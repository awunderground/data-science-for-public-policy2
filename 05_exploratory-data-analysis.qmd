---
title: "Exploratory Data Analysis"
abstract: This chapter introduces exploratory data analysis (EDA), an essential process for data scientists and researchers to use to understand their data.
format: 
  html:
    toc: true
    code-line-numbers: true

editor_options: 
  chunk_output_type: console
---

![Bruce McCandless II in space in 1984.](images/header-images/Bruce_McCandless_II_during_EVA_in_1984.jpg)

```{r}
#| label: rmarkdown-setup
#| echo: false
#| include: false
library(tidyverse)
library(here)

exercise_number <- 1
```

## Reading in Data

We've already covered reading in csv files using the `read_csv()` function from the `readr` package, but you may also need to read in data that is stored in a number of other common file formats:

### Excel Spreadsheets

We introduced [readxl](https://readxl.tidyverse.org/) in @sec-readxl. As a reminder, it is not a core tidyverse package, so it needs to be explicitly loaded in each R session. 

Many excel files can be read with the simple syntax `data <- read_excel(path = "relative/file/path/to/data")`. In cases where the Excel spreadsheet contains multiple sheets, you can use the `sheet` argument to specify the sheet to read as a string (the name of the sheet) or an integer (the position of the sheet). You can also read only components of a sheet using Excel-style cell-ranges (ex: `A3:L44`). 

### STATA, SAS, and SPSS files

[haven](https://haven.tidyverse.org/) is a tidyverse package for reading data from SAS (`read_sas()`), STATA (`read_dta()`), and SPSS (`read_sav()`) files. Like the `readxl` package, it is not a core tidyverse package and also needs to be explicitly loaded in each R session.

Note that the haven package can only read and write STATA `.dta` files through version 15 of STATA. For files created in more recent versions of STATA, the `readstat13` package's `read.dta13` file can be used.

### Zip Files

You may also want to read in data that is saved in a zip file. In order to do this, you can use the `unzip()` function to unzip the files using the following syntax: `unzip(zipfile = "path/to/zip/file", exdir = "path/to/directory/for/unzipped/files")`.

Often times, you may want to read in a zip file from a website into R. In order to do this, you will need to first download the zip file to your computer using the `download.file()` function, unzip the file using `unzip()` and then read in the data using the appropriate function for the given file type.

To download the week 40 public use file data for the Census Household Pulse Survey, run the following code:

```{r}
#| eval: false
base_url <- "https://www2.census.gov/programs-surveys/demo/datasets/hhp/"
week_url <- "2021/wk40/HPS_Week40_PUF_CSV.zip"

pulse_url <- paste0(base_url, week_url)

# creates data directory in working directory
# gives warning if directory already exists
dir.create("data")

# For Mac, *.nix systems:
download.file(
  pulse_url, 
  destfile = "data/pulse40.zip"
)

# For Windows systems, you need to add the mode = "wb" 
# argument to prevent an invalid zip file 
download.file(
  pulse_url, 
  destfile = "data/pulse40.zip", 
  mode = "wb"
)

```

```{r}
#| echo: false
#| include: false
pulse_url <- "https://www2.census.gov/programs-surveys/demo/datasets/hhp/2021/wk40/HPS_Week40_PUF_CSV.zip"

dest_path <- here("data", "pulse40.zip")

if (!file.exists(dest_path)){
  # For Mac, *.nix systems:
  download.file(pulse_url, 
                destfile = dest_path
  )
}

```




::: callout

#### [`r paste("Exercise", exercise_number)`]{style="color:#1696d2;"}

1. Copy and paste the code chunk above and keep the appropriate `download.file` command for your computer.
2. Write code using the `unzip()` function to unzip the zip file downloaded. Set `zipfile` to be the same the `destfile` parameter you used in part 1.  Set `exdir` to be the same directory where you just downloaded the zip file. Run both of these commands.
3. Examine the unzipped files and select the appropriate function to read in the `pulse2021_puf_40` file. Write code to read that file into R, assigning the output to the `pulse` object.

```{r}
#| echo: false
#| include: false

unzip(
  zipfile = here("data", 
                 "pulse40.zip"), 
  exdir = here("data/")
)

pulse <- read_csv(
  here("data", 
       "pulse2021_puf_40.csv")
)

```

:::


## Column Names

As discussed earlier in the course, dataframe columns - like other objects - should be given names that are ["concise and meaningful"](https://style.tidyverse.org/syntax.html#object-names). Generally column names should be nouns and only use lowercase letters, numbers, and underscores `_` (this is referred to as snake case). Columns should not begin with numbers. You should not include white space in column names (e.g "Birth Month" = bad, "birth_month" = good). It is also best practice for column names to be singular (use "birth_month" instead of "birth_months").

The [janitor package](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html) is a package that contains a number of useful functions to clean data in accordance with the tidyverse principles. One such function is the `clean_names()` function, which converts column names to snake case according to the tidyverse type guide (along with some other useful cleaning functions outlined in the link above). The `clean_names()` function works well with the `|>` operator.



::: callout

#### [`r paste("Exercise", exercise_number)`]{style="color:#1696d2;"}

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

1. Take a look at the column names in the Pulse data file you read in for the exercise earlier.
2. Then edit the command in the R script that you wrote to read in the CSV file to pipe the results of that command to the `janitor::clean_names()` function. Note that you may have to install and import the janitor package first.
3. Now look at the column names again after running the modified command. How have they changed?

```{r}
#| echo: false
#| include: false

pulse <- pulse |>
  janitor::clean_names()

```
:::

As discussed in the introduction to the tidyverse, you can also directly rename columns using the `rename()` function from the `dplyr` package as follows: `rename(data, new_col = old_col)`.



## Data Overview

Once you've imported your data, a common first step is to get a very high-level summary of your data.

As introduced in the introduction to the tidyverse, the `glimpse()` function provides a quick view of your data, printing the type and first several values of each column in the dataset to the console.

```{r}
#| label: glimpse-storms
glimpse(x = storms)

```

The `summary()` function enables you to quickly understand the distribution of a numeric or categorical variable. For a numeric variable, summary() will return the minimum, first quartile, median, mean, third quartile, and max values. For a categorical (or factor) variable, summary() will return the number of observations in each category. If you pass a dataframe to summary() it will summarize every column in the dataframe. You can also call summary on a single variable as shown below:

```{r}
summary(storms$wind)

```

The `str()` function compactly displays the internal structure of any R object. If you pass a dataframe to `str` it will print the column type and first several values for each column in the dataframe, similar to the `glimpse()` function.

Getting a high-level overview of the data can help you identify what questions you need to ask of your data during the exploratory data analysis process. The rest of this lecture will outline several questions that you should always ask when exploring your data - though this list is not exhaustive and will be informed by your specific data and analysis!

## Are my columns the right types?

We'll read in the population-weighted centroids for the District of Columbia exported from the Missouri Census Data Center's [geocorr2014 tool](https://mcdc.missouri.edu/applications/geocorr2014.html).

```{r}
dc_centroids <- read_csv(
  paste0(
    "https://raw.githubusercontent.com/awunderground/awunderground-data/",
    "main/geocorr/geocorr2014_dc.csv"
  )
)

glimpse(dc_centroids)

```

We see that all of the columns have been read in as character vectors because the second line of the csv file has a character description of each column. By default, `read_csv` uses the first 1,000 rows of data to infer the column types of a file. We can avoid this by skipping the first two lines of the csv file and manually setting the column names.


```{r}
#save the column names from the dataframe
col_names <- dc_centroids |> names()

dc_centroids <- read_csv(
  paste0(
    "https://raw.githubusercontent.com/awunderground/awunderground-data/",
    "main/geocorr/geocorr2014_dc.csv"
  ),
  col_names = col_names,
  skip = 2
)

glimpse(dc_centroids)

```

You can convert column types by using the `as.*` set of functions. For example, we could convert the `county` column to a character vector as follows: `mutate(dc_centroids, county = as.character(county))`. We can also set the column types when reading in the data with `read_csv()` using the `col_types` argument. For example: 

```{r}
dc_centroids <- read_csv(
  paste0(
    "https://raw.githubusercontent.com/awunderground/awunderground-data/",
    "main/geocorr/geocorr2014_dc.csv"
  ),
  col_names = col_names,
  skip = 2,
  col_types = c("county" = "character")
)

glimpse(dc_centroids)

```

As you remember from week 1, a vector in R can only contain one data type. If R does not know how to convert a value in the vector to the given type, it may introduce NA values by coercion. For example:


```{r}
as.numeric(c("20", "10", "10+", "25", "~8"))
```

### String manipulation with `stringr`

Before converting column types, it is critical to clean the column values to ensure that NA values aren't accidentally introduced by coercion. We can use the `stringr` package introduced in @sec-strings to clean character data. A few reminders:

-  This package is part of the core tidyverse and is automatically loaded with `library(tidyverse)`.
-  The [stringr cheat sheet](https://github.com/rstudio/cheatsheets/blob/main/strings.pdf) offers a great guide to the `stringr` functions. 

To demonstrate some of the `stringr` functions, let's create a `state` column with the two digit state FIPS code for DC and a `geoid` column in the `dc_centroid` dataframe which contains the 11-digit census tract FIPS code, which can be useful for joining this dataframe with other dataframes that commonly use the FIPS code as a unique identifier. We will need to first remove the period from the `tract` column and then concatenate the `county` and `tract` columns into a `geoid` column. We can do that using `stringr` as follows:


```{r}
dc_centroids <- dc_centroids |>
  mutate(
    #replace first instance of pattern
    tract = str_replace(tract, "\\.", ""), 
    #join multiple strings into single string
    geoid = str_c(county, tract, sep = "")
  )

```



::: callout

#### [`r paste("Exercise", exercise_number)`]{style="color:#1696d2;"}

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

1. Copy the code above into an R script and edit it to add the creation of a variable called `state` that is equal to the first two characters of the `county` variable using the `str_sub()` function.

```{r}
#| echo: false
#| include: false
dc_centroids <- dc_centroids |>
  mutate(
    #replace first instance of pattern
    tract = str_replace(tract, "\\.", ""), 
    #join multiple strings into single string
    geoid = str_c(county, tract, sep = ""),
    state = str_sub(county, 1, 2)
  )

```

:::

:::{.callout-note}
Note that the `str_replace()` function uses regular expressions to match the pattern that gets replaced. Regular expressions is a concise and flexible tool for describing patterns in strings - but it's syntax can be complex and not particularly intuitive. [This vignette](https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html) provides a useful introduction to regular expressions, and when in doubt - there are plentiful Stack Overflow posts to help when you search your specific case. ChatGPT is often effective at writing regular expressions, but recall our warnings about using ChatGPT in Chapter 1. 
:::

### Date manipulation with `lubridate`

The `lubridate` package, which we introduced in @sec-dates, makes it much easier to work with dates and times in R. As a reminder, `lubridate` is part of the tidyverse, but it is not a core tidyverse package. It must be explicitly loaded in each session with `library(lubridate)`.

We'll use a dataset on political violence and protest events across the continent of Africa in 2022 from the [Armed Conflict Location & Event Data Project (ACLED)](https://acleddata.com/) to illustrate the power of `lubridate`. The `lubridate` package allows users to easily and quickly parse date-time variables in a range of different formats. For example, the `ymd()` function takes a date variable in year-month-day format (e.g. 2022-01-31) and converts it to a date-time format. The different formats are outlined in the [lubridate documentation](https://lubridate.tidyverse.org/). 



```{r}
library(lubridate)
acled_2022 <- read_csv(
  paste0(
    "https://raw.githubusercontent.com/awunderground/awunderground-data/",
    "main/acled/acled_africa_2022.csv")
  ) |>
  janitor::clean_names() |>
  mutate(event_date = ymd(event_date)) |>
  select(event_date, region, country, event_type, sub_event_type)


```

Creating datetime columns enables the use of a number of different operations, such as filtering based on date:

```{r}
acled_2022 |>
  filter(event_date > "2022-06-01")

```


Or calculating durations:

```{r}
acled_2022 |>
  #number of days into 2022 event occurred
  mutate(days_into_2022 = event_date - ymd("2022-01-01")) 

```

Or extracting components of dates:

```{r}
acled_2022 |>
  mutate(event_month = month(event_date)) #month event occurred

```

Datetimes can also much more easily be plotted using `ggplot2`. For example, it is easy to visualize the distribution of events across the year:



```{r}
acled_2022 |> 
  ggplot(aes(x = event_date)) + 
  geom_freqpoly(binwidth = 1) # each bin is 1 day

```


For more information on lubridate, see the [lubridate cheat sheet](https://www.rstudio.com/resources/cheatsheets/).

[Source: R for Data Science, Ch 16](https://r4ds.had.co.nz/dates-and-times.html)

### Categorical and Factor Variables

Categorical variables can be stored as characters in R. The `case_when()` function makes it very easy to create categorical variables based on other columns. For example:



```{r}
pulse <- pulse |>
  mutate(
    hisp_rrace = case_when(
      rrace == 1 ~ "White alone, not Hispanic",
      rrace == 2 ~ "Black alone, not Hispanic",
      rrace == 3 ~ "Asian alone, not Hispanic",
      rrace == 4 ~ "Two or more races + Other races, not Hispanic",
      TRUE ~ NA_character_)
  )
```



Factors are a data type specifically made to work with categorical variables. The `forcats` library in the core tidyverse is made to work with factors. Factors are particularly valuable if the values have a ordering that is not alphanumeric. 



```{r}
x1 <- c("Dec", "Apr", "Jan", "Mar")

month_levels <- c(
  "Jan", "Feb", "Mar", "Apr", "May", "Jun", 
  "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"
)

y1 <- factor(x1, levels = month_levels)

sort(x1)
sort(y1)

```



Factors are also valuable if you want to show all possible values of the categorical variable, even when they have no observations.



```{r}
table(x1)
table(y1)

```



## Is there missing data?

Before you work with any dataset, you should understand how missing values are encoded. The best place to find this information is the data dictionary - which you should always read before working with any new dataset! 

This is particularly important because while R automatically recognizes standard missing values as NA, it doesn't recognize non-standard encodings like numbers representing missing values, "missing", "na", "N/A", etc. 

Non-standard missing values should be converted to NA before conducting analysis. One way of doing this is with `mutate` and the `if_else` or `case_when` functions.



::: callout

#### [`r paste("Exercise", exercise_number)`]{style="color:#1696d2;"}

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

1. Go to the folder where you unzipped the Pulse data from earlier and open the data dictionary file. How does this dataset represent missing values for the `RECVDVACC` variable?
2. Using `mutate` and `if_else` or `case_when`, replace the missing values in the `recvdvacc` column with NA. 



```{r}
#| eval: false
#| include: false

pulse <- pulse |>
  mutate(
    recvdvacc = if_else(
      condition = recvdvacc %in% c(-88, -99), 
      true = NA_real_, 
      false = recvdvacc
    )
  )

```

:::

Once you have converted all missing value encodings to NA, the next question you need to ask is how you want to handle missing values in your analysis. The right approach will depend on what the missing value represents and the goals of your analysis.

- *Leave as NA*: This can be the best choice when the missing value truly represents a case when the true value is unknown. You will need to handle NAs by setting `na.rm = TRUE` in functions or filtering using `is.na()`. One drawback of this approach is that if the values aren't missing at random (e.g. smokers may be less likely to answer survey questions about smoking habits), your results may be biased. Additionally, this can cause you to lose observations and reduce power of analyses. 
- *Replace with 0*: This can be the best choice if a missing value represents a count of zero for a given entity. For example, a dataset on the number of Housing Choice Voucher tenants by zip code and quarter may have a value of NA if there were no HCV tenants in the given zip code and quarter.
- *Impute missing data*: Another approach is imputing the missing data with a reasonable value. There are a number of different imputation approaches:

    - *Mean/median/mode imputation*: Fills the missing values with the column mean or median. This approach is very easy to implement, but can artifically reduce variance in your data and be sensitive to outliers in the case of mean imputation. 
    - *Predictive imputation*: Fills the missing values with a predicted value based on a model that has been fit to the data or calculated probabilities based on other columns in the data. This is a more complex approach but is likely more accurate (for example, it can take into account variable correlation).
    
These alternatives are discussed in more detail in a later chapter covering data imputation. 

The `replace_na()` function in `dplyr` is very useful for replacing NA values in one or more columns.



```{r}
df <- tibble(x = c(1, 2, NA), y = c("a", NA, "b"))

# Using replace_na to replace one column
df |> 
  mutate(x = replace_na(x, 0))

# Using replace_na to replace multiple columns with different values
df |> 
  replace_na(list(x = 0, y = "unknown"))

# Using if_else to perform mean imputation
df |> 
  mutate(x = if_else(is.na(x), mean(x, na.rm = TRUE), x))

```



## Do I have outliers or unexpected values?

### Identifying Outliers/Unexpected Values

Using R to examine the distribution of your data is one way to identify outliers or unexpected values. For example, we can examine the distribution of the `bodywt` variable in the `msleep` dataset both by examining the mathematical distribution using the `summary()` function and visually using `ggplot`.



```{r}
summary(msleep$bodywt)
```



```{r}
#| fig-height: 1

msleep |>
  ggplot(aes(bodywt, 1)) +
  geom_point(alpha = 0.2) +
  scale_y_continuous(breaks = 0) +
  labs(y = NULL) +
  theme_bw() +
  theme(panel.border = ggplot2::element_blank())

```



### Unit Tests

Writing tests in R is a great way to test that your data does not have unexpected/incorrect values. These tests can also be used to catch mistakes that can be introduced by errors in the data cleaning process. There are a number of R packages that have tools for writing tests, including:

* [testthat](https://testthat.r-lib.org/index.html)
* [assertthat](https://cran.r-project.org/web/packages/assertthat/readme/README.html)
* [assertr](https://cran.r-project.org/web/packages/assertr/vignettes/assertr.html)



```{r}
#| error: true

library(assertr)

df <- tibble(age = c(20, 150, 47, 88),
             height = c(60, 2, 72, 66))

df |>
  assertr::verify(age < 120) |>
  summarise(mean(age, na.rm = TRUE))

```



Critically, adding the test caused the code to return an error **before** calculating the mean of the age variable. This is a feature, not a bug! It can prevent you from introducing errors into your analyses. Moreover, by writing a set of tests in your analysis code, you can run the same checks every time you perform the analysis which can help you catch errors caused by changes in the input data. 

### Handling Outliers/Unexpected Values

When you identify outliers or unexpected values, you will have to decide how you want to handle those values in your data. The proper way to handle those values will depend on the reason for the outlier value and the objectives of your analysis.

* If the outlier is caused by data errors, such as an implausible age or population value, you can replace the outlier values with `NA` using `mutate` and `if_else` or `case_when` as described above.
* If the outlier represents a different population than the one you are studying - e.g. the different consumption behaviors of individual consumers versus wholesale business orders - you can remove it from the data. 
* You can transform the data, such as taking the natural log to reduce the variation caused by outliers.
* You can select a different analysis method that is less sensitive to outliers, such as using the median rather than the mean to measure central tendency. 



::: callout

#### [`r paste("Exercise", exercise_number)`]{style="color:#1696d2;"}

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

1. Read the column descriptions in the [csv file for the DC centroids]("https://raw.githubusercontent.com/awunderground/awunderground-data/main/geocorr/geocorr2014_dc.csv"). 
2. Use one of the methods above to identify whether the `pop10` column contains any outliers. According to the [Census Bureau](https://www.census.gov/programs-surveys/geography/about/glossary.html#par_textimage_13), tracts generally have a population between 1,200 and 8,000 people. 
3. Calculate the mean of the `pop10` column in the `dc_centroids` dataframe, but first write one test using `assertr::verify()` to test for invalid values based on the column definition.

```{r}
#| include: false
#| eval: false

summary(dc_centroids$pop10)

dc_centroids |>
  assertr::verify(pop10 > 1000) |>
  summarise(mean_pop = mean(pop10, na.rm = TRUE))


```
:::


## Data Quality Assurance

Data quality assurance is the foundation of high quality analysis. Four key questions that you should always ask when considering using a dataset for analysis are:

1. Does the data suit the research question? Examine the data quality (missingness, accuracy) of key columns, the number of observations in subgroups of interest, etc.
2. Does the data accurately represent the population of interest? Think about the data generation process (e.g. using 311 calls or Uber ride data to measure potholes) - are any populations likely to be over or underrepresented? Use tools like Urban's [Spatial Equity Data Tool](https://apps.urban.org/features/equity-data-tool/) to test data for unrepresentativeness.
3. Is the data gathering reproducible? Wherever possible, eliminate manual steps to gather or process the data. This includes using reproducible processes for data ingest such as APIs or reading data directly from the website rather than manually downloading files. All edits to the data should be made programmatically (e.g. skipping rows when reading data rather than deleting extraneous rows manually). Document the source of the data including the URL, the date of the access, and specific metadata about the vintage.
4. How can you verify that you are accessing and using the data correctly? This may include writing tests to ensure that raw and calculated data values are plausible, comparing summary statistics against those provided in the data documentation (if applicable), published tables/statistics from the data provider, or published tables/statistics from a trusted third party. 

## Conclusion
This chapter outlined EDA in great detail. Some key reminders are:
- R and the tidyverse have great packages for reading in data (`readr` for csv files, `readxl` for Excel files, `haven` for STATA, SAS, and SPSS files). 

- `janitor::clean_names()` is a great way to get your column name structure to "snake case".
- `glimpse()` is an easy way to get summary statistics on your columns. 
- `stringr` and `lubridate` can help you handle string and date manipulation, respectively. 
- There a many good options for handling missing data. Be thoughtful!
- Unit tests are a good way to ensure your data is in an expected format. 

