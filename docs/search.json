[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Public Policy Part II",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "09_nonparametric-2.html#sec-review9",
    "href": "09_nonparametric-2.html#sec-review9",
    "title": "1  Nonparametric Statistics 2",
    "section": "1.1 Review",
    "text": "1.1 Review\nStatistical inference is a procedure for making judgements about population parameters using data and assumptions.\nWe’ve learned about many types of tests for statistical inference. For example, consider the following tests:\n\nOne-sample t-test to determine if a population mean is different from a key value like 0.\nTwo-sample t-test to determine if two population means are different from one another.\nANOVA to determine if multiple population means are different from other means.\nTwo-sample F-tests to determine if two population variances are different from one another.\n\nDifferent tests apply in different applied situations. For example, Higgins (2004) identifies at least four approaches to testing hypotheses:\n\nUse normal-theory methods like t-tests. Unfortunately, the assumptions of these tests may not be met.\nTransform data and use normal-theory methods like t-tests. Unfortunately, the assumptions of these tests may still not be met.\nUse tests based on other common distributions like the exponential distribution. There may be insufficient data to determine the form or the data may come from a difficult distribution with unknown parameters or characteristics.\nUse nonparametric statistics.\n\nWe use library(infer) for statistical inference in R1.\nlibrary(infer) has a more powerful workflow for other types of tests built around four verbs:\n\nspecify() is used to specify the variable or variables of interest.\nhypothesize() is used to declare a null hypothesis.\ngenerate() is used to generate data from the null hypothesis using parametric and nonparametric approaches.\ncalculate() is used to calculate the distribution of statistics from data created by generate() to form the null hypothesis.\n\nThe four verbs can be piped together as we will see in later examples. For now, let’s focus on t_test(), which is a short cut for running t-tests in R.\n\n1.1.1 Example 1\nWe simulate some data with 8 observations from \\(X \\sim N(\\mu = 0, \\sigma = 1)\\) and 8 observations from \\(X \\sim N(\\mu = 1, \\sigma = 1)\\).\n\nset.seed(20230402)\n\nsim_data <- bind_rows(\n  tibble(group = \"1\", x = rnorm(n = 8, mean = 0, sd = 1)),\n  tibble(group = \"2\", x = rnorm(n = 8, mean = 1, sd = 1))\n)\n\n\n1-Sample T-Test\nLet’s start with group 1 and test if \\(\\mu\\) is statistically significantly different than 0:\n\\[H_0: \\mu = 0\\]\n\\[H_a: \\mu \\ne 0\\]\n\nsim_data |>\n  filter(group == \"1\") |>\n  t_test(response = x, mu = 0)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1     -1.21     7   0.264 two.sided     -0.421    -1.24    0.398\n\n\nBased on the p-value and t-statistic, we have insufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) level.\n\nNow, let’s work with group 2 using the same hypotheses.\n\nsim_data |>\n  filter(group == \"2\") |>\n  t_test(response = x, mu = 0)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1      3.01     7  0.0196 two.sided      0.954    0.205     1.70\n\n\nBased on the p-value and t-statistic, there is sufficient evidence to reject the null hypothesis in favor of the alternative hypothesis at the \\(\\alpha = 0.05\\) significance level. It is likely that the population mean, \\(\\mu\\), for group 2 is different than 0.\n\n\n2-Sample T-Test\nNext, let’s implement a two-sample t-test to test if the population means from group 1 and group 2 are different.\n\\[H_0: \\mu_1 = \\mu_2\\]\n\\[H_a: \\mu_1 \\ne \\mu_2\\]\n\nsim_data |>\n  t_test(\n    formula = x ~ group, \n    order = c(\"1\", \"2\"),\n    alternative = \"two-sided\"\n  )\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1     -2.93  13.9  0.0111 two.sided      -1.37    -2.38   -0.367\n\n\nBased on the p-value and t-statistic, there is sufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) significance level. It is likely that the population means are different.\n\n\n\n\n\n\nWarning\n\n\n\nImportant assumptions support the inferences in Example 1. We will unpack these assumptions later.\n\n\n\n\n\n1.1.2 Example 2\nLet’s consider a real example from Ashraf, Karlan, and Yin (2006). They created a commitment savings product for a Philippine bank called SEED (Save, Earn, Enjoy Deposits). They evaluated the effect of the commitment savings product on the level of household savings (among other outcomes) using a randomized control trial (RCT). The authors shared their data on the Harvard Dataverse. J-PAL offers a brief summary.\nseed.csv contains simplified data from their analysis data.\n\nseed <- read_csv(here(\"data\", \"seed.csv\"))\n\nglimpse(seed)\n\nRows: 1,777\nColumns: 5\n$ group2    <chr> \"treatment\", \"control\", \"treatment\", \"control\", \"control\", \"…\n$ group3    <chr> \"treatment\", \"marketing\", \"treatment\", \"marketing\", \"control…\n$ totbal    <dbl> 121.32, 125.19, 529.39, 101.80, 118.04, 532.69, 598.41, 548.…\n$ newtotbal <dbl> 4.62, 0.00, 4139.34, 105.94, 0.00, 362.74, 2323.50, 373.13, …\n$ balchange <dbl> -116.70, -125.19, 3609.95, 4.14, -118.04, -169.95, 1725.09, …\n\n\n\n1-Sample T-Test\nLet’s start with a simple 1-sample T-Test to see if the population mean for change in balance after one year of the SEED accounts is statistically significantly different than zero. Our null and alternative hypotheses are\n\\[H_0: \\mu_{\\text{balance change}} = 0\\]\n\\[H_a: \\mu_{\\text{balance change}} \\ne 0\\]\n\nseed |>\n  t_test(response = balchange, mu = 0)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1      2.72  1776 0.00652 two.sided       293.     81.9     503.\n\n\nWe reject the null hypothesis in favor of the alternative hypothesis. We conclude that the mean change in bank account balances is different than zero.\nWe make inferences with data and assumptions. Did our assumptions hold in this case? The assumptions for a 1-sample T-Test are\n\n\\(x_1, x_2, ..., x_n\\) is an IID random sample from a population.\nThe population is normally distributed or the sample size is large enough for the Central Limit Theorem to apply.\n\nBoth assumptions are reasonable. We can assume that the sample is identically distributed because the sample was drawn from one population: clients of the Green Bank of Caraga. It also seems reasonable to assume that a household that received the commitment savings product did not somehow influence a control household’s level of savings. Thus the IID assumption is reasonable.\nThe study assessed more than 1,000 households, suggesting that we need not be concerned with the distribution’s normality.\nThe average doesn’t represent the experience of the average person. Maybe we want to run a statistical test on the median. Unfortunately, normal-theory statistics doesn’t give us a tool for this. This could be a big challenge based on key percentiles for the change in account balance:\n\nseed |>\n  summarize(\n    q = seq(0, 1, 0.1), \n    balchange = quantile(balchange, probs = q)\n  )\n\n# A tibble: 11 × 2\n       q balchange\n   <dbl>     <dbl>\n 1   0     -2169. \n 2   0.1    -526. \n 3   0.2    -280  \n 4   0.3    -191. \n 5   0.4    -117. \n 6   0.5     -63.0\n 7   0.6       0  \n 8   0.7       0  \n 9   0.8      20.9\n10   0.9     323. \n11   1    114001. \n\n\n\n\n2-Sample T-Test\nNext, let’s run a 2-sample t-test to compare the means from the control and treatment group.2\n\\[H_0: \\mu_{control} = \\mu_{treatment}\\]\n\\[H_a: \\mu_{control} \\ne \\mu_{treatment}\\]\n\nseed |>\n  t_test(\n    formula = balchange ~ group2, \n    order = c(\"treatment\", \"control\"),\n    alternative = \"two-sided\"\n  )\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1      1.56 1064.   0.118 two.sided       350.    -89.0     788.\n\n\nThere is insufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) significance level. The mean change in account balance is not statistically significantly different for the treatment and control groups.\nWe make inferences with data and assumptions? Did our assumptions hold in this case? The assumptions for a 2-sample T-Test are\n\n\\(x_1, x_2, ..., x_n\\) is an IID random sample from population 1. \\(y_1, y_2, ..., y_n\\) is an IID random sample from a population 2.\nThe populations are normally distributed or the sample size is large enough for the Central Limit Theorem to apply.\nThe populations have equal variances (possibly unknown).\n\nAssumptions 1 and 2 hold, but we don’t know if assumption 3 holds. There are statistical tests for the equivalence of two population variances. In this case, we don’t even need the tests. The two samples have dramatically different variances.\n\nseed |>\n  group_by(group2) |>\n  summarize(var(balchange))\n\n# A tibble: 2 × 2\n  group2    `var(balchange)`\n  <chr>                <dbl>\n1 control           5514806.\n2 treatment        37127545.\n\n\nMany applications of 2-sample t-tests incorrectly ignore assumption 3 but situations where we want to make inferences about the difference in means when population variances differ is widespread. This is known as the Behrens-Fisher problem.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\n\nSimulate two-sample data with 8 observations each like in Section 1.1.1 using the log-normal distribution with rlnorm().\nFirst, run a 2-sample t-test where the simulated data have very similar means.\nSecond, run a 2-sample t-test where the simulated data have very different means."
  },
  {
    "objectID": "09_nonparametric-2.html#permutation-test",
    "href": "09_nonparametric-2.html#permutation-test",
    "title": "1  Nonparametric Statistics 2",
    "section": "1.2 Permutation Test",
    "text": "1.2 Permutation Test\n\n\n\n\n\n\nCombination\n\n\n\nA combination is the exhaustive reshuffling of \\(n\\) objects into two groups without regard for order.\nLet \\(k\\) be the number of objects in the first group. There are \\(C(n, k) = {n \\choose k} = \\frac{n!}{(n - k)!k!}\\) combinations.\nConsider the objects A, B, C, D. Suppose we want to combine them into two groups of equal size. There are \\(C(4, 2) = {4 \\choose 2} = 6\\) combinations.\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nA, B\nC, D\n\n\nA, C\nB, D\n\n\nA, D\nB, C\n\n\nB, C\nA, D\n\n\nB, D\nA, C\n\n\nC, D\nA, B\n\n\n\n\n\nThe permutation test is typically used to compare parameters from two or more populations. The algorithm for a permutation test is as follows:\n\nDetermine some parameter of interest for two or more populations (like the mean or median). The groups can be treatment and control groups.\nCalculate the difference in statistics for each group. This is the test statistic or observed statistic.\nDetermine every possible combination of assignment of each observation to the groups (e.g. treatment and control group).\nFor each combination:\n\nCalculate the difference in the determined statistic (e.g. mean or median) between the two groups.\nStore that difference in a vector. This vector is called the permutation distribution.\n\nDetermine the p-value by comparing the test statistic to the permutation distribution.\n\nFor a gentle (and alpaca-themed) visualized introduction to this content, see this blog."
  },
  {
    "objectID": "09_nonparametric-2.html#permutation-distribution",
    "href": "09_nonparametric-2.html#permutation-distribution",
    "title": "1  Nonparametric Statistics 2",
    "section": "1.3 Permutation Distribution",
    "text": "1.3 Permutation Distribution\nThe permutation distribution is a vector of statistics calculated on combinations3 of the data. :::\n\n\n\n\n\n\nPermutation principle\n\n\n\nThe permutation principle states that the permutation distribution is an appropriate reference distribution and can be used to determine p-values and statistical significance. [Higgins (2004)]4\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the data used in a permutation test are selected randomly or come from a designed experiment, then we can make inferences about the population. Otherwise, we can only make inferences about the sample.\n\n\n\n1.3.1 Example 1 (Revisited)\nLet’s revisit Example 1 using permutation methods. In general we use library(infer) to implement permutations tests. Here we implement the method from scratch for demonstration.\n\n2-Sample Difference of Means\n\n#' Implement a permutation test for means\n#'\n#' @param x A numeric vector\n#' @param group_size The group size of group 1\n#'\n#' @return A permutation distribution \n#' \npermute_means <- function(x, group_size = 8) {\n  \n  # combn() returns a matrix with group_size rows\n  # and a column for every combination of integers associated with x\n  indices <- combn(x = 1:length(x), m = group_size)\n  \n  index <- 1:ncol(indices)\n\n  # 1. extract a vector of indices stored as a column in the indices matrix\n  # 2. extract only the observations in x included in the vector of indices\n  # 3. take the mean of those observations\n  dist <- map_dbl(\n    .x = index, \n    .f = ~ mean(x[indices[, .x]]) - mean(x[-indices[, .x]])\n  )\n  \n  return(dist)\n  \n}\n\npermute_means() permutes the indices of the values in x to permute the values of x. In this case, with group_size = 8 the first column has indices 1, 2, 3, 4, 5, 6, 7, 8 and the last column has indices 9, 10, 11, 12, 13, 14, 15, 16. The remaining columns express every possible permutation of the values 1:16 into two groups of size 8.\n\n# create the permutation distribution\npermutation_dist <- permute_means(sim_data$x, 8)\n\n# visualize the permutation distribution\ntibble(x = permutation_dist) |>\n  ggplot(aes(x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n# calculate the test statistic\nD <- mean(sim_data[sim_data$group == \"1\", ]$x) -\n  mean(sim_data[sim_data$group == \"2\", ]$x)\n\n# calculate a p-value\nmean(D > permutation_dist)\n\n[1] 0.006915307\n\n\nThe p-value is very small. There is sufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) level. It is likely that the population means of the two samples are different.\n\n\n2-Sample Difference of Medians\n\n#' Implement a permutation test for medians\n#'\n#' @param x A numeric vector\n#' @param group_size The group size of group 1\n#'\n#' @return A permutation distribution \n#' \npermute_medians <- function(x, group_size = 8) {\n  \n  indices <- combn(1:length(x), group_size)\n  \n  index <- 1:ncol(indices)\n\n  dist <- map_dbl(\n    .x = index, \n    .f = ~ median(x[indices[, .x]]) - median(x[-indices[, .x]])\n  )\n  \n  return(dist)\n  \n}\n\n# calculate the permutation distribution\npermutation_dist <- permute_medians(sim_data$x, 8)\n\n# visualize the permutation distribution\ntibble(x = permutation_dist) |>\n  ggplot(aes(x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n# calculate the test statistic\nD <- median(sim_data[sim_data$group == \"1\", ]$x) -\n  median(sim_data[sim_data$group == \"2\", ]$x)\n\n# calculate the p-value\nmean(D > permutation_dist)\n\n[1] 0.02206682\n\n\nThe p-value is small. There is sufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) level. It is likely that the population medians (location parameters) of the two samples are different.\n\n\n\n1.3.2 Example 2 (Revisited)\nLet \\(n\\) be the number of observations in the data and \\(k\\) be the number of observations in the larger group. Then there are \\({n \\choose k} = \\frac{n!}{k!(n - k)!}\\) combinations of the data. This can be computationally very expensive.\nConsider the data from example 2, which contains 1,777 observations. That’s 935 observations in the control group and 842 observations in the treatment group.\n\nchoose(nrow(seed), sum(seed$group2 == \"control\"))\n\n[1] Inf\n\n\nFortunately, sampling permutations can give a decent approximation of the full permutation test.\n\n2-Sample Difference in Means\nWe will now use library(infer) to calculate the observed statistic, generate the null distribution, and calculate the p-value.\nRecall our null and alternative hypotheses.\n\\[H_0: \\mu_{control} = \\mu_{treatment}\\]\n\\[H_a: \\mu_{control} \\ne \\mu_{treatment}\\]\n\n# calculate the test statistic\npoint_estimate <- seed |>\n  specify(balchange ~ group2) |>\n  calculate(stat = \"diff in means\", order = c(\"control\", \"treatment\"))\n\n# generate the permutation distribution\nperm_dist <- seed |>\n  specify(balchange ~ group2) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 5000, type = \"permute\") |>\n  calculate(stat = \"diff in means\", order = c(\"control\", \"treatment\"))\n\n# visualize the p-value\nperm_dist |>\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n\n\n# calculate the p-value\nperm_dist |>\n  get_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.109\n\n\nOnce again, there is insufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) significance level. The mean change in account balance is not statistically significantly different for the treatment and control groups.\n\n\n2-Sample Difference in Medians\nUsing normal-theory statistics, we don’t have clear strategies for statistical tests about the median. This is simple with nonparametric statistics.\n\n# calculate the test statistic\npoint_estimate <- seed |>\n  specify(balchange ~ group2) |>\n  calculate(stat = \"diff in medians\", order = c(\"control\", \"treatment\"))\n\n# generate the permutation distribution\nperm_dist <- seed |>\n  specify(balchange ~ group2) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 5000, type = \"permute\") |>\n  calculate(stat = \"diff in medians\", order = c(\"control\", \"treatment\"))\n\n# visualize the p-value\nperm_dist |>\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n\n\n# calculate the p-value\nperm_dist |>\n  get_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1  0.0012\n\n\nWe have sufficient evidence to reject the null hypothesis in favor if the alternative hypothesis at the \\(\\alpha = 0.05\\) level. It is likely that the population median for group 1 is different than the population median for group 2.\nInterestingly, the median change for the treatment group is slightly less negative than the control group.\n\nseed |>\n  group_by(group2) |>\n  summarize(median = median(balchange))\n\n# A tibble: 2 × 2\n  group2    median\n  <chr>      <dbl>\n1 control    -84.6\n2 treatment  -38.1\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\n\nUse the same log-normal data simulated in the earlier exercise.\nFirst, run a 2-sample permutation test for the difference in means using very similar population means.\nSecond, run a 2-sample permutation test for the difference in means using very different population means.\n\n\n\n\n\n\n1.3.3 Other techniques\nStatistical tests are often selected because of their coverage probabilities and power.\n\n\n\n\n\n\nCoverage probability\n\n\n\nCoverage probability is how close the stated level of confidence is to the actual level of confidence.5\nFor example, a given statistical test, sampling procedure, and population may generate confidence intervals that contain the true parameter of interest 93% of the time for a 95% confidence interval.\n\n\n\n\n\n\n\n\nPower\n\n\n\nPower is the probability of rejecting \\(H_0\\) if it is false.\n\n\nWe want tests with correct coverage probabilities and high statistical power. If the assumptions are met then parametric tests often have more power than nonparametric tests. The power advantage can often flip when assumptions start failing.\nSome nonparametric tests, which are often variations on the permutation test, have more power than the permutation test. Often, the Wilcoxon Rank Sum test, which is uses ranks instead of values, has higher power. Another test to explore is the Kruskal-Wallis test."
  },
  {
    "objectID": "09_nonparametric-2.html#nonparametric-bootstrap",
    "href": "09_nonparametric-2.html#nonparametric-bootstrap",
    "title": "1  Nonparametric Statistics 2",
    "section": "1.4 Nonparametric Bootstrap",
    "text": "1.4 Nonparametric Bootstrap\nWe will investigate one more technique for density estimation focused on estimating sampling distributions.\n\n\n\n\n\n\nNonparametric Bootstrap Sample\n\n\n\nThe nonparametric bootstrap sample is a random sample with replacement.\n\n\n\n\n\n\n\n\nSampling Distribution\n\n\n\nA sampling distribution of a statistic is the distribution we would observe if a statistic was calculated on many different samples from the same population distribution.\n\n\nSampling distributions are fundamental to statistics. We frequently use the normal distribution or Student’s t distribution to make inferences about population means and population regression coefficients.\n\n\n\n\n\n\nConfidence interval\n\n\n\nGiven a sampling procedure and a population, the target parameter is in the confidence interval X% of the time over many random samples.6\n\n\nThe idea of repeated sampling is fundamental to sampling distributions, confidence intervals, and statistical inference in frequentist statistics. The nonparametric bootstrap leverages the idea of repeated sampling to allow for statistical inference under a minimal number of assumptions. This makes it possible to perform inference:\n\nWhen a known sampling distribution for a statistic isn’t available\nWhen the sample is too small to apply the central limit theorem\n\n\n1.4.1 Basic Bootstrap\nSuppose we are interested in an unknown parameter \\(\\theta\\).\n\nCompute \\(\\hat\\theta\\) from the original data.\nTake \\(B\\) bootstrap samples of size \\(n\\) from the original data.\nCalculate \\(\\hat\\theta_{b,i}\\) on each bootstrap sample.\n\nThe vector \\(\\hat{\\vec\\theta_b}\\) is the bootstrap distribution and it has multiple uses. Importantly, it is a nonparametric estimate of the sampling distribution of \\(\\hat\\theta\\).\n\n\n1.4.2 Example\n\nB <- 5000\nb <- vector(mode = \"numeric\", length = B)\n\nfor (i in seq_along(b)) {\n  \n  x_boot <- sample(seed$balchange, replace = TRUE)\n  \n  b[i] <- median(x_boot)\n  \n}\n\ntibble(b) |>\n  ggplot(aes(b)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can use this permutation distribution to calculate bias (Section 1.4.3), standard errors (Section 1.4.4), and confidence intervals (Section 1.4.6).\n\n\n1.4.3 Bias\nSometimes estimators of population parameters are biased7. Bootstrapping can be used to estimate the bias in an estimator.\nLet \\(\\theta\\) be the parameter of interest and \\(\\hat\\theta\\) be an estimator. \\(\\hat\\theta\\) is unbiased if \\(E[\\hat\\theta] = \\theta\\).\nAccordingly, \\(bias(\\hat\\theta) = E[\\hat\\theta] - \\theta\\).\n\\(E[\\hat\\theta]\\) is just the sample mean of the sampling distribution.\n\\[\n\\hat{bias}(\\hat\\theta) = \\bar{\\hat{\\theta^*}} - \\hat\\theta\n\\]\nwhere \\(\\bar{\\hat\\theta^*} = \\frac{1}{b} \\sum_{b = 1}^B \\hat\\theta_b\\).\nBasically, to estimate bias we subtract our estimate from the mean of the sampling distribution, which is our bootstrap distribution. This estimate of bias can be used to correct for bias.\nIf \\(\\frac{|bias|}{se} \\le 0.25\\), then it is likely unnecessary to correct for bias (Rizzo 2008). We will next see how to estimate \\(se\\).\n\n\n1.4.4 Standard errors\nSuppose we want to estimate the standard error of an estimator \\(\\hat\\theta\\). To do this, simply take the standard deviation of the bootstrap replicates.\n\\[\n\\hat{se}(\\hat\\theta^*) = \\sqrt{\\frac{1}{B-1} \\sum_{b = 1}^B \\left(\\hat\\theta_b - \\bar{\\hat\\theta^*}\\right)^2}\n\\]\nwhere \\(\\bar{\\hat\\theta^*} = \\frac{1}{b} \\sum_{b = 1}^B \\hat\\theta_b\\).\nHistorically, the bootstrap was used to estimate the standard errors of estimates. The jackknife, another resampling method, can be used to estimate bias and standard errors. We won’t focus on this. Instead, we will estimate confidence intervals (Chernick and LaBudde 2011).\n\n\n1.4.5 Example\nFirst, let’s repeat the SEED example from above using the boot() function. boot() expects a custom function with the argument x for the data and the argument i for the index for statistics. We useR = 5000 for B <- 5000 in the earlier example.\n\nlibrary(boot)\n\nseed_bootstrap <- boot(\n  data = seed$balchange, \n  statistic = function(x, i) median(x[i]), \n  R = 5000\n)\n\nseed_bootstrap\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = seed$balchange, statistic = function(x, i) median(x[i]), \n    R = 5000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1*   -63.04 -0.9602388    7.817336\n\n\nlibrary(boot) and boot() are useful when we want to see bias. They are also useful when we are interested in statistics not included in library(infer). Otherwise, we will use library(infer).\n\nbootstrap_distribution <- seed |>\n  specify(response = balchange) |>\n  hypothesize(null = \"point\", med = 0) |>\n  generate(reps = 5000, type = \"bootstrap\") |>\n  calculate(stat = \"median\")\n\nbootstrap_distribution |>\n  visualize()\n\n\n\n\n\n\n1.4.6 Confidence intervals\nUnder certain assumptions it is simple to construct confidence intervals for parameters like the population mean.\nThere are several ways to calculate confidence intervals using bootstrapping:\n\nPercentile method\nBCA\nt-pivot\n\nApproach three doesn’t always work but probably has the most desirable properties. Approach 2 is a suitable backup. Approach 1 often has the worst properties, but is simple and will be sufficient in many applications.\n\nPercentile Method\nFor each bootstrap sample indexed by \\(b = 1, ..., B\\),\n\nCompute \\(\\hat\\theta\\) from the original data.\nTake \\(B\\) bootstrap samples of size \\(n\\) from the data.\nCompute \\(\\hat\\theta_b\\) for each bootstrap sample.\nFor \\(\\alpha = 0.05\\), find the 2.5th and 97.th percentiles of \\(\\hat{\\vec\\theta}\\).\n\n\n\n\n1.4.7 Example\n\npoint_estimate <- seed |>\n  specify(response = balchange) |>\n  calculate(stat = \"median\")\n\nbootstrap_distribution |>\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n\n\nbootstrap_distribution |>\n  get_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0\n\nbootstrap_distribution |>\n  get_confidence_interval(\n    point_estimate = point_estimate,\n    level = 0.95,\n    type = \"se\"\n  )\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1    -78.2    -47.8\n\n\nThe confidence intervals are comparable. We’re good to go.\nStudents can take multiple statistics classes and never learn how to calculate a 95% confidence interval for a median. It’s simple with bootstrapping.\nIn this case, it’s striking that the balance for the median SEED account declined over the period and there is sufficient evidence that the median is statistically significantly different than $0. In fact, it’s negative!"
  },
  {
    "objectID": "09_nonparametric-2.html#more-methods",
    "href": "09_nonparametric-2.html#more-methods",
    "title": "1  Nonparametric Statistics 2",
    "section": "1.5 More Methods",
    "text": "1.5 More Methods\nM-Estimation and quantile regression are two nonparametric techniques that we will not discuss that are common public policy analysis.\nM-Estimation is useful for estimating conditional means in the presence of outliers. Quantile regression is useful for estimating conditional quantiles like the conditional median.\n\n\n\n\n\n\nExercise 3\n\n\n\n\n\n\n\nStart with the seed data. Drop observations where group3 == \"marketing\".\nUse library(infer) and bootstrapping to test if the median is statistically significantly different than zero.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\n\n\nlibrary(infer) allows for tests about the median but not other percentiles.\n\nStart with the full seed data.\nUse boot() and boot.ci() to test if the 75th percentile for balchange is different than 0.\n\n\n\n\n\n\n\n\n\n\nAshraf, N., D. Karlan, and W. Yin. 2006. “Tying Odysseus to the Mast: Evidence From a Commitment Savings Product in the Philippines.” The Quarterly Journal of Economics 121 (2): 635–72. https://doi.org/10.1162/qjec.2006.121.2.635.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001. “Interval Estimation for a Binomial Proportion.” Statistical Science 16 (2). https://doi.org/10.1214/ss/1009213286.\n\n\nChernick, Michael R., and Robert A. LaBudde. 2011. An Introduction to Bootstrap Methods with Applications to r. Hoboken, N.J: Wiley.\n\n\nHiggins, James J. 2004. An Introduction to Modern Nonparametric Statistics. Pacific Grove, CA: Brooks/Cole.\n\n\nRizzo, Maria L. 2008. Statistical Computing with r. Chapman & Hall/CRC Computer Science and Data Analysis Series. Boca Raton: Chapman & Hall/CRC."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ashraf, N., D. Karlan, and W. Yin. 2006. “Tying Odysseus to the\nMast: Evidence From a Commitment Savings Product in the\nPhilippines.” The Quarterly Journal of Economics 121\n(2): 635–72. https://doi.org/10.1162/qjec.2006.121.2.635.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial Proportion.”\nStatistical Science 16 (2). https://doi.org/10.1214/ss/1009213286.\n\n\nChernick, Michael R., and Robert A. LaBudde. 2011. An Introduction\nto Bootstrap Methods with Applications to r. Hoboken, N.J: Wiley.\n\n\nHiggins, James J. 2004. An Introduction to Modern Nonparametric\nStatistics. Pacific Grove, CA: Brooks/Cole.\n\n\nRizzo, Maria L. 2008. Statistical Computing with r. Chapman\n& Hall/CRC Computer Science and Data Analysis Series. Boca Raton:\nChapman & Hall/CRC."
  }
]