[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Public Policy Part II",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "08_nonparametric-1.html#sec-review8",
    "href": "08_nonparametric-1.html#sec-review8",
    "title": "1  Nonparametric Curve Fitting",
    "section": "1.1 Review",
    "text": "1.1 Review\n\nStatistical models serve three major purposes\n\nSummary\nInference\nPrediction\n\nWe adopted nonparametric techniques like KNN, decision trees, and random forests for making accurate predictions.\nIn certain applications, these data-driven approaches work well."
  },
  {
    "objectID": "08_nonparametric-1.html#nonparametric-statistics",
    "href": "08_nonparametric-1.html#nonparametric-statistics",
    "title": "1  Nonparametric Curve Fitting",
    "section": "1.2 Nonparametric Statistics",
    "text": "1.2 Nonparametric Statistics\n\n\n\n\n\n\nParametric Statistics\n\n\n\nParametric statistics requires the form of the population distribution to be completely specified except for a finite number of parameters.\nFor example, the population is normally distributed with two parameters: mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)).\n\n\n\n\n\n\n\n\nNonparametric Statistics\n\n\n\nNonparametric statistics1 require a minimal number of assumptions about the form of the population distribution.\nHiggins (2004) offers two key examples of minimal assumptions:\n\nThe data are from a population with a continuous distribution.\nThe population depends on location and scale parameters.\n\n\n\nThere are nonparametric test for many common families of statistical inference. For example,\n\n1-sample location parameter test (like a 1-sample t-test).\n2-sample location parameter test (like a 2-sample t-test).\nK-sample location parameter test (like ANOVA).\n2-sample scale parameter test (like a 2-sample F-test).\n\nNonparametric statistics can also be grouped by their statistical ideas2. Many of these ideas are incredibly simple and could be, as my professor used to say, derived while sitting on the beach.\n\nSmoothing: Fitting curves without specifying the functional forms of the curves. This can be done in one-dimension to estimate a probability distribution function and in multiple dimension for estimating smoothed conditional means.\nPermutation: Exhaustively reconfigure the data to create a sampling distribution for a statistic.\nBootstrap: Repeatedly sample from the data and calculate a statistical to create a sampling distribution for a statistic.\n\nThis set of notes focuses on techniques for estimating probability density functions (PDFs) with smoothing when the data don’t conform to a common PDF like the normal distribution or exponential distribution. The next set of notes with focus on the permutation and bootstrap for estimating sampling distributions (PDFs for statistics)."
  },
  {
    "objectID": "08_nonparametric-1.html#estimating-probability-density-functions",
    "href": "08_nonparametric-1.html#estimating-probability-density-functions",
    "title": "1  Nonparametric Curve Fitting",
    "section": "1.3 Estimating Probability Density Functions",
    "text": "1.3 Estimating Probability Density Functions\nSo far, we’ve explored parametric techniques for estimating densities and probability density functions. We looked at data and fit common probability distributions with a finite number of parameters to the data. For instance, for a normal distribution the sufficient statistics or finite number of parameters are the mean and standard deviation. Unfortunately, common distributions may not fit the data.\nIn this section, we will look at data-driven nonparametric approaches to estimating probability density functions.\nSuppose we have have a random sample of data \\(x_1, x_2, ..., x_n\\) for one variable and the data come from a population with a continuous probability density function. We represent the PDF as \\(f_X(x)\\). Our goal is to estimate \\(f_X(x)\\) with \\(\\hat{f_X}(x)\\) using a data-driven approach and without specify a model or functional form.\nWe will discuss two approaches to this problem:\n\nRelative frequency histogram\nKernel density estimators (KDEs)\n\nWe can use relative frequency histograms and KDEs to 1. visualize data and 2. generate new data with random sampling.\n\n1.3.1 Relative Frequency Histograms\n\n\n\n\n\n\nRelative Frequency Histogram\n\n\n\nRelative frequency histograms are histograms normalized such that the bars sum to 1 that can be used to estimate PDFs.\nNote: PDFs must integrate to 1.\n\n\nThe procedure for creating a relative frequency histogram is fairly simple:\n\nPartition the range of the data into \\(k\\) bins with equal width. The widths are called binwidths.\nCount the number of observations in each bin.\nDivide the counts by n * binwidth to normalize the histogram\n\nLet’s consider a formal definition from (Higgins 2004).\nPartition the range of the data into bins such that \\(a_1 &lt; a_2 &lt; ... &lt; a_k\\). Let \\(a_{i + 1} - a_i\\) be the binwidth. The density estimate for a value \\(x\\) in an interval \\(a_i &lt; x \\le a_{i + 1}\\) is\n\\[\\hat{f}(x) = \\frac{\\text{number of observations} \\in (a_i, a_{i + 1}]}{n \\cdot (a_{i + 1} - a_i)}\\]\n\nExample\nConsider three random samples of varying sizes from standard normal distributions.\n\nset.seed(20230617)\ndata &lt;- tibble(\n  set = c(\n    rep(\"n=40\", times = 40), \n    rep(\"n=100\", times = 100), \n    rep(\"n=10,000\", times = 10000)\n  ),\n  x = c(rnorm(n = 40), rnorm(n = 100), rnorm(n = 10000))\n) |&gt;\n  mutate(set = factor(set, levels = c(\"n=40\", \"n=100\", \"n=10,000\")))\n\nLet’s use library(ggplot2) and the after_stat() function to create relative frequency histograms for the three random samples. Remember, the relative frequency histograms are \\(\\hat{f}(x)\\).\nIn this example, we know \\(f_X(x)\\). It is the PDF for \\(X \\sim N(\\mu = 0, \\sigma = 1)\\). Next, superimpose the known PDF on top of \\(\\hat{f_X}(x)\\).\n\ndata |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = 0, sd = 1)) +\n  facet_wrap(~ set)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nFigure 1.1: Relative frequency histograms fit to sample data from standard normal distributions with three different sample sizes.\n\n\n\n\n\n\n\nNumber of Bins and Binwidth\nHistograms and relative frequency histograms are very sensitive to the selection of binwidths. geom_histogram() defaults to 30 bins, but this can be overridden with the binwidth and bins argument.\nSeveral papers have developed rules for determining the number of bins or binwidth including Scott and Sain (2005).\nAn important takeaway is that increasing \\(k\\), the number of bins, can lead to noisier bars but more precision in the estimate, while decreasing \\(k\\) results in less noisy bars but imprecise estimates. Figure 1.2 recreates Figure 1.1 with fewer bins.\n\ndata |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 10) +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = 0, sd = 1)) +\n  facet_wrap(~ set)\n\n\n\nFigure 1.2: Relative frequency histograms fit to sample data from standard normal distributions with three different sample sizes.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nRun hist(cars$speed, plot = FALSE) and look at the results.\nWrite R code to convert $counts into $density based on the process outlined above.\n\n\n\n\n\n\n1.3.2 Kernel Density Estimation\nWe will introduce kernel density estimators by considering two related concepts.\n\nConcept #1\nWe were able to simplify and fit complex probability densities using mixture distributions. In fact, as \\(k\\), the number of mixture distributions increased, we were able to fit increasingly complex probability densities. Unfortunately, this process created overparameterized models.\nKernel density estimation, a nonparametric approach, takes a slightly different approach. It places a normal distribution on every observation in the data set and then combines these normal distributions into a complex probability density. We no longer need to estimate many parameters and specify the number of clusters, but the KDE is computationally expensive and requires storing all of the original data.\n\n\nConcept #2\nRelative frequency histograms have a couple of issues:\n\nThey are very sensitive to binwidths\nThey are not smooth\n\n\n\n\n\n\n\nKernel Density Estimators\n\n\n\nKernel Density Estimators are weighted relative frequencies of the number of values near some value. The counts are normalized so the entire KDE integrates to 1. KDEs are a data-driven method for estimating PDFs.\nKernel density estimators are smoothed histograms. Fundamentally, they use a weighted average of data points near \\(x_0\\) to generate \\(\\hat{f}(x_0)\\). Here, \\(x_0\\) is the point where we are estimating the probability density.\n\n\n\n\nKDE Process\n\n\n\n\n\n\nTip\n\n\n\nKernel: A kernel (more precisely density kernel) is a function that maps real values to real positive values. The kernel must be symmetric and integrate to 1.\nIn many ways, density kernels are a subset of probability density functions. Often, we will use the normal or Gaussian PDF as a kernel.\n\n\nLet \\(w(z)\\) be a kernel (symmetric probability distribution) centered at \\(0\\). This kernel can give more weight to observations near \\(x_0\\) and less weight to observations far from \\(x_0\\). Common choices include the Normal kernel and boxcar kernel.\nLet \\(\\triangle\\) be a bandwidth. Bandwidths are like binwidths in relative frequency histograms. The bandwidth controls how much weight data points near\nThe kernel density estimate (KDE) is\n\\[\n\\hat{f}(x) = \\frac{1}{n\\triangle} \\sum_{i = 1}^n w\\left(\\frac{x_0 - x_i}{\\triangle}\\right)\n\\tag{1.1}\\]\nBased on Equation 1.1, we need to pick a weighting function (\\(W(z)\\)) and a bandwidth (\\(\\triangle\\)). We will almost always use the normal distribution and bw.nrd0 bandwidth3. This means we can define a much simpler process:\n\nPlace a normal distribution on each observation in the data\nSum up the densities at \\(x_0\\)\nDivide by \\(n\\) to normalize the distribution\n\nConsider an example where x = c(1, 4, 4.5, 5, 6, 8, 10). Figure 1.3 visually shows the creation of the KDE for x.\n\n\n\n\nFigure 1.3: The KDE (black) is the Sum of the Normal Distributions (red) Normalized by the Number of Observations\n\n\n\n\n\n\n\nKDE’s in R\nThere are several ways to use kernel density estimators in R. We will explore geom_density() and density().\nThe use of geom_density() is nearly identical to geom_histogram(). The function has a bw argument to change the bandwidth and a kernel argument to change the kernel.\n\ndata &lt;- expand_grid(\n  x = c(1, 4, 4.5, 5, 6, 8, 10)\n)\n\ndata |&gt;\n  ggplot(aes(x = x)) +\n  geom_density()\n\ndata |&gt;\n  ggplot(aes(x = x)) +\n  geom_density(kernel = \"triangular\")\n\nFigure 1.4: geom_density() with different kernels\n\n\n\n\n\n(a) Gaussian kernel\n\n\n\n\n\n\n\n\n\n(b) Triangular kernel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nThe log-normal distribution is a distribution whose natural log is normally distributed. It is useful for modeling variables with a right skew like income and wealth.\n\nSample 100 observations from a log-normal distribution using rlnorm() with meanlog = 0 and sdlog = 1.\nCreate a relative frequency histogram for the variable.\nAdd a KDE with geom_density() in red.\nUse stat_function(), like in Figure 1.1, to add the theoretical PDF to the visualization in blue. You should now have three layers.\nDuplicate the code and set n to 10000. Is something off?"
  },
  {
    "objectID": "08_nonparametric-1.html#nonparametric-curve-smoothing",
    "href": "08_nonparametric-1.html#nonparametric-curve-smoothing",
    "title": "1  Nonparametric Curve Fitting",
    "section": "1.4 Nonparametric Curve Smoothing",
    "text": "1.4 Nonparametric Curve Smoothing\nWe now move to bivariate data where we have \\(x_1, x_2, ..., x_n\\) and \\(y_1, y_2, ..., y_n\\). Our goal is to flexibly estimate \\(f(x)\\) with \\(\\hat{f}(x)\\).4\n\\(\\hat{f}(x)\\) is an estimate of the conditional mean \\(E\\left[Y|X=x\\right]\\). Recall that linear regression is a parametric approach, with a finite number of parameters, for estimating this conditional mean. When we plug in a specific value, \\(x_0\\), we get back the conditional mean for \\(y\\) at the specific value \\(x_0\\).\nWe’re focused on nonparametric approaches to estimate \\(E\\left[Y|X=x\\right]\\). We want to make a minimal number of assumptions and we don’t want to specify a functional form. This is a very data-driven approach, which is advantageous when the data follow a clear pattern but the functional form isn’t easy to specify with just main effects or low-order polynomials.\n\n1.4.1 Approaches\nThere are three major nonparametric approaches to curve smoothing.\n\nKernel methods. Kernel methods are related to kernel density estimation for estimating probability density functions. Here, we need to estimate the joint PDF \\(f_{X,Y}(x, y)\\) and the marginal PDF \\(f_X(x)\\). Next, integrate out \\(f_X(x)\\) to approximate \\(E\\left[Y|X=x\\right]\\). This approach can be highly biased in the tails of \\(x\\) and in sparse regions of \\(x\\).(Hastie, Tibshirani, and Friedman 2009)\nRegression splines. Regression splines break \\(x\\) into ordered regions and then fit \\(\\hat{f}(x)\\) as a series of piece-wise low-order polynomial regressions. Special conditions ensure that \\(\\hat{f}(x)\\) is continuous where splines meet.\nLocal linear regression and locally estimated scatter plot smoothing (LOESS). These methods use weighted linear regression estimated on observations near \\(x_0\\), where \\(x_0\\) is a specific point in the domain where we want to estimate \\(\\hat{f}\\)\n\n\n\n1.4.2 K-Nearest Neighbors\nK-Nearest Neighbors (KNN) average is an estimate of the regression function/conditional mean \\(E\\left[Y|X=x\\right]\\).\nLet \\(N_k(x_0)\\) be the \\(k\\) closest observations to \\(x_0\\). Closeness is often measured with Euclidean distance. Let \\(Ave\\) denote the mean. Then\n\\[\n\\hat{f}(x_0) = Ave\\left(y_i | x_i \\in N_k(x_0)\\right)\n\\tag{1.2}\\]\nis an estimate of the conditional mean of \\(y\\) at point \\(x_0\\).\nConsider an example with simulated data.\n\nset.seed(20201004)\n\nx &lt;- runif(n = 100, min = 0, max = 10)\n\ndata1 &lt;- bind_cols(\n  x = x,\n  y = 10 * sin(x) + x + 20 + rnorm(n = length(x), mean = 0, sd = 2)\n)\n\n\n\n\n\nFigure 1.5: KNN average on simulated data with \\(k = 3\\).\n\n\n\n\n\nKNN Average has a couple of issues.\n\n\\(\\hat{f}(x)\\) is often a discontinuous stepwise function.\nAll observations in \\(N_k(x_0)\\) are given the same weight even though some values are nearer to \\(x_0\\) than other values.\n\n\n\n1.4.3 Local Linear Regression and LOESS\nLocal linear regression fits a separate weighted linear regression model for each \\(x_0\\) on a subset of the data. Only the closest observations are used. LOESS, a specific local linear regression model, is a curve smoothing approach for data visualization.\nThe conditional mean for LOESS is a fairly simple weighted linear regression model.\n\\[\n\\hat{f}(x_0) = \\hat{l}(x_0)\n\\]\nAll we need to do is calculate weights for the weighted linear regression. Span, the \\(k\\) closest observations considered for each linear regression model is the only hyperparameter for LOESS. We will use the tricube weight function using the following procedure:\n\nCalculate the distance from \\(x_0\\) to all observations in the data.\nPick the \\(k\\) closest values.\nDivide all chosen distances by the maximum distance so that all distances are in \\([0, 1]\\).\nApply the tricube function, \\((1 - u ^ 3) ^ 3\\) to the scaled distances.\n\nOnce the weights are calculated, simply plug them into a linear regression model using the \\(k\\) closest observations to \\(x_0\\).\nLet’s look at the math that explains the algorithm. The LOESS conditional mean uses weighted least squares to find \\(\\hat{l}(x_0)\\) that minimizes\n\\[\n\\sum_{x_i \\in N_k(x_o)} \\left[y_i - l(x_i)\\right]^2W\\left(\\frac{|x_0 - x_i|}{\\triangle_{x_o}}\\right)\n\\]\n\n\\(x_i \\in N_k(x_o)\\) is the \\(k\\) closest observations to \\(x_0\\)\n\\(l(x_i)\\) is a weighted linear regression model\n\\(|x_0 - x_i|\\) is a vector of distances\n\\(\\triangle_{x_o}\\) is the maximum distance in the vector \\(|x_0 - x_i|\\)\n\\(W()\\) is a kernel\n\nFigure 1.6 demonstrates LOESS on simulated data.\n\n\n\n\nFigure 1.6: LOESS fit on simulated data. Red points are predicted values a \\(x_0 \\in \\{2, 4, 8\\}\\). Dashed lines are fitted local linear regression models.\n\n\n\n\n\nIn general, LOESS is very poor for extrapolating beyond the range of \\(x\\) and even in places where data are sparse. In practice, local linear regression models and LOESS are mostly used for visualizing trends in scatter plots using a smoothed conditional mean line.\n\n\n1.4.4 Code\n\ngeom_smooth()\nLOESS is not limited to simple linear regression models. For example, geom_smooth() includes a 2nd-order polynomial for each local regression model by default. Let’s fit a\n\ndata1 |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nFigure 1.7: ?(caption)\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse geom_smooth() to fit a LOESS model to the cars data with x = speed and y = dist.\nUse geom_smooth() to fit a linear regression model to the cars data with x = speed and y = dist. Make the line red.\n\n\n\n\n\nloess()\nloess() can be used to fit a local linear regression model. It contains many arguments than be used to fine-tune the fit include span, degree, and surface.\n\ndata1_loess &lt;- loess(formula = y ~ x, data = data1, degree = 1)\n\ndata1_loess\n\nCall:\nloess(formula = y ~ x, data = data1, degree = 1)\n\nNumber of Observations: 100 \nEquivalent Number of Parameters: 2.76 \nResidual Standard Error: 5.446 \n\n\nOnce we have a LOESS object, we can use predict() to calculate the conditional mean.\n\npredict(data1_loess, newdata = tibble(x = c(1, 5, 8)))\n\n       1        2        3 \n25.78555 22.38631 30.15598 \n\n\nLOESS is nonparametric and fits a new model for each unique \\(x_0\\). LOESS is memory-based meaning that almost all computation is done when predict() is run. This is computationally expensive and LOESS will not work well with large data sets.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nGenerate the simulated data from above.\nGenerate the plot Figure 1.7.\nUse loess() and predict() to generate a conditional mean for \\(x_0 = 6\\).\nAdd the predicted values to the data visualization using geom_point() and color = \"red\".\n\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n\n\nHiggins, James J. 2004. An Introduction to Modern Nonparametric Statistics. Pacific Grove, CA: Brooks/Cole.\n\n\nScott, David W., and Stephan R. Sain. 2005. “Multidimensional Density Estimation.” In, 24:229–61. Elsevier. https://doi.org/10.1016/S0169-7161(04)24009-3."
  },
  {
    "objectID": "08_nonparametric-1.html#footnotes",
    "href": "08_nonparametric-1.html#footnotes",
    "title": "1  Nonparametric Curve Fitting",
    "section": "",
    "text": "R.A. Fisher, a key developer of frequentist statistics, developed an early nonparametric test called Fisher’s Exact Test. The use of nonparametric statistics has increased with swelling computing power.↩︎\nMethods based on binomial distribution are also common but we will skip them.↩︎\n\\(\\text{bandwidth} = 0.9 min\\left(\\hat{\\sigma}, \\frac{IQR}{1.34}\\right)\\)↩︎\nNote that \\(f_x(x)\\) above meant a probability density function. In this case, \\(f(x)\\) is a function that relates \\(y_1, y_2, ..., y_n\\) to \\(x_1, x_2, ..., x_n\\).↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. Springer Series in Statistics. New York, NY:\nSpringer.\n\n\nHiggins, James J. 2004. An Introduction to Modern Nonparametric\nStatistics. Pacific Grove, CA: Brooks/Cole.\n\n\nScott, David W., and Stephan R. Sain. 2005. “Multidimensional\nDensity Estimation.” In, 24:229–61. Elsevier. https://doi.org/10.1016/S0169-7161(04)24009-3."
  }
]