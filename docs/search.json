[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Public Policy Part II",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "07_microsimulation.html",
    "href": "07_microsimulation.html",
    "title": "1  Microsimulation",
    "section": "",
    "text": "2 Review\nIt is often important to ask what would be the policy impact of…“ on a population or subpopulation.\nOne common approach is to look at representative agents. For example, we could construct one observation that is representative and pass its values into a calculator. Then, we could extrapolate this experience to other observations.\nAnother common approach is to look at aggregated data. For example, we could look at county-level insurance coverage in Medicaid expansion and non-expansion states and then extrapolate to other health care expansions.\nOrcutt (1957) suggested a radically different approach. Instead of using a representative agent or aggregated data to project outcomes, model outcomes for individual units and aggregate the results. Potential units-of-analysis include people, households, and firms. Models include anything from simple accounting rules to complex behavioral and demographic models.\nIt took decades for this approach to see wide adoption because of data and computational limitations, but it is now commonplace in policy evaluation.\nWe’ll first look at calculators, which are an important tool for representative agent methods and microsimulation.\nThese two examples show the value of using calculators to explore potentially harmful marriage disincentives like marriage penalties, benefits cliffs, and extremely high marginal tax rates.\nMicrosimulation requires many assumptions and significant investment, but is useful for a few reasons:\nMicrosimulation is widely used in government, not-for-profits, and academia:\nMicrosimulation is also widely cited in popular publications around important debates:\nMicrosimulation models range from simple calculators applied to representative microdata to very complex dynamic models."
  },
  {
    "objectID": "07_microsimulation.html#basic-microsimulation",
    "href": "07_microsimulation.html#basic-microsimulation",
    "title": "1  Microsimulation",
    "section": "5.1 Basic Microsimulation",
    "text": "5.1 Basic Microsimulation\n\n\n\n\n\n\nAccounting Rules\n\n\n\nAccounting rules are the basic calculations associated with government law and programs like taxes, Social Security benefits, and Medicare.\nAccounting rules are sometimes called arithmetic rules because they are typically limited to addition, subtraction, multiplication, division, and simple if-else logic.\n\n\nLet’s start with a very simple algorithm for microsimulation modeling:\n\nConstruct a sample that represents the population of interest.\nApply accounting rules.\nAggregate.\n\nThe simplest microsimulation models essentially apply tax models similar to TurboTax to a representative set of microdata.\nWe can summarize the differences using summary statistics that demonstrate the distribution of outcomes. For example, it is common to look at deciles or key percentiles to understand the heterogeneity of changes.\nWe can construct a baseline simulation by applying current law for step 2. Next, we can construct an alternative or counterfactual simulation by changing step 2 to a proposed policy. Finally, we can difference current law and the counterfactual to estimate the impact of a policy.\n\n5.1.1 Example 2\nLet’s consider a simple example where we apply the benefit calculator from earlier to families from the 2022 Annual Social and Economic Supplement to the Current Population Survey.\nTo keep things simple, we only consider families of the respondent to the CPS. Furthermore, we will ignore survey weights.\n\n\nCode\n# if file doesn't exist in data, then download\nif (!file.exists(here(\"data\", \"cps_microsim.csv\"))) {\n  \n  cps_extract_request &lt;- define_extract_cps(\n    description = \"2018-2019 CPS Data\",\n    samples = \"cps2022_03s\",\n    variables = c(\"YEAR\", \"NCHILD\", \"FTOTVAL\")\n  )\n  \n  submitted_extract &lt;- submit_extract(cps_extract_request)\n  \n  downloadable_extract &lt;- wait_for_extract(submitted_extract)\n  \n  data_files &lt;- download_extract(\n    downloadable_extract,\n    download_dir = here(\"data\")\n  )\n  \n  cps_data &lt;- read_ipums_micro(data_files)\n  \n  cps_data |&gt;\n    filter(PERNUM == 1) |&gt;\n    mutate(\n      FTOTVAL = zap_labels(FTOTVAL),\n      NCHILD = zap_labels(NCHILD)\n    ) |&gt;\n    select(SERIAL, YEAR, ASECWT, NCHILD, FTOTVAL) |&gt;\n    rename_with(tolower) |&gt;\n    write_csv(here(\"data\", \"cps_microsim.csv\"))\n  \n}\n\n\n\nasec &lt;- read_csv(here(\"data\", \"cps_microsim.csv\"))\n\nRows: 59148 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): serial, year, asecwt, nchild, ftotval\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproposal1 &lt;- asec |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    )\n  )\n\nproposal1\n\n# A tibble: 59,148 × 6\n   serial  year asecwt nchild ftotval benefit\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      1  2022  2553.      0   44220       0\n 2      5  2022  1587.      0   82752       0\n 3      6  2022  1493.      0   60001       0\n 4      9  2022   898.      3  123814       0\n 5     10  2022  2031.      0   31184       0\n 6     11  2022  1987.      0       0       0\n 7     12  2022  1978.      0   19783       0\n 8     13  2022  1098.      2   72370       0\n 9     15  2022  2334.      0   44000       0\n10     17  2022  1978.      0     400       0\n# ℹ 59,138 more rows"
  },
  {
    "objectID": "07_microsimulation.html#distributional-analysis",
    "href": "07_microsimulation.html#distributional-analysis",
    "title": "1  Microsimulation",
    "section": "5.2 Distributional Analysis",
    "text": "5.2 Distributional Analysis\nAggregate analysis and representative agent analysis often mask important heterogeneity. The first major advantage of microsimulation is the ability to apply distributional analysis.\n\n\n\n\n\n\nDistributional Analysis\n\n\n\nDistributional analysis is the calculation and interpretation of statistics outside of the mean, median, and total. The objective is to understand a range of outcomes instead of typical outcomes.\n\n\nHere, we expand step 3 from the basic microsimulation algorithm to include a range of statistics. The most common statistics are percentiles or outcomes for ntiles.\n\n5.2.1 Example 3\nConsider the previous example. Let’s summarize the mean benefit by family income decile. We can use the ntile() function to construct ntiles(). We can use min() and max() in summarize() to define the bounds of the ntiles.\n\ndistributional_table &lt;- proposal1 |&gt;\n  mutate(ftotval_decile = ntile(ftotval, n = 10)) |&gt;\n  group_by(ftotval_decile) |&gt;\n  summarize(\n    min_income = min(ftotval),\n    max_income = max(ftotval),\n    mean_benefit = mean(benefit)\n  )\n\ndistributional_table\n\n# A tibble: 10 × 4\n   ftotval_decile min_income max_income mean_benefit\n            &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1              1     -19935      14160       1019. \n 2              2      14160      25200        921. \n 3              3      25200      37432        863. \n 4              4      37433      50100        622. \n 5              5      50100      65122        266. \n 6              6      65124      84000         59.6\n 7              7      84000     108006          0  \n 8              8     108010     143210          0  \n 9              9     143221     205154          0  \n10             10     205163    2320191          0  \n\n\n\n\nCode\ndistributional_table |&gt;\n  ggplot(aes(ftotval_decile, mean_benefit)) +\n  geom_col() +\n  geom_text(aes(label = scales::label_dollar()(mean_benefit)), vjust = -1) +\n  scale_x_continuous(breaks = 1:10) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1)),\n    labels = scales::label_dollar() \n  ) +\n  labs(\n    title = \"Distribution of Average Benefits Under the Williams Benefit Proposal\",\n    x = \"Familiy Income Decile\",\n    y = \"Average Benefit\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nApply the calculator from the earlier exercise to the CPS data.\nAggregate outcomes for the 5th, 15th, and 25th percentiles.\nAggregate outcomes for the bottom 10 vintiles."
  },
  {
    "objectID": "07_microsimulation.html#counterfactual-analysis",
    "href": "07_microsimulation.html#counterfactual-analysis",
    "title": "1  Microsimulation",
    "section": "5.3 Counterfactual Analysis",
    "text": "5.3 Counterfactual Analysis\n\n\n\n\n\n\nCounterfactual\n\n\n\nA counterfactual is a situation that would be true under different circumstances.\n\n\nThe second major advantage of microsimulation is the ability to implement counterfactuals and evaluate “what-if” situations. Consider a few examples:\n\nWhat could happen to the distribution of post-tax income if the top marginal tax rate is increased by 5 percentage points?\nWhat could happen to median Social Security benefits in 2030 if the retirement age is increased by 2 months every year beginning in 2024?\nWhat could happen to total student loan balances if federal student loan interest accumulation is paused for 6 more months.\n\nWe update our microsimulation algorithm to include counterfactual analysis.\n\nConstruct a sample that represents the population of interest.\nApply accounting rules that reflect current circumstances. This is the baseline microsimulation.\nApply accounting rules that reflect counterfactual circumstances. This is the counterfactual microsimulation.\nAggregate results with a focus on the difference between the baseline microsimulation and the counterfactual simulation.\n\n\n5.3.1 Example 4\nLet’s pretend the new_tax_credit() is current law. It is our baseline. Suppose a legislator proposes reforms to the law. This is our counterfactual. Here are the proposed changes:\n\nEliminate benefits for families with zero income to promote work.\nEliminate the $20,000 income exclusion to reduce benefits for higher earners.\n\n\n#' Calculate the benefit from the new tax credit\n#'\n#' @param num_children A numeric for the number of children\n#' @param family_income A numeric for family income\n#'\n#' @return Numeric benefit in dollars\nnewer_tax_credit &lt;- function(num_children, family_income) {\n  \n  dplyr::case_when(\n    family_income == 0 ~ 0,\n    num_children &gt;= 3 ~ pmax(0, 6000 - 0.1 * family_income),\n    num_children == 2 ~ pmax(0, 4500 - 0.1 * family_income),\n    num_children == 1 ~ pmax(0, 3000 - 0.1 * family_income),\n    TRUE ~ 0\n  )\n\n}\n\n\nproposal2 &lt;- asec |&gt;\n  mutate(\n    benefit_baseline = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    ),\n    benefit_counteractual = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = newer_tax_credit\n    )\n  )\n\nproposal2 |&gt;\n  mutate(benefit_change = benefit_counteractual - benefit_baseline) |&gt;\n    mutate(ftotval_decile = ntile(ftotval, n = 10)) |&gt;\n  group_by(ftotval_decile) |&gt;\n  summarize(\n    min_income = min(ftotval),\n    max_income = max(ftotval),\n    mean_benefit = mean(benefit_change)\n  )\n\n# A tibble: 10 × 4\n   ftotval_decile min_income max_income mean_benefit\n            &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1              1     -19935      14160       -374. \n 2              2      14160      25200       -436. \n 3              3      25200      37432       -557. \n 4              4      37433      50100       -467. \n 5              5      50100      65122       -242. \n 6              6      65124      84000        -59.6\n 7              7      84000     108006          0  \n 8              8     108010     143210          0  \n 9              9     143221     205154          0  \n10             10     205163    2320191          0  \n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nRun the baseline and counterfactual simulations.\nCreate a bar chart representing the change in benefits."
  },
  {
    "objectID": "07_microsimulation.html#extrapolation",
    "href": "07_microsimulation.html#extrapolation",
    "title": "1  Microsimulation",
    "section": "5.4 Extrapolation",
    "text": "5.4 Extrapolation\n\n\n\n\n\n\nExtrapolation\n\n\n\nExtrapolation is the extension of microsimulation models to unobserved time periods. Most often, microsimulation models are extrapolated into the future.\n\n\nMost microsimulation models incorporate time. For example, many models look at a few or many years. We now add a new step 2 to the microsimulation algorithm, where we we can project demographic and economic outcomes into the future or past before applying accounting rules and aggregating the results.\n\nConstruct a sample that represents the population of interest.\nExtrapolate the population of interest over multiple time periods.\nApply accounting rules.\nAggregate.\n\nSome sophisticated models treat time as continuous. More often, microsimulation treats time as discrete. For example, models represent time every month or every year.\nExtrapolation adds uncertainty and assumptions to microsimulation. It is important to be clear about what microsimulation is and isn’t.\n\n\n\n\n\n\nProjection\n\n\n\nA projection explores what could happen under a given set of assumptions. It is always correct under the assumptions.\nStats Canada\n\n\n\n\n\n\n\n\nForecast\n\n\n\nA forecast attempts to predict the most-likely future.\nStats Canada\n\n\nMost microsimulation models are projections, not forecasts.\n\n5.4.1 Transitions\n\n\n\n\n\n\nStatic Microsimulation\n\n\n\nStatic microsimulation models do not subject units to individual transitions between time periods \\(t - 1\\) and \\(t\\) or behavioral responses.\n\n\nStatic models typically only deal with one time period or they deal with multiple time periods but reweight the data to match expected totals and characteristics over time. Basically, individual decisions affect the distribution of outcomes but have little impact on overall outcomes.\n\n\n\n\n\n\nDynamic Microsimulation\n\n\n\nDynamic microsimulation models subject individual units to transitions between time periods \\(t - 1\\) and \\(t\\). This is sometimes referred to as “aging” the population. Or they subject individual units to behavioral responses.\n\n\nTransitions from period \\(t - 1\\) to period \\(t\\) are key to dynamic microsimulation. Transitions can be deterministic or stochastic. An example of a deterministic transition is an individual always joining Medicare at age 65. An example of a stochastic transition is an unmarried individual marrying at age 30 with probability \\(p_1\\) and remaining unmarried with probability \\(p_2\\).\nTransition models are fundamental to stochastic transitions. Transition models include transition probability models (categorical variables) and processes based on probability distributions (continuous variables).\nHere a few examples:\n\nIf an individual will acquire more education.\nIf an individual will work.\nIf an individual will marry, divorce, or widow.\nIf an individual will have children.\nIf an individual will retire.\nIf an individual will die.\n\nTransition models are often taken from existing literature or estimated on panel data. The data used to estimate these models must have at least two time periods.\nIt is common to extrapolate backwards, which is also known as backcasting, to evaluate microsimulation models against history. Backcasting can also add important longitudinal information to records like detailed earnings histories for calculating Social Security benefits.\n\n\n5.4.2 Example 5\nLet’s extrapolate our 2022 CPS data to 2023 and 2024 using transition models for number of children and family total income.\nThe transition model for number of children is very simple. 15% of families lose a child, 80% of families observe no change, and 5% of families gain one child.\n\n#' Extrapolate the number of children\n#'\n#' @param num_children A numeric for the number of children in time t\n#'\n#' @return A numeric for the number in time t + 1\n#'\nchildren_hazard &lt;- function(num_children) {\n  \n  change &lt;- sample(x = c(-1, 0, 1), size = 1, prob = c(0.15, 0.8, 0.05))\n  \n  pmax(num_children + change, 0)\n  \n}\n\nThe transition model for family income is very simple. The proportion change is drawn from a normal distribution with \\(\\mu = 0.02\\) and \\(\\sigma = 0.03\\).\n\n#' Extrapolate family income\n#'\n#' @param num_children A numeric for family income in time t\n#'\n#' @return A numeric for family income in time t + 1\n#'\nincome_hazard &lt;- function(family_income) {\n  \n  change &lt;- rnorm(n = 1, mean = 0.02, sd = 0.03)\n  \n  family_income + family_income * change\n  \n}\n\nFor simplicity, we combine both transition models into one function.\n\n#' Extrapolate the simple CPS\n#'\n#' @param data A tibble with nchild and ftotval\n#'\n#' @return A data frame in time t + 1\n#'\nextrapolate &lt;- function(data) {\n  \n  data |&gt;\n    mutate(\n      nchild = map_dbl(.x = nchild, .f = children_hazard),\n      ftotval = map_dbl(.x = ftotval, .f = income_hazard),\n      year = year + 1\n    )\n\n}\n\nFinally, we extrapolate.\n\n# make the stochastic results reproducible\nset.seed(20230812)\n\n# extrapolate using t to create t + 1\nasec2023 &lt;- extrapolate(asec)\nasec2024 &lt;- extrapolate(asec2023)\n\n# combine\nasec_extrapolated &lt;- bind_rows(\n  asec,\n  asec2023,\n  asec2024\n)\n\nThe benefit and income amounts in new_tax_credit() are not indexed for inflation. Let’s see how benefits change over time with the extrapolated data.\n\nasec_extrapolated |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    )\n  ) |&gt;\n  group_by(year) |&gt;\n  summarize(\n    mean_ftotval = mean(ftotval),\n    total_nchild = sum(nchild),\n    mean_benefit = mean(benefit)\n  )\n\n# A tibble: 3 × 4\n   year mean_ftotval total_nchild mean_benefit\n  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1  2022       97768.        46296         375.\n2  2023       99705.        45615         379.\n3  2024      101700.        44848         379.\n\n\nEven though incomes grew and the number of children declined, it turns out that enough families went from zero children to one child under the transition probability model for children that average benefits remained about the same."
  },
  {
    "objectID": "07_microsimulation.html#beyond-accounting-rules",
    "href": "07_microsimulation.html#beyond-accounting-rules",
    "title": "1  Microsimulation",
    "section": "5.5 Beyond Accounting Rules",
    "text": "5.5 Beyond Accounting Rules\n\n\n\n\n\n\nBehavioral Responses\n\n\n\nA behavioral response is an assumed reaction to changing circumstances in a microsimulation model.\nFor example, increasing Social Security benefits may crowd out retirement savings (Chetty et al. 2014).\n\n\nUntil now, we’ve only considered accounting rules within each time period. Accounting rules are appealing because they don’t require major assumptions, but they are typically insufficient. It is often necessary to consider how units will respond to a changing environment.\nBehavioral responses are often elasticities estimated with econometric analysis. Behavioral responses are also a huge source of assumptions and uncertainty for microsimulation."
  },
  {
    "objectID": "07_microsimulation.html#building-a-representative-population",
    "href": "07_microsimulation.html#building-a-representative-population",
    "title": "1  Microsimulation",
    "section": "5.6 Building a Representative Population",
    "text": "5.6 Building a Representative Population\nBuilding a starting sample and constructing data for estimating transition models is difficult. We briefly outline a few techniques of interest.\n\n5.6.1 Synthetic Starting Data\nWe’ve adopted a cross-sectional starting population. Some microsimulation models adopt a synthetic1 approach where every observation is simulated from birth.\n\n\n5.6.2 Data Linkage and Imputation\nMany microsimulation models need more variables than are included in any one source of information. For example, retirement models often need demographic information, longitudinal earnings information, and tax information. Many microsimulation models rely on data linkage and data imputation techniques to augment their starting data.\n\n\n\n\n\n\nData Linkage\n\n\n\nData linkage is the process of using distance-based rules or probabilistic models to connect an observation in one source of data to an observation in another source of data.\n\n\n\n\n\n\n\n\nData imputation\n\n\n\nData imputation is the process of using models to predict values where data is unobserved. The data could be missing because of nonresponse or because the information was not gathered in the data collection process.\n\n\n\n\n5.6.3 Validation\n\n\n\n\n\n\nValidation\n\n\n\nValidation is the process of reviewing results to determine their validity. Techniques include comparing statistics, visual comparisons, and statistical comparisons like the Kolmogorov-Smirnov test for the equivalence of two distributions.\n\n\nVisual and statistical validation are essential to evaluating the quality of a microsimulation model. If validation looks poor, then a modeler can redo other parts of the microsimulation workflow or they can reweight or align the data.\n\n\n5.6.4 Reweighting\n\n\n\n\n\n\nReweighting\n\n\n\nReweighting is the process of adjusting observation weights in a data set so aggregate weighted statistics from the data set hit specified targets.\n\n\nSuppose a well-regarded source of information says mean income is $50,000 but a microsimulation model estimates mean income of $45,000. We can use reweighting to plausibly adjust the weights in the microsimulation model so mean income is $50,000.\nTechniques include post-stratification and calibration. Kolenikov (2016) offers a good introduction.\n\n\n5.6.5 Alignment\n\n\n\n\n\n\nAlignment\n\n\n\nAlignment is the process of adjusting model coefficient or predicted values so output aligns with specified targets.\n\n\nSuppose a well-regarded source of information says mean income is $50,000 but a microsimulation model estimates mean income of $45,000. We can adjust the intercept in linear regression models or adjust predicted values so mean income is $50,000. Li and O’Donoghue (2014) offers a good introduction."
  },
  {
    "objectID": "07_microsimulation.html#assumptions",
    "href": "07_microsimulation.html#assumptions",
    "title": "1  Microsimulation",
    "section": "5.7 Assumptions",
    "text": "5.7 Assumptions\nMicrosimulation is only as useful as its assumptions. We will review a few key assumptions present in many microsimulation models.\n\n5.7.1 Open vs. Closed\n\n\n\n\n\n\nClosed Model\n\n\n\nA closed microsimulation model models the life cycle of all units in the model. For example, two existing units marry.\n\n\n\n\n\n\n\n\nOpen Model\n\n\n\nAn open microsimulation model allows for the on-demand generation of new, but mature, units in the model.\n\n\n\n\n5.7.2 Independence\nAnother important assumption deals with the relationship between units in the model. Should observations be treated as wholly independent or do they interact? For example, if someone takes a job is it more difficult for another individual to take the job?\nInteractions can be explicit or implicit.\n\n\n\n\n\n\nExplicit Interaction\n\n\n\nExplicit interaction allows the actions of one unit to affect other units during extrapolation or behavioral models.\nFor example, models of marriage markets may account for changing economic circumstances among potential matches.\n\n\n\n\n\n\n\n\nImplicit Interaction\n\n\n\nImplicit interaction allows the actions of one unit to affect other units in post-processing.\nFor example, reweighting and alignment techniques allow outcomes for one unit to affect other units through intercepts in models and new weights.\n\n\n\n\n5.7.3 Markov Assumption\n\n\n\n\n\n\nMarkov Assumption\n\n\n\nThe Markov assumptions states that the only factors affecting a transition from period \\(t - 1\\) to period \\(t\\) are observable in \\(t - 1\\).\nThe Markov assumption only considers memory or history to the extent that it is observable in period \\(t - 1\\). For example, \\(t - 2\\) may affect educational attainment or college savings in \\(t - 1\\), which can affect the transition from \\(t - 1\\) to \\(t\\), but \\(t - 2\\) will never be explicitly included."
  },
  {
    "objectID": "07_microsimulation.html#uncertainty",
    "href": "07_microsimulation.html#uncertainty",
    "title": "1  Microsimulation",
    "section": "5.8 Uncertainty",
    "text": "5.8 Uncertainty\n\n\n\n\n\n\nAletoric Uncertainty\n\n\n\nAleatoric uncertainty is uncertainty due to probabilistic randomness.\n\n\n\n\n\n\n\n\nEpistemic Uncertainty\n\n\n\nEpistemic uncertainty is uncertainty due to lack of knowledge of the underlying system.\n\n\nThe microsimulation field has a poor track record of quantifying uncertainty. Most estimates do not contain standard errors or even crude distributions of outcomes.\nMcClelland, Khitatrakun, and Lu (2020) explored adding confidence intervals to microsimulation models using normal approximations and bootstrapping methods2. They find that normal approximations work well in most cases unless policy changes affect a small number of returns.\nAcross microsimulations for five policy approaches, they estimate modest confidence intervals. This makes sense. First, microsimulation aggregates many units that have their own sources of uncertainty. This is different than earlier examples of Monte Carlo simulation that focused on one observation at a time. Aggregation reduces variance.3\nSecond, the authors only consider uncertainty because of sampling variation. This fails to capture aleatoric uncertainty from statistical matching, imputation, and projection. Furthermore, these methods fail to capture epistemic uncertainty. The confidence intervals, in other words, assume the model is correct.\nGiven these shortcomings, it is important to be clear about assumptions, transparent about implementations, and humble about conclusions. All useful models are wrong. The hope is to be as little wrong as possible."
  },
  {
    "objectID": "07_microsimulation.html#microsimulation-model-examples",
    "href": "07_microsimulation.html#microsimulation-model-examples",
    "title": "1  Microsimulation",
    "section": "5.9 Microsimulation Model Examples",
    "text": "5.9 Microsimulation Model Examples\nFirst, let’s outline a few characteristics of microsimulation models.\n\n\n\n\n\n\nCompiled Programming Languages\n\n\n\nCompiled programming languages have an explicit compiling step where code is converted to assembly language and ultimately binary code.\nC++, Fortran, and Java are examples of compiled programming languages.\n\n\n\n\n\n\n\n\nScripting Programming Languages\n\n\n\nScripting programming languages, also known as interpreted programming languages, do not have a compiling step.\nR, Python, and Julia are examples of scripting programming languages.\n\n\nAruoba and Fernndez-Villaverde (2018) benchmark several programming languages on the same computing task. Lower-level, compiled programming languages dominate higher-level, scripting programming languages like R and Python. Julia is the lone bright spot that blends usability and performance.\nMany microsimulation models are written in lower-level, compiled programming languages. Fortran may seem old, but there is a reason it has stuck around for microsimulation.\n\n\n\n\n\n\nGeneral Microsimulation Models\n\n\n\nGeneral microsimulation models contain a wide range of behaviors and population segments.\n\n\n\n\n\n\n\n\nSpecialized Microsimulation Models\n\n\n\nSpecialized microsimulation models focus on a limit set of behaviors or population segments. ~ Stats Canada\n\n\n\n5.9.1 Simulating the 2020 Census\n\nName: Simulating the 2020 Census\nAuthors: Diana Elliott, Steven Martin, Jessica Shakesprere, Jessica Kelly\nGeneral or specific: Specific\nLanguage: Python\nPurpose: The authors simulate various Decennial Census response factors and evaluate the distribution of responses to the Decennial Census.\n\n\n\n5.9.2 TPC Microsimulation in the Cloud\n\nName: TPC Microsimulation in the Cloud\nAuthors: The TPC microsimulation team, Jessica Kelly, Kyle Ueyama, Alyssa Harris\nGeneral or specific: Specific\nLanguage: Fortran\nPurpose: The authors take TPC’s tax microsimulation model and move it to the cloud. This allows TPC to reverse the typical microsimulation process. Instead of describing policies and observing the outcomes, they can describe desirable outcomes and then use grid search to back out policies that achieve those outcomes.\n\n\n\n5.9.3 Modeling Income in the Near Term (MINT)4\n\nName: Modeling Income in the Near Term (MINT)\nAuthors: Karen E. Smith and many other people\nGeneral or specific: General\nLanguage: SAS5\nPurpose: The Social Security Administration uses MINT to evaluate the distributional impact of various Social Security policy proposals.\n\n\n\n\n\nAruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A Comparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien Nielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and Crowd-Out in Retirement Savings Accounts: Evidence from Denmark*.” The Quarterly Journal of Economics 129 (3): 1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response Adjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary Alignment Methods in Microsimulation Models.” Journal of Artificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020. “Estimating Confidence Intervals in a Tax Microsimulation Model.” International Journal of Microsimulation 13 (2): 2–20. https://doi.org/10.34196/IJM.00216.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.” The Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528."
  },
  {
    "objectID": "07_microsimulation.html#footnotes",
    "href": "07_microsimulation.html#footnotes",
    "title": "1  Microsimulation",
    "section": "",
    "text": "Synthetic will carry many meanings this semester.↩︎\nTheir implementation of bootstrapping is clever and uses replicate weights to simplify computation and manage memory.↩︎\nThe standard error of the mean reduces at a rate of \\(\\sqrt{n}\\) as the sample size increases.↩︎\nMINT comically models income and many other variables for at least 75 years into the future.↩︎\nThe Dynamic Simulation of Income Model (DYNASIM) is a related to MINT and is written in Fortran.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A\nComparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien\nNielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and\nCrowd-Out in Retirement Savings Accounts: Evidence from\nDenmark*.” The Quarterly Journal of Economics 129 (3):\n1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response\nAdjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary\nAlignment Methods in Microsimulation Models.” Journal of\nArtificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020.\n“Estimating Confidence Intervals in a Tax Microsimulation\nModel.” International Journal of Microsimulation 13 (2):\n2–20. https://doi.org/10.34196/IJM.00216.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.”\nThe Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528."
  }
]