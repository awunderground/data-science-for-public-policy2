[
  {
    "objectID": "01_advanced-quarto.html#sec-review",
    "href": "01_advanced-quarto.html#sec-review",
    "title": "2  Advanced Quarto",
    "section": "2.1 Review",
    "text": "2.1 Review\n\n2.1.1 Motivation\nThere are many problems worth avoiding in an analysis:\n\nCopying-and-pasting, transposing, and manual repetition\nRunning code out-of-order\nMaintaining parallel documents like a script for analysis and a doc for narrative\nCode written for computers that is tough to parse by humans\n\nNot convinced? Maybe we just want to make cool stuff like websites, blogs, books, and slide decks.\nQuarto, a literate statistical programming framework for R, Python, and Julia helps us solve many of these problems. Quarto uses\n\nplain text files ending in .qmd that are similar to .R and .Rmd files\nlibrary(knitr)\npandoc1\n\nQuarto uses library(knitr) and pandoc to convert plain text .qmd documents into rich output documents like these class notes. The “Render” button appears in RStudio with a .qmd file is open in the editor window.\nClicking the “Render” button begins the process of rendering .qmd files.\n\n\n\n\n\n\n\n\n\nWhen the button is clicked, Quarto calls library(knitr) and renders .qmd (Quarto files) into .md (Markdown files), which Pandoc then converts into any specified output type. Quarto and library(knitr) don’t need to be explicitly loaded as the entire process is handled by clicking the “Render” button in RStudio.\n\n\n\n\n\n\n\n\n\nSource: Quarto website\nQuarto, library(knitr), and Pandoc are all installed with RStudio. You will need to install a LaTeX distribution to render PDFs. We recommend library(tinytex) as a LaTeX distribution (installation instructions).\n\n\n\n\n\n\nExercise 1\n\n\n\n\nClick the new script button in RStudio and add a “Quarto Document”.\nGive the document a name, an author, and ensure that HTML is selected.\nSave the document as “hello-quarto.qmd”.\nClick “Render”.\n\n\n\nQuarto has three main ingredients:\n\nYAML header\nMarkdown text\nCode chunks\n\n\n\n2.1.2 (1) YAML Header\nYAML stands for “yet another markup language”. The YAML header contains meta information about the document including output type, document settings, and parameters that can be passed to the document. The YAML header starts with --- and ends with ---.\nHere is the simplest YAML header for a PDF document:\n---\nformat: pdf\n---\nYAML headers can contain many output specific settings. This YAML header creates an HTML document with code folding and a floating table of contents:\n---\nformat: \n  html:\n    embed-resources: true\n    code-fold: true\n    toc: true\n---  \nParameters can be specified as follows\n---\nformat: pdf\nparams:\n  state: \"Virginia\"\n---\nNow state can be referred to anywhere in R code as params$state. Parameters are useful for a couple of reasons:\n\nWe can clearly change key values for a Quarto document in the YAML header.\nWe can create a template and programmatically iterate the template over a set of values with the quarto_render() function and library(purrr). This blog outlines the idea. The Mobility Metrics Data Tables and SLFI State Fiscal Briefs are key examples of this workflow.\n\n\n\n\n\n\n\nWarning\n\n\n\nUnlike R Markdown, images and other content are not embedded in .html from Quarto by default. Be sure to include embed-resources: true in YAML headers to embed content and make documents easier to share.\nSuppose we embed an image called image.png in a Quarto document called example.qmd, which, when rendered, creates example.html. If we don’t include embed-resources: true, then we will need to share image.png and example.html to see the embedded image. This is also true for other files like .css.\n\n\n\n\n2.1.3 (2) Markdown text\nMarkdown is a shortcut for HyperText Markup Language (HTML). Essentially, simple meta characters corresponding to formatting are added to plain text.\nTitles and subtitltes\n------------------------------------------------------------\n\n# Title 1\n\n## Title 2\n\n### Title 3\n\n\nText formatting \n------------------------------------------------------------\n\n*italic*  \n\n**bold**   \n\n`code`\n\nLists\n------------------------------------------------------------\n\n* Bulleted list item 1\n* Item 2\n  * Item 2a\n  * Item 2b\n\n1. Item 1\n2. Item 2\n\nLinks and images\n------------------------------------------------------------\n\n[text](http://link.com)\n\n![Penguins](images/penguins.png)\n\n\n2.1.4 (3) Code chunks\n\n\n\n\n\nMore frequently, code is added in code chunks:\n\n```{r}\n2 + 2\n```\n\n[1] 4\n\n\nThe first argument inline or in a code chunk is the language engine. Most commonly, this will just be a lower case r. knitr allows for many different language engines:\n\nR\nJulia\nPython\nSQL\nBash\nRcpp\nStan\nJavascript\nCSS\n\nQuarto has a rich set of options that go inside of the chunks and control the behavior of Quarto.\n\n```{r}\n#| label: important-calculation\n#| eval: false\n\n2 + 2\n```\n\nIn this case, eval makes the code not run. Other chunk-specific settings can be added inside the brackets. Here2 are the most important options:\n\n\n\nOption\nEffect\n\n\n\n\necho: false\nHides code in output\n\n\neval: false\nTurns off evaluation\n\n\noutput: false\nHides code output\n\n\nwarning: false\nTurns off warnings\n\n\nmessage: false\nTurns off messages\n\n\nfig-height: 8\nChanges figure width in inches3\n\n\nfig-width: 8\nChanges figure height in inches4\n\n\n\nDefault settings for the entire document can be changed in the YAML header with the execute option:\nexecute:\n  warning: false\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAdd date: today to your YAML header after title. This will update every time the document is rendered.\nCopy the Markdown table from this table generator and add it to your .qmd document.\nCreate a scatter plot of the cars data with library(ggplot2). Adjust the figure width and height using options within the chunk.\nClick “Render”.\n\n\n\n\n\n2.1.5 Organizing a Quarto Document\nIt is important to clearly organize a Quarto document and the constellation of files that typically support an analysis.\n\nAlways use .Rproj files.\nUse sub-directories to sort images, .css, data.\n\nLater, we will learn how to use library(here) to effectively organize sub-directories."
  },
  {
    "objectID": "01_advanced-quarto.html#math-notation",
    "href": "01_advanced-quarto.html#math-notation",
    "title": "2  Advanced Quarto",
    "section": "2.2 Math Notation",
    "text": "2.2 Math Notation\nThis course uses probability and statistics. Occasionally, we want to easily communicate with mathematical notation. For example, it may be convenient to type that \\(X\\) is a random variable that follows a standard normal distribution (mean = 0 and standard deviation = 1).\n\\[X \\sim N(\\mu = 0, \\sigma = 1)\\]\n\n2.2.1 Math Mode\nUse $ to start and stop in-line math notation and $$ to start multi-line math notation. Math notation uses LaTeX’s syntax for mathematical notation.\nHere’s an example with in-line math:\nConsider a binomially distributed random variable, $X \\sim binom(n, p)$. \nConsider a binomially distributed random variable, \\(X \\sim binom(n, p)\\).\nHere’s an example with a chunk of math:\n$$\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n$${#eq-binomial}\n\\[\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n\\tag{2.1}\\]\n\n\n2.2.2 Important Syntax\nMath mode recognizes basic math symbols available on your keyboard including +, -, *, /, &gt;, &lt;, (, and ).\nMath mode contains all greek letters. For example, \\alpha (\\(\\alpha\\)) and \\beta (\\(\\beta\\)).\n\n\nTable 2.1: My Caption\n\n\nLaTeX\nSymbol\n\n\n\n\n\\alpha\n\\(\\alpha\\)\n\n\n\\beta\n\\(\\beta\\)\n\n\n\\gamma\n\\(\\gamma\\)\n\n\n\\Delta\n\\(\\Delta\\)\n\n\n\\epsilon\n\\(\\epsilon\\)\n\n\n\\theta\n\\(\\theta\\)\n\n\n\\pi\n\\(\\pi\\)\n\n\n\\sigma\n\\(\\sigma\\)\n\n\n\\chi\n\\(\\chi\\)\n\n\n\n\nMath mode also recognizes \\(\\log(x)\\) (\\log(x)) and \\(\\sqrt{x}\\) (\\sqrt{x}).\nSuperscripts (^) are important for exponentiation and subscripts (_) are important for adding indices. y = x ^ 2 renders as \\(y = x ^ 2\\) and x_1, x_2, x_3 renders as \\(x_1, x_2, x_3\\). Brackets are useful for multi-character superscripts and subscripts like \\(s_{11}\\) (s_{11}).\nIt is useful to add symbols to letters. For example, \\bar{x} is useful for sample means (\\(\\bar{x}\\)), \\hat{y} is useful for predicted values (\\(\\hat{y}\\)), and \\vec{\\beta} is useful for vectors of coefficients (\\(\\vec{\\beta}\\)).\nMath mode supports fractions with \\frac{x}{y} (\\(\\frac{x}{y}\\)), big parentheses with \\left(\\right) (\\(\\left(\\right)\\)), and brackets with \\left[\\right] (\\(\\left[\\right]\\)).\nMath mode has a symbol for summation. Let’s combine it with bars, fractions, subscripts, and superscipts to show sample mean \\bar{x} = \\frac{1}{n}\\sum_i^n x_i, which looks like \\(\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\).\n\\sim is how to add the tilde for distributed as. For example, X \\sim N(\\mu = 0, \\sigma = 1) shows the normal distribution \\(X \\sim N(\\mu = 0, \\sigma = 1)\\).\nMatrices are are a little bit more work in math mode. Consider the follow variance-covariance matrix:\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\[\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\]\nThis guide provides and exhaustive look at math options in Quarto.\n\n\n\n\n\n\nWarning\n\n\n\nMath mode is finicky! Small errors like mismatched parentheses or superscript and subscript errors will cause Quarto documents to fail to render. Write math carefully and render early and often.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse math mode to type out the equation for root mean square error (RMSE).\nDo you divide by n or n - 1?"
  },
  {
    "objectID": "01_advanced-quarto.html#cross-references",
    "href": "01_advanced-quarto.html#cross-references",
    "title": "2  Advanced Quarto",
    "section": "2.3 Cross References",
    "text": "2.3 Cross References\nCross references are useful for organizing documents that include sections, figures, tables, and equations. Cross references create hyperlinks within documents that jump to the locations of these elements. Linking sections, figures, tables, or equations helps readers navigate the document.\nCross references also automatically number the referenced elements. This means that if there are two tables (ie. Table 1 and Table 2) and a table is added between the two tables, all of the table numbers and references to the tables will automatically update.\nCross references require two bits of code within a Quarto document:\n\nA label associated with the section, figure, table, or equation.\nA reference to the labelled section, figure, table, or equation.\n\nLabels are written in brackets or as arguments in code chunks, and begin with the the type object being linked. References begin with @ followed by the label of object being linked.\n\n2.3.1 Sections\nLinking sections helps readers navigate between sections. Use brackets to label sections after headers and always begin labels with sec-. Then you can reference that section with @sec-.\n## Review {#sec-review}\n\nSee @sec-review if you are totally lost.\nThe cross references shows up like this: See Section 2.1 if you are totally lost.\nIt can be helpful to turn on section numbering with number-sections: true in the YAML header. Additionally, Markdown has a native method for linking between sections.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nAdd a few section headers to your Quarto document.\nAdd a cross reference to one of the section headers.\n\n\n\n\n\n2.3.2 Figures\n\n\n\nFigure 2.1: Penguins\n\n\nWe can reference figures like Figure 2.1 with @fig-penguins.\n\n\n2.3.3 Tables\nWe can link to tables in our documents. For example, we can link to the greek table with @tbl-greek Table 2.1.\n\n\n2.3.4 Equations\nWe can link to equations in our documents. For example, we can link to the binomial distribution earlier with @eq-binomial Equation 5.4.\n\n\n\n\n\n\nExercise 5\n\n\n\n\nAdd a cross reference to your RMSE equation from earlier."
  },
  {
    "objectID": "01_advanced-quarto.html#citations",
    "href": "01_advanced-quarto.html#citations",
    "title": "2  Advanced Quarto",
    "section": "2.4 Citations",
    "text": "2.4 Citations\n\n2.4.1 Zotero\nZotero is a free and open-source software for organizing research and managing citations.\n\n\n\n\n\n\nDigital Object Identifier (DOI)\n\n\n\nDOIs are persistent identifiers that uniquely identify objects including many academic papers. For example, 10.1198/jcgs.2009.07098 identifies “A Layered Grammar of Graphics” by Hadley Wickham.\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\nInstall Zotero.\nFind the DOI for “Tidy Data” by Hadley Wickham.\nClick the magic wand in Zotero and paste the DOI.\n\n\n\n\n\n\n\n\n\n\n\nReview the new entry in Zotero.\n\n\n\n\n\n2.4.2 Zotero Integration\nZotero has a powerful integration with Quarto. In practice, it’s one click to add a DOI to Zotero and then one click to add a citation to Quarto.\nRStudio automatically adds My Library from Zotero. Simply switch to the Visual Editor (top left in RStudio), click “Insert”, and click “Citation”. This will open a prompt to insert a citation into the Quarto document.\nThe citation is automatically added with parentheses to go at the end of sentences. Delete the square brackets to convert the citation to an in-line citation.\nInserting the citation automatically adds the citation to the references section. Deleting the reference automatically deletes the citation from the references section.\nZotero Groups are useful for sharing citations and Zotero Group Libraries need to be added to RStudio. To set this up:\nTo set this up, in RStudio:\n\nGo to Tools and select “Global Options”\nSelect “RMarkdown” and then click “Citations”\nFor “Use Libraries” choose “Selected Libraries”\nSelect the group libraries to add\n\n\n\n\n\n\n\nExercise 7\n\n\n\n\nCite “Tidy Data” by Hadley Wickham in your Quarto document.\nClick “Render”"
  },
  {
    "objectID": "01_advanced-quarto.html#more-resources",
    "href": "01_advanced-quarto.html#more-resources",
    "title": "2  Advanced Quarto",
    "section": "2.5 More Resources",
    "text": "2.5 More Resources\n\nQuarto Guide\nIterating fact sheets and web pages with Quarto"
  },
  {
    "objectID": "01_advanced-quarto.html#footnotes",
    "href": "01_advanced-quarto.html#footnotes",
    "title": "2  Advanced Quarto",
    "section": "",
    "text": "Pandoc is free software that converts documents between markup formats. For example, Pandoc can convert files to and from markdown, LaTeX, jupyter notebook (ipynb), and Microsoft Word (.docx) formats, among many others. You can see a comprehensive list of files Pandoc can convert on their About Page.↩︎\nThis table was typed as Markdown code. But sometimes it is easier to use a code chunk to create and print a table. Pipe any data frame into knitr::kable() to create a table that will be formatted in the output of a rendered Quarto document.↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more.↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more.↩︎"
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#sec-review6a",
    "href": "06_advanced-unsupervised-ml.html#sec-review6a",
    "title": "10  Mixture Distributions and Mixture Modeling",
    "section": "10.1 Review 1",
    "text": "10.1 Review 1\nWe’ll start with a review of multivariate normal distributions. In particular, this exercise demonstrates the impact of the variance-covariance matrix on the shape of multivariate normal distributions.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nLoad library(mvtnorm).\nCopy and paste the following code. This code will not obviously not run as is. We will add tibbles to independent, sigma1 and sigma2 in the steps below.\n\n\nbind_rows(\n  independent = ,\n  var_covar1 = ,\n  var_covar2 = ,\n  .id = \"source\"\n)\n\n\nCreate a tibble with V1 and V2. For both variables, use rnorm() to sample 1,000 observations from a standard normal distribution. Add the results to independent.\nUsing the following variance-covariance matrix, sample 1,000 observations from a multivariate-normal distribution. Add the results for sigma1 and use as_tibble().\n\n\nsigma1 &lt;- matrix(\n  c(1, 0,\n    0, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\n\nUsing the following variance-covariance matrix, sample 1,000 observations from a multivariate-normal distribution. Add the results for sigma2 and use as_tibble().\n\n\nsigma2 &lt;- matrix(\n  c(1, 0.8,\n    0.8, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\n\nCreate a scatter plot with V1 on the x-axis and V2 on the y-axis. Facet based on source.\n\n\n\n\nlibrary(mvtnorm)\n\nsigma1 &lt;- matrix(\n  c(1, 0,\n    0, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\nsigma2 &lt;- matrix(\n  c(1, 0.8,\n    0.8, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\nbind_rows(\n  independent = tibble(\n    V1 = rnorm(n = 1000),\n    V2 = rnorm(n = 1000)\n  ),\n  sigma1 = rmvnorm(\n    n = 1000, \n    sigma = sigma1\n  ) |&gt;\n    as_tibble(),\n  sigma2 = rmvnorm(\n    n = 1000, \n    sigma = sigma2\n  ) |&gt;\n    as_tibble(),\n  .id = \"source\"\n) |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point() +\n  facet_wrap(~ source)"
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#a-new-type-of-random-variable",
    "href": "06_advanced-unsupervised-ml.html#a-new-type-of-random-variable",
    "title": "10  Mixture Distributions and Mixture Modeling",
    "section": "10.2 A New Type of Random Variable",
    "text": "10.2 A New Type of Random Variable\nWe learned about common univariate and multivariate distributions. For each of the distributions, there are well-defined and straightforward ways to sample values from the distribution. We can also manipulate these distributions to calculate probabilities.\nThe real world is complicated, and we will quickly come across data where we struggle to find a common probability distributions.\nFigure Figure 10.1 shows a relative frequency histogram for the duration of eruptions at Old Faithful in Yellowstone National Park.\n\n# faithful is a data set built into R\nfaithful |&gt;\n  ggplot(aes(eruptions, y = after_stat(density))) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 10.1: Distribution of waiting times between eruptions at the Old Faithful geyser in Yellowstone National Park.\n\n\n\n\nThis distribution looks very complicated. But what if we break this distribution into pieces? In this case, what if we think of the distribution as a combination of two normal distributions?\n\n\nCode\n# show geyser as two normal distribution\nlibrary(mclust)\n\ngmm_geyser &lt;- Mclust(\n  data = dplyr::select(faithful, eruptions), \n  G = 2\n)\n\n\nbind_cols(\n  faithful,\n  cluster = gmm_geyser$classification\n) |&gt;\n  ggplot(aes(eruptions, y = after_stat(density), \n             fill = factor(cluster))) +\n  geom_histogram() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nLatent Variable\n\n\n\nA latent variable is a variable that isn’t directly observed but can be inferred through other variables and modeling. Sometimes the latent variable is meaningful but unobserved. Sometimes it isn’t meaningful.\nLatent variables are sometimes called hidden variables.\n\n\nBreaking complex problems into smaller pieces is good. These latent variables will allow us to do some cools things:\n\nSimply express complicated probability distributions\nMake inferences about complex populations\nCluster data\n\nIn this set of notes, we’ll use latent variables to\n\nConstruct mixture distributions\nCluster data\n\nLet’s consider a “data generation story” different than anything we considered in Chapter 5. Instead of sampling directly from one known probability distribution, we will sample in two stages (Hastie, Tibshirani, and Friedman 2009).\n\nSample from a discrete probability distribution with \\(k\\) unique values (i.e. Bernoulli distribution when \\(k = 2\\) and categorical distribution when \\(k &gt; 2\\)).\nSample from one of \\(k\\) different distributions conditional on the outcome of step 1.\n\nThis new sampling procedure aligns closely with the idea of hierarchical sampling and hierarchical models. It is also sometimes called ancestral sampling (Bishop 2006, 430).\nThis two-step approach dramatically increases the types of distributions at our disposal because we are no longer limited to individual common univariate distributions like a single normal distribution or a single uniform distribution. The two-step approach is also the foundation of two related tools:\n\nMixture distributions: Distributions expressed as the linear combination of other distributions. Mixture distributions can be very complicated distributions expressed in terms of simple distributions with known properties.\nMixture modeling: Statistical inference about sub-populations made only with pooled data without labels for the sub populations.\n\nWith mixture distributions, we care about the overall distribution and don’t care about the latent variables.\nWith mixture modeling, we use the overall distribution to learn about the latent variables/sub populations/clusters in the data."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#mixture-distributions",
    "href": "06_advanced-unsupervised-ml.html#mixture-distributions",
    "title": "10  Mixture Distributions and Mixture Modeling",
    "section": "10.3 Mixture Distributions",
    "text": "10.3 Mixture Distributions\n\n\n\n\n\n\nMixture Distribution\n\n\n\nA mixture distribution is a probabilistic model that is a linear combination of common probability distributions.\nA discrete mixture distribution can be expressed as\n\\[\np_{mixture}(x) = \\sum_{k = 1}^K \\pi_kp(x)\n\\]\nwhere \\(K\\) is the number of mixtures and \\(\\pi_k\\) is the weight of each PMF included in the mixture distribution.\nA continuous mixture distribution can be expressed as\n\\[\np_{mixture}(x) = \\sum_{k = 1}^K \\pi_kf(x)\n\\]\nwhere \\(K\\) is the number of mixtures and \\(\\pi_k\\) is the weight of each PDF included in the mixture distribution.\n\n\n\n10.3.1 Example 1\nLet’s consider a concrete example with a Bernoulli distribution and two normal distributions.\n\nSample \\(X \\sim Bern(p = 0.25)\\)\nSample from \\(Y \\sim N(\\mu = 0, \\sigma = 2)\\) if \\(X = 0\\) and \\(Y \\sim (\\mu = 4, \\sigma = 2)\\) if \\(X = 1\\).\n\nNow, let’s sample from a Bernoulli distribution and then sample from one of two normal distributions using R code.\n\ngenerate_data &lt;- function(n) {\n  \n  step1 &lt;- sample(x = c(0, 1), size = n, replace = TRUE, prob = c(0.75, 0.25))\n  \n  step1 &lt;- sort(step1)\n  \n  step2 &lt;- c(\n    rnorm(n = sum(step1 == 0), mean = 0, sd = 2),\n    rnorm(n = sum(step1 == 1), mean = 5, sd = 1)\n  )\n  \n  tibble::tibble(\n    x = step1,\n    y = step2\n  )\n\n}\n\nset.seed(1)\n\ngenerate_data(n = 1000) |&gt;\n  ggplot(aes(x = y, y = after_stat(density))) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis marginal distribution looks complex but the process of creating the marginal distribution is simple.\nIn fact, consider this quote from Bishop (2006) (Page 111):\n\nBy using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.\n\n\n\n\n\n\n\nComponent\n\n\n\nA component is each common probability distribution that is combined to create a mixture distribution. For example, a mixture of two Gaussian distributions has two components.\n\n\n\n\n\n\n\n\nMixing Coefficient\n\n\n\nA mixing coefficient is the probability associated with a component with a component in a mixture distribution. Mixing coefficients must sum to 1.\nWe’ll use \\(\\pi_k\\) for population mixing coefficients and \\(p_k\\) for sample mixing coefficients. Mixing coefficients are also called mixing weights and mixing probabilities.\n\n\nMixture distributions are often overparameterized, which means they have an excessive number of parameters. For a univariate mixture of normals with \\(k\\) components, we have \\(k\\) means, \\(k\\) standard deviations, and \\(k\\) mixing coefficients.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nSample 1,000 observations from a mixture of three normal distributions with the following parameters:\n\n\n\\(p_1 = p_2 = p_3\\)\n\\(\\mu_1 = -3\\), \\(\\mu_2 = 0\\), \\(\\mu_3 = 3\\)\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3 = 1\\)\n\n\nCreate a relative frequency histogram of the values.\n\n\n\n\n\n10.3.2 Example 2\nSuppose we used statistical inference to infer some parameters for the geysers example above. We will describe how to estimate these paramaters later.\n\n\\(p_1 =\\) 0.3485696 and \\(p_2 =\\) 0.6514304\n\\(\\bar{x_1} =\\) 2.0189927 and \\(\\bar{x_2} =\\) 4.2737083\n\\(s_1 =\\) 0.2362355and \\(s_2 =\\) 0.4365146\n\nThe mixture density is\n\\[\nf_{mixture}(x) = p_1f(x|\\mu = \\bar{x_1}, \\sigma = s_1) + p_2f(x|\\mu = \\bar{x_2},\\sigma=s_2)\n\\tag{10.1}\\]\n\ngeyser_density &lt;- function(x, model) {\n  \n  probs &lt;- model$parameters$pro\n  \n  d1 &lt;- dnorm(\n    x, \n    mean =  model$parameters$mean[1], \n    sd = sqrt(model$parameters$variance$sigmasq[1])\n  )\n  \n  d2 &lt;- dnorm(\n    x, \n    mean =  model$parameters$mean[2], \n    sd = sqrt(model$parameters$variance$sigmasq[2])\n  )\n  \n  probs[1] * d1 + probs[2] * d2\n  \n}\n\nmm &lt;- tibble(\n  x = seq(0, 5, 0.01),\n  f_x = map_dbl(x, geyser_density, model = gmm_geyser)\n) \n\nggplot() +\n  geom_histogram(data = faithful, mapping = aes(x = eruptions, y = after_stat(density))) +\n  geom_line(data = mm, mapping = aes(x, f_x), color = \"red\") + \n  labs(\n    title = \"\",\n    subtitles = \"Observed data in black, inferred distribution in red\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#sec-review6b",
    "href": "06_advanced-unsupervised-ml.html#sec-review6b",
    "title": "10  Mixture Distributions and Mixture Modeling",
    "section": "10.4 Review #2",
    "text": "10.4 Review #2\n\n10.4.1 Multivariate Normal Distribution\nThe multivariate normal distribution is a higher-dimensional version of the univariate normal distribution. The MVN distribution has a vector of means of length \\(k\\) and a \\(k\\)-by-\\(k\\) variance-covariance matrix.\nWe show that a random vector is multivariate normally distributed with\n\\[\n\\vec{X} \\sim \\mathcal{N}(\\vec\\mu, \\boldsymbol\\Sigma)\n\\tag{10.2}\\]\nThe PDF of a multivariate normally distributed random variable is\n\\[\nf(x) = (2\\pi)^{-k/2}det(\\boldsymbol\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\vec{x} - \\vec\\mu)^T\\boldsymbol\\Sigma^{-1}(\\vec{x} - \\vec\\mu)\\right)\n\\tag{10.3}\\]\n\n\n10.4.2 K-Means Clustering\nK-Means Clustering is a heuristic-based approach to finding latent groups in data. The algorithm assigns each observation to one and only one group through a two step iteration that minimizes the Euclidean distance between observations and centroids for each group.\n\nSetupStep 1Step 2Step 3Step 4Step 5\n\n\nConsider the following data set.\n\n\nCode\ndata &lt;- tibble(x = c(1, 2, 1, 4, 7, 10, 8),\n               y = c(5, 4, 4, 3, 7, 8, 5))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 1: Randomly place K centroids in your n-dimensional vector space\n\n\nCode\ncentroids &lt;- tibble(x = c(2, 5),\n                  y = c(5, 5),\n                  cluster = c(\"a\", \"b\"))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 2: Calculate the nearest centroid for each point using a distance measure\n\n\nCode\ncentroids &lt;- tibble(x = c(2, 5),\n                  y = c(5, 5),\n                  cluster = c(\"a\", \"b\"))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  geom_line(aes(x = c(4, 2), y = c(3, 5)), linetype = \"dashed\") +  \n  geom_line(aes(x = c(4, 5), y = c(3, 5)), linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 3: Assign each point to the nearest centroid\n\n\nCode\ndata$cluster &lt;- c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\")\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 4: Recalculate the position of the centroids based on the means of the assigned points\n\n\nCode\ncentroids2 &lt;- data %&gt;%\n  group_by(cluster) %&gt;%\n  summarize(x = mean(x), y = mean(y))\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids, aes(x, y), size = 4, alpha = 0.25) +\n  geom_point(data = centroids2, aes(x, y, color = cluster), size = 4) +  \n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 5: Repeat steps 2-4 until no points change cluster assignments\n\n\nCode\ndata$cluster &lt;- c(\"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\")\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids2, aes(x, y, color = cluster), size = 4) +  \n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse library(tidyclust) to cluster the faithful data into three clusters."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#mixture-modelingmodel-based-clustering",
    "href": "06_advanced-unsupervised-ml.html#mixture-modelingmodel-based-clustering",
    "title": "10  Mixture Distributions and Mixture Modeling",
    "section": "10.5 Mixture Modeling/Model-Based Clustering",
    "text": "10.5 Mixture Modeling/Model-Based Clustering\nUntil now, we’ve assumed that we’ve known all parameters when working with mixture distributions. What if we want to learn these parameters/make inferences about these parameters?\nThe process of making inferences about latent groups is related to K-Means Clustering. While K-Means Clustering is heuristic based, mixture modeling formalize the process of making inferences about latent groups using probability models. Gaussian mixture models (GMM) are a popular mixture model.\n\n\n\n\n\n\nMixture Modeling\n\n\n\nMixture modeling is the process of making inferences about sub populations using data that contain sub population but no labels for the sub populations."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#gaussian-mixture-modeling-gmm",
    "href": "06_advanced-unsupervised-ml.html#gaussian-mixture-modeling-gmm",
    "title": "10  Mixture Distributions and Mixture Modeling",
    "section": "10.6 Gaussian Mixture Modeling (GMM)",
    "text": "10.6 Gaussian Mixture Modeling (GMM)\n\n\n\n\n\n\nGaussian Mixture Modeling (GMM)\n\n\n\nGaussian mixture modeling (GMM) is mixture modeling that uses normal and multivariate normal distributions.\n\n\n\n\n\n\n\n\nHard Assignment\n\n\n\nHard assignment assigns an observation in a clustering model to one and only one group.\n\n\n\n\n\n\n\n\nSoft Assignment\n\n\n\nSoft assignment assigns an observation in a clustering model to all groups with varying weights or probabilities.\n\n\n\n\n\n\n\n\nResponsibilities\n\n\n\nSoft assignments are quantified with responsibilities. Responsibilities are the probability that a given observation belongs to a given group. The soft assignments for an observation sum to 1.\nWe quantified responsibilities with \\(\\pi_k\\) for mixture distributions. Responsibilities are parameters we will infer during mixture modeling.\n\n\nThere are two main differences between K-Means Clustering and GMM.\n\nInstead of calculating Euclidean distance from each observation to each group centroid, we use multivariate normal distributions to calculate the probability that an observation belongs to each group.\n\nObservations close to the means of a mixture will have a high relative probability of belonging to that mixture.\nObservations far from the means of a mixture will have a low relative probability of belonging to that mixture.\n\nInstead of simply updating \\(k\\) group centroids, we must update \\(k\\) multivariate normal distributions. This requires calculating a vector of means and a variance-covariance matrix for each of the \\(k\\) groups.\n\n\n10.6.1 Example 3\nThe parameters in example 2 were estimated using GMM. Let’s repeat a similar exercise with the faithful using eruptions and waiting instead of just eruptions. We’ll assume there are three groups.\n\n# fit GMM\ngmm2_geyser &lt;- Mclust(faithful, G = 3)\n\nLet’s plot the multivariate normal distributions. Figure 10.2 shows the centroids (stars) and shapes (ellipses) of the distributions in black. The colors represent hard assignments to groups and the size of the points represent the uncertainty of the assignments with larger points having more uncertainty.\n\n# plot fitted model\nplot(gmm2_geyser, what = \"uncertainty\")\n\n\n\n\nFigure 10.2: Uncertainty plot from a GMM\n\n\n\n\nWe can also summarize the model with library(broom).\n\nlibrary(broom)\n\naugment(gmm2_geyser)\n\n# A tibble: 272 × 4\n   eruptions waiting .class .uncertainty\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1      3.6       79 1          2.82e- 2\n 2      1.8       54 2          8.60e-13\n 3      3.33      74 1          3.26e- 3\n 4      2.28      62 2          3.14e- 7\n 5      4.53      85 3          1.17e- 2\n 6      2.88      55 2          3.09e- 3\n 7      4.7       88 3          2.99e- 3\n 8      3.6       85 1          2.39e- 2\n 9      1.95      51 2          5.23e-12\n10      4.35      85 3          5.52e- 2\n# ℹ 262 more rows\n\ntidy(gmm2_geyser)\n\n# A tibble: 3 × 5\n  component  size proportion mean.eruptions mean.waiting\n      &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1         1    40      0.166           3.79         77.5\n2         2    97      0.356           2.04         54.5\n3         3   135      0.478           4.46         80.8\n\nglance(gmm2_geyser)\n\n# A tibble: 1 × 7\n  model     G    BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 EEE       3 -2314. -1126.    11     NA   272\n\n\n\n\n10.6.2 mclust\nThe previous example uses library(mclust)1 and library(broom).\nMclust() is the main function for fitting Gaussian Mixture Models. The function contains several different types of models for the variances of the multivariate normal distributions. The defaults are sensible. G is the number of groups. If G isn’t specified, then Mclust() will try 1:9 and pick the G with the lowest BIC (defined below).\nplot() with what = \"uncertainty\" creates a very useful data visualization for seeing the multivariate normal distributions and classifications for low-dimensional GMM.\nglance(), tidy(), and augment() from library(broom) return important information about the assignments, groups, and model diagnostics.\n\n\n10.6.3 Estimation\nSuppose we have \\(n\\) observations, \\(k\\) groups, and \\(p\\) variables. A single GMM will have\n\nan \\(n\\) by \\(k\\) matrix of responsibilities\n\\(k\\) vectors of means of length \\(p\\)\n\\(k\\) \\(p\\) by \\(p\\) variance-covariance matrices\n\nWe want the maximum likelihood estimates for all of the parameters in the model. For technical reasons, it is very difficult to get these estimates using popular methods like stochastic gradient descent.\nInstead, we will use expectations maximization (EM) to find the parameters. We also used EM for K-Means clustering.\n\n\n\n\n\n\n\nRandomly initialize all of the parameters. Calculate the log-likelihood.\nE-Step: Update the responsibilities assuming the means and variance-covariance matrices are known.\nM-Step: Estimate new means and variance-covariance matrices assuming the responsibilities are known. The means and variance-covariance matrices are calculated using weighted MLE where the responsibilities are the weights.\nCalculate the log-likelihood. Go back to step 2 if the log-likelihood improves by at least as much as the stopping threshold.\n\n\n\n\nThis algorithm is computationally efficient, but it is possible for it to find a local maximum log-likelihood without finding the global maximum log-likelihood.\nFor a more mathematical description of this process, see Elements of Statistical Learning Section 6.8 (Hastie, Tibshirani, and Friedman 2009). A highly descriptive comparison to kmeans (with Python code) can be seen here.\n\n\n10.6.4 Example 4\nLet’s consider a policy-relevant example using data from the Small Area Health Insurance Estimates (SAHIE) Program.\nFirst, we pull the 2016 county-level estimates of the uninsured rate. We label a state as an expansion state if it expanded data before 2015-01-01. We use this date with 2016 data because of policy lags.\n\nlibrary(censusapi)\n\nsahie &lt;- getCensus(\n  name = \"timeseries/healthins/sahie\",\n  vars = c(\"GEOID\", \"PCTUI_PT\"),\n  region = \"county:*\",\n  time = 2016\n) |&gt;\n  as_tibble()\n\nNext, we pull data from the Kaiser Family Foundation about the expansion dates of Medicaid under the Patient Protection and Affordable Care Act.\n\nstates &lt;- tribble(\n  ~state, ~state_fips, ~implementation_date,\n  \"Alabama\", \"01\", NA,\n  \"Alaska\", \"02\", \"2015-09-15\",\n  \"Arizona\", \"04\", \"2014-01-01\",\n  \"Arkansas\", \"05\", \"2014-01-01\",\n  \"California\", \"06\", \"2014-01-01\",\n  \"Colorado\", \"08\", \"2014-01-01\",\n  \"Connecticut\", \"09\", \"2014-01-01\",\n  \"Delaware\", \"10\", \"2014-01-01\",\n  \"District of Columbia\", \"11\", \"2014-01-01\",\n  \"Florida\", \"12\", NA,\n  \"Georgia\", \"13\", NA,\n  \"Hawaii\", \"15\", \"2014-01-01\",\n  \"Idaho\", \"16\", \"2020-01-01\",\n  \"Illinois\", \"17\", \"2014-01-01\",\n  \"Indiana\", \"18\", \"2015-02-01\",\n  \"Iowa\", \"19\", \"2014-01-01\",\n  \"Kansas\", \"20\", NA,\n  \"Kentucky\", \"21\", \"2014-01-01\", \n  \"Louisiana\", \"22\", \"2016-07-01\",\n  \"Maine\", \"23\", \"2018-07-02\",\n  \"Maryland\", \"24\", \"2014-01-01\",\n  \"Massachusetts\", \"25\", \"2014-01-01\",\n  \"Michigan\", \"26\", \"2014-04-01\",\n  \"Minnesota\", \"27\", \"2014-01-01\",\n  \"Mississippi\", \"28\", NA,\n  \"Missouri\", \"29\", \"2021-07-01\",\n  \"Montana\", \"30\", \"2016-01-01\",\n  \"Nebraska\", \"31\", \"2020-10-01\",\n  \"Nevada\", \"32\", \"2014-01-01\", \n  \"New Hampshire\", \"33\", \"2014-08-15\",\n  \"New Jersey\", \"34\", \"2014-01-01\",\n  \"New Mexico\", \"35\", \"2014-01-01\",\n  \"New York\", \"36\", \"2014-01-01\", \n  \"North Carolina\", \"37\", NA,\n  \"North Dakota\", \"38\", \"2014-01-01\", \n  \"Ohio\", \"39\", \"2014-01-01\",\n  \"Oklahoma\", \"40\", \"2021-07-01\", \n  \"Oregon\", \"41\", \"2014-01-01\", \n  \"Pennsylvania\", \"42\", \"2015-01-01\", \n  \"Rhode Island\", \"44\", \"2014-01-01\", \n  \"South Carolina\", \"45\", NA,\n  \"South Dakota\", \"46\", \"2023-07-01\", \n  \"Tennessee\", \"47\", NA,\n  \"Texas\", \"48\", NA,\n  \"Utah\", \"49\", \"2020-01-01\",\n  \"Vermont\", \"50\", \"2014-01-01\",\n  \"Virginia\", \"51\", \"2019-01-01\", \n  \"Washington\", \"53\", \"2014-01-01\",\n  \"West Virginia\", \"54\", \"2014-01-01\",\n  \"Wisconsin\", \"55\", NA,\n  \"Wyoming\", \"56\", NA\n) %&gt;%\n  mutate(implementation_date = ymd(implementation_date))\n\nsahie &lt;- left_join(\n  sahie, \n  states,\n  by = c(\"state\" = \"state_fips\")\n) |&gt;\n  filter(!is.na(PCTUI_PT)) |&gt; \n  mutate(expanded = implementation_date &lt; \"2015-01-01\") %&gt;%\n  mutate(expanded = replace_na(expanded, FALSE))\n\nWe use GMM to cluster the data.\n\nuni &lt;- select(sahie, PCTUI_PT)\n\nset.seed(1)\nuni_mc &lt;- Mclust(uni, G = 2)\n\nglance(uni_mc)\n\n# A tibble: 1 × 7\n  model     G     BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 V         2 -18541. -9250.     5     NA  3141\n\n\nWhen we compare .class to expansion, we see that the model does an good job of labeling counties’ expansion status without observing counties’ expansion status.\n\nbind_cols(\n  sahie,\n  augment(uni_mc)\n) |&gt;\n  count(expanded, .class)\n\nNew names:\n• `PCTUI_PT` -&gt; `PCTUI_PT...5`\n• `PCTUI_PT` -&gt; `PCTUI_PT...9`\n\n\n# A tibble: 4 × 3\n  expanded .class     n\n  &lt;lgl&gt;    &lt;fct&gt;  &lt;int&gt;\n1 FALSE    1        442\n2 FALSE    2       1509\n3 TRUE     1       1030\n4 TRUE     2        160\n\n\n\n\n10.6.5 BIC\nLikelihood quantifies how likely observed data are given a set of parameters. If \\(\\theta\\) is a vector of parameters, then \\(L(\\theta |x) = f(x |\\theta)\\) is the likelihood function.\nWe often don’t know the exact number of latent groups in the data. We need a way to compare models with varying numbers of groups. Simply picking the model with the maximum likelihood will lead to models with too many groups.\nThe Bayesian information criterion (BIC) is an alternative to likelihoods that penalizes models for having many parameters. Let \\(L\\) be the likelihood, \\(m\\) the number of free parameters, and \\(n\\) the number of observations.\n\\[\nBIC = -2log(L) + mlog(n)\n\\tag{10.4}\\]\nWe will choose models that minimize BIC. Ideally, we will use v-fold cross validation for this process.\n\n\n10.6.6 Example 5\nThe Mclust() function will try G = 1:9 when G isn’t specified. Mclust() will also try 14 different variance models for the mixture models.\n\n\n\n\n\n\nImportant\n\n\n\nWe want to minimize BIC but library(mclust) is missing a negative sign. So we want to maximize the BIC plotted by library(mclust). You can read more here.\n\n\nWe can plot the BICs with plot() and view the optimal model with glance().\n\nfaithful_gmm &lt;- Mclust(faithful)\n\nplot(faithful_gmm, what = \"BIC\")\n\n\n\nglance(faithful_gmm)\n\n# A tibble: 1 × 7\n  model     G    BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 EEE       3 -2314. -1126.    11     NA   272"
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#bernoulli-mixture-modeling-bmm",
    "href": "06_advanced-unsupervised-ml.html#bernoulli-mixture-modeling-bmm",
    "title": "10  Mixture Distributions and Mixture Modeling",
    "section": "10.7 Bernoulli Mixture Modeling (BMM)",
    "text": "10.7 Bernoulli Mixture Modeling (BMM)\nLet’s consider a data generation story based on the Bernoulli distribution. Now, each variable, \\(X_1, X_2, ..., X_D\\), is draw from a mixture of \\(K\\) Bernoulli distributions.\n\\[\nX_d  = \\begin{cases}\nBern(p_1) \\text{ with probability }\\pi_1 \\\\\nBern(p_2) \\text{ with probability }\\pi_2 \\\\\n\\vdots \\\\\nBern(p_K) \\text{ with probability }\\pi_K\n\\end{cases}\n\\tag{10.5}\\]\nLet \\(i\\) be an index for each mixture that contributes to the random variable. The probability mass function of the random variable is written as\n\\[\nP(X_d) = \\Pi_{i = 1}^Kp_i^{x_i} (1 - p_i)^{1 - x_i}\n\\tag{10.6}\\]\nLet’s consider a classic example from Bishop (2006) and Murphy (2022). The example uses the MNIST database, which contains 70,000 handwritten digits. The digits are stored in 784 variables, from a 28 by 28 grid, with values ranging from 0 to 255, which indicate the darkness of the pixel.\nTo prepare the data, we divide each pixel by 255 and then turn the pixels into indicators with values under 0.5 as 0 and values over 0.5 as 1. Figure Figure 10.3 visualizes the first four digits after reading in the data and applying pre-processing.\n\nsource(here::here(\"R\", \"visualize_digit.R\"))\n\nmnist &lt;- read_csv(here::here(\"data\", \"mnist_binary.csv\"))\n\nglimpse(dplyr::select(mnist, 1:10))\n\nRows: 60,000\nColumns: 10\n$ label    &lt;dbl&gt; 5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4…\n$ pix_28_1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_5 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_9 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nvisualize_digit(mnist, 1)\nvisualize_digit(mnist, 2)\nvisualize_digit(mnist, 3)\nvisualize_digit(mnist, 4)\n\n\n\n\n\n\n\n(a) 5\n\n\n\n\n\n\n\n(b) 0\n\n\n\n\n\n\n\n(c) 4\n\n\n\n\n\n\n\n(d) 1\n\n\n\n\nFigure 10.3: First Four Digits\n\n\n\nThe digits are labelled in the MNIST data set but we will ignore the labels and use Bernoulli Mixture Modeling to learn the latent labels or groups. We will treat each pixel as its own Bernoulli distribution and cluster observations using mixtures of 784 Bernoulli distributions. This means each cluster will contain \\(784\\) parameters.\n\n10.7.1 Two Digit Example\nLet’s start with a simple example using just the digits “1” and “8”. We’ll use library(flexmix) by Leisch (2004). library(flexmix) is powerful but uses different syntax than we are used to.\n\nThe function flexmix() expects a matrix.\nThe formula expects the entire matrix on the left side of the ~.\nWe specify the distribution used during the maximization (M) step with model = FLXMCmvbinary().\n\n\nlibrary(flexmix)\n\nLoading required package: lattice\n\nmnist_18 &lt;- mnist |&gt;\n  filter(label %in% c(\"1\", \"8\")) |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nThe starting assignments are random, so we set a seed.\n\nset.seed(20230612)\nmnist_18_clust &lt;- flexmix(\n  formula = mnist_18 ~ 1, \n  k = 2, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 100)\n)\n\nThe MNIST data are already labelled, so we can compare our assignments to the labels if we convert the “soft assignments” to “hard assignments”. Note that most applications won’t have labels.\n\nmnist |&gt;\n  filter(label %in% c(\"1\", \"8\")) |&gt;  \n  bind_cols(cluster = mnist_18_clust@cluster) |&gt;\n  count(label, cluster)\n\n# A tibble: 4 × 3\n  label cluster     n\n  &lt;dbl&gt;   &lt;int&gt; &lt;int&gt;\n1     1       1   482\n2     1       2  6260\n3     8       1  5610\n4     8       2   241\n\n\nFigure 10.4 shows the estimated \\(p_i\\) for each pixel for each cluster. The figure shows 784 \\(p_i\\) for \\(k = 1\\) and 784 \\(p_i\\) for \\(k = 2\\). We see that the estimated parameters closely resemble the digits.\nOf course, each digit can differ from these images because everyone writes differently. In some ways, these are average digits across many version of the digits.\n\nmeans_18 &lt;- rbind(\n  t(parameters(mnist_18_clust, component = 1)),\n  t(parameters(mnist_18_clust, component = 2))\n) |&gt;\n  as_tibble() |&gt;\n  mutate(label = NA)\n\n\nvisualize_digit(means_18, 1)\nvisualize_digit(means_18, 2)\n\n\n\n\n\n\n\n(a) 8\n\n\n\n\n\n\n\n(b) 1\n\n\n\n\nFigure 10.4: Estimated Parameters for Each Cluster\n\n\n\nThe BMM does a good job of labeling the digits and recovering the average shape of the digits.\n\n\n10.7.2 Ten Digit Example\nLet’s now consider an example that uses all 10 digits.\nIn most applications, we won’t know the number of latent variables. First, we sample 1,0002 digits and run the model with \\(k = 2, 3, ..., 12\\). We’ll calculate the BIC for each hyperparameter and pick the \\(k\\) with lowest BIC.\n\nset.seed(20230613)\nmnist_sample &lt;- mnist |&gt;\n  slice_sample(n = 1000) |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nsteps &lt;- stepFlexmix(\n  formula = mnist_sample ~ 1, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 100, minprior = 0),\n  k = 2:12, \n  nrep = 1\n)\n\n\\(k = 7\\) provides the lowest BIC. This is probably because digits like 3 and 8 are very similar.\n\nsteps\n\n\nCall:\nstepFlexmix(formula = mnist_sample ~ 1, model = FLXMCmvbinary(), \n    control = list(iter.max = 100, minprior = 0), k = 2:12, nrep = 1)\n\n   iter converged  k k0    logLik      AIC      BIC      ICL\n2    43      TRUE  2  2 -196191.7 395521.4 403221.6 403227.9\n3    30      TRUE  3  3 -188722.8 382153.6 393706.4 393713.9\n4    32      TRUE  4  4 -182949.0 372176.0 387581.4 387585.1\n5    27      TRUE  5  5 -178955.2 365758.4 385016.4 385019.7\n6    35      TRUE  6  6 -175448.7 360315.5 383426.1 383428.5\n7    37      TRUE  7  7 -171697.0 354381.9 381345.1 381347.8\n8    37      TRUE  8  8 -171282.5 355123.1 385938.8 385941.1\n9    38      TRUE  9  9 -169213.3 352554.6 387223.0 387224.9\n10   25      TRUE 10 10 -165521.6 346741.2 385262.2 385263.7\n11   34      TRUE 11 11 -162919.3 343106.5 385480.1 385481.8\n12   26      TRUE 12 12 -162253.5 343345.0 389571.1 389572.7\n\n\nNext, we run the BMM on the full data with \\(k = 7\\).\n\nmnist_full &lt;- mnist |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nmnist_clust &lt;- flexmix(\n  formula = mnist_full ~ 1, \n  k = 7, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 200, minprior = 0)\n)\n\nThe MNIST data are already labelled, so we can compare our assignments to the labels if we convert the “soft assignments” to “hard assignments”. Note that most applications won’t have labels. The rows of the table are the digits. The columns of the table are the clusters. We can see, for example, that most of the 0’s are clustered in cluster 5.\n\nlabels &lt;- mnist |&gt;\n  bind_cols(cluster = mnist_clust@cluster)\n\ntable(labels$label, labels$cluster)\n\n   \n       1    2    3    4    5    6    7\n  0    5  357  282  289 4875    1  114\n  1   36  288   40   35    0 6319   24\n  2  114  166  652   73   50  163 4740\n  3  263  473 4786   80   41  260  228\n  4 3384 1779    4  139    7   53  476\n  5  325 2315 2367  173  109   59   73\n  6    9   86   41 4365   42  128 1247\n  7 3560 2395   21    0   25  234   30\n  8  257 2582 2369   57   32  445  109\n  9 3739 1797  109    5   26  136  137\n\n\nFigure 10.5 shows the estimated \\(p_i\\) for each pixel for each cluster. The following visualize the \\(784K\\) parameters that we estimated. It shows 784 \\(p_i\\) for \\(k = 1, 2, ..., 7\\) clusters. We see that the estimated parameters closely resemble the digits.\n\nmeans &lt;- rbind(\n  t(parameters(mnist_clust, component = 1)),\n  t(parameters(mnist_clust, component = 2)),\n  t(parameters(mnist_clust, component = 3)),\n  t(parameters(mnist_clust, component = 4)),\n  t(parameters(mnist_clust, component = 5)),\n  t(parameters(mnist_clust, component = 6)),\n  t(parameters(mnist_clust, component = 7))\n) |&gt;\n  as_tibble() |&gt;\n  mutate(label = NA)\n\nvisualize_digit(means, 1)\nvisualize_digit(means, 2)\nvisualize_digit(means, 3)\nvisualize_digit(means, 4)\nvisualize_digit(means, 5)\nvisualize_digit(means, 6)\nvisualize_digit(means, 7)\n\n\n\n\n\n\n\n(a) 3, 7, and 9\n\n\n\n\n\n\n\n(b) 5, 7, and 8\n\n\n\n\n\n\n\n(c) 3\n\n\n\n\n\n\n\n(d) 6\n\n\n\n\n\n\n\n\n\n(e) 0\n\n\n\n\n\n\n\n(f) 1\n\n\n\n\n\n\n\n(g) 2\n\n\n\n\nFigure 10.5: Estimated Parameters for Each Cluster\n\n\n\nThe example with all digits doesn’t result in 10 distinct mixtures but it does a fairly good job of structuring finding structure in the data. Without labels and considering the variety of messy handwriting, this is a useful model."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#considerations",
    "href": "06_advanced-unsupervised-ml.html#considerations",
    "title": "10  Mixture Distributions and Mixture Modeling",
    "section": "10.8 Considerations",
    "text": "10.8 Considerations\nMixture modeling is difficult for a couple of reasons:\n\nWe need to assume a model. It can be difficult to assume a multivariate distribution that fits the data in all dimensions of interest.\nThe models are overparameterized and can take a very long time to fit.\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n\n\nLeisch, Friedrich. 2004. “FlexMix: A General Framework for Finite Mixture Models and Latent Class Regression in R.” Journal of Statistical Software 11 (8). https://doi.org/10.18637/jss.v011.i08.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#footnotes",
    "href": "06_advanced-unsupervised-ml.html#footnotes",
    "title": "10  Mixture Distributions and Mixture Modeling",
    "section": "",
    "text": "library(tidyclust) currently doesn’t support mixture modeling. I hope this will change in the future.↩︎\nThis is solely to save computation time.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A\nComparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nAshraf, N., D. Karlan, and W. Yin. 2006. “Tying Odysseus to the\nMast: Evidence From a Commitment Savings Product in the\nPhilippines.” The Quarterly Journal of Economics 121\n(2): 635–72. https://doi.org/10.1162/qjec.2006.121.2.635.\n\n\nBarrientos, Andrés F., Aaron R. Williams, Joshua Snoke, and Claire McKay\nBowen. 2021. “A Feasibility Study of Differentially Private\nSummary Statistics and Regression Analyses with Evaluations on\nAdministrative and Survey Data.” https://doi.org/10.48550/ARXIV.2110.12055.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nBlumenstock, Joshua. n.d. “Calling for Better Measurement:\nEstimating an Individual’s Wealth and Well-Being from\nMobile Phone Transaction Records.” Center for Effective\nGlobal Action. https://escholarship.org/uc/item/8zs63942.\n\n\nBlumenstock, Joshua, Gabriel Cadamuro, and Robert On. 2015.\n“Predicting Poverty and Wealth from Mobile Phone Metadata.”\nScience 350 (6264): 1073–76. https://doi.org/10.1126/science.aac4420.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial Proportion.”\nStatistical Science 16 (2). https://doi.org/10.1214/ss/1009213286.\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical\nInference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.\n\n\nChernick, Michael R., and Robert A. LaBudde. 2011. An Introduction\nto Bootstrap Methods with Applications to r. Hoboken, N.J: Wiley.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien\nNielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and\nCrowd-Out in Retirement Savings Accounts: Evidence from\nDenmark*.” The Quarterly Journal of Economics 129 (3):\n1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nFellegi, I. P. 1972. “On the Question of Statistical\nConfidentiality.” Journal of the American Statistical\nAssociation 67 (337): 7–18. https://www.jstor.org/stable/2284695?seq=1#metadata_info_tab_contents.\n\n\nGinsberg, Jeremy, Matthew H. Mohebbi, Rajan S. Patel, Lynnette Brammer,\nMark S. Smolinski, and Larry Brilliant. 2009. “Detecting Influenza\nEpidemics Using Search Engine Query Data.” Nature 457\n(7232): 1012–14. https://doi.org/10.1038/nature07634.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. Springer Series in Statistics. New York, NY:\nSpringer.\n\n\nHiggins, James J. 2004a. An Introduction to Modern Nonparametric\nStatistics. Pacific Grove, CA: Brooks/Cole.\n\n\n———. 2004b. An Introduction to Modern Nonparametric Statistics.\nPacific Grove, CA: Brooks/Cole.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response\nAdjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLeisch, Friedrich. 2004. “FlexMix: A General Framework for Finite\nMixture Models and Latent Class Regression in\nR.” Journal of Statistical\nSoftware 11 (8). https://doi.org/10.18637/jss.v011.i08.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary\nAlignment Methods in Microsimulation Models.” Journal of\nArtificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020.\n“Estimating Confidence Intervals in a Tax Microsimulation\nModel.” International Journal of Microsimulation 13 (2):\n2–20. https://doi.org/10.34196/IJM.00216.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An\nIntroduction. Adaptive Computation and Machine Learning Series.\nCambridge, Massachusetts: The MIT Press.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.”\nThe Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528.\n\n\nPotash, Eric, Joe Brew, Alexander Loewi, Subhabrata Majumdar, Andrew\nReece, Joe Walsh, Eric Rozier, Emile Jorgenson, Raed Mansour, and Rayid\nGhani. 2015. “KDD ’15: The 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining.” In, 2039–47.\nSydney NSW Australia: ACM. https://doi.org/10.1145/2783258.2788629.\n\n\nRavn, Signe, Ashley Barnwell, and Barbara Barbosa Neves. 2020.\n“What Is “Publicly Available Data”?\nExploring Blurred PublicPrivate Boundaries and Ethical\nPractices Through a Case Study on Instagram.” Journal of\nEmpirical Research on Human Research Ethics 15 (1-2): 40–45. https://doi.org/10.1177/1556264619850736.\n\n\nRizzo, Maria L. 2008. Statistical Computing with r. Chapman\n& Hall/CRC Computer Science and Data Analysis Series. Boca Raton:\nChapman & Hall/CRC.\n\n\nSalganik, Matthew J. 2018. Bit by Bit: Social Research in the\nDigital Age. Princeton: Princeton University Press.\n\n\nScott, David W., and Stephan R. Sain. 2005. “Multidimensional\nDensity Estimation.” In, 24:229–61. Elsevier. https://doi.org/10.1016/S0169-7161(04)24009-3.\n\n\nSomepalli, Gowthami, Singla, Micah Goldblum, Jonas Geiping, and Tom\nGoldstein. 2023. “Diffusion Art or Digital Forgery? Investigating\nData Replication in Diffusion Models.” Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 6048–58. https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualie, and Model\nData. 2nd edition. Sebastopol, CA: O’Reilly.\n\n\nZheng, Vivian. 2020. “How Urban Piloted Data Science Techniques to\nCollect Land-Use Reform Data.” https://urban-institute.medium.com/how-urban-piloted-data-science-techniques-to-collect-land-use-reform-data-475409903b88."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Public Policy",
    "section": "",
    "text": "Welcome\nThis book is the notes for Advanced Data Science for Public Policy in the McCourt School of Public Policy at Georgetown University."
  },
  {
    "objectID": "index.html#acknowledgements-more-to-come",
    "href": "index.html#acknowledgements-more-to-come",
    "title": "Data Science for Public Policy",
    "section": "Acknowledgements (More to come!)",
    "text": "Acknowledgements (More to come!)\nThis book has benefited from many great teachers, collaborators, and students. First, I want to thank Gabe Morrison for excellent reviews and proofreading. Second, I want to thank Alex Engler and Alena Stern for collaborating on Intro to Data Science for Public Policy during six semesters and eight classes."
  },
  {
    "objectID": "05_simulation-and-sampling.html#sec-review5",
    "href": "05_simulation-and-sampling.html#sec-review5",
    "title": "5  Simulation and Sampling",
    "section": "5.1 Review",
    "text": "5.1 Review\n\n\n\n\n\n\nPopulation\n\n\n\nA population is the entire set of observations of interest.\nFor example, a population could be everyone residing in France at a point in time. A different population could be every American ages 65 or older.\n\n\n\n\n\n\n\n\nParameter\n\n\n\nA parameter is a numerical quantity that summarizes a population.\nFor example, the population mean and population standard deviation describe important characteristics of many populations.\nMore generally, location parameters, scale parameters, and shape parameters describe many populations.\n\n\n\n\n\n\n\n\nRandom Sample\n\n\n\nA random sample is a random subset of a population.\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nA statistic is a numerical quantity that summarizes a sample.\nFor example, the sample mean and sample standard deviation describe important characteristics of many random samples.\n\n\nParameters are to populations what statistics are to samples. The process of learning about population parameters from statistics calculated from samples is called statistical inference."
  },
  {
    "objectID": "05_simulation-and-sampling.html#introduction",
    "href": "05_simulation-and-sampling.html#introduction",
    "title": "5  Simulation and Sampling",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nSimulation and sampling are important tools for statistics and data science. After reviewing/introducing basic concepts about probability theory and probability distributions, we will discuss two important applications of simulation and sampling.\n\nMonte Carlo simulation: A class of methods where values are repeatedly sampled/simulated from theoretical distributions that model a data generation process. Theoretical distributions, like the normal distribution, have closed-from representations and a finite number of parameters like mean and variance.\nResampling methods: A class of methods where values are repeatedly sampled from observed data to approximate repeatedly sampling from a population. Bootstrapping is a common resampling method.\n\nMonte Carlo methods and resampling methods have a wide range of applications. Monte Carlo simulation is used by election forecasters to predict electoral outcomes and econometricians to understand the properties of estimators. Resampling methods are used in machine learning and causal inference. Both are fundamental methods for agent-based models including microsimulation."
  },
  {
    "objectID": "05_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "href": "05_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "title": "5  Simulation and Sampling",
    "section": "5.3 Fundamentals of Probability Theory",
    "text": "5.3 Fundamentals of Probability Theory\n\n\n\n\n\n\nRandom Variable\n\n\n\n\\(X\\) is a random variable if its value is unknown and/or could change.\n\n\n\\(X\\) could be the outcome from the flip of a coin or the roll of a die. \\(X\\) could also be the amount of rain next July 4th.\n\n\n\n\n\n\nExperiment\n\n\n\nAn experiment is a process that results in a fixed set of possible outcomes.\n\n\n\n\n\n\n\n\nSet\n\n\n\nA set is a collection of objects.\n\n\n\n\n\n\n\n\nSample Space\n\n\n\nA sample space is the set of all possible outcomes for an experiment. We will denote a sample space with \\(\\Omega\\).\n\n\n\n\n\n\n\n\nDiscrete Random Variable\n\n\n\nA set is countable if there is a one-to-one correspondence from the elements of the set to some (finite) or all (countably infinite) positive integers (i.e. 1 = heads and 2 = tails).\nA random variable is discrete if its sample space is countable (finite or countably infinite).\n\n\n\n\n\n\n\n\nContinuous Random Variable\n\n\n\nA random variable is continuous if its sample space is any value in a \\(\\mathbb{R}\\) (real) interval.\nThere are infinite possible values in a real interval so the sample space is uncountable."
  },
  {
    "objectID": "05_simulation-and-sampling.html#discrete-random-variables",
    "href": "05_simulation-and-sampling.html#discrete-random-variables",
    "title": "5  Simulation and Sampling",
    "section": "5.4 Discrete Random Variables",
    "text": "5.4 Discrete Random Variables\n\n\n\n\n\n\nProbability Mass Function\n\n\n\nA probability mass function (PMF) computes the probability of an event in the sample space of a discrete random variable.\n\\[\np(x) = P(X = x)\n\\tag{5.1}\\]\nwhere \\(0 \\le p(x) \\le 1\\) and \\(\\sum_{x \\in \\Omega} p(x) = 1\\)\n\n\n\n\nCode\ntibble(\n  a = factor(1:6),\n  `P(X = a)` = rep(1 / 6, 6)\n) |&gt;\n  ggplot(aes(a, `P(X = a)`)) +\n  geom_col() +\n  labs(title = \"PMF for rolling a fair die\")\n\n\n\n\nFigure 5.1: PMF for rolling a fair die\n\n\n\n\n\nNow we can make statements like \\(P(X = a)\\). For example, \\(P(X = 3) = \\frac{1}{6}\\).\n\n5.4.1 Bernoulli Distribution\nA Bernoulli random variable takes on the value \\(1\\) with probability \\(p\\) and \\(0\\) with probability \\(1 - p\\). It is often used to represent coins. When \\(p = \\frac{1}{2}\\) we refer to the coin as “fair”.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Ber(p)\n\\tag{5.2}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) =\n\\begin{cases}\n1 - p &\\text{ if } x = 0 \\\\\np &\\text{ if } x = 1\n\\end{cases} = p^x(1 - p)^{1-x}\n\\tag{5.3}\\]\n\n\n5.4.2 Binomial Distribution\nA binomial random variable is the number of events observed in \\(n\\) repeated Bernoulli trials.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Bin(n, p)\n\\tag{5.4}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) = {n \\choose x} p^x(1 - p)^{n - x}\n\\tag{5.5}\\]\nWe can calculate the theoretical probability of a given draw from a binomial distribution using this PDF. For example, suppose we have a binomial distribution with \\(10\\) trials and \\(p = \\frac{1}{2}\\). The probability of drawing exactly six \\(1\\)s and four \\(0\\)s is\n\\[\np(X = 6) = \\frac{10!}{6!4!} 0.5^6(1 - 0.5)^{10 - 6} \\approx 0.2051\n\\tag{5.6}\\]\nWe can do similar calculations for each value between \\(0\\) and \\(10\\).\nWe can also take random draws from the distribution. Figure 5.2 shows 1,000 random draws from a binomial distribution with 10 trials and p = 0.5. The theoretical distribution is overlaid in red.\n\n\nCode\ntibble(\n  x = rbinom(n = 1000, size = 10, prob = 0.5)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(breaks = 0:10) +\n  geom_point(data = tibble(x = 0:10, y = map_dbl(0:10, dbinom, size = 10, prob = 0.5)),\n             aes(x, y),\n             color = \"red\") +\n  labs(\n    title = \"1,000 samples of a binomial RV\",\n    subtitle = \"Size = 10; prob = 0.5\",\n    y = NULL\n  ) \n\n\n\n\nFigure 5.2: 1,000 random draws from a binomial distribution with 10 trials and p = 0.5\n\n\n\n\n\n\n\n\n\n\n\nSampling Error\n\n\n\nSampling error is the difference between sample statistics (estimates of population parameters) and population parameters.\n\n\nThe difference between the red dots and black bars in Figure 5.2 is caused by sampling error.\n\n\n5.4.3 Distributions Using R\nMost common distributions have R functions to\n\ncalculate the density of the pdf/pmf for a specific value\ncalculate the probability of observing a value less than \\(a\\)\ncalculate the value associated with specific quantiles of the pdf/pmf\nrandomly sample from the probability distribution\n\nLet’s consider a few examples:\nThe following answers the question: “What is the probability of observing 10 events in 10 trials when p = 0.5?”\n\ndbinom(x = 10, size = 10, prob = 0.5)\n\n[1] 0.0009765625\n\n\n\nThe following answers the question: “What’s the probability of observing 3 or fewer events in 10 trials when p = 0.5”\n\npbinom(q = 3, size = 10, prob = 0.5)\n\n[1] 0.171875\n\n\n\nThe following answers the question: “What is a 10th percentile number of events to see in 10 trials when p = 0.5?\n\nqbinom(p = 0.1, size = 10, prob = 0.5)\n\n[1] 3\n\n\n\nThe following randomly draw ten different binomially distributed random variables.\n\nrbinom(n = 10, size = 10, prob = 0.5)\n\n [1] 4 5 4 8 4 8 7 8 4 5\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nAdd dbinom() for 0, 1, and 2 when size = 10 and prob = 0.5.\nCalculate pbinom() with size = 10 and prob = 0.5. Why do you get the same answer?\nPlug the result from step 1/step 2 into qbinom() with size = 10 and prob = 0.5. Why do you get the answer 2?\nUse rbinom() with size = 10 and prob = 0.5 to sample 10,000 binomially distributed random variables and assign the output to x. Calculate mean(x &lt;= 2). How does the answer compare to step 1/step 2?\n\n\n\n\n\n\n\n\n\nPseudo-random numbers\n\n\n\nComputers use pseudo-random numbers to generate samples from probability distributions. Modern pseudo-random samplers are very random.\nUse set.seed() to make pseudo-random sampling reproducible.\n\n\n\n\n5.4.4 Poisson Random Variable\nA poisson random variable is the number of events that occur in a fixed period of time. For example, a poisson distribution can be used to model the number of visits in an emergency room between 1AM and 2AM.\nWe show that a random variable is poisson-distributed with\n\\[\nX \\sim Pois(\\lambda)\n\\tag{5.7}\\]\nThe parameter \\(\\lambda\\) is both the mean and variance of the poisson distribution. The PMF of a poisson random variable is\n\\[\np(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\tag{5.8}\\]\nFigure 5.3 shows 1,000 draws from a poisson distribution with \\(\\lambda = 10\\).\n\n\nCode\nset.seed(20200905)\n\ntibble(\n  x = rpois(1000, lambda = 10)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(limits = c(0, 29)) +\n  stat_function(fun = dpois, n = 30, color = \"red\", args = list(lambda = 10)) + \n  labs(\n    title = \"1,000 samples of a Poisson RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\nFigure 5.3: 1,000 samples of a Poisson RV\n\n\n\n\n\n\n\n5.4.5 Categorical Random Variable\nWe can create a custom discrete probability distribution by enumerating the probability of each event in the sample space. For example, the PMF for the roll of a fair die is\n\\[\np(x) =\n\\begin{cases}\n\\frac{1}{6} & \\text{if } x = 1\\\\\n\\frac{1}{6} & \\text{if } x = 2\\\\\n\\frac{1}{6} & \\text{if } x = 3\\\\\n\\frac{1}{6} & \\text{if } x = 4\\\\\n\\frac{1}{6} & \\text{if } x = 5\\\\\n\\frac{1}{6} & \\text{if } x = 6\n\\end{cases}\n\\tag{5.9}\\]\nThis PMF is visualized in Figure 5.1. We can sample from this PMF with\n\nsample(x = 1:6, size = 1)\n\n[1] 3\n\n\nWe can also sample with probabilities that differ for each event:\n\nsample(\n  x = c(\"rain\", \"sunshine\"), \n  size = 1, \n  prob = c(0.1, 0.9)\n)\n\n[1] \"sunshine\"\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nSample 1,000 observations from a Poisson distribution with \\(\\lambda = 20\\).\nSample 1,000 observations from a normal distribution with \\(\\mu = 20\\) and \\(\\sigma = \\sqrt{20}\\).\nVisualize and compare both distribution.\n\n\n\nWhen \\(\\lambda\\) is sufficiently large, the normal distribution is a reasonable approximation of the poisson distribution."
  },
  {
    "objectID": "05_simulation-and-sampling.html#continuous-random-variables",
    "href": "05_simulation-and-sampling.html#continuous-random-variables",
    "title": "5  Simulation and Sampling",
    "section": "5.5 Continuous Random Variables",
    "text": "5.5 Continuous Random Variables\n\n\n\n\n\n\nProbability Density Function (PDF)\n\n\n\nA probability density function is a non-negative, integrable function for each real value \\(x\\) that shows the relative probability of values of \\(x\\) for an absolutely continuous random variable \\(X\\).\nWe note PDF with \\(f_X(x)\\).\n\n\n\n\n\n\n\n\nCumulative Distribution Function (CDF)\n\n\n\nA cumulative distribution function (cdf) shows the probability of a random variable \\(X\\) taking on any value less than or equal to \\(x\\).\nWe note CDF with \\(F_X(x) = P(X \\le x)\\)\n\n\nHere is the PDF for a standard normal random variable:\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"PDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\n\n\n\nIf we integrate the entire function we get the CDF.\nCumulative Density Function (CDF): A function of a random variable \\(X\\) that returns the probability that the value \\(X &lt; x\\).\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = pnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"CDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n5.5.1 Uniform Distribution\nUniform random variables have equal probability for every value in the sample space. The distribution has two parameters: minimum and maximum. A standard uniform random has minimum = 0 and maximum = 1.\nWe show that a random variable is uniform distributed with\n\\[\nX \\sim U(a, b)\n\\tag{5.10}\\]\nThe PDF of a uniform random variable is\n\\[\nf(x) =\n\\begin{cases}\n\\frac{1}{b - a} & \\text{if } x \\in [a, b] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\tag{5.11}\\]\nStandard uniform random variables are useful for generating other random processes and imputation.\n\n\nCode\nset.seed(20200904)\n\ntibble(\n  x = runif(1000)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dunif, n = 101, color = \"red\") + \n  labs(\n    title = \"1,000 samples of a standard uniform RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n5.5.2 Normal Distribution\nThe normal distribution is the backbone of statistical inference because of the central limit theorem.\nWe show that a random variable is normally distributed with\n\\[\nX \\sim N(\\mu, \\sigma)\n\\tag{5.12}\\]\nThe PDF of a normally distributed random variable is\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right) ^ 2\\right]\n\\tag{5.13}\\]\n\n\n\n\n\n\nFundamental Probability Formula for Intervals\n\n\n\nThe probability that an absolutely continuous random variable takes on any specific value is always zero because the sample space is uncountable. Accordingly, we express the probability of observing events within a region for absolutely continuous random variables.\nIf \\(X\\) has a PDF and \\(a &lt; b\\), then\n\\[\nP(a \\le X \\le b) = P(a \\le X &lt; b) = P(a &lt; X \\le b) = P(a &lt; X &lt; b) = \\int_a^bf(x)dx = F_X(b) - F_X(a)\n\\tag{5.14}\\]\n\n\nThe last portion of this inequality is fundamental to working with continuous probability distributions and is the backbone of much of any intro to statistics course. For example, the probability, \\(P(X &lt; 0)\\) is represented by the blue region below.\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  geom_area(stat = \"function\", fun = dnorm, fill = \"blue\", xlim = c(-4, 0)) +\n  labs(\n    title = \"PDF for a standard normal random variable\",\n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nStudent’s t-distribution and the normal distribution are closely related.\n\nUse pnorm() to calculate \\(P(X &lt; -1)\\) for a standard normal distribution.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 10.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 100.\n\n\n\nObserve how the normal distribution becomes a better approximation for Student’s t-distribution when the degrees of freedom increases.\n\n\n5.5.3 Exponential Distribution\nAn exponential random variable is the wait time between events for a poisson random variable. It is useful for modeling wait time. For example, an exponential distribution can be used to model the wait time between arrivals in an emergency room between 1AM and 2AM. It has one parameter: rate (\\(\\lambda\\)).\nWe show that a random variable is exponentially distributed with\n\\[\nX \\sim Exp(\\lambda)\n\\tag{5.15}\\]\nThe PDF of an exponential random variable is\n\\[\nf(x) = \\lambda\\exp(-\\lambda x)\n\\tag{5.16}\\]\n\n\nCode\ntibble(\n  x = rexp(n = 1000, rate = 1)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_density() +\n  stat_function(fun = dexp, n = 101, args = list(rate = 1), color = \"red\") + \n  labs(\n    title = \"1,000 samples of an exponential RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n5.5.4 Other Distributions\n\nGeometric RV: Number of Bernoulli trials up to and including the \\(1^{st}\\) event\nNegative Binomial RV: Number of Bernoulli trials up to and including the \\(r^{th}\\) event\nGamma RV: Time until the \\(\\alpha\\) person arrives"
  },
  {
    "objectID": "05_simulation-and-sampling.html#parametric-density-estimation",
    "href": "05_simulation-and-sampling.html#parametric-density-estimation",
    "title": "5  Simulation and Sampling",
    "section": "5.6 Parametric Density Estimation",
    "text": "5.6 Parametric Density Estimation\nA key exercise in statistics is selecting a probability distribution to represent data and then learning the parameters of probability distributions from the data. The process is often called model fitting.\nWe are focused on parametric density estimation. Later, we will focus on nonparameteric density estimation. This section will focus on frequentist inference of population parameters from observed data. Later, we will adopt a Bayesian approach to inference.\n\n5.6.1 Maximum Likelihood Estimation\nAll of the probability distributions we have observed have a finite number of parameters. Maximum likelihood estimation is a common method for estimating these parameters.\nThe general process is\n\nPick the probability distribution that fits the observed data.\nIdentify the finite number of parameters associated with the probability distribution.\nCalculate the parameters that maximize the probability of the observed data.\n\n\n\n\n\n\n\nLikelihood\n\n\n\nLet \\(\\vec{x}\\) be observed data and \\(\\theta\\) be a parameter or parameters from a chosen probability distribution. The likelihood is the joint probability of the observed data conditional on values of the parameters.\nThe likelihood of discrete data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n p(x_i|\\theta)\n\\tag{5.17}\\]\nThe likelihood of continuous data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n f(x_i|\\theta)\n\\tag{5.18}\\]\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\n\nMaximum likelihood estimation is a process for estimating parameters for a given distribution that maximizes the log likelihood.\nIn other words, MLEs find the estimated parameters that maximize the probability of observing the observed set of data.\n\n\nWe won’t unpack how to derive the maximum likelihood estimators1 but it is easy to look up most MLEs.\n\nBinomial distribution MLEs\nSuppose we have a sample of data \\(x_1, ..., x_m\\). If the number of trials \\(n\\) is already known, then \\(p\\) is the only parameter for the binomial distribution that needs to be estimated. The MLE for \\(p\\) is \\(\\hat{p} = \\frac{\\sum_{i = 1}^n x_i}{mn}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat{p}\\).\n\nset.seed(20230909)\nx &lt;- rbinom(n = 8, size = 10, prob = 0.3)\n\nx\n\n[1] 4 3 6 3 4 3 3 2\n\n\n\nmle_binom &lt;- sum(x) / (10 * 8)\n\nmle_binom\n\n[1] 0.35\n\n\n\n\nNormal distribution MLEs\n\\(\\mu\\) and \\(\\sigma\\) are the parameters of a normal distribution. The MLEs for a normal distribution are \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i = \\bar{x}\\) and \\(s^2 = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^2\\).2\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i\\) and \\(\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\).\n\nset.seed(20230909)\nx &lt;- rnorm(n = 200, mean = 0, sd = 1)\n\n\nmean_hat &lt;- mean(x)\n\nmean_hat\n\n[1] 0.02825125\n\nsigma2_hat &lt;- mean((x - mean(x)) ^ 2)\n\nsigma2_hat\n\n[1] 0.8119682\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = mean_hat, sd = sqrt(sigma2_hat)))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nExponential distribution MLEs\n\\(\\lambda\\) is the only parameter of an exponential distribution. The MLE for an exponential distribution is \\(\\hat\\lambda = \\frac{1}{\\bar{x}}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\frac{1}{\\bar{x}}\\).\n\nset.seed(20230909)\nx &lt;- rexp(n = 200, rate = 10)\n\nmle_exp &lt;- 1 / mean(x)\n\nmle_exp\n\n[1] 10.58221\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dexp, color = \"red\", args = list(rate = mle_exp))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCreate the vector in the code chunk below:\n\n\n\nCode\nx &lt;- c(\n  30970.787, 10901.544, 15070.015, 10445.772, 8972.258, \n  15759.614, 13341.328, 18498.858, 134462.066, 17498.930, \n  7112.306, 27336.795, 75526.381, 110123.606, 32910.618, \n  16764.452, 21244.380, 18952.455, 954373.470, 4219.635,\n  7078.766, 27657.996, 18337.097, 14566.525, 14220.000, \n  21457.202, 9322.311, 26018.018, 96325.728, 26780.329, \n  25833.356, 10719.360, 8642.935, 29302.623, 10517.174,\n  33831.547, 339077.456, 5805.707, 141505.710, 28168.790, \n  10446.378, 4993.349, 27502.949, 35519.162, 45761.505, \n  26163.096, 72163.668, 15515.435, 69396.895, 84972.590, \n  67248.460, 26966.374, 24624.339, 4779.110, 23330.279,\n  196311.913, 20517.739, 80257.587, 32108.466, 9735.061, \n  20502.579, 2544.004, 165909.040, 20949.512, 16643.695, \n  30267.741, 8359.024, 13355.154, 8425.988, 4491.550,\n  32071.872, 61648.149, 75074.135, 62842.985, 26040.648, \n  68733.979, 63368.710, 11157.211, 5782.610, 3629.674, \n  44399.230, 2852.381, 8200.453, 41249.003, 15006.791,\n  808974.653, 30705.915, 6341.954, 28208.144, 5409.821,\n  54566.805, 10894.864, 4583.550, 31110.875, 43474.872, \n  69059.161, 33054.574, 8789.910, 218887.477, 11051.292, \n  3366.743, 63853.329, 68756.561, 48031.259, 11707.191,\n  26593.634, 8868.942, 19225.309, 27704.670, 10666.549, \n  47151.963, 20343.604, 123932.502, 33030.986, 5412.023, \n  23540.382, 9894.513, 52742.541, 21397.990, 25100.143,\n  23757.882, 48347.300, 4325.134, 23816.776, 11907.656, \n  24179.849, 25967.574, 7531.294, 15131.240, 21595.781, \n  40473.936, 35390.849, 4060.563, 55334.157, 37058.771, \n  34050.456, 17351.500, 7453.829, 48131.565, 10576.746,\n  26450.754, 33592.986, 21425.018, 34729.337, 77370.078, \n  5819.325, 9067.356, 19829.998, 20120.706, 3637.042, \n  44812.638, 22930.229, 29683.776, 76366.822, 15464.594, \n  1273.101, 53036.266, 2846.294, 114076.200, 14492.680, \n  55071.554, 31597.849, 199724.125, 52332.510, 98411.129, \n  43108.506, 6580.620, 12833.836, 8846.348, 7599.796, \n  6952.447, 30022.143, 24829.739, 40784.581, 8997.219,\n  3786.354, 11515.298, 116515.617, 137873.967, 3282.185,\n  107886.676, 13184.850, 51083.235, 2907.886, 51827.538, \n  37564.196, 23196.399, 20169.037, 9020.364, 11118.250, \n  56930.060, 11657.302, 84642.584, 44948.450, 16610.166, \n  5509.231, 4770.262, 15614.233, 5993.999, 22628.114\n)\n\n\n\nVisualize the data with a relative frequency histogram.\nCalculate the MLEs for a normal distribution and add a normal distribution to the visualization in red.\nCalculate the MLEs for a log-normal distribution and add a log-normal distribution to the visualization in blue."
  },
  {
    "objectID": "05_simulation-and-sampling.html#multivariate-random-variables",
    "href": "05_simulation-and-sampling.html#multivariate-random-variables",
    "title": "5  Simulation and Sampling",
    "section": "5.7 Multivariate Random Variables",
    "text": "5.7 Multivariate Random Variables\nWe’ve explored univariate or marginal distributions thus far. Next, we will focus on multivariate distributions.\n\n\n\n\n\n\nMultivariate Distribution\n\n\n\nA multivariate distribution is a probability distribution that shows the probability (discrete) or relative probability (continuous) of more than one random variable.\nMultivariate distributions require describing characteristics of random variables and the relationships between random variables.\n\n\n\n5.7.1 Multivariate Normal Distribution\nThe multivariate normal distribution is a higher-dimensional version of the normal distribution.\nInstead of a single mean and a single variance, the \\(k\\)-dimensional multivariate normal distribution has a vector of means of length \\(k\\) and a \\(k\\)-by-\\(k\\) variance-covariance matrix3. The vector describes the central tendencies of each dimension of the multivariate distribution and the matrix describe the variance of the distributions and relationships between the distributions.\nWe show that a random vector is multivariate normally distributed with\n\\[\n\\vec{X} \\sim \\mathcal{N}(\\vec\\mu, \\boldsymbol\\Sigma)\n\\tag{5.19}\\]\nThe PDF of a multivariate normally distributed random variable is\n\\[\nf(x) = (2\\pi)^{-k/2}det(\\boldsymbol\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\vec{x} - \\vec\\mu)^T\\boldsymbol\\Sigma^{-1}(\\vec{x} - \\vec\\mu)\\right)\n\\tag{5.20}\\]\nFunctions for working with multi-variate normal distributions from library(MASS). Figure 5.4 shows three different random samples from 2-dimensional multivariate normal distributions.\n\nSigma1 &lt;- matrix(c(1, 0.8, \n                   0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n  \nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma1) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nSigma2 &lt;- matrix(c(1, 0.2, \n                   0.2, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma2) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nSigma3 &lt;- matrix(c(1, -0.8, \n                   -0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma3) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nFigure 5.4: Samples from Multivariate Normal Distributions\n\n\n\n\n\n(a) Strong Positive Covariance\n\n\n\n\n\n\n\n(b) Weak Covariance\n\n\n\n\n\n\n\n(c) Strong Negative Covariance"
  },
  {
    "objectID": "05_simulation-and-sampling.html#monte-carlo-methods",
    "href": "05_simulation-and-sampling.html#monte-carlo-methods",
    "title": "5  Simulation and Sampling",
    "section": "5.8 Monte Carlo Methods",
    "text": "5.8 Monte Carlo Methods\nSimulation methods, including Monte Carlo simulation, are used for policy analysis:\n\nFiveThirtyEight and the New York Times use Monte Carlo simulation to predict the outcomes of elections.\nThe Social Security Administration uses microsimulation to evaluate the distributional impact of Social Security reforms.\nThe Census Bureau uses simulation to understand the impact of statistical disclosure control on released data.\nEconometricians and statisticians use Monte Carlo simulation to demonstrate the properties of estimators.\n\nWe can make probabilistic statements about common continuous random variables because their PDFs are integrable or at least easy enough to approximate with lookup tables. We can make probabilistic statements about common discrete random variables with summation.\nBut we often want to make probabilistic statements about uncommon or complex probability distributions. Maybe the probability distribution of the random variable doesn’t have a tractable integral (i.e. the area under the curve can’t practically be computed). Or maybe there are too many potential outcomes (e.g. rays of light emitting from a light bulb in the Marble Science video linked at the top).\nMonte Carlo: A Monte Carlo method estimates a deterministic quantity using stochastic (random) sampling.\nMonte Carlo but easier this time: A Monte Carlo method takes hundreds or thousands of independent samples from a random variable or variables and then approximates fixed population quantities with summaries of those draws. The quantities could be population parameters like a population mean or probabilities.\nMonte Carlo methods have three major applications:\n\nSampling – Monte Carlo simulation allows for sampling from complex probability distributions. The samples can be used to model real-world events (queues), to model outcomes with uncertain model inputs (election modeling), to generate fake data with known parameters to evaluate statistical methods (model selection when assumptions fail), and to draw from the posteriors of Bayesian models.\nNumerical integration – Integration, as noted above, is important to calculating probabilities and ultimately calculating quantities like expected value or the intervals. Monte Carlo methods can approximate multidimensional integrals that will never be directly solved by computers or simplify estimating probabilities when there are uncountably many potential outcomes (Solitaire).\nOptimization – Monte Carlo methods can be used for complex optimization. We will not focus on optimization.\n\nLet’s explore some examples:\n\n5.8.1 Example 1: Coin Tossing\nWe can calculate the proportion of tosses of a fair coin that we expect to turn up heads by finding the expected value of the binomial distribution and dividing by the number of tosses. But suppose we can’t… Or maybe we wish to confirm our calculations with simulations…\nLet’s try repeated sampling from a binomial distribution to approximate this process:\n\n#' Count the proportion of n tosses that turn up heads\n#'\n#' @param n An integer for the number of tosses\n#'\n#' @return The proportion of n tosses that turn up heads\n#' \ncount_heads &lt;- function(n) {\n  \n  # toss the fair coin n times\n  coin_tosses &lt;- rbinom(n = n, size = 1, prob = 0.5)\n    \n  coin_tosses &lt;- if_else(coin_tosses == 1, \"heads\", \"tails\")\n  \n  # calculate the proportion of heads\n  prop_heads &lt;- mean(coin_tosses == \"heads\")\n  \n  return(prop_heads)\n  \n}\n\nLet’s toss the coin ten times.\n\nset.seed(11)\ncount_heads(n = 10)\n\n[1] 0.3\n\n\nOk, we got 0.3, which we know isn’t close to the expected proportion of 0.5. What if we toss the coin 1 million times.\n\nset.seed(20)\ncount_heads(n = 1000000)\n\n[1] 0.499872\n\n\nOk, that’s more like it.\n\\[\\cdot\\cdot\\cdot\\]\nMonte Carlo simulation works because of the law of large numbers. The law of large numbers states that the probability that the average of trials differs from the expected value converges to zero as the number of trials goes to infinity.\nMonte Carlo simulation basically repeats the ideas behind frequentist inferential statistics. If we can’t measure every unit in a population then we can sample a representative population and estimate parameters about that population.\nThe keys to Monte Carlo simulation are randomness and independent and identically distributed sampling (i.i.d.).\n\n\n5.8.2 Example 2: Bootstrap Sampling\nOn average, a bootstrap sample includes about 63% of the observations from the data that are sampled. This means that an individual bootstrap sample excludes 37% of the observations from the source data!\nSo if we bootstrap sample from a vector of length 100, then \\(\\frac{63}{100}\\) values will end up in the bootstrap sample on average and \\(\\frac{37}{100}\\) of the values will be repeats on average.\nWe can explore this fact empirically with Monte Carlo simulation using repeated samples from a categorical distribution. We will use sample().\n\n#' Calculate the proportion of unique values from a vector of integers included \n#' in a bootstrap sample\n#'\n#' @param integers A vector of integers\n#'\n#' @return The proportion of integers included in the bootstrap sample\n#' \ncount_uniques &lt;- function(integers) {\n  \n  # generate a bootstrap sample\n  samples &lt;- sample(integers, size = length(integers), replace = TRUE)\n  \n  # calculate the proportion of unique values from the original vector\n  prop_unique &lt;- length(unique(samples)) / length(integers)\n  \n  return(prop_unique)\n  \n}\n\nLet’s bootstrap sample 100,000 times.\n\n# pre-allocate the output vector for efficient computation\nprop_unique &lt;- vector(mode = \"numeric\", length = 100000)\nfor (i in seq_along(prop_unique)) {\n  \n  prop_unique[i] &lt;- count_uniques(integers = 1:100)\n  \n}\n\nFinally, calculate the mean proportion and estimate the expected value.\n\nmean(prop_unique)\n\n[1] 0.6337935\n\n\nWe can also calculate a 95% confidence interval using the bootstrap samples.\n\nquantile(prop_unique, probs = c(0.025, 0.975))\n\n 2.5% 97.5% \n 0.57  0.69 \n\n\n\n\n5.8.3 Example 3: \\(\\pi\\)\nConsider one of the examples from Marble Science: Monte Carlo Simulation. Imagine we don’t know \\(\\pi\\) but we know that the equation for the area of a square is \\(r ^ 2\\) and the equation for the area of a circle is \\(\\pi r ^ 2\\). If we know the ratio of the areas of the circle and the square, then we can solve for \\(\\pi\\).\n\\[\n\\frac{\\text{Area of Cirle}}{\\text{Area of Square}} = \\frac{\\pi r ^ 2}{r ^ 2} = \\pi\n\\tag{5.21}\\]\nThis is simply solved with Monte Carlo simulation. Randomly sample a bivariate uniform random variables and count how frequently the values are inside of the square or inside the circle.\n\nexpand_grid(\n  x = seq(0, 4, 0.1),\n  y = seq(0, 2, 0.1)\n) |&gt;\nggplot() +\n  ggforce::geom_circle(aes(x0 = 2, y0 = 1, r = 1), fill = \"blue\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), fill = \"red\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 3, ymin = 0, ymax = 2), fill = NA, color = \"black\") +\n  coord_fixed()\n\n\n\n\n\nnumber_of_samples &lt;- 2000000\n\n# sample points in a rectangle with x in [0, 3] and y in [0, 2]\nset.seed(20210907)\nsamples &lt;- tibble(\n  x = runif(number_of_samples, min = 0, max = 3),\n  y = runif(number_of_samples, min = 0, max = 2)\n)\n\n# calculate if (x, y) is in the circle, the square, or neither\nsamples &lt;- samples |&gt;\n  mutate(\n    in_square = between(x, 0, 1) & between(y, 0, 1),\n    in_circle = (x - 2) ^ 2 + (y - 1) ^ 2 &lt; 1\n  ) \n\n# calculate the proportion of samples in each shape\nprop_in_shapes &lt;- samples |&gt;\n  summarize(\n    prop_in_square = mean(in_square), \n    prop_in_circle = mean(in_circle)\n  ) \n\n# calculate the ratio\nprop_in_shapes |&gt;\n  mutate(prop_in_circle / prop_in_square) |&gt;\n  print(digits = 3)\n\n# A tibble: 1 × 3\n  prop_in_square prop_in_circle `prop_in_circle/prop_in_square`\n           &lt;dbl&gt;          &lt;dbl&gt;                           &lt;dbl&gt;\n1          0.166          0.524                            3.15\n\n\nThe answer approximates \\(\\pi\\)!\n\n\n5.8.4 Example 4: Simple Linear Regression\nThe goal of statistical inference is to use data, statistics, and assumptions to infer parameters and probabilities about a population. Typically we engage in point estimation and interval estimation.\nSometimes it is useful to reverse this process to understand and confirm the properties of estimators. That means starting with known population parameters, simulating hundreds or thousands of samples from that population, and then observing point estimates and interval estimates over those samples.\n\nLinear Regression Assumptions\n\nThe population model is of the linear form \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\nThe estimation data come from a random sample or experiment\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) independently and identically distributed (i.i.d.)\n\\(x\\) has variance and there is no perfect collinearity in \\(x\\)\n\n\n\nStatistics\nIf we have one sample of data, we can estimate points and intervals with the following estimators:\nThe residual standard error is\n\\[\n\\hat\\sigma = \\frac{\\sum e_i^2}{(n - 2)}\n\\tag{5.22}\\]\nThe estimate of the slope is\n\\[\n\\hat\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\n\\tag{5.23}\\]\nThe standard error of the estimate of the slope, which can be used to calculate t-statistics and confidence intervals, is\n\\[\n\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2}\n\\tag{5.24}\\]\nThe estimate of the intercept term is\n\\[\n\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1\\bar{x}\n\\tag{5.25}\\]\nThe standard error of the intercept is\n\\[\n\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]}\n\\tag{5.26}\\]\n\n\nMonte Carlo Simulation\nConsider a simple linear regression model with the following population model:\n\\[y = 5 + 15x + \\epsilon\\]\nWe can calculate the above statistics over repeated sampling and confirm their asymptotic properties with Monte Carlo simulation.\nFirst, create 1,000 random samples from the population.\n\nset.seed(20210906)\n\ndata &lt;- map(\n  .x = 1:1000,\n  .f = ~ tibble(\n    x = rnorm(n = 10000, mean = 0, sd = 2),\n    epsilon = rnorm(n = 10000, mean = 0, sd = 10),\n    y = 5 + 15 * x + epsilon\n  )\n)\n\nNext, estimate a simple linear regression model for each draw of the population. This step includes calculating \\(\\hat\\sigma\\), \\(\\hat\\beta_1\\), \\(\\hat\\beta_0\\), \\(\\hat{SE}(\\hat\\beta_1)\\), and \\(\\hat{SE}(\\hat\\beta_0)\\).\n\nestimated_models &lt;- map(\n  .x = data,\n  .f = ~ lm(y ~ x, data = .x)\n)\n\nNext, we extract the coefficients and confidence intervals.\n\ncoefficients &lt;- map_df(\n  .x = estimated_models,\n  .f = tidy,\n  conf.int = TRUE\n)\n\nLet’s look at estimates of the residual standard error. The center of the distribution closely matches the population standard deviation of the error term.\n\nmodel_metrics &lt;- map_df(\n  .x = estimated_models,\n  .f = glance\n) \n\nmodel_metrics |&gt;\n  ggplot(aes(sigma)) +\n  geom_histogram() +\n  labs(title = \"Plot of the estimated residual standard errors\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLet’s plot the coefficients. The centers approximately match the population intercept of 5 and slope of 15.\n\ncoefficients |&gt;\n  ggplot(aes(estimate)) +\n  geom_histogram() +\n  facet_wrap(~term, scales = \"free_x\") +\n  labs(title = \"Coefficients estimates across 10,000 samples\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe standard deviation of the coefficients also matches the standard errors.\n\\[\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2} = \\sqrt\\frac{10^2}{40,000} = 0.05\\]\n\\[\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]} = \\sqrt{10^2\\left[\\frac{1}{10,000} + 0\\right]} = 0.1\\]\n\ncoefficients |&gt;\n  group_by(term) |&gt;\n  summarize(\n    mean(estimate), \n    sd(estimate)\n  )\n\n# A tibble: 2 × 3\n  term        `mean(estimate)` `sd(estimate)`\n  &lt;chr&gt;                  &lt;dbl&gt;          &lt;dbl&gt;\n1 (Intercept)             5.00         0.100 \n2 x                      15.0          0.0482\n\n\nLet’s look at how often the true parameter is inside the 95% confidence interval. It’s close although not exactly 95%.\n\ncoefficients |&gt;\n  filter(term == \"x\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 15 & conf.high &gt;= 15))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1           0.959\n\ncoefficients |&gt;\n  filter(term == \"(Intercept)\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 5 & conf.high &gt;= 5))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1            0.95\n\n\n\n\n\n5.8.5 Example 5: Queuing Example\nSuppose we have a queue at a Social Security field office. Let \\(t\\) be time. When the office opens, \\(t = 0\\) and the queue is empty.\nLet, \\(T_i\\) be the interarrival time and \\(T_i \\sim exp(\\lambda_1)\\)\nLet, \\(S_i\\) be the service time time and \\(S_I \\sim exp(\\lambda_2)\\)\nFrom these two random variables, we can calculate the arrival times, departure times, and wait times for each customer.\n\nThe arrival times are the cumulative sum of the interarrival times.\nThe wait times are zero if a person arrives after the person before them and the difference between the prior person’s departure and the current person’s arrival otherwise.\nThe departure time is arrival time plus the wait time plus the service time.\n\n\nset.seed(19920401)\nqueue &lt;- generate_queue(t = 100, lambda = 1, mu = 1)\n\nqueue\n\n# A tibble: 100 × 5\n   interarrival_time arrival_time service_time wait_time departure_time\n               &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1             0.467        0.467       5.80        0              6.27\n 2             1.97         2.43        0.0892      3.83           6.36\n 3             2.70         5.13        1.26        1.22           7.61\n 4             0.335        5.47        4.85        2.14          12.5 \n 5             0.372        5.84        1.89        6.62          14.3 \n 6             1.72         7.56        0.507       6.78          14.9 \n 7             2.28         9.84        0.932       5.01          15.8 \n 8             0.339       10.2         1.18        5.61          17.0 \n 9             2.54        12.7         1.42        4.25          18.4 \n10             0.572       13.3         0.157       5.10          18.5 \n# ℹ 90 more rows\n\n\n\nflow &lt;- tibble::tibble(\n  time = c(queue$arrival_time, queue$departure_time),\n  type = c(rep(\"arrival\", length(queue$arrival_time)), \n           rep(\"departure\", length(queue$departure_time))), \n  change = c(rep(1, length(queue$arrival_time)), rep(-1, length(queue$departure_time))),\n) |&gt;\n  arrange(time) |&gt; \n  filter(time &lt; 100) |&gt; \n  mutate(queue = cumsum(change) - 1)\n\nflow |&gt;\n  ggplot(aes(time, queue)) +\n  geom_step() +\n  labs(title = \"Simulated queue at the Social Security office\")\n\n\n\n\nThis is interesting, but it’s still only one draw from a Monte Carlo simulation. What if we are interested in the distribution of wait times for the fifth customer?\n\n#' Generate wait times at the queue\n#'\n#' @param person_number An integer for the person of interest\n#' @param iterations An integer for the number of Monte Carlo iterations\n#' @param t A t for the maximum time\n#'\n#' @return A vector of wait times\n#' \ngenerate_waits &lt;- function(person_number, iterations, t) {\n  \n  wait_time &lt;- vector(mode = \"numeric\", length = iterations)\n  for (i in seq_along(wait_time)) {\n    \n    wait_time[i] &lt;- generate_queue(t = t, lambda = 1, mu = 1)$wait_time[person_number]\n    \n  }\n  \n  return(wait_time)\n  \n}\n\nset.seed(20200908)\nwait_time &lt;- generate_waits(person_number = 5, iterations = 10000, t = 50)\n\nmean(wait_time)\n\n[1] 1.464371\n\nquantile(wait_time, probs = c(0.025, 0.5, 0.975))\n\n     2.5%       50%     97.5% \n0.0000000 0.9222193 5.8742015 \n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCreate a Monte Carlo simulation of an unfair coin toss where p = 0.6.\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nSuppose we have three independent normally-distributed random variables.\n\\[X_1 \\sim N(\\mu = 0, \\sigma = 1)\\]\n\\[X_2 \\sim N(\\mu = 1, \\sigma = 1)\\]\n\\[X_3 \\sim N(\\mu = 2, \\sigma = 1)\\]\n\nUse Monte Carlo simulation with 10,000 repetitions to estimate how often \\(X_{i1} &lt; X_{i2} &lt; X_{i3}\\).\n\n\n\n\n\n5.8.6 More examples of Monte Carlo simulation\n\nfivethirtyeight 2020 election forecast use\nU.S. Census Bureau simulation of data collection operations\n\n\nMarkov Chain Monte Carlo\nBayesian statisticians estimate posterior distributions of parameters that are combinations of prior distributions and sampling distributions. Outside of special cases, posterior distributions are difficult to identify. Accordingly, most Bayesian estimation uses an extension of Monte Carlo simulation called Markov Chain Monte Carlo or MCMC.\n\n\n\n5.8.7 One Final Note\nMonte Carlo simulations likely underestimate uncertainty. Monte Carlo simulations only capture aleatoric uncertainty and they don’t capture epistemic uncertainty.\nAleatoric uncertainty: Uncertainty due to probabilistic variety\nEpistemic uncertainty: Uncertainty due to a lack of knowledge\nIn other words, Monte Carlo simulations estimates assume the model is correct, which is almost certainly never fully true. Be transparent. Be humble."
  },
  {
    "objectID": "05_simulation-and-sampling.html#sampling-from-observed-data",
    "href": "05_simulation-and-sampling.html#sampling-from-observed-data",
    "title": "5  Simulation and Sampling",
    "section": "5.9 Sampling from Observed Data",
    "text": "5.9 Sampling from Observed Data\nUntil now, we’ve only discussed sampling from closed-form theoretical distributions. We also called this process simulation. There are many applications where we may want to sample from observed data.\nWe can break these methods into two general approaches:\n\nSampling\nResampling\n\n\n5.9.1 Sampling\n\n\n\n\n\n\nSampling\n\n\n\nSampling is the process of selecting a subset of data. Probability sampling is the process of selecting a sample when the selection uses randomization.\n\n\nSampling has many applications:\n\nReducing costs for the collection of data\nImplementing machine learning algorithms\nResampling\n\n\n\n5.9.2 Resampling\n\n\n\n\n\n\nResampling\n\n\n\nResampling is the process of repeatedly sampling from observed data to approximate the generation of new data.\n\n\nThere are at least three popular resampling methods:\n\nCross Validation: Partitioning the data and shuffling the partitions to understand the accuracy of predictive models.\nBootstrap sampling: Repeated sampling with replacement to estimate sampling distributions from observed data.\nJackknife: Leave-one-out sampling to estimate the bias and standard error of a statistic.\n\nWe focused on cross-validation for machine learning and predictive modeling in data science for public policy. We will use this approach again for predictive modeling.\nWe will also learn about bootstrap sampling when we discuss nonparametric statistics.\n\n\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical Inference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning."
  },
  {
    "objectID": "05_simulation-and-sampling.html#footnotes",
    "href": "05_simulation-and-sampling.html#footnotes",
    "title": "5  Simulation and Sampling",
    "section": "",
    "text": "(Casella and Berger 2002) offers a robust introduction to deriving maximum likelihood estimators.↩︎\nNote that the MLE for variance is biased.↩︎\nCorrelation may be more familiar than covariance. Sample correlation is standardized sample covariance. \\(Corr(\\vec{x}, \\vec{y}) = \\frac{Cov(\\vec{x}, \\vec{y})}{S_{\\vec{x}}S_{\\vec{y}}}\\). Correlation is also between -1 and 1 inclusive. Covariance can take on any real value.↩︎"
  }
]