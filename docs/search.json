[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Public Policy Part II",
    "section": "",
    "text": "Welcome\nThis book is the notes for Advanced Data Science for Public Policy in the McCourt School of Public Policy at Georgetown University."
  },
  {
    "objectID": "index.html#acknowledgements-more-to-come",
    "href": "index.html#acknowledgements-more-to-come",
    "title": "Data Science for Public Policy Part II",
    "section": "Acknowledgements (More to come!)",
    "text": "Acknowledgements (More to come!)\nThis book has benefited from many great teachers, collaborators, and students. First, I want to thank Gabe Morrison for excellent reviews and proofreading. Second, I want to thank Alex Engler and Alena Stern for collaborating on Intro to Data Science for Public Policy during six semesters and eight classes."
  },
  {
    "objectID": "01_advanced-quarto.html#sec-review",
    "href": "01_advanced-quarto.html#sec-review",
    "title": "1  Advanced Quarto",
    "section": "1.1 Review",
    "text": "1.1 Review\n\n1.1.1 Motivation\nThere are many problems worth avoiding in an analysis:\n\nCopying-and-pasting, transposing, and manual repetition\nRunning code out-of-order\nMaintaining parallel documents like a script for analysis and a doc for narrative\nCode written for computers that is tough to parse by humans\n\nNot convinced? Maybe we just want to make cool stuff like websites, blogs, books, and slide decks.\nQuarto, a literate statistical programming framework for R, Python, and Julia helps us solve many of these problems. Quarto uses\n\nplain text files ending in .qmd that are similar to .R and .Rmd files\nlibrary(knitr)\npandoc1\n\nQuarto uses library(knitr) and pandoc to convert plain text .qmd documents into rich output documents like these class notes. The “Render” button appears in RStudio with a .qmd file is open in the editor window.\nClicking the “Render” button begins the process of rendering .qmd files.\n\n\n\n\n\n\n\n\n\nWhen the button is clicked, Quarto calls library(knitr) and renders .qmd (Quarto files) into .md (Markdown files), which Pandoc then converts into any specified output type. Quarto and library(knitr) don’t need to be explicitly loaded as the entire process is handled by clicking the “Render” button in RStudio.\n\n\n\n\n\n\n\n\n\nSource: Quarto website\nQuarto, library(knitr), and Pandoc are all installed with RStudio. You will need to install a LaTeX distribution to render PDFs. We recommend library(tinytex) as a LaTeX distribution (installation instructions).\n\n\n\n\n\n\nExercise 1\n\n\n\n\nClick the new script button in RStudio and add a “Quarto Document”.\nGive the document a name, an author, and ensure that HTML is selected.\nSave the document as “hello-quarto.qmd”.\nClick “Render”.\n\n\n\nQuarto has three main ingredients:\n\nYAML header\nMarkdown text\nCode chunks\n\n\n\n1.1.2 (1) YAML Header\nYAML stands for “yet another markup language”. The YAML header contains meta information about the document including output type, document settings, and parameters that can be passed to the document. The YAML header starts with --- and ends with ---.\nHere is the simplest YAML header for a PDF document:\n---\nformat: pdf\n---\nYAML headers can contain many output specific settings. This YAML header creates an HTML document with code folding and a floating table of contents:\n---\nformat: \n  html:\n    embed-resources: true\n    code-fold: true\n    toc: true\n---  \nParameters can be specified as follows\n---\nformat: pdf\nparams:\n  state: \"Virginia\"\n---\nNow state can be referred to anywhere in R code as params$state. Parameters are useful for a couple of reasons:\n\nWe can clearly change key values for a Quarto document in the YAML header.\nWe can create a template and programmatically iterate the template over a set of values with the quarto_render() function and library(purrr). This blog outlines the idea. The Mobility Metrics Data Tables and SLFI State Fiscal Briefs are key examples of this workflow.\n\n\n\n\n\n\n\nWarning\n\n\n\nUnlike R Markdown, images and other content are not embedded in .html from Quarto by default. Be sure to include embed-resources: true in YAML headers to embed content and make documents easier to share.\nSuppose we embed an image called image.png in a Quarto document called example.qmd, which, when rendered, creates example.html. If we don’t include embed-resources: true, then we will need to share image.png and example.html to see the embedded image. This is also true for other files like .css.\n\n\n\n\n1.1.3 (2) Markdown text\nMarkdown is a shortcut for HyperText Markup Language (HTML). Essentially, simple meta characters corresponding to formatting are added to plain text.\nTitles and subtitltes\n------------------------------------------------------------\n\n# Title 1\n\n## Title 2\n\n### Title 3\n\n\nText formatting \n------------------------------------------------------------\n\n*italic*  \n\n**bold**   \n\n`code`\n\nLists\n------------------------------------------------------------\n\n* Bulleted list item 1\n* Item 2\n  * Item 2a\n  * Item 2b\n\n1. Item 1\n2. Item 2\n\nLinks and images\n------------------------------------------------------------\n\n[text](http://link.com)\n\n![Penguins](images/penguins.png)\n\n\n1.1.4 (3) Code chunks\n\n\n\n\n\nMore frequently, code is added in code chunks:\n\n```{r}\n2 + 2\n```\n\n[1] 4\n\n\nThe first argument inline or in a code chunk is the language engine. Most commonly, this will just be a lower case r. knitr allows for many different language engines:\n\nR\nJulia\nPython\nSQL\nBash\nRcpp\nStan\nJavascript\nCSS\n\nQuarto has a rich set of options that go inside of the chunks and control the behavior of Quarto.\n\n```{r}\n#| label: important-calculation\n#| eval: false\n\n2 + 2\n```\n\nIn this case, eval makes the code not run. Other chunk-specific settings can be added inside the brackets. Here2 are the most important options:\n\n\n\nOption\nEffect\n\n\n\n\necho: false\nHides code in output\n\n\neval: false\nTurns off evaluation\n\n\noutput: false\nHides code output\n\n\nwarning: false\nTurns off warnings\n\n\nmessage: false\nTurns off messages\n\n\nfig-height: 8\nChanges figure width in inches3\n\n\nfig-width: 8\nChanges figure height in inches4\n\n\n\nDefault settings for the entire document can be changed in the YAML header with the execute option:\nexecute:\n  warning: false\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAdd date: today to your YAML header after title. This will update every time the document is rendered.\nCopy the Markdown table from this table generator and add it to your .qmd document.\nCreate a scatter plot of the cars data with library(ggplot2). Adjust the figure width and height using options within the chunk.\nClick “Render”.\n\n\n\n\n\n1.1.5 Organizing a Quarto Document\nIt is important to clearly organize a Quarto document and the constellation of files that typically support an analysis.\n\nAlways use .Rproj files.\nUse sub-directories to sort images, .css, data.\n\nLater, we will learn how to use library(here) to effectively organize sub-directories."
  },
  {
    "objectID": "01_advanced-quarto.html#math-notation",
    "href": "01_advanced-quarto.html#math-notation",
    "title": "1  Advanced Quarto",
    "section": "1.2 Math Notation",
    "text": "1.2 Math Notation\nThis course uses probability and statistics. Occasionally, we want to easily communicate with mathematical notation. For example, it may be convenient to type that \\(X\\) is a random variable that follows a standard normal distribution (mean = 0 and standard deviation = 1).\n\\[X \\sim N(\\mu = 0, \\sigma = 1)\\]\n\n1.2.1 Math Mode\nUse $ to start and stop in-line math notation and $$ to start multi-line math notation. Math notation uses LaTeX’s syntax for mathematical notation.\nHere’s an example with in-line math:\nConsider a binomially distributed random variable, $X \\sim binom(n, p)$. \nConsider a binomially distributed random variable, \\(X \\sim binom(n, p)\\).\nHere’s an example with a chunk of math:\n$$\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n$${#eq-binomial}\n\\[\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n\\tag{1.1}\\]\n\n\n1.2.2 Important Syntax\nMath mode recognizes basic math symbols available on your keyboard including +, -, *, /, &gt;, &lt;, (, and ).\nMath mode contains all greek letters. For example, \\alpha (\\(\\alpha\\)) and \\beta (\\(\\beta\\)).\n\n\nTable 1.1: My Caption\n\n\nLaTeX\nSymbol\n\n\n\n\n\\alpha\n\\(\\alpha\\)\n\n\n\\beta\n\\(\\beta\\)\n\n\n\\gamma\n\\(\\gamma\\)\n\n\n\\Delta\n\\(\\Delta\\)\n\n\n\\epsilon\n\\(\\epsilon\\)\n\n\n\\theta\n\\(\\theta\\)\n\n\n\\pi\n\\(\\pi\\)\n\n\n\\sigma\n\\(\\sigma\\)\n\n\n\\chi\n\\(\\chi\\)\n\n\n\n\nMath mode also recognizes \\(\\log(x)\\) (\\log(x)) and \\(\\sqrt{x}\\) (\\sqrt{x}).\nSuperscripts (^) are important for exponentiation and subscripts (_) are important for adding indices. y = x ^ 2 renders as \\(y = x ^ 2\\) and x_1, x_2, x_3 renders as \\(x_1, x_2, x_3\\). Brackets are useful for multi-character superscripts and subscripts like \\(s_{11}\\) (s_{11}).\nIt is useful to add symbols to letters. For example, \\bar{x} is useful for sample means (\\(\\bar{x}\\)), \\hat{y} is useful for predicted values (\\(\\hat{y}\\)), and \\vec{\\beta} is useful for vectors of coefficients (\\(\\vec{\\beta}\\)).\nMath mode supports fractions with \\frac{x}{y} (\\(\\frac{x}{y}\\)), big parentheses with \\left(\\right) (\\(\\left(\\right)\\)), and brackets with \\left[\\right] (\\(\\left[\\right]\\)).\nMath mode has a symbol for summation. Let’s combine it with bars, fractions, subscripts, and superscipts to show sample mean \\bar{x} = \\frac{1}{n}\\sum_i^n x_i, which looks like \\(\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\).\n\\sim is how to add the tilde for distributed as. For example, X \\sim N(\\mu = 0, \\sigma = 1) shows the normal distribution \\(X \\sim N(\\mu = 0, \\sigma = 1)\\).\nMatrices are are a little bit more work in math mode. Consider the follow variance-covariance matrix:\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\[\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\]\nThis guide provides and exhaustive look at math options in Quarto.\n\n\n\n\n\n\nWarning\n\n\n\nMath mode is finicky! Small errors like mismatched parentheses or superscript and subscript errors will cause Quarto documents to fail to render. Write math carefully and render early and often.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse math mode to type out the equation for root mean square error (RMSE).\nDo you divide by n or n - 1?"
  },
  {
    "objectID": "01_advanced-quarto.html#cross-references",
    "href": "01_advanced-quarto.html#cross-references",
    "title": "1  Advanced Quarto",
    "section": "1.3 Cross References",
    "text": "1.3 Cross References\nCross references are useful for organizing documents that include sections, figures, tables, and equations. Cross references create hyperlinks within documents that jump to the locations of these elements. Linking sections, figures, tables, or equations helps readers navigate the document.\nCross references also automatically number the referenced elements. This means that if there are two tables (ie. Table 1 and Table 2) and a table is added between the two tables, all of the table numbers and references to the tables will automatically update.\nCross references require two bits of code within a Quarto document:\n\nA label associated with the section, figure, table, or equation.\nA reference to the labelled section, figure, table, or equation.\n\nLabels are written in brackets or as arguments in code chunks, and begin with the the type object being linked. References begin with @ followed by the label of object being linked.\n\n1.3.1 Sections\nLinking sections helps readers navigate between sections. Use brackets to label sections after headers and always begin labels with sec-. Then you can reference that section with @sec-.\n## Review {#sec-review}\n\nSee @sec-review if you are totally lost.\nThe cross references shows up like this: See Section 1.1 if you are totally lost.\nIt can be helpful to turn on section numbering with number-sections: true in the YAML header. Additionally, Markdown has a native method for linking between sections.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nAdd a few section headers to your Quarto document.\nAdd a cross reference to one of the section headers.\n\n\n\n\n\n1.3.2 Figures\n\n\n\nFigure 1.1: Penguins\n\n\nWe can reference figures like Figure 1.1 with @fig-penguins.\n\n\n1.3.3 Tables\nWe can link to tables in our documents. For example, we can link to the greek table with @tbl-greek Table 1.1.\n\n\n1.3.4 Equations\nWe can link to equations in our documents. For example, we can link to the binomial distribution earlier with @eq-binomial Equation 5.4.\n\n\n\n\n\n\nExercise 5\n\n\n\n\nAdd a cross reference to your RMSE equation from earlier."
  },
  {
    "objectID": "01_advanced-quarto.html#citations",
    "href": "01_advanced-quarto.html#citations",
    "title": "1  Advanced Quarto",
    "section": "1.4 Citations",
    "text": "1.4 Citations\n\n1.4.1 Zotero\nZotero is a free and open-source software for organizing research and managing citations.\n\n\n\n\n\n\nDigital Object Identifier (DOI)\n\n\n\nDOIs are persistent identifiers that uniquely identify objects including many academic papers. For example, 10.1198/jcgs.2009.07098 identifies “A Layered Grammar of Graphics” by Hadley Wickham.\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\nInstall Zotero.\nFind the DOI for “Tidy Data” by Hadley Wickham.\nClick the magic wand in Zotero and paste the DOI.\n\n\n\n\n\n\n\n\n\n\n\nReview the new entry in Zotero.\n\n\n\n\n\n1.4.2 Zotero Integration\nZotero has a powerful integration with Quarto. In practice, it’s one click to add a DOI to Zotero and then one click to add a citation to Quarto.\nRStudio automatically adds My Library from Zotero. Simply switch to the Visual Editor (top left in RStudio), click “Insert”, and click “Citation”. This will open a prompt to insert a citation into the Quarto document.\nThe citation is automatically added with parentheses to go at the end of sentences. Delete the square brackets to convert the citation to an in-line citation.\nInserting the citation automatically adds the citation to the references section. Deleting the reference automatically deletes the citation from the references section.\nZotero Groups are useful for sharing citations and Zotero Group Libraries need to be added to RStudio. To set this up:\nTo set this up, in RStudio:\n\nGo to Tools and select “Global Options”\nSelect “RMarkdown” and then click “Citations”\nFor “Use Libraries” choose “Selected Libraries”\nSelect the group libraries to add\n\n\n\n\n\n\n\nExercise 7\n\n\n\n\nCite “Tidy Data” by Hadley Wickham in your Quarto document.\nClick “Render”"
  },
  {
    "objectID": "01_advanced-quarto.html#more-resources",
    "href": "01_advanced-quarto.html#more-resources",
    "title": "1  Advanced Quarto",
    "section": "1.5 More Resources",
    "text": "1.5 More Resources\n\nQuarto Guide\nIterating fact sheets and web pages with Quarto"
  },
  {
    "objectID": "01_advanced-quarto.html#footnotes",
    "href": "01_advanced-quarto.html#footnotes",
    "title": "1  Advanced Quarto",
    "section": "",
    "text": "Pandoc is free software that converts documents between markup formats. For example, Pandoc can convert files to and from markdown, LaTeX, jupyter notebook (ipynb), and Microsoft Word (.docx) formats, among many others. You can see a comprehensive list of files Pandoc can convert on their About Page.↩︎\nThis table was typed as Markdown code. But sometimes it is easier to use a code chunk to create and print a table. Pipe any data frame into knitr::kable() to create a table that will be formatted in the output of a rendered Quarto document.↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more.↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more.↩︎"
  },
  {
    "objectID": "02_advanced-data-cleaning.html#sec-review2",
    "href": "02_advanced-data-cleaning.html#sec-review2",
    "title": "2  Advanced Data Cleaning",
    "section": "2.1 Review",
    "text": "2.1 Review\nR for Data Science (2e) displays the first steps of the data science process as “Import”, “Tidy”, and “Transform”. DSPP1 introduced important techniques for importing data like read_csv() and querying web APIs, for tidying data like pivot_longer(), and for transforming data like mutate().\n\n\n\n\n\n\nExercise 1\n\n\n\n\nUse mutate() and case_when() to add a new variable called speed_cat to cars where the values are \"slow\" when speed &lt; 10, \"moderate\" when speed &lt; 20, and \"fast\" otherwise."
  },
  {
    "objectID": "02_advanced-data-cleaning.html#sec-import",
    "href": "02_advanced-data-cleaning.html#sec-import",
    "title": "2  Advanced Data Cleaning",
    "section": "2.2 Import",
    "text": "2.2 Import\n\n2.2.1 library(here)\nDeveloping Quarto documents in subdirectories is a pain. When interactively running code in the console, file paths are read as if the .qmd file is in the same folder as the .Rproj. When clicking render, paths are treated as if they are in the subdirectory where the .qmd file is.\nlibrary(here) resolves headaches around file referencing in project-oriented workflows.\nLoading library(here) will print your working directory.\n\nlibrary(here)\n\nhere() starts at /Users/aaronwilliams/presentations/data-science-for-public-policy2\n\n\nAfter this, here() will use reasonable heuristics to find project files using relative file paths. When placing Quarto documents in a directory below the top-level directory, use here() and treat each folder and file as a different string.\nBefore\n\nread_csv(\"data/raw/important-data.csv\")\n\nAfter\n\nread_csv(here(\"data\", \"raw\", \"important-data.csv\"))\n\n\n\n2.2.2 library(readxl)\nWe will focus on reading data from Excel workbooks. Excel is a bad tool with bad design that has led to many analytical errors. Unfortunately, it’s a dominant tool for storing data and often enters the data science workflow.\nlibrary(readxl) is the premier package for reading data from .xls and .xlsx files. read_excel(), which works like read_csv(), loads data from .xls and .xlsx files. Consider data from the Urban Institute’s Debt in America feature accessed through the Urban Institute Data Catalog.\n\nlibrary(readxl)\n\nread_excel(here(\"data\", \"state_dia_delinquency_ 7 Jun 2022.xlsx\"))\n\n# A tibble: 51 × 28\n   fips  state_name          state Share with Any Debt …¹ Share with Any Debt …²\n   &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;                  &lt;chr&gt;                 \n 1 01    Alabama             AL    .3372881               .5016544              \n 2 02    Alaska              AK    .1672429               .221573               \n 3 04    Arizona             AZ    .2666938               .3900013              \n 4 05    Arkansas            AR    .3465793               .5426918              \n 5 06    California          CA    .2087713               .2462195              \n 6 08    Colorado            CO    .213803                .3554938              \n 7 09    Connecticut         CT    .2194708               .3829038              \n 8 10    Delaware            DE    .2866829               .469117               \n 9 11    District of Columb… DC    .2232908               .3485817              \n10 12    Florida             FL    .2893825               .3439322              \n# ℹ 41 more rows\n# ℹ abbreviated names: ¹​`Share with Any Debt in Collections, All`,\n#   ²​`Share with Any Debt in Collections, Communities of Color`\n# ℹ 23 more variables:\n#   `Share with Any Debt in Collections, Majority White Communities` &lt;chr&gt;,\n#   `Median Debt in Collections, All` &lt;chr&gt;,\n#   `Median Debt in Collections, Communities of Color` &lt;chr&gt;, …\n\n\nread_excel() has several useful arguments:\n\nsheet selects the sheet to read.\nrange selects the cells to read and can use Excel-style ranges like “C34:D50”.\nskip skips the selected number of rows.\nn_max selects the maximum number of rows to read.\n\nExcel encourages bad habits and untidy data, so these arguments are useful for extracting data from messy Excel workbooks.\nreadxl_example() contains a perfect example. The workbook contains two sheets, which we can see with excel_sheets().\n\nreadxl_example(\"clippy.xlsx\") |&gt;\n  excel_sheets()\n\n[1] \"list-column\"    \"two-row-header\"\n\n\nAs is common with many Excel workbooks, the second sheet contains a second row of column names with parenthetical comments about each column.1\n\nreadxl_example(\"clippy.xlsx\") |&gt;  \n  read_excel(sheet = \"two-row-header\")\n\n# A tibble: 2 × 4\n  name       species              death                 weight    \n  &lt;chr&gt;      &lt;chr&gt;                &lt;chr&gt;                 &lt;chr&gt;     \n1 (at birth) (office supply type) (date is approximate) (in grams)\n2 Clippy     paperclip            39083                 0.9       \n\n\nThis vignette suggests a simple solution to this problem.\n\n# extract the column names\ncol_names &lt;- readxl_example(\"clippy.xlsx\") |&gt;  \n  read_excel(sheet = \"two-row-header\", n_max = 0) |&gt;\n  names()\n\n# load the data and add the column names\nreadxl_example(\"clippy.xlsx\") |&gt;  \n    read_excel(\n      sheet = \"two-row-header\", \n      skip = 2,\n      col_names = col_names\n    )\n\n# A tibble: 1 × 4\n  name   species   death               weight\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dttm&gt;               &lt;dbl&gt;\n1 Clippy paperclip 2007-01-01 00:00:00    0.9\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nThe IRS Statistics of Income Division is one of the US’s 13 principal statistical agencies. They publish rich information derived from tax returns. We will focus on Table 1, Adjusted Gross Income (AGI) percentiles by state.\n\nRead in the 52 cells in the first column that contain “United States”, all 50 states, and the “District of Columbia”.\nIdentify the cells containing data for “Adjusted gross income floor on percentiles”. Read in the data with read_excel(). Either programmatically read in the column names (i.e. “Top 1 Percent”, …) or assign them with col_names().\nUse bind_cols() to combine the data from step 1 and step 2.\n\n\n\nlibrary(tidyxl) contains tools for working with messy Excel workbooks, library(openxlsx) contains tools for creating Excel workbooks with R, and library(googlesheets4) contains tools for working with Google Sheets."
  },
  {
    "objectID": "02_advanced-data-cleaning.html#sec-tidy",
    "href": "02_advanced-data-cleaning.html#sec-tidy",
    "title": "2  Advanced Data Cleaning",
    "section": "2.3 Tidy",
    "text": "2.3 Tidy\nThe defining opinion of the tidyverse is its wholehearted adoption of tidy data. Tidy data has three features:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a dataframe.\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. ~ Hadley Wickham\n\nlibrary(tidyr) is the main package for tidying untidy data. We’ll practice some skills using examples from three workbooks from the IRS SOI.\npivot_longer() is commonly used for tidying data and for making data longer for library(ggplot2). pivot_longer() reorients data so that key-value pairs expressed as column name-column value are column value-column value in adjacent columns. pivot_longer() has three essential arguments:\n\ncols is a vector of columns to pivot (or not pivot).\nnames_to is a string for the name of the column where the old column names will go (i.e. “series” in the figure).\nvalues_to is a string for the name of the column where the values will go (i.e. “rate” in the figure).\n\n\n\n\n\n\npivot_wider() is the inverse of pivot_longer().\n\nTidying Example 1\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable1 &lt;- tribble(\n  ~state, ~agi2006, ~agi2016, ~agi2020,\n  \"Alabama\", 95067, 114510, 138244,\n  \"Alaska\", 17458, 23645, 26445,\n  \"Arizona\", 146307, 181691, 245258\n)\n\ntable1\n\n# A tibble: 3 × 4\n  state   agi2006 agi2016 agi2020\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Alabama   95067  114510  138244\n2 Alaska    17458   23645   26445\n3 Arizona  146307  181691  245258\n\n\n\n\nYear is a variable. This data is untidy because year is included in the column names.\n\ntable1 &lt;- tribble(\n  ~state, ~agi2006, ~agi2016, ~agi2020,\n  \"Alabama\", 95067, 114510, 138244,\n  \"Alaska\", 17458, 23645, 26445,\n  \"Arizona\", 146307, 181691, 245258\n)\n\ntable1\n\n# A tibble: 3 × 4\n  state   agi2006 agi2016 agi2020\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Alabama   95067  114510  138244\n2 Alaska    17458   23645   26445\n3 Arizona  146307  181691  245258\n\npivot_longer(\n  data = table1, \n  cols = -state, \n  names_to = \"year\", \n  values_to = \"agi\"\n)\n\n# A tibble: 9 × 3\n  state   year       agi\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama agi2006  95067\n2 Alabama agi2016 114510\n3 Alabama agi2020 138244\n4 Alaska  agi2006  17458\n5 Alaska  agi2016  23645\n6 Alaska  agi2020  26445\n7 Arizona agi2006 146307\n8 Arizona agi2016 181691\n9 Arizona agi2020 245258\n\n\nThe year column isn’t useful yet. We’ll fix that later.\n\n\n\n\nlibrary(tidyr) contains several functions to split values into multiple cells.\n\nseparate_wider_delim() separates a value based on a delimeter and creates wider data.\nseparate_wider_position() separates a value based on position and creates wider data.\nseparate_longer_delim() separates a value based on a delimeter and creates longer data.\nseparate_longer_position() separates a value based on position and creates longer data.\n\n\n\nTidying Example 2\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable2 &lt;- tribble(\n  ~state, ~`agi2006|2016|2020`,\n  \"Alabama\", \"95067|114510|138244\",\n  \"Alaska\", \"17458|23645|26445\",\n  \"Arizona\", \"146307|181691|245258\"\n)\n\ntable2\n\n# A tibble: 3 × 2\n  state   `agi2006|2016|2020` \n  &lt;chr&gt;   &lt;chr&gt;               \n1 Alabama 95067|114510|138244 \n2 Alaska  17458|23645|26445   \n3 Arizona 146307|181691|245258\n\n\n\n\nThe values for 2006, 2016, and 2020 are all squished into one cell.\n\ntable2 &lt;- tribble(\n  ~state, ~`agi2006|2016|2020`,\n  \"Alabama\", \"95067|114510|138244\",\n  \"Alaska\", \"17458|23645|26445\",\n  \"Arizona\", \"146307|181691|245258\"\n)\n\ntable2\n\n# A tibble: 3 × 2\n  state   `agi2006|2016|2020` \n  &lt;chr&gt;   &lt;chr&gt;               \n1 Alabama 95067|114510|138244 \n2 Alaska  17458|23645|26445   \n3 Arizona 146307|181691|245258\n\nseparate_wider_delim(\n  data = table2, \n  cols = `agi2006|2016|2020`, \n  delim = \"|\",\n  names = c(\"2006\", \"2016\", \"2020\")\n) |&gt;\n  pivot_longer(\n    cols = -state,\n    names_to = \"year\", \n    values_to = \"agi\"\n  )\n\n# A tibble: 9 × 3\n  state   year  agi   \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; \n1 Alabama 2006  95067 \n2 Alabama 2016  114510\n3 Alabama 2020  138244\n4 Alaska  2006  17458 \n5 Alaska  2016  23645 \n6 Alaska  2020  26445 \n7 Arizona 2006  146307\n8 Arizona 2016  181691\n9 Arizona 2020  245258\n\n\n\n\n\n\nbind_rows() combines data frames by stacking the rows.\n\none &lt;- tribble(\n  ~id, ~var,\n  \"1\", 3.14,\n  \"2\", 3.15,\n)\n\ntwo &lt;- tribble(\n  ~id, ~var,\n  \"3\", 3.16,\n  \"4\", 3.17,\n)\n\nbind_rows(one, two)\n\n# A tibble: 4 × 2\n  id      var\n  &lt;chr&gt; &lt;dbl&gt;\n1 1      3.14\n2 2      3.15\n3 3      3.16\n4 4      3.17\n\n\nbind_cols() combines data frames by appending columns.\n\nthree &lt;- tribble(\n  ~id, ~var1,\n  \"1\", 3.14,\n  \"2\", 3.15,\n)\n\nfour &lt;- tribble(\n  ~id, ~var2,\n  \"1\", 3.16,\n  \"2\", 3.17,\n)\n\nbind_cols(three, four)\n\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...3`\n\n\n# A tibble: 2 × 4\n  id...1  var1 id...3  var2\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 1       3.14 1       3.16\n2 2       3.15 2       3.17\n\n\nWhen possible, we recommend using relational joins like left_join() to combine by columns because it is easy to miss-align rows with bind_cols().\n\nleft_join(\n  x = three,\n  y = four,\n  by = \"id\"\n)\n\n# A tibble: 2 × 3\n  id     var1  var2\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1      3.14  3.16\n2 2      3.15  3.17\n\n\n\n\nTidying Example 3\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable3_2006 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", \"95067\",\n  \"Alaska\", \"17458\",\n  \"Arizona\", \"146307\"\n)\n\ntable3_2006\n\n# A tibble: 3 × 2\n  state   agi   \n  &lt;chr&gt;   &lt;chr&gt; \n1 Alabama 95067 \n2 Alaska  17458 \n3 Arizona 146307\n\ntable3_2016 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", \"114510\",\n  \"Alaska\", \"23645\",\n  \"Arizona\", \"181691\"\n)\n\ntable3_2016\n\n# A tibble: 3 × 2\n  state   agi   \n  &lt;chr&gt;   &lt;chr&gt; \n1 Alabama 114510\n2 Alaska  23645 \n3 Arizona 181691\n\ntable3_2020 &lt;- tribble(\n  ~state, ~`agi`,\n  \"Alabama\", \"138244\",\n  \"Alaska\", \"26445\",\n  \"Arizona\", \"245258\"\n)\n\ntable3_2020\n\n# A tibble: 3 × 2\n  state   agi   \n  &lt;chr&gt;   &lt;chr&gt; \n1 Alabama 138244\n2 Alaska  26445 \n3 Arizona 245258\n\n\n\n\nThe variable year is contained in the data set names. The .id argument in bind_rows() allows us to create the year variable.\n\ntable3_2006 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 95067,\n  \"Alaska\", 17458,\n  \"Arizona\", 146307\n)\n\ntable3_2006\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama  95067\n2 Alaska   17458\n3 Arizona 146307\n\ntable3_2016 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 114510,\n  \"Alaska\", 23645,\n  \"Arizona\", 181691\n)\n\ntable3_2016\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama 114510\n2 Alaska   23645\n3 Arizona 181691\n\ntable3_2020 &lt;- tribble(\n  ~state, ~`agi`,\n  \"Alabama\", 138244,\n  \"Alaska\", 26445,\n  \"Arizona\", 245258\n)\n\ntable3_2020\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama 138244\n2 Alaska   26445\n3 Arizona 245258\n\nbind_rows(\n  `2006` = table3_2006,\n  `2016` = table3_2016,\n  `2020` = table3_2020,\n  .id = \"year\"\n)\n\n# A tibble: 9 × 3\n  year  state      agi\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 2006  Alabama  95067\n2 2006  Alaska   17458\n3 2006  Arizona 146307\n4 2016  Alabama 114510\n5 2016  Alaska   23645\n6 2016  Arizona 181691\n7 2020  Alabama 138244\n8 2020  Alaska   26445\n9 2020  Arizona 245258\n\n\n\n\n\n\nRelational joins are fundamental to working with tidy data. Tidy data can only contain one unit of observation (e.g. county or state not county and state). When data exist on multiple levels, they must be stored in separate tables that can later be combined.\n\n\n\n\n\n\nMutating Joins\n\n\n\nMutating joins add new variables to a data frame by matching observations from one data frame to observations in another data frame.\n\n\n\n\n\n\n\n\nFiltering Joins\n\n\n\nFiltering joins drop observations based on the presence of their key (identifier) in another data frame.\nFor example, we may have a list of students in detention and a list of all students. We can use a filtering join to create a list of student not in detention.\n\n\nFor now, we will focus on mutating joins. Let their be two data frames x and y and let both data frames have a key variable that uniquely identifies rows.\n\nleft_join(x, y) appends variables from y on to x but only keeps observations from x.\nfull_join(x, y) appends variables from y on to x and keeps all observations from x and y.\nanti_join(x, y) returns all observations from x without a match in y. anti_join() is traditionally only used for filtering joins, but it is useful for writing tests for mutating joins.\n\nTo learn more, read the Joins chapter of R for Data Science (2e). library(tidylog) is a useful function for monitoring the behavior of joins.\n\n\nTidying Example 4\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable4a &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 95067,\n  \"Alaska\", 17458,\n  \"Arizona\", 146307\n)\n\ntable4a\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama  95067\n2 Alaska   17458\n3 Arizona 146307\n\ntable4b &lt;- tribble(\n  ~state, ~returns,\n  \"Alabama\", 1929941,\n  \"Alaska\", 322369,\n  \"Arizona\", 2454951\n)\n\ntable4b\n\n# A tibble: 3 × 2\n  state   returns\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Alabama 1929941\n2 Alaska   322369\n3 Arizona 2454951\n\n\n\n\nThese data are tidy! But keeping the data in two separate data frames may not make sense. Let’s use full_join() to combine the data and anti_join() to see if there are mismatches.\n\ntable4a &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 95067,\n  \"Alaska\", 17458,\n  \"Arizona\", 146307\n)\n\ntable4a\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama  95067\n2 Alaska   17458\n3 Arizona 146307\n\ntable4b &lt;- tribble(\n  ~state, ~returns,\n  \"Alabama\", 1929941,\n  \"Alaska\", 322369,\n  \"Arizona\", 2454951\n)\n\ntable4b\n\n# A tibble: 3 × 2\n  state   returns\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Alabama 1929941\n2 Alaska   322369\n3 Arizona 2454951\n\nfull_join(table4a, table4b, by = \"state\")\n\n# A tibble: 3 × 3\n  state      agi returns\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 Alabama  95067 1929941\n2 Alaska   17458  322369\n3 Arizona 146307 2454951\n\nanti_join(table4a, table4b, by = \"state\")\n\n# A tibble: 0 × 2\n# ℹ 2 variables: state &lt;chr&gt;, agi &lt;dbl&gt;\n\nanti_join(table4b, table4a, by = \"state\")\n\n# A tibble: 0 × 2\n# ℹ 2 variables: state &lt;chr&gt;, returns &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse pivot_longer() to make the SOI percentile data from the earlier exercise longer. After the transformation, there should be one row per percentile per state.\n\n\n\nTo see more examples, read the tidy data section in R for Data Science (2e)"
  },
  {
    "objectID": "02_advanced-data-cleaning.html#sec-transform",
    "href": "02_advanced-data-cleaning.html#sec-transform",
    "title": "2  Advanced Data Cleaning",
    "section": "2.4 Transform",
    "text": "2.4 Transform\n\n2.4.1 Strings\nCheck out the stringr cheat sheet.\nlibrary(stringr) contains powerful functions for working with strings in R. In data analysis, we may need to detect matches, subset strings, work with the lengths of strings, modify strings, and join and split strings.\n\nDetecting Matches\nstr_detect() is useful for detecting matches in strings, which can be useful with filter(). Consider the executive orders data set and suppose we want to return executive orders that contain the word \"Virginia\".\n\neos &lt;- read_csv(here(\"data\", \"executive-orders.csv\")) |&gt;\n  filter(!is.na(text)) |&gt;\n  group_by(executive_order_number) |&gt;\n  summarize(text = list(text)) |&gt;\n  mutate(text = map_chr(text, ~paste(.x, collapse = \" \")))\n\nRows: 196537 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): text, president\ndbl  (1): executive_order_number\ndate (1): signing_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\neos\n\n# A tibble: 1,126 × 2\n   executive_order_number text                                                  \n                    &lt;dbl&gt; &lt;chr&gt;                                                 \n 1                  12890 \"Executive Order 12890 of December 30, 1993 Amendment…\n 2                  12944 \"Executive Order 12944 of December 28, 1994 Adjustmen…\n 3                  12945 \"Executive Order 12945 of January 20, 1995 Amendment …\n 4                  12946 \"Executive Order 12946 of January 20, 1995 President'…\n 5                  12947 \"Executive Order 12947 of January 23, 1995 Prohibitin…\n 6                  12948 \"Executive Order 12948 of January 30, 1995 Amendment …\n 7                  12949 \"Executive Order 12949 of February 9, 1995 Foreign In…\n 8                  12950 \"Executive Order 12950 of February 22, 1995 Establish…\n 9                  12951 \"Executive Order 12951 of February 22, 1995 Release o…\n10                  12952 \"Executive Order 12952 of February 24, 1995 Amendment…\n# ℹ 1,116 more rows\n\neos |&gt;\n  filter(str_detect(string = text, pattern = \"Virginia\"))\n\n# A tibble: 6 × 2\n  executive_order_number text                                                   \n                   &lt;dbl&gt; &lt;chr&gt;                                                  \n1                  13150 Executive Order 13150 of April 21, 2000 Federal Workfo…\n2                  13508 Executive Order 13508 of May 12, 2009 Chesapeake Bay P…\n3                  13557 Executive Order 13557 of November 4, 2010 Providing an…\n4                  13775 Executive Order 13775 of February 9, 2017 Providing an…\n5                  13787 Executive Order 13787 of March 31, 2017 Providing an O…\n6                  13934 Executive Order 13934 of July 3, 2020 Building and Reb…\n\n\n\n\nSubsetting Strings\nstr_sub() can subset strings based on positions within the string. Consider an example where we want to extract state FIPS codes from county FIPS codes.\n\ntibble(fips = c(\"01001\", \"02013\", \"04001\")) |&gt;\n  mutate(state_fips = str_sub(fips, start = 1, end = 2))\n\n# A tibble: 3 × 2\n  fips  state_fips\n  &lt;chr&gt; &lt;chr&gt;     \n1 01001 01        \n2 02013 02        \n3 04001 04        \n\n\n\n\nManaging Lengths\nstr_pad() is useful for managing lengths. Consider the common situation when zeros are dropped from the beginning of FIPS codes.\n\ntibble(fips = c(1, 2, 4)) |&gt;\n  mutate(fips = str_pad(fips, side = \"left\", pad = \"0\", width = 2))\n\n# A tibble: 3 × 1\n  fips \n  &lt;chr&gt;\n1 01   \n2 02   \n3 04   \n\n\n\n\nModifying Strings\nstr_replace(), str_replace_all(), str_remove(), and str_remove_all() can delete or modify parts of strings. Consider an example where we have course names and we want to delete everything except numeric digits.2\n\ntibble(course = c(\"PPOL 670\", \"GOVT 8009\", \"PPOL 6819\")) |&gt;\n  mutate(course = str_remove(course, pattern = \"[:alpha:]*\\\\s\"))\n\n# A tibble: 3 × 1\n  course\n  &lt;chr&gt; \n1 670   \n2 8009  \n3 6819  \n\n\nstr_c() and str_glue() are useful for joining strings. Consider an example where we want to “fill in the blank” with a variable in a data frame.\n\ntibble(fruit = c(\"apple\", \"banana\", \"cantelope\")) |&gt;\n  mutate(sentence = str_glue(\"my favorite fruit is {fruit}\"))\n\n# A tibble: 3 × 2\n  fruit     sentence                      \n  &lt;chr&gt;     &lt;glue&gt;                        \n1 apple     my favorite fruit is apple    \n2 banana    my favorite fruit is banana   \n3 cantelope my favorite fruit is cantelope\n\n\n\ntibble(fruit = c(\"apple\", \"banana\", \"cantelope\")) |&gt;\n  mutate(\n    another_sentence = \n      str_c(\"Who doesn't like a good \", fruit, \".\")\n    )\n\n# A tibble: 3 × 2\n  fruit     another_sentence                  \n  &lt;chr&gt;     &lt;chr&gt;                             \n1 apple     Who doesn't like a good apple.    \n2 banana    Who doesn't like a good banana.   \n3 cantelope Who doesn't like a good cantelope.\n\n\nThis workflow is useful for building up URLs when accessing APIs, scraping information from the Internet, and downloading many files.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nUse mutate() and library(stringr) to create a variable for year from the earlier SOI exercise. For instance, \"agi2006\" should be \"2006\".\nUse as.numeric() to convert the string from step 1 into a numeric value.\nCreate a data visualization with year on the x-axis.\n\n\n\n\n\n\n2.4.2 Factors\nCheck out the forcats cheat sheet.\nMuch of our work focuses on four of the six types of atomic vectors: logical, integer, double, and character. R also contains augmented vectors like factors.\nFactors are categorical data stored as integers with a levels attribute. Character vectors often work well for categorical data and many of R’s functions convert character vectors to factors. This happens with lm().\nFactors have many applications:\n\nGiving the levels of a categorical variable non-alpha numeric order in a ggplot2 data visualization.\nRunning calculations on data with empty groups.\nRepresenting categorical outcome variables in classification models.\n\n\nFactor Basics\n\nx1 &lt;- factor(c(\"a\", \"a\", \"b\", \"c\"), levels = c(\"d\", \"c\", \"b\", \"a\"))\n\nx1\n\n[1] a a b c\nLevels: d c b a\n\nattributes(x1)\n\n$levels\n[1] \"d\" \"c\" \"b\" \"a\"\n\n$class\n[1] \"factor\"\n\nlevels(x1)\n\n[1] \"d\" \"c\" \"b\" \"a\"\n\n\nx1 has order but it isn’t ordinal. Sometimes we’ll come across ordinal factor variables, like with the diamonds data set. Unintentional ordinal variables can cause unexpected errors. For example, including ordinal data as predictors in regression models will lead to different estimated coefficients than other variable types.\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\nx2 &lt;- factor(\n  c(\"a\", \"a\", \"b\", \"c\"), \n  levels = c(\"d\", \"c\", \"b\", \"a\"),\n  ordered = TRUE\n)\n\nx2\n\n[1] a a b c\nLevels: d &lt; c &lt; b &lt; a\n\nattributes(x2)\n\n$levels\n[1] \"d\" \"c\" \"b\" \"a\"\n\n$class\n[1] \"ordered\" \"factor\" \n\nlevels(x2)\n\n[1] \"d\" \"c\" \"b\" \"a\"\n\n\nFigure 2.1 shows how we can use a factor to give a variable a non-alpha numeric order and preserve empty levels. In this case, February and March have zero tropical depressions, tropical storms, and hurricanes and we want to demonstrate that emptiness.\n\n# use case_match to convert integers into month names\nstorms &lt;- storms |&gt;\n  mutate(\n    month = case_match(\n      month,\n      1 ~ \"Jan\",\n      4 ~ \"Apr\",\n      5 ~ \"May\",\n      6 ~ \"Jun\",\n      7 ~ \"Jul\",\n      8 ~ \"Aug\",\n      9 ~ \"Sep\",\n      10 ~ \"Oct\",\n      11 ~ \"Nov\",\n      12 ~ \"Dec\"\n    )\n  )\n\n# create data viz without factors\nstorms |&gt;\n  count(month) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n# add factor variable\nmonths &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n            \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nstorms &lt;- storms |&gt;\n  mutate(month = factor(month, levels = months)) \n\n# create data viz with factors\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n(a) Figure without a factor\n\n\n\n\n\n\n\n(b) Figure with a factor\n\n\n\n\nFigure 2.1: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\nFactors also change the behavior of summary functions like count().\n\nstorms |&gt;\n  count(month)\n\n# A tibble: 10 × 2\n   month     n\n   &lt;fct&gt; &lt;int&gt;\n 1 Jan      70\n 2 Apr      66\n 3 May     201\n 4 Jun     779\n 5 Jul    1603\n 6 Aug    4440\n 7 Sep    7509\n 8 Oct    3077\n 9 Nov    1109\n10 Dec     212\n\nstorms |&gt;\n  count(month, .drop = FALSE)\n\n# A tibble: 12 × 2\n   month     n\n   &lt;fct&gt; &lt;int&gt;\n 1 Jan      70\n 2 Feb       0\n 3 Mar       0\n 4 Apr      66\n 5 May     201\n 6 Jun     779\n 7 Jul    1603\n 8 Aug    4440\n 9 Sep    7509\n10 Oct    3077\n11 Nov    1109\n12 Dec     212\n\n\nlibrary(forcats) simplifies many common operations on factor vectors.\n\n\nChanging Order\nfct_relevel(), fct_rev(), and fct_reorder() are useful functions for modifying the order of factor variables. Figure 2.2 demonstrates using fct_rev() to flip the order of a categorical axis in ggplot2.\n\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\nstorms |&gt;\n  mutate(month = fct_rev(month)) |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n(a) Descending\n\n\n\n\n\n\n\n(b) Ascending\n\n\n\n\nFigure 2.2: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\nFigure 2.3 orders the factor variable based on the number of observations in each category using fct_reorder(). fct_reorder() can order variables based on more sophisticated summaries than just magnitude. For example, it can order box-and-whisker plots based on the median or even something as arbitrary at the 60th percentile.\n\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  mutate(month = fct_reorder(.f = month, .x = n, .fun = median)) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n(a) Alpha-numeric\n\n\n\n\n\n\n\n(b) Magnitude\n\n\n\n\nFigure 2.3: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\n\n\nChanging Values\nFunctions like fct_recode() and fct_lump_min() are useful for changing factor variables. Figure 2.4 combines categories with fewer than 1,000 observations into an \"Other\" group.\n\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\nstorms |&gt;\n  mutate(month = fct_lump_min(month, min = 1000)) |&gt;  \n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n(a) All\n\n\n\n\n\n\n\n(b) Lumped\n\n\n\n\nFigure 2.4: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\n\n\n\n2.4.3 Dates and Date-Times\nCheck out the lubridate cheat sheet.\nThere are many ways to store dates.\n\nMarch 14, 1992\n03/14/1992\n14/03/1992\n14th of March ’92\n\nOne way of storing dates is the best. The ISO 8601 date format is an international standard with appealing properties like fixed lengths and self ordering. The format is YYYY-MM-DD.\nlibrary(lubridate) has useful functions that will take dates of any format and convert them to the ISO 8601 standard.\n\nlibrary(lubridate)\n\nmdy(\"March 14, 1992\")\n\n[1] \"1992-03-14\"\n\nmdy(\"03/14/1992\")\n\n[1] \"1992-03-14\"\n\ndmy(\"14/03/1992\")\n\n[1] \"1992-03-14\"\n\ndmy(\"14th of March '92\")\n\n[1] \"1992-03-14\"\n\n\nThese functions return variables of class \"Date\".\n\nclass(mdy(\"March 14, 1992\"))\n\n[1] \"Date\"\n\n\nlibrary(lubridate) also contains functions for parsing date times into ISO 8601 standard. Times are slightly trickier because of time zones.\n\nmdy_hms(\"12/02/2021 1:00:00\")\n\n[1] \"2021-12-02 01:00:00 UTC\"\n\nmdy_hms(\"12/02/2021 1:00:00\", tz = \"EST\")\n\n[1] \"2021-12-02 01:00:00 EST\"\n\nmdy_hms(\"12/02/2021 1:00:00\", tz = \"America/Chicago\")\n\n[1] \"2021-12-02 01:00:00 CST\"\n\n\nBy default, library(lubridate) will put the date times in Coordinated Universal Time (UTC), which is the successor to Greenwich Mean Time (GMT). I recommend carefully reading the data dictionary if time zones are important for your analysis or if your data cross time zones. This is especially important during time changes (e.g. “spring forward” and “fall back”).\nFortunately, if you encode your dates or date-times correctly, then library(lubridate) will automatically account for time changes, time zones, leap years, leap seconds, and all of the quirks of dates and times.\n\n\n\n\n\n\nExercise 5\n\n\n\n\ndates &lt;- tribble(\n  ~date,\n  \"12/01/1987\",\n  \"12/02/1987\",\n  \"12/03/1987\"\n)\n\n\nCreate the dates data from above with tribble().\nUse mutate() to convert the date column to the ISO 8601 standard (YYYY-MM-DD).\n\n\n\n\nExtracting Components\nlibrary(lubridate) contains functions for extracting components from dates like the year, month, day, and weekday. Conisder the follow data set about full moons in Washington, DC in 2023.\n\nfull_moons &lt;- tribble(\n  ~full_moon,\n  \"2023-01-06\",\n  \"2023-02-05\",\n  \"2023-03-07\",\n  \"2023-04-06\",\n  \"2023-05-05\",\n  \"2023-06-03\",\n  \"2023-07-03\",\n  \"2023-08-01\",\n  \"2023-08-30\",\n  \"2023-09-29\",\n  \"2023-10-28\",\n  \"2023-11-27\",\n  \"2023-12-26\"\n) |&gt;\n  mutate(full_moon = as_date(full_moon))\n\nSuppose we want to know the weekday of each full moon.\n\nfull_moons |&gt;\n  mutate(week_day = wday(full_moon, label = TRUE))\n\n# A tibble: 13 × 2\n   full_moon  week_day\n   &lt;date&gt;     &lt;ord&gt;   \n 1 2023-01-06 Fri     \n 2 2023-02-05 Sun     \n 3 2023-03-07 Tue     \n 4 2023-04-06 Thu     \n 5 2023-05-05 Fri     \n 6 2023-06-03 Sat     \n 7 2023-07-03 Mon     \n 8 2023-08-01 Tue     \n 9 2023-08-30 Wed     \n10 2023-09-29 Fri     \n11 2023-10-28 Sat     \n12 2023-11-27 Mon     \n13 2023-12-26 Tue     \n\n\n\n\nMath\nlibrary(lubridate) easily handles math with dates and date-times. Suppose we want to calculate the number of days since American Independence Day:\n\ntoday() - as_date(\"1776-07-04\")\n\nTime difference of 90306 days\n\n\nIn this case, subtraction creates an object of class difftime represented in days. We can use the difftimes() function to calculate differences in other units.\n\ndifftime(today(), as_date(\"1776-07-04\"), units = \"mins\")\n\nTime difference of 130040640 mins\n\n\n\n\nPeriods\nPeriods track clock time or a calendar time. We use periods when we set a recurring meetings on a calendar and when we set an alarm to wake up in the morning.\nThis can lead to some interesting results. Do we always add 365 days when we add 1 year to a date? With periods, this isn’t true. Sometimes we add 366 days during leap years. For example,\n\nstart &lt;- as_date(\"1999-03-14\")\nend &lt;- start + years(1)\n\nend\n\n[1] \"2000-03-14\"\n\nend - start\n\nTime difference of 366 days\n\n\n\n\nDurations\nDurations track the passage of physical time in exact seconds. Durations are like sand falling into an hourglass. Duration functions start with d like dyears() and dminutes().\n\nstart &lt;- as_date(\"1999-03-14\")\nend &lt;- start + dyears(1)\n\nend\n\n[1] \"2000-03-13 06:00:00 UTC\"\n\n\nNow we always add 365 days, but we see that March 13th is one year after March 14th.\n\n\nIntervals\nUntil now, we’ve focused on points in time. Intervals have length and have a starting point and an ending point.\nSuppose classes start on August 23rd and proceed every week for a while. Do any of these dates conflict with Georgetown’s fall break?\n\nclasses &lt;- as_date(\"2023-08-23\") + weeks(0:15)\n\nfall_break &lt;- interval(as_date(\"2023-11-22\"), as_date(\"2023-11-26\"))\n\nclasses %within% fall_break\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE  TRUE FALSE FALSE\n\n\nWe focused on dates, but many of the same principles hold for date-times.\n\n\n\n\n\n\nExercise 6\n\n\n\n\nCreate a date object for your birth date.\nCalculate the number of days since your birth date.\nCreate a vector of your birthdays from your birth date for the next 120 years. Do you use periods or durations?\n\n\n\n\n\n\n2.4.4 Missing Data\nMissing data are ever present in data analysis. R stores missing values as NA, which are contagious and are fortunately difficult to ignore.\nreplace_na() is the quickest function to replace missing values. It is a shortcut for a specific instance of if_else().\n\nx &lt;- c(1, NA, 3)\n\nif_else(condition = is.na(x), true = 2, false = x)\n\n[1] 1 2 3\n\nreplace_na(x, replace = 2)\n\n[1] 1 2 3\n\n\nWe recommend avoiding arguments like na.rm and using filter() for structurally missing values and replace_na() or imputation for nonresponse.\n\n\n\n\n\n\nExercise 7\n\n\n\nLet’s focus on different data shared by SOI. Now we’ll focus on individual income and tax data by state.\nThis Excel workbook is a beast. For instance, it isn’t clear how the hierarchy works. I expected all of the rows nested under “Number of returns” to sum up to the number of returns. Unfortunately, the rows are not disjoint. Also, the merged cells for column headers are very difficult to use with programming languages.\n\nStart with 20in01al.xlsx.\nCreate a tidy data frame with rows 10 through 12 (“Number of single returns”, “Number of joint returns”, and “Number of head of household returns”) disaggregated by “size of adjusted gross income”."
  },
  {
    "objectID": "02_advanced-data-cleaning.html#footnotes",
    "href": "02_advanced-data-cleaning.html#footnotes",
    "title": "2  Advanced Data Cleaning",
    "section": "",
    "text": "The instinct to include these comments is good. The execution is poor because it creates big headaches for people using programming languages. I suggest using a data dictionary instead.↩︎\nThis example uses regular expressions (regex). Visit R4DS (2e) for a review of regex.↩︎"
  },
  {
    "objectID": "03_functions.html#sec-review3",
    "href": "03_functions.html#sec-review3",
    "title": "3  Advanced R Programming",
    "section": "3.1 Review",
    "text": "3.1 Review\n\n3.1.1 Relational Data\nWe’ve almost exclusively used data frames up to this point. We leveraged relations within our data to pick subsets of the data with functions like filter().\n\nlibrary(tidyverse)\n\nmsleep |&gt; \n  filter(name == \"Cow\")\n\n# A tibble: 1 × 11\n  name  genus vore  order   conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cow   Bos   herbi Artiod… domesticated           4       0.7       0.667    20\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nImportantly, we almost never used indices or selected data by position, which can lead to errors if the underlying data change. An example of using indices and selecting data by position would be to pick data from row number 5 and column number 4. This idea of using the relations in data reduces the chances of making mistakes and leads to clearer code."
  },
  {
    "objectID": "03_functions.html#programming",
    "href": "03_functions.html#programming",
    "title": "3  Advanced R Programming",
    "section": "3.2 Programming",
    "text": "3.2 Programming\n\n3.2.1 Selecting Data\nThere are other ways to subset data, which are important when working with objects other than data frames. We will focus on [], [[]], and $.\n\nAtomic Vectors\nMuch of our work focuses on four of the six types of atomic vectors: logical, integer, double, and character. [] is useful for subsetting atomic vectors. Consider a vector with the first six letters of the alphabet:\n\nletters_short &lt;- letters[1:6]\n\nWe can use positive integers to subset to the first and fifth letters of the alphabet.\n\nletters_short[c(1, 5)]\n\n[1] \"a\" \"e\"\n\n\nWe can use negative integers to subset to the everything but the first and fifth letters of the alphabet.\n\nletters_short[c(-1, -5)]\n\n[1] \"b\" \"c\" \"d\" \"f\"\n\n\nWe can use Booleans (trues and falses) to subset to the first and fifth letters of the alphabet.\n\nletters_short[c(TRUE, FALSE, FALSE, FALSE, TRUE, FALSE)]\n\n[1] \"a\" \"e\"\n\n\nThis may seem silly, but we have many ways to create Booleans that we can then use to subset a vector.\n\nbooleans &lt;- letters_short %in% c(\"a\", \"e\")\n\nbooleans\n\n[1]  TRUE FALSE FALSE FALSE  TRUE FALSE\n\nletters_short[booleans]\n\n[1] \"a\" \"e\"\n\n\nWe can use a character vector to subset a named vector.\n\nnamed_vector &lt;- c(a = 1, b = 2, c = 3)\n\nnamed_vector\n\na b c \n1 2 3 \n\nnamed_vector[c(\"a\", \"c\")]\n\na c \n1 3 \n\n\nWe are able to select more than one element with [], which will not be true of [[]] and $. One thing to look out for is vector recycling. Let’s go back to letters_short, which is length six, but consider some indices of varying lengths.\n\nletters_short[TRUE]\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\"\n\nletters_short[c(TRUE, FALSE)]\n\n[1] \"a\" \"c\" \"e\"\n\nletters_short[c(TRUE, FALSE, TRUE)]\n\n[1] \"a\" \"c\" \"d\" \"f\"\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWow, R recycles the Booleans. Six is divisible by 1, 2, and 3, so there are many ways to recycle the index to subset letters_short. This is dangerous and can quietly cause analtyic errors.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCreate letters_short.\nTry subsetting the vectors with indices with length four or five. What happens?\n\n\n\n\n\nLists\n[[]] and $ are useful for subsetting lists. Both can be used to subset data frames, but I recommending avoiding this.\nUnlike [], which returns multiple elements, [[]] and $ can only return a single element and [[]] and $ simplify objects by removing a layer of hierarchy.\n[[]] can select an element by position or name, while $ can only select an element by name. Consider a list with the first six letters of the alphabet.\n\nalphabet &lt;- list(\n  vowels = c(\"a\", \"e\"),\n  consonants = c(\"b\", \"c\", \"d\", \"f\")\n)\n\nWe can use [] to select the first or second element. In both cases, we get back a smaller list.\n\nalphabet[1]\n\n$vowels\n[1] \"a\" \"e\"\n\nclass(alphabet[1])\n\n[1] \"list\"\n\nalphabet[2]\n\n$consonants\n[1] \"b\" \"c\" \"d\" \"f\"\n\nclass(alphabet[2])\n\n[1] \"list\"\n\n\nWe can use [[]] to select the first or second element. Now, we get back a vector instead of a list. [[]] simplified the object by removing a level of hierarchy.\n\nalphabet[[1]]\n\n[1] \"a\" \"e\"\n\nclass(alphabet[[1]])\n\n[1] \"character\"\n\n\nWe can also use [[]] to select an object by name.\n\nalphabet[[\"vowels\"]]\n\n[1] \"a\" \"e\"\n\nclass(alphabet[[\"vowels\"]])\n\n[1] \"character\"\n\n\nWe can use $ to select either vector by name.\n\nalphabet$vowels\n\n[1] \"a\" \"e\"\n\nclass(alphabet$vowels)\n\n[1] \"character\"\n\n\nReferring to objects by name should make for code that is more robust to changing data.\n\nBeforeAfter\n\n\n\nalphabet1 &lt;- list(\n  vowels = c(\"a\", \"e\"),\n  consonants = c(\"b\", \"c\", \"d\", \"f\")\n)\n\nalphabet1[[2]]\n\n[1] \"b\" \"c\" \"d\" \"f\"\n\nalphabet1[[\"consonants\"]]\n\n[1] \"b\" \"c\" \"d\" \"f\"\n\n\n\n\n\nalphabet2 &lt;- list(\n  vowels = c(\"a\", \"e\"),\n  confusing = \"y\",\n  consonants = c(\"b\", \"c\", \"d\", \"f\")\n)\n\nalphabet2[[2]]\n\n[1] \"y\"\n\nalphabet2[[\"consonants\"]]\n\n[1] \"b\" \"c\" \"d\" \"f\"\n\n\n\n\n\nSubsetting lists can be difficult. Fortunately, RStudio has a tool than can help. Click on a list in your global environment. Navigate to the far right and click the list button with a green arrow. This will generate code and add it to the Console.\n\nInterestingly, this tool avoids $ and uses [[]] to pick the vector by name.\n\nalphabet[[\"vowels\"]]\n\n[1] \"a\" \"e\"\n\n\n\n\n\n3.2.2 Control Flow\n\nFor Loops\nLoops are a fundamental programming tool for iteration; however, they are less common in R than in other programming languages. We previously focused on the Map-Reduce framework and library(purrr) instead of for loops for iteration.\nFor loops have two main pieces: 1. a header and 2. a body. Headers define the number of iterations and potential inputs to the iteration. Bodies are iterated once per iteration. Here is a very simple example:\n\nfor (i in 1:10) {\n  \n  print(i)\n  \n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nWe can use headers several different ways. Like above, we may just want to repeat the values in the index.\n\nfruit &lt;- c(\"apple\", \"banana\", \"cantelope\")\nfor (b in fruit) {\n  \n  print(b)\n  \n}\n\n[1] \"apple\"\n[1] \"banana\"\n[1] \"cantelope\"\n\n\nWe can use the header as an index.\n\nfruit &lt;- c(\"apple\", \"banana\", \"cantelope\")\nfor (i in 1:3) {\n  \n  print(fruit[i])\n  \n}\n\n[1] \"apple\"\n[1] \"banana\"\n[1] \"cantelope\"\n\n\nWe can leverage the index to use results from previous iterations.\n\nresult &lt;- c(1, NA, NA, NA) \nfor (i in 2:4) {\n  \n  result[i] &lt;- result[i - 1] * 2\n  \n}\n\nresult\n\n[1] 1 2 4 8\n\n\nWe’ve now seen three different ways to use the header.\n\nSimply repeat the elements in the header (e.g. print i).\nUse the elements in the header as an index (e.g. select the \\(i^{th}\\) element).\nUse the header to reference past iterations (e.g. i - 1)\n\n\n\n\n\n\n\nDon’t grow vectors!\n\n\n\nIt is tempting to initialize a vector and then grow the vector with a for loop and c(). It is also tempting to initialize a data frame and then grow the data frame with bind_rows(). Because of R’s design, this is computationally very inefficient.\nThis is slow!:\n\nvec &lt;- c(1)\n\nfor (i in 2:10) {\n  \n  vec &lt;- c(vec, i)\n  \n  \n}\n\nvec\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\nIt is essential to pre-allocate vectors and then fill them in. It is also easy to make mistakes when creating indices (e.g. 1:length(x) may end up as c(1, 0)). seq_along() is a helpful alternative to :. The following pre-allocates a vector and then uses the length of the vector to create an index.\n\nnumbers &lt;- vector(mode = \"numeric\", length = 5)\n\nnumbers\n\n[1] 0 0 0 0 0\n\nfor (i in seq_along(numbers)) {\n  \n  numbers[i] &lt;- i\n  \n}\n\nnumbers\n\n[1] 1 2 3 4 5\n\n\nLet’s consider a simple random walk with 100 steps. In this case, the person starts at location zero and random takes one step forward or one step back.\n\nposition &lt;- vector(mode = \"numeric\", length = 100)\n\nset.seed(20230530)\nfor (iter in 2:length(position)) {\n  \n  position[iter] &lt;- position[iter - 1] + sample(x = c(-1, 1), size = 1)\n  \n  \n}\n\nposition\n\n  [1]  0 -1  0  1  2  1  0  1  0  1  2  1  0  1  2  1  2  1  2  1  2  1  2  1  2\n [26]  3  4  5  4  3  2  3  4  3  4  3  2  3  2  3  2  3  2  3  2  3  2  1  2  3\n [51]  2  3  4  3  4  5  6  5  6  5  6  7  6  5  6  7  8  9 10 11 12 13 12 13 12\n [76] 11 10  9 10  9  8  9 10 11 12 13 12 11 12 11 12 13 12 13 14 15 14 15 14 15\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nCreate the following list:\n\n\nalphabet &lt;- list(\n  vowels = c(\"a\", \"e\"),\n  confusing = \"y\",\n  consonants = c(\"b\", \"c\", \"d\", \"f\")\n)\n\n\nWrite a for loop and use str_to_upper() to transform all letters to upper case.\n\n\n\n\n\nWhile Loops\nWhile loops are similar to for loops; however, instead of predetermining the number of iterations in the header, while loops determine a condition in the header and run until that condition is met. For loops can be rewritten as while loops. It can be useful to track the iteration number. Consider a simple example where we double x every iteration while x &lt; 1000.\n\niteration &lt;- 0\nx &lt;- 2\n\nwhile (x &lt; 1000) {\n  \n  iteration &lt;- iteration + 1\n  x &lt;- x * 2\n  \n}\n\nx \n\n[1] 1024\n\niteration\n\n[1] 9\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nWrite the random walk from the for loop above as a while loop. Stop the while loop when position &lt; -10 or position &gt; 10. How many iterations did it take?\n\n\n\n\n\nif, else, and else if\nif_else() and case_when() apply conditional logic to a vector. We most frequently use those functions inside of mutate() to create a new variable or manipulate an existing variable.\nR also has if, else, and else if, which are used to select sections of code to run. This is incredibly useful when programming outside of data manipulation. For example, we can use if to download a file only if it doesn’t already exist.\n\nif (!file.exists(\"data.csv\")) {\n\n  download.file(url = \"web-url.csv\", destfile = \"data.csv\")\n\n}\n\nSelection control flow has two important pieces. First, there is a conditional statement inside (). If the condition is TRUE, then evaluate. If it is FALSE, then don’t evaluate. Second, there is a body contained in {}. Note the formatting in the above example.\nThe conditional statement must be a single TRUE or FALSE. If your statement involves more than one Boolean, then consider using all(), which evaluates to TRUE if everything is TRUE and any(), which evaluates to TRUE if anay element is TRUE.\nLet’s consider a more sophisticated example.\n\nif (distribution == \"normal\") {\n  \n  x &lt;- rnorm(n = 100)\n  \n} else if (distribution == \"poisson\") {\n  \n  x &lt;- rpois(n = 100, lambda = 8)\n  \n} else {\n  \n  stop(\"distribution mast be normal or poissoin\")\n  \n}\n\nThis style of using if, else if, and else is fundamental for including options in custom functions."
  },
  {
    "objectID": "03_functions.html#custom-functions",
    "href": "03_functions.html#custom-functions",
    "title": "3  Advanced R Programming",
    "section": "3.3 Custom Functions",
    "text": "3.3 Custom Functions\n\n3.3.1 Motivation\nCustom functions are an essential building block for good analyses. Custom functions are useful for abiding by the DRY (don’t repeat yourself) principle. Under our conception of DRY, we should create a function any time we do something three times.\nCopying-and-pasting is typically bad because it is easy to make mistakes and we typically want a single source source of truth in a script. Custom functions also promote modular code design and testing.\nThe bottom line: we want to write clear functions that do one and only one thing that are sufficiently tested so we are confident in their correctness.\n\n\n3.3.2 Examples\nLet’s consider a couple of examples from (Barrientos et al. 2021). This paper is a large-scale simulation of formally private mechanisms, which relates to several future chapters of this book.\nDivision by zero, which returns NaN, can be a real pain when comparing confidential and noisy results when the confidential value is zero. This function simply returns 0 when the denominator is 0.\n\n#' Safely divide number. When zero is in the denominator, return 0. \n#'\n#' @param numerator A numeric value for the numerator\n#' @param denominator A numeric value for the denominator\n#'\n#' @return A numeric ratio\n#'\nsafe_divide &lt;- function(numerator, denominator) {\n  \n  if (denominator == 0) {\n    \n    return(0)\n    \n  } else {\n    \n    return(numerator / denominator)\n    \n  }\n}\n\nThis function\n\nImplements the laplace or double exponential distribution, which isn’t included in base R.\nApplies a technique called the laplace mechanism.\n\n\n#' Apply the laplace mechanism\n#'\n#' @param eps Numeric epsilon privacy parameter\n#' @param gs Numeric global sensitivity for the statistics of interest\n#'\n#' @return\n#' \nlap_mech &lt;- function(eps, gs) {\n  \n  # Checking for proper values\n  if (any(eps &lt;= 0)) {\n    stop(\"The eps must be positive.\")\n  }\n  if (any(gs &lt;= 0)) {\n    stop(\"The GS must be positive.\")\n  }\n  \n  # Calculating the scale\n  scale &lt;- gs / eps\n\n  r &lt;- runif(1)\n\n  if(r &gt; 0.5) {\n    r2 &lt;- 1 - r\n    x &lt;- 0 - sign(r - 0.5) * scale * log(2 * r2)\n  } else {\n    x &lt;- 0 - sign(r - 0.5) * scale * log(2 * r)\n  }\n  \n  return(x)\n}\n\n\n\n3.3.3 Basics\nR has a robust system for creating custom functions. To create a custom function, use function():\n\nsay_hello &lt;- function() {\n  \n  \"hello\"\n   \n}\n\nsay_hello()\n\n[1] \"hello\"\n\n\nOftentimes, we want to pass parameters/arguments to our functions:\n\nsay_hello &lt;- function(name) {\n  \n  paste(\"hello,\", name)\n   \n}\n\nsay_hello(name = \"aaron\")\n\n[1] \"hello, aaron\"\n\n\nWe can also specify default values for parameters/arguments:\n\nsay_hello &lt;- function(name = \"aaron\") {\n  \n  paste(\"hello,\", name)\n   \n}\n\nsay_hello()\n\n[1] \"hello, aaron\"\n\nsay_hello(name = \"alex\")\n\n[1] \"hello, alex\"\n\n\nsay_hello() just prints something to the console. More often, we want to perform a bunch of operations and the then return some object like a vector or a data frame. By default, R will return the last unassigned object in a custom function. It isn’t required, but it is good practice to wrap the object to return in return().\nIt’s also good practice to document functions. With your cursor inside of a function, go Insert &gt; Insert Roxygen Skeleton:\n\n#' Say hello\n#'\n#' @param name A character vector with names\n#'\n#' @return A character vector with greetings to name\n#' \nsay_hello &lt;- function(name = \"aaron\") {\n  \n  greeting &lt;- paste(\"hello,\", name)\n  \n  return(greeting)\n  \n}\n\nsay_hello()\n\n[1] \"hello, aaron\"\n\n\nAs you can see from the Roxygen Skeleton template above, function documentation should contain the following:\n\nA description of what the function does\nA description of each function argument, including the class of the argument (e.g. string, integer, dataframe)\nA description of what the function returns, including the class of the object\n\nTips for writing functions:\n\nFunction names should be short but effectively describe what the function does. Function names should generally be verbs while function arguments should be nouns. See the Tidyverse style guide for more details on function naming and style.\nAs a general principle, functions should each do only one task. This makes it much easier to debug your code and reuse functions!\nUse :: (e.g. dplyr::filter()) when writing custom functions. This will create stabler code and make it easier to develop R packages.\n\n\n\n3.3.4 Functions with Multiple Outputs\nWhen return() is reached in a function, return() is evaluated and evaluation ends and R leaves the function.\n\nsow_return &lt;- function() {\n  \n  return(\"The function stops!\")\n  \n  return(\"This never happens!\")\n  \n}\n\nsow_return()\n\n[1] \"The function stops!\"\n\n\nIf the end of a function is reached without calling return(), the value from the last evaluated expression is returned.\nWe prefer to include return() at the end of functions for clarity even though return() doesn’t change the behavior of the function.\nSometimes we want to return more than one vector or data frame. list() is very helpful in these siutations.\n\nsummarize_results &lt;- function(x) {\n  \n  mean_x &lt;- mean(x)\n  \n  median_x &lt;- median(x)\n  \n  results &lt;- list(\n    mean = mean_x,\n    median = median_x\n  )\n  \n  return(results)\n  \n}\n\nsummarize_results(x = 1:10)\n\n$mean\n[1] 5.5\n\n$median\n[1] 5.5\n\n\n\n\n3.3.5 Referential Transparency\nR functions, like mathematical functions, should always return the exact same output for a given set of inputs1. This is called referential transparency. R will not enforce this idea, so you must write good code.\n\nBad!\n\nbad_function &lt;- function(x) {\n  \n  x * y\n  \n}\n\ny &lt;- 2\nbad_function(x = 2)\n\n[1] 4\n\ny &lt;- 3\nbad_function(x = 2)\n\n[1] 6\n\n\n\n\nGood!\n\ngood_function &lt;- function(x, y) {\n  \n  x * y\n  \n}\n  \ny &lt;- 2\ngood_function(x = 2, y = 1)\n\n[1] 2\n\ny &lt;- 3\ngood_function(x = 2, y = 1)\n\n[1] 2\n\n\nBruno Rodriguez has a book and a blog that explores this idea further."
  },
  {
    "objectID": "03_functions.html#debugging",
    "href": "03_functions.html#debugging",
    "title": "3  Advanced R Programming",
    "section": "3.4 Debugging",
    "text": "3.4 Debugging\nR code inside of custom functions can be tougher to troubleshoot than R code outside of custom functions. Fortunately, R has a powerful debugging tool.\nThe debugger requires putting custom functions in their own scripts. This is covered in Section 3.6.\nTo set up the debugger, simply select the red dot to the left of a line of code in a custom function and then source the custom function. After, there should be a red dot next to the defined function in the global environment.2\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 3.1: Setting up the debugger\n\n\nNow, when the function is called it will stop at the red dot (the stop point). Importantly, the environment should reflect the environment inside of the function instead of the global environment.\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 3.2: Using the debugger\n\n\nFinally, RStudio gives several controls for the debugger. There is a button to Continue to the end of the function. There is a button to Stop execution.\nThere is also a button with two brackets and a green arrow. This steps the debugger into another function. This is incredibly useful when functions are nested inside of functions.\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCreate a custom function with at least three different pieces of R code.\nSave the function in a .R script with the same name as the function.\nClick the little red dot to the left of first line of code in the .R script.\nSource the function with the source button at the top right.\nCall the function. You should enter the debugger."
  },
  {
    "objectID": "03_functions.html#benchmarking",
    "href": "03_functions.html#benchmarking",
    "title": "3  Advanced R Programming",
    "section": "3.5 Benchmarking",
    "text": "3.5 Benchmarking\nBenchmarking is the process of estimating the run time of code. Oftentimes, benchmarking is used to compare multiple pieces of code to pick the more performant code. This raises a couple of issues:\n\nComputing environments differ. My MacBook Pro with Apple M1 chips typically outperforms my work computer.\nOther software can slow performance. When I open up Spotify my R processes typically slow down.\n\nWe can’t solve problem 1 with an R package, but we can solve problem 2 by running tests multiple times. library(microbenchmark) makes this very easy.\nSuppose we are interested in the median of a vector of 1 million numbers. We can easily calculate this with median() or quantile(). Suppose we are concerned about computation speed, so lets test the code performance:\n\nlibrary(microbenchmark)\n\nx &lt;- 1:1000000\n\nmicrobenchmark::microbenchmark(\n  median(x),\n  quantile(x, probs = 0.5)\n)\n\nWarning in microbenchmark::microbenchmark(median(x), quantile(x, probs = 0.5)):\nless accurate nanosecond times to avoid potential integer overflows\n\n\nUnit: milliseconds\n                     expr      min       lq     mean   median       uq\n                median(x) 6.467135 6.894191 7.784298 6.952350 8.276834\n quantile(x, probs = 0.5) 3.872409 4.060271 4.553908 4.094055 4.835909\n       max neval cld\n 41.251289   100  a \n  6.817931   100   b\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nLet’s compare %&gt;% and |&gt; to see if they have comparable computation times. Consider this example from Stack Overflow, which shows |&gt; is clearly better.\n\nLoad library(microbenchmark) and add the microbenchmark() function.\nCreate x1 &lt;- 1:1000000, x2 &lt;- 1:1000000, and x3 &lt;- 1:1000000\nTest median(x1), x2 |&gt; median(), and x3 %&gt;% median().3"
  },
  {
    "objectID": "03_functions.html#sec-organizing-an-analysis",
    "href": "03_functions.html#sec-organizing-an-analysis",
    "title": "3  Advanced R Programming",
    "section": "3.6 Organizing an Analysis",
    "text": "3.6 Organizing an Analysis\nWe recommend writing functions for data analysis. We need a plan for how to add custom functions to our workflow built on RStudio projects and Quarto.\nWe typically recommending adding a directory called R or src in a project directory and then sourcing scripts in to Quarto documents. Keeping functions in separate scripts makes the functions easier to use in multiple documents and simplifies the debugging process outlined above.\nWe typically only add one function to an R script in the R/ directory and name the script after the function (without parentheses). Next, we source function scripts at the top of Quarto documents after loading packages with the source(). library(here) is essential if when sourcing from a Quarto document that is in a subdirectory of the project."
  },
  {
    "objectID": "03_functions.html#packages",
    "href": "03_functions.html#packages",
    "title": "3  Advanced R Programming",
    "section": "3.7 Packages",
    "text": "3.7 Packages\nAt some point, the same scripts or data are used often enough or widely enough to justify moving from sourced R scripts to a full-blown R package. R packages make it easier to\n\nMake it easier to share and version code.\nImprove documentation of functions and data.\nMake it easier to test code.\nOften lead to fun hex stickers.\n\n\n3.7.1 Use This\nlibrary(usethis) includes an R package template. The following will add all necessary files for an R package to a directory called testpackage/ and open an RStudio package.\n\nlibrary(usethis)\ncreate_package(\"/Users/adam/testpackage\")\n\n\n\n3.7.2 Package contents\nThe template includes a lot of different files and directories. We will focus on the minimum sufficient set of files for building a package.\nDESCRIPTION contains the meta information about the package. Important lines include the package version and the license. Package versions are useful for tracking the version of the package used with an analysis. library(usethis) has a helper function for picking a license.\nPackage: testpackage\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\n\n\n3.7.3 Functions\nFunctions go in the R directory, just like when we sourced functions earlier. Be sure to reference packages directly with :: (e.g. stats::lm()).\n\nRoxygen\nIt is essential to use Roxygen skeletons with custom functions. RStudio makes this simple. Place the cursor in a function and select Code &gt; Insert Roxygen Skeleton. The Roxygen skeleton creates help documentation for a package, which can be accessed with ?.\n#' Title\n#'\n#' @param\n#'\n#' @return\n#' @export\n#'\n#' @examples\nThe title should be a brief description of the function. param describes each input to the function and return describes the output of the function.\n\n\nTests"
  },
  {
    "objectID": "03_functions.html#unit-testing",
    "href": "03_functions.html#unit-testing",
    "title": "3  Advanced R Programming",
    "section": "3.8 Unit testing",
    "text": "3.8 Unit testing\nUnit testing is the systematic testing of functions to ensure correctness."
  },
  {
    "objectID": "03_functions.html#test-coverage",
    "href": "03_functions.html#test-coverage",
    "title": "3  Advanced R Programming",
    "section": "3.9 Test coverage",
    "text": "3.9 Test coverage\nTest coverage is the scope and quality of tests performed on a code base."
  },
  {
    "objectID": "03_functions.html#references",
    "href": "03_functions.html#references",
    "title": "3  Advanced R Programming",
    "section": "3.10 References",
    "text": "3.10 References\n\n\n\n\nBarrientos, Andrés F., Aaron R. Williams, Joshua Snoke, and Claire McKay Bowen. 2021. “A Feasibility Study of Differentially Private Summary Statistics and Regression Analyses with Evaluations on Administrative and Survey Data.” https://doi.org/10.48550/ARXIV.2110.12055."
  },
  {
    "objectID": "03_functions.html#footnotes",
    "href": "03_functions.html#footnotes",
    "title": "3  Advanced R Programming",
    "section": "",
    "text": "This rule won’t exactly hold if the function contains random or stochastic code. In those cases, the function should return the same output every time if the seed is set with set.seed().↩︎\nAccording to Hadley Wickham, “You can think of an environment as a bag of names. Each name points to an object stored elsewhere in memory.” For more details, see the Environments chapter of Advanced R.↩︎\nDon’t be surprised when microbenchmark::microbenchmark() returns a row with expr median(x2) instead of x2 |&gt; median(). This is expected behavior because the base pipe is implemented as a syntax transformation. You can read more about this in this blog.↩︎"
  },
  {
    "objectID": "04_web-scraping.html#sec-review4",
    "href": "04_web-scraping.html#sec-review4",
    "title": "4  Web Scraping",
    "section": "4.1 Review",
    "text": "4.1 Review\nWe explored pulling data from web APIs in DSPP1. With web APIs, stewards are often carefully thinking about how to share information. This will not be the case with web scraping.\nWe also explored extracting data from Excel workbooks in Section 02. We will build on some of the ideas in that section.\nRecall that if we have a list of elements, we can extract the \\(i^{th}\\) element with [[]]. For example, we can extract the third data frame from a list of data frames called data with data[[3]].\nRecall that we can use map() to iterate a function across each element of a vector. Consider the following example:\n\ntimes2 &lt;- function(x) x * 2\n\nx &lt;- 1:3\n\nmap(.x = x, .f = times2)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6"
  },
  {
    "objectID": "04_web-scraping.html#introduction-and-motivation",
    "href": "04_web-scraping.html#introduction-and-motivation",
    "title": "4  Web Scraping",
    "section": "4.2 Introduction and Motivation",
    "text": "4.2 Introduction and Motivation\nThe Internet is an immense source of information for research. Sometimes we can easily download data of interest in an ideal format with the click of a download button or a single API call.\nBut it probably won’t be long until we need data that require many download button clicks. Or worse, we may want data from web pages that don’t have a download button at all.\nConsider a few examples.\n\nThe Urban Institute’s Boosting Upward Mobility from Poverty project programmatically downloaded 51 .xslx workbooks when building the Upward Mobility Data Tables.\nWe worked with the text of executive orders going back to the Clinton Administration when learning text analysis in DSPP1. Unfortunately, the Federal Register doesn’t publish a massive file of executive orders. So we iterated through websites for each executive order, scraped the text, and cleaned the data.\nThe Urban Institute scraped course descriptions from Florida community colleges to understand opportunities for work-based learning.\nThe Billion Prices Project web scraped millions of prices each day from online retailers. The project used the data to construct real-time price indices that limited political interference and to research concepts like price stickiness.\n\nWe will explore two approaches for gathering information from the web.\n\nIteratively downloading files: Sometimes websites contain useful information across many files that need to be separately downloaded. We will use code to download these files. Ultimately, these files can be combined into one larger data set for research.\nScraping content from the body of websites: Sometimes useful information is stored as tables or lists in the body of websites. We will use code to scrape this information and then parse and clean the result.\n\nSometimes we download many PDF files using the first approach. A related method that we will not cover that is useful for gathering information from the web is extracting text data from PDFs."
  },
  {
    "objectID": "04_web-scraping.html#legal-and-ethical-considerations",
    "href": "04_web-scraping.html#legal-and-ethical-considerations",
    "title": "4  Web Scraping",
    "section": "4.3 Legal and Ethical Considerations",
    "text": "4.3 Legal and Ethical Considerations\nIt is important to consider the legal and ethical implications of any data collection. Collecting data from the web through methods like web scraping raises serious ethical and legal considerations.\n\n4.3.1 Legal1\nDifferent countries have different laws that affect web scraping. The United States has different laws and legal interpretations than countries in Europe, which are largely regulated by the European Union. In general, the United States has more relaxed policies than the European when it comes to gathering data from the web.\nR for Data Science (2e) contains a clear and approachable rundown of legal consideration for gathering information for the web. We adopt their three-part standard of “public, non-personal, and factual”, which relate to terms of service, personally identifiable information, and copyright.\nWe will focus solely on laws in the United States.\n\nTerms of Service\nThe legal environment for web scraping is in flux, but US Courts have created an environment that is legally supportive of gathering public information from the web.\nFirst, we need to understand how many websites bar web scraping. Second, we need to understand when we can ignore these rules.\n\n\n\n\n\n\nTerms of Service\n\n\n\nA terms of service is a list of rules posted by the provider of a website, web service, or software.\n\n\nTerms of Service for many websites bar web scraping.\nFor example, LinkedIn’s Terms of Service says users agree to not “Develop, support or use software, devices, scripts, robots or any other means or processes (including crawlers, browser plugins and add-ons or any other technology) to scrape the Services or otherwise copy profiles and other data from the Services;”\nThis sounds like the end of web scraping, but as Wickham, Çetinkaya-Rundel, and Grolemund (2023) note, Terms of Service end up being a “legal land grab” for companies. It isn’t clear how LinkedIn would legally enforce this. HiQ Labs v. LinkedIn from the United States Court of Appeals for the Ninth Circuit bars Computer Fraud and Abuse Act (CFAA) claims against web scraping public information.2\nWe follow a simple guideline: it is acceptable to scrape information when we don’t need to create an account.\n\n\n\n4.3.2 PII\n\n\n\n\n\n\nPersonal Identifiable Information\n\n\n\nPersonal Identifiable Information (PII) is any information that can be used to directly identify an individual.\n\n\nPublic information on the Internet often contains PII, which raises legal and ethical challenges. We will discuss the ethics of PII later.\nThe legal considerations are trans-Atlantic. The General Data Protection Regulation (GDPR) is a European Union regulation about information privacy. It contains strict rules about the collection and storage of PII. It applies to almost everyone collecting data inside the EU. The GDPR is also extraterritorial, which means its rules can apply outside of the EU under certain circumstances like when an American company gathers information about EU individuals.\nWe will avoid gathering PII, so we don’t need to consider PII.\n\nCopyright\n\n\n\n\n\n\nCopyright Law\n\n\n\n\nCopyright protection subsists, in accordance with this title, in original works of authorship fixed in any tangible medium of expression, now known or later developed, from which they can be perceived, reproduced, or otherwise communicated, either directly or with the aid of a machine or device. Works of authorship include the following categories:\n\n\nliterary works;\n\n\nmusical works, including any accompanying words;\n\n\ndramatic works, including any accompanying music;\n\n\npantomimes and choreographic works;\n\n\npictorial, graphic, and sculptural works;\n\n\nmotion pictures and other audiovisual works;\n\n\nsound recordings; and\n\n\narchitectural works.\n\n\nIn no case does copyright protection for an original work of authorship extend to any idea, procedure, process, system, method of operation, concept, principle, or discovery, regardless of the form in which it is described, explained, illustrated, or embodied in such work.\n\n17 U.S.C.\n\n\nOur final legal concern for gathering information from the Internet is copyright law. We have two main options for avoiding copyright limitations.\n\nWe can avoid copyright protections by not scraping authored content in the protected categories (i.e. literary works and sound recordings). Fortunately, factual data are not typically protected by copyright.\nWe can scrape information that is fair use. This is important if we want to use images, films, music, or extended text as data.\n\nWe will focus on data that are not copyrighted.\n\n\n\n4.3.3 Ethical\nWe now turn to ethical considerations and some of the best-practices for gathering information from the web. In general, we will aim to be polite, give credit, and respect individual information.\n\nBe polite\nIt is expensive and time-consuming to host data on the web. Hosts experience a small burden every time we access a website. This burden is small but can quickly grow with repeated queries. Just like with web APIs, we want to pace the burden of our access to be polite.\n\n\n\n\n\n\nRate Limiting\n\n\n\nRate limiting is the intentional slowing of web traffic for a user or users.\n\n\nWe will use Sys.sleep() in custom functions to slow our web scraping and ease the burden of our web scraping on web hosts.\n\n\n\n\n\n\nrobots.txt\n\n\n\nrobots.txt tells web crawlers and scrapers which URLs the crawler is allowed to access on a website.\n\n\nMany websites contain a robots.txt file. Consider examples from the Urban Institute and White House.\nWe can manually look at the robots.txt. For example, just visit https://www.urban.org/robots.txt or https://www.whitehouse.gov/robots.txt. We can also use library(polite), which will automatically look at the robots.txt.\n\n\nGive Credit\nAcademia and the research profession undervalue the collection and curation of data. Generally speaking, no one gets tenure for constructing even the most important data sets. It is important to give credit for data accessed from the web. Ideally, add the citation to Zotero and then easily add it to your manuscript in Quarto.\nBe sure to make it easy for others to cite data sets that you create. Include an example citation like IPUMS or create a DOI for your data.\nThe rise of generative AI models like GPT-3, Stable Diffusion, DALL-E 2 makes urgent considerations of giving credit. These models consume massive amounts of training data, and it isn’t clear where the training data come from or the legal and ethical implications of the training data.3\nConsider a few current events:\n\nSarah Silverman is suing OpenAI because she “never gave permission for OpenAI to ingest the digital version of her 2010 book to train its AI models, and it was likely stolen from a ‘shadow library’ of pirated works.”\nSomepalli et al. (2023) use state-of-the-art image retrieval models to find that generative AI models like the popular the popular Stable Diffusion model “blatantly copy from their training data.” This is a major problem if the training data are copyrighted. The first page of their paper (here) contains some dramatic examples.\nFinally, this Harvard Business Review article discusses the intellectual property problem facing generative AI.\n\n\n\nRespect Individual Information\nData science methods should adhere to the same ethical standards as any research method. The social sciences have ethical norms about protecting privacy (discussed later) and informed consent.\n\n\n\n\n\n\nDiscussion\n\n\n\nIs it appropriate to collect and share public PII?\nDo these norms apply to data that is already public on the Internet?\n\n\nLet’s consider an example. In 2016, researchers posted data about 70,000 OkCupid accounts. The data didn’t contain names but did contain usernames. The data also contained many sensitive variables including topics like sexual habits and politics.\nThe release drew strong reactions from some research ethicists including Michael Zimmer and Os Keyes.4\nFellegi (1972) defines data privacy as the ability “to determine what information about ourselves we will share with others”. Maybe OkCupid users made the decision to forego confidentiality when they published their accounts. Many institutional ethics committees do not require informed consent for public data.\nRavn, Barnwell, and Barbosa Neves (2020) do a good job developing a conceptual framework that bridges the gap between the view that all public data require informed consent and the view that no public data require informed consent.\nIt’s possible to conceive of a web scraping research project that is purely observational that adheres to the ethical standards of research and contains potentially disclosive information about individuals. Fortunately, researchers can typically use Institutional Review Boards and research ethicists to navigate these questions.\nAs a basic standard, we will avoid collecting PII and use anonymization techniques to limit the risk of re-identification.\nWe will also focus on applications where the host of information crudely shares the information. There are ample opportunities to create value by gathering information from government sources and converting it into more useful formats. For example, the government too often shares information in .xls and .xlsx files, clunky web interfaces, and PDFs."
  },
  {
    "objectID": "04_web-scraping.html#programatically-downloading-data",
    "href": "04_web-scraping.html#programatically-downloading-data",
    "title": "4  Web Scraping",
    "section": "4.4 Programatically Downloading Data",
    "text": "4.4 Programatically Downloading Data\nThe County Health Rankings & Roadmaps is a source of state and local information.\nSuppose we are interested in Injury Deaths at the state level. We can click through the interface and download a .xlsx file for each state.\n\n4.4.1 Downloading a Single File\n\nStart here.\nUsing the interface at the bottom of the page, we can navigate to the page for “Virginia.”\nNext, we can click “View State Data.”\nNext, we can click “Download Virginia data sets.”\n\nThat’s a lot of clicks to get here.\nIf we want to download “2023 Virginia Data”, we can typically right click on the link and select “Copy Link Address”. This should return one of the following two URLS:\nhttps://www.countyhealthrankings.org/sites/default/files/media/document/2023%20County%20Health%20Rankings%20Virginia%20Data%20-%20v2.xlsx\nhttps://www.countyhealthrankings.org/sites/default/files/media/document/2023 County Health Rankings Virginia Data - v2.xlsx\nSpaces are special characters in URLs and they are sometimes encoded as %20. Both URLs above work in the web browser, but only the URL with %20 will work with code.\nAs we’ve seen several times before, we could use read_csv() to directly download the data from the Internet if the file was a .csv.5 We need to download this file because it is an Excel file, which we can do with download.file() provided we include a destfile.\n\ndownload.file(\n  url = \"https://www.countyhealthrankings.org/sites/default/files/media/document/2023%20County%20Health%20Rankings%20Virginia%20Data%20-%20v2.xlsx\", \n  destfile = \"data/virginia-injury-deaths.xlsx\"\n)\n\n\n\n4.4.2 Downloading Multiple Files\nIf we click through and find the links for several states, we see that all of the download links follow a common pattern. For example, the URL for Vermont is\nhttps://www.countyhealthrankings.org/sites/default/files/media/document/2023 County Health Rankings Vermont Data - v2.xlsx\nThe URLs only differ by \"Virginia\" and \"Vermont\". If we can create a vector of URLs by changing state name, then it is simple to iterate downloading the data. We will only download data for two states, but we can imagine downloading data for many states or many counties. Here are three R tips:\n\npaste0() and str_glue() from library(stringr) are useful for creating URLs and destination files.\nwalk() from library(purrr) can iterate functions. It’s like map(), but we use it when we are interested in the side-effect of a function.6\nSometimes data are messy and we want to be polite. Custom functions can help with rate limiting and cleaning data.\n\n\ndownload_chr &lt;- function(url, destfile) {\n\n  download.file(url = url, destfile = destfile)\n\n  Sys.sleep(0.5)\n\n}\n\nstates &lt;- c(\"Virginia\", \"Vermont\")\n\nurls &lt;- paste0(\n  \"https://www.countyhealthrankings.org/sites/default/files/\",\n  \"media/document/2023%20County%20Health%20Rankings%20\",\n  states,\n  \"%20Data%20-%20v2.xlsx\"\n)\n\noutput_files &lt;- paste0(\"data/\", states, \".xlsx\")\n\nwalk2(.x = urls, .y = output_files, .f = download_chr)\n\n\n\n\n\n\n\nExercise 1\n\n\n\nSOI Tax Stats - Historic Table 2 provides individual income and tax data, by state and size of adjusted gross income. The website contains a bulleted list of URLs and each URL downloads a .xlsx file.\n\nUse download.file() to download the file for Alabama.\nExplore the URLs using “Copy Link Address”.\nIterate pulling the data for Alabama, Alaska, and Arizona."
  },
  {
    "objectID": "04_web-scraping.html#web-scraping-with-rvest",
    "href": "04_web-scraping.html#web-scraping-with-rvest",
    "title": "4  Web Scraping",
    "section": "4.5 Web Scraping with rvest",
    "text": "4.5 Web Scraping with rvest\nWe now pivot to situations where useful information is stored in the body of web pages.\n\n4.5.1 Web Design\nIt’s simple to build a website with Quarto because it abstracts away most of web development. For example, Markdown is just a shortcut to write HTML. Web scraping requires us to learn more about web development than when we use Quarto.\nThe user interface of websites can be built with just HTML, but most websites contain HTML, CSS, and JavaScript. The development the interface of websites with HTML, CSS, and JavaScript is called front-end web development.\n\n\n\n\n\n\nHyper Text Markup Language\n\n\n\nHyper Text Markup Language (HTML) is the standard language for creating web content. HTML is a markup language, which means it has code for creating structure and and formatting.\n\n\nThe following HTML generates a bulleted list of names.\n&lt;ul&gt;\n  &lt;li&gt;Alex&lt;/li&gt;\n  &lt;li&gt;Aaron&lt;/li&gt;\n  &lt;li&gt;Alena&lt;/li&gt;\n&lt;/ul&gt;\n\n\n\n\n\n\nCascading Style Sheets\n\n\n\nCascading Style Sheets (CSS) describes hot HTML elements should be styled when they are displayed.\n\n\nFor example, the following CSS adds extra space after sections with ## in our class notes.\n.level2 {\n  margin-bottom: 80px;\n}\n\n\n\n\n\n\nJavaScript\n\n\n\nJavaScript is a programming language that runs in web browsers and is used to build interactivity in web interfaces.\n\n\nQuarto comes with default CSS and JavaScript. library(leaflet) and Shiny are popular tools for building JavaScript applications with R. We will focus on web scraping using HTML and CSS.\nFirst, we will cover a few important HTML concepts. W3Schools offers a thorough introduction. Consider the following simple website built from HTML:\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Hello World!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1 class='important'&gt;Bigger Title!&lt;/h1&gt;\n&lt;h2 class='important'&gt;Big Title!&lt;/h1&gt;\n&lt;p&gt;My first paragraph.&lt;/p&gt;\n&lt;p id='special-paragraph'&gt;My first paragraph.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nAn HTML element is a start tag, some content, and an end tag. Every start tag has a matching end tag. For example, &lt;body and &lt;/body&gt;. &lt;html&gt;, &lt;head&gt;, and &lt;body&gt; are required elements for all web pages. Other HTML elements include &lt;h1&gt;, &lt;h2&gt;, and &lt;p&gt;.\nHTML attributes are name/value pairs that provide additional information about elements. HTML attributes are optional and are like function arguments for HTML elements.\nTwo HTML attributes, classes and ids, are particularly important for web scraping.\n\nHTML classes are HTML attributes that label multiple HTML elements. These classes are useful for styling HTML elements using CSS. Multiple elements can have the same class.\nHTML ids are HTML attributes that label one HTML element. Ids are useful for styling singular HTML elements using CSS. Each ID can be used only one time in an HTML document.\n\nWe can view HTML for any website by right clicking in our web browser and selecting “View Page Source.”7\n\n\n\n\n\n\nExercise 2\n\n\n\n\nInspect the HTML behind this list of “Hello World examples”.\nInspect the HTML behind the Wikipedia page for Jerzy Neyman.\n\n\n\nSecond, we will explore CSS. CSS relies on HTML elements, HTML classes, and HTML ids to style HTML content. CSS selectors can directly reference HTML elements. For example, the following selectors change the style of paragraphs and titles.\np {\n  color: red;\n}\n\nh1 {\n  font-family: wingdings;\n}\nCSS selectors can reference HTML classes. For example, the following selector changes the style of HTML elements with class='important'.\n.important {\n  font-family: wingdings;\n}\nCSS selectors can reference also reference HTML IDs. For example, the following selector changes the style of the one element with id='special-paragraph'\n#special-paragraph {\n  color: pink;\n}\nWe can explore CSS by right clicking and selecting Inspect. Most modern websites have a lot of HTML and a lot of CSS. We can find the CSS for specific elements in a website with the button at the top left of the new window that just appeared.\n\n\n\nInspecting CSS\n\n\n\n\n4.5.2 Tables\nlibrary(rvest) is the main tool for scraping static websites with R. We’ll start with examples that contain information in HTML tables.8\nHTML tables store information in tables in websites using the &lt;table&gt;, &lt;tr&gt;, &lt;th&gt;, and &lt;td&gt;. If the data of interest are stored in tables, then it can be trivial to scrape the information.\nConsider the Wikipedia page for the 2012 Presidential Election. We can scrape all 46 tables from the page with two lines of code. We use the WayBack Machine to ensure the content is stable.\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\ntables &lt;- read_html(\"https://web.archive.org/web/20230814004444/https://en.wikipedia.org/wiki/2012_United_States_presidential_election\") |&gt;\n  html_table()\n\nSuppose we are interested in the table about presidential debates. We can extract that element from the list of tables.\n\ntables[[18]]\n\n# A tibble: 12 × 9\n   `Presidential candidate`     Party `Home state` `Popular vote` `Popular vote`\n   &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;         \n 1 \"Presidential candidate\"     Party Home state   Count          Percentage    \n 2 \"Barack Hussein Obama II\"    Demo… Illinois     65,915,795     51.06%        \n 3 \"Willard Mitt Romney\"        Repu… Massachuset… 60,933,504     47.20%        \n 4 \"Gary Earl Johnson\"          Libe… New Mexico   1,275,971      0.99%         \n 5 \"Jill Ellen Stein\"           Green Massachuset… 469,627        0.36%         \n 6 \"Virgil Hamlin Goode Jr.\"    Cons… Virginia     122,389        0.11%         \n 7 \"Roseanne Cherrie Barr\"      Peac… Utah         67,326         0.05%         \n 8 \"Ross Carl \\\"Rocky\\\" Anders… Just… Utah         43,018         0.03%         \n 9 \"Thomas Conrad Hoefling\"     Amer… Nebraska     40,628         0.03%         \n10 \"Other\"                      Other Other        217,152        0.17%         \n11 \"Total\"                      Total Total        129,085,410    100%          \n12 \"Needed to win\"              Need… Needed to w… Needed to win  Needed to win \n# ℹ 4 more variables: Electoralvote &lt;chr&gt;, `Running mate` &lt;chr&gt;,\n#   `Running mate` &lt;chr&gt;, `Running mate` &lt;chr&gt;\n\n\nOf course, we want to be polite. library(polite) makes this very simple. “The three pillars of a polite session are seeking permission, taking slowly and never asking twice.”\nWe’ll use bow() to start a session and declare our user agent, and scrape() instead of read_html().9\n\nlibrary(polite)\n\nsession &lt;- bow(\n  url = \"https://web.archive.org/web/20230814004444/https://en.wikipedia.org/wiki/2012_United_States_presidential_election\",\n  user_agent = \"Georgetown students learning scraping -- arw109@georgetown.edu\"\n)\n\nsession\n\n&lt;polite session&gt; https://web.archive.org/web/20230814004444/https://en.wikipedia.org/wiki/2012_United_States_presidential_election\n    User-agent: Georgetown students learning scraping -- arw109@georgetown.edu\n    robots.txt: 1 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n\nelection_page &lt;- session |&gt;\n  scrape() \n  \ntables &lt;- election_page |&gt;\n  html_table()\n\ntables[[18]]\n\n# A tibble: 12 × 9\n   `Presidential candidate`     Party `Home state` `Popular vote` `Popular vote`\n   &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;         \n 1 \"Presidential candidate\"     Party Home state   Count          Percentage    \n 2 \"Barack Hussein Obama II\"    Demo… Illinois     65,915,795     51.06%        \n 3 \"Willard Mitt Romney\"        Repu… Massachuset… 60,933,504     47.20%        \n 4 \"Gary Earl Johnson\"          Libe… New Mexico   1,275,971      0.99%         \n 5 \"Jill Ellen Stein\"           Green Massachuset… 469,627        0.36%         \n 6 \"Virgil Hamlin Goode Jr.\"    Cons… Virginia     122,389        0.11%         \n 7 \"Roseanne Cherrie Barr\"      Peac… Utah         67,326         0.05%         \n 8 \"Ross Carl \\\"Rocky\\\" Anders… Just… Utah         43,018         0.03%         \n 9 \"Thomas Conrad Hoefling\"     Amer… Nebraska     40,628         0.03%         \n10 \"Other\"                      Other Other        217,152        0.17%         \n11 \"Total\"                      Total Total        129,085,410    100%          \n12 \"Needed to win\"              Need… Needed to w… Needed to win  Needed to win \n# ℹ 4 more variables: Electoralvote &lt;chr&gt;, `Running mate` &lt;chr&gt;,\n#   `Running mate` &lt;chr&gt;, `Running mate` &lt;chr&gt;\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nInstall and load library(rvest).\nInstall and load library(polite).\nScrape the Presidential debates table from the Wikipedia article for the 2008 presidential election.\n\n\n\n\n\n4.5.3 Other HTML Content\nSuppose we want to scrape every URL in the body of the 2012 Presidential Election webpage. html_table() no longer works.\nWe could manually poke through the source code to find the appropriate CSS selectors. Fortunately, SelectorGadget often eliminates this tedious work by telling you the name of the html elements that you click on.\n\nClick the SelectorGadget gadget browser extension. You may need to click the puzzle piece to the right of the address bar and then click the SelectorGadget browser extension.\nSelect an element you want to scrape. The elements associated with the CSS selector provided at the bottom will be in green and yellow.\n\nIf SelectorGadget selects too few elements, select additional elements. If SelectorGadget selects too many elements, click those elements. They should turn red.\n\nEach click should refine the CSS selector.\nAfter a few clicks, it’s clear we want p a. This should select any element a in p. a is the element for URLs.\nWe’ll need a few more functions to finish this example.\n\nhtml_elements() filters the output of read_html()/scrape() based on the provided CSS selector. html_elements() can return multiple elements while html_element() always returns one element.\nhtml_text2() retrieves text from HTML elements.\nhtml_attrs() retrieves HTML attributes from HTML elements. html_attrs() can return multiple attributes while html_attr() always returns one attribute.\n\n\ntibble(\n  text = election_page |&gt;\n    html_elements(css = \"p a\") |&gt;\n    html_text2(),\n  url = election_page |&gt;\n    html_elements(css = \"p a\") |&gt;\n    html_attr(name = \"href\")\n)\n\n# A tibble: 355 × 2\n   text                  url                                                    \n   &lt;chr&gt;                 &lt;chr&gt;                                                  \n 1 Barack Obama          /web/20230814004444/https://en.wikipedia.org/wiki/Bara…\n 2 Democratic            /web/20230814004444/https://en.wikipedia.org/wiki/Demo…\n 3 Barack Obama          /web/20230814004444/https://en.wikipedia.org/wiki/Bara…\n 4 Democratic            /web/20230814004444/https://en.wikipedia.org/wiki/Demo…\n 5 presidential election /web/20230814004444/https://en.wikipedia.org/wiki/Unit…\n 6 Democratic            /web/20230814004444/https://en.wikipedia.org/wiki/Demo…\n 7 President             /web/20230814004444/https://en.wikipedia.org/wiki/Pres…\n 8 Barack Obama          /web/20230814004444/https://en.wikipedia.org/wiki/Bara…\n 9 running mate          /web/20230814004444/https://en.wikipedia.org/wiki/Runn…\n10 Vice President        /web/20230814004444/https://en.wikipedia.org/wiki/Vice…\n# ℹ 345 more rows\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nSuppose we are interested in examples of early websites. Wikipedia has a list of URLs from before 1995.\n\nAdd the SelectorGadget web extension to your browser.\nUse library(polite) and library(rvest) to scrape() the following URL.\n\nhttps://web.archive.org/web/20230702163608/https://en.wikipedia.org/wiki/List_of_websites_founded_before_1995\n\nWe are interested in scraping the names of early websites and their URLs. Use SelectorGadget to determine the CSS selectors associated with these HTML elements.\nCreate a tibble with a variable called name and a variable called url.\nRemove duplicate rows with distinct() or filter().\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nFind your own HTML table of interest to scrape.\nUse library(rvest) and library(polite) to scrape the table.\n\n\n\n\n\n\n\nFellegi, I. P. 1972. “On the Question of Statistical Confidentiality.” Journal of the American Statistical Association 67 (337): 7–18. https://www.jstor.org/stable/2284695?seq=1#metadata_info_tab_contents.\n\n\nRavn, Signe, Ashley Barnwell, and Barbara Barbosa Neves. 2020. “What Is “Publicly Available Data”? Exploring Blurred PublicPrivate Boundaries and Ethical Practices Through a Case Study on Instagram.” Journal of Empirical Research on Human Research Ethics 15 (1-2): 40–45. https://doi.org/10.1177/1556264619850736.\n\n\nSomepalli, Gowthami, Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023. “Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 6048–58. https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualie, and Model Data. 2nd edition. Sebastopol, CA: O’Reilly."
  },
  {
    "objectID": "04_web-scraping.html#footnotes",
    "href": "04_web-scraping.html#footnotes",
    "title": "4  Web Scraping",
    "section": "",
    "text": "We are not lawyers. This is not official legal advise. If in-doubt, please contact a legal professional.↩︎\nThis blog and this blog support this statement. Again, we are not lawyers and the HiQ Labs v. LinkedIn decision is complicated because of its long history and conclusion in settlement.↩︎\nThe scale of crawling is so great that there is concern about models converging once all models use the same massive training data. Common Crawl is one example. This isn’t a major issue for generating images but model homogeneity is a big concern in finance.↩︎\nWho deserves privacy is underdiscussed and inconsistent. Every year, newspapers across the country FOIA information about government employees and publish their full names, job titles, and salaries.↩︎\nConsequently, code that may once have worked can break, but using read_csv(&lt;file_path&gt;) to access data once it has been downloaded will work consistently.↩︎\nThe only difference between map() and walk() is their outputs. map() returns the results of a function in a list. walk() returns nothing when used without assignment, and we never use walk() with assignment. walk() is useful when we don’t care about the output of functions and are only interested in their “side-effects”. Common functions to use with walk() are ggsave() and write_csv(). For more information on walk(), see Advanced R.↩︎\nWe recommend using Google Chrome, which has excellent web development tools.↩︎\nIf a website is static, that means that the website is not interactive and will remain the same unless the administrator actively makes changes. Hello World examples is an example of a static website.↩︎\nThe polite documentation describes the bow() function as being used to “introduce the client to the host and ask for permission to scrape (by inquiring against the host’s robots.txt file).”↩︎"
  },
  {
    "objectID": "05_simulation-and-sampling.html#sec-review5",
    "href": "05_simulation-and-sampling.html#sec-review5",
    "title": "5  Simulation and Sampling",
    "section": "5.1 Review",
    "text": "5.1 Review\n\n\n\n\n\n\nPopulation\n\n\n\nA population is the entire set of observations of interest.\nFor example, a population could be everyone residing in France at a point in time. A different population could be every American ages 65 or older.\n\n\n\n\n\n\n\n\nParameter\n\n\n\nA parameter is a numerical quantity that summarizes a population.\nFor example, the population mean and population standard deviation describe important characteristics of many populations.\nMore generally, location parameters, scale parameters, and shape parameters describe many populations.\n\n\n\n\n\n\n\n\nRandom Sample\n\n\n\nA random sample is a random subset of a population.\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nA statistic is a numerical quantity that summarizes a sample.\nFor example, the sample mean and sample standard deviation describe important characteristics of many random samples.\n\n\nParameters are to populations what statistics are to samples. The process of learning about population parameters from statistics calculated from samples is called statistical inference."
  },
  {
    "objectID": "05_simulation-and-sampling.html#introduction",
    "href": "05_simulation-and-sampling.html#introduction",
    "title": "5  Simulation and Sampling",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nSimulation and sampling are important tools for statistics and data science. After reviewing/introducing basic concepts about probability theory and probability distributions, we will discuss two important applications of simulation and sampling.\n\nMonte Carlo simulation: A class of methods where values are repeatedly sampled/simulated from theoretical distributions that model a data generation process. Theoretical distributions, like the normal distribution, have closed-from representations and a finite number of parameters like mean and variance.\nResampling methods: A class of methods where values are repeatedly sampled from observed data to approximate repeatedly sampling from a population. Bootstrapping is a common resampling method.\n\nMonte Carlo methods and resampling methods have a wide range of applications. Monte Carlo simulation is used by election forecasters to predict electoral outcomes and econometricians to understand the properties of estimators. Resampling methods are used in machine learning and causal inference. Both are fundamental methods for agent-based models including microsimulation."
  },
  {
    "objectID": "05_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "href": "05_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "title": "5  Simulation and Sampling",
    "section": "5.3 Fundamentals of Probability Theory",
    "text": "5.3 Fundamentals of Probability Theory\n\n\n\n\n\n\nRandom Variable\n\n\n\n\\(X\\) is a random variable if its value is unknown and/or could change.\n\n\n\\(X\\) could be the outcome from the flip of a coin or the roll of a die. \\(X\\) could also be the amount of rain next July 4th.\n\n\n\n\n\n\nExperiment\n\n\n\nAn experiment is a process that results in a fixed set of possible outcomes.\n\n\n\n\n\n\n\n\nSet\n\n\n\nA set is a collection of objects.\n\n\n\n\n\n\n\n\nSample Space\n\n\n\nA sample space is the set of all possible outcomes for an experiment. We will denote a sample space with \\(\\Omega\\).\n\n\n\n\n\n\n\n\nDiscrete Random Variable\n\n\n\nA set is countable if there is a one-to-one correspondence from the elements of the set to some (finite) or all (countably infinite) positive integers (i.e. 1 = heads and 2 = tails).\nA random variable is discrete if its sample space is countable (finite or countably infinite).\n\n\n\n\n\n\n\n\nContinuous Random Variable\n\n\n\nA random variable is continuous if its sample space is any value in a \\(\\mathbb{R}\\) (real) interval.\nThere are infinite possible values in a real interval so the sample space is uncountable."
  },
  {
    "objectID": "05_simulation-and-sampling.html#discrete-random-variables",
    "href": "05_simulation-and-sampling.html#discrete-random-variables",
    "title": "5  Simulation and Sampling",
    "section": "5.4 Discrete Random Variables",
    "text": "5.4 Discrete Random Variables\n\n\n\n\n\n\nProbability Mass Function\n\n\n\nA probability mass function (PMF) computes the probability of an event in the sample space of a discrete random variable.\n\\[\np(x) = P(X = x)\n\\tag{5.1}\\]\nwhere \\(0 \\le p(x) \\le 1\\) and \\(\\sum_{x \\in \\Omega} p(x) = 1\\)\n\n\n\n\nCode\ntibble(\n  a = factor(1:6),\n  `P(X = a)` = rep(1 / 6, 6)\n) |&gt;\n  ggplot(aes(a, `P(X = a)`)) +\n  geom_col() +\n  labs(title = \"PMF for rolling a fair die\")\n\n\n\n\nFigure 5.1: PMF for rolling a fair die\n\n\n\n\n\nNow we can make statements like \\(P(X = a)\\). For example, \\(P(X = 3) = \\frac{1}{6}\\).\n\n5.4.1 Bernoulli Distribution\nA Bernoulli random variable takes on the value \\(1\\) with probability \\(p\\) and \\(0\\) with probability \\(1 - p\\). It is often used to represent coins. When \\(p = \\frac{1}{2}\\) we refer to the coin as “fair”.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Ber(p)\n\\tag{5.2}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) =\n\\begin{cases}\n1 - p &\\text{ if } x = 0 \\\\\np &\\text{ if } x = 1\n\\end{cases} = p^x(1 - p)^{1-x}\n\\tag{5.3}\\]\n\n\n5.4.2 Binomial Distribution\nA binomial random variable is the number of events observed in \\(n\\) repeated Bernoulli trials.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Bin(n, p)\n\\tag{5.4}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) = {n \\choose x} p^x(1 - p)^{n - x}\n\\tag{5.5}\\]\nWe can calculate the theoretical probability of a given draw from a binomial distribution using this PDF. For example, suppose we have a binomial distribution with \\(10\\) trials and \\(p = \\frac{1}{2}\\). The probability of drawing exactly six \\(1\\)s and four \\(0\\)s is\n\\[\np(X = 6) = \\frac{10!}{6!4!} 0.5^6(1 - 0.5)^{10 - 6} \\approx 0.2051\n\\tag{5.6}\\]\nWe can do similar calculations for each value between \\(0\\) and \\(10\\).\nWe can also take random draws from the distribution. Figure 5.2 shows 1,000 random draws from a binomial distribution with 10 trials and p = 0.5. The theoretical distribution is overlaid in red.\n\n\nCode\ntibble(\n  x = rbinom(n = 1000, size = 10, prob = 0.5)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(breaks = 0:10) +\n  geom_point(data = tibble(x = 0:10, y = map_dbl(0:10, dbinom, size = 10, prob = 0.5)),\n             aes(x, y),\n             color = \"red\") +\n  labs(\n    title = \"1,000 samples of a binomial RV\",\n    subtitle = \"Size = 10; prob = 0.5\",\n    y = NULL\n  ) \n\n\n\n\nFigure 5.2: 1,000 random draws from a binomial distribution with 10 trials and p = 0.5\n\n\n\n\n\n\n\n\n\n\n\nSampling Error\n\n\n\nSampling error is the difference between sample statistics (estimates of population parameters) and population parameters.\n\n\nThe difference between the red dots and black bars in Figure 5.2 is caused by sampling error.\n\n\n5.4.3 Distributions Using R\nMost common distributions have R functions to\n\ncalculate the density of the pdf/pmf for a specific value\ncalculate the probability of observing a value less than \\(a\\)\ncalculate the value associated with specific quantiles of the pdf/pmf\nrandomly sample from the probability distribution\n\nLet’s consider a few examples:\nThe following answers the question: “What is the probability of observing 10 events in 10 trials when p = 0.5?”\n\ndbinom(x = 10, size = 10, prob = 0.5)\n\n[1] 0.0009765625\n\n\n\nThe following answers the question: “What’s the probability of observing 3 or fewer events in 10 trials when p = 0.5”\n\npbinom(q = 3, size = 10, prob = 0.5)\n\n[1] 0.171875\n\n\n\nThe following answers the question: “What is a 10th percentile number of events to see in 10 trials when p = 0.5?\n\nqbinom(p = 0.1, size = 10, prob = 0.5)\n\n[1] 3\n\n\n\nThe following randomly draw ten different binomially distributed random variables.\n\nrbinom(n = 10, size = 10, prob = 0.5)\n\n [1] 3 8 2 7 5 4 6 3 6 4\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nAdd dbinom() for 0, 1, and 2 when size = 10 and prob = 0.5.\nCalculate pbinom() with size = 10 and prob = 0.5. Why do you get the same answer?\nPlug the result from step 1/step 2 into qbinom() with size = 10 and prob = 0.5. Why do you get the answer 2?\nUse rbinom() with size = 10 and prob = 0.5 to sample 10,000 binomially distributed random variables and assign the output to x. Calculate mean(x &lt;= 2). How does the answer compare to step 1/step 2?\n\n\n\n\n\n\n\n\n\nPseudo-random numbers\n\n\n\nComputers use pseudo-random numbers to generate samples from probability distributions. Modern pseudo-random samplers are very random.\nUse set.seed() to make pseudo-random sampling reproducible.\n\n\n\n\n5.4.4 Poisson Random Variable\nA poisson random variable is the number of events that occur in a fixed period of time. For example, a poisson distribution can be used to model the number of visits in an emergency room between 1AM and 2AM.\nWe show that a random variable is poisson-distributed with\n\\[\nX \\sim Pois(\\lambda)\n\\tag{5.7}\\]\nThe parameter \\(\\lambda\\) is both the mean and variance of the poisson distribution. The PMF of a poisson random variable is\n\\[\np(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\tag{5.8}\\]\nFigure 5.3 shows 1,000 draws from a poisson distribution with \\(\\lambda = 10\\).\n\n\nCode\nset.seed(20200905)\n\ntibble(\n  x = rpois(1000, lambda = 10)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(limits = c(0, 29)) +\n  stat_function(fun = dpois, n = 30, color = \"red\", args = list(lambda = 10)) + \n  labs(\n    title = \"1,000 samples of a Poisson RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\nFigure 5.3: 1,000 samples of a Poisson RV\n\n\n\n\n\n\n\n5.4.5 Categorical Random Variable\nWe can create a custom discrete probability distribution by enumerating the probability of each event in the sample space. For example, the PMF for the roll of a fair die is\n\\[\np(x) =\n\\begin{cases}\n\\frac{1}{6} & \\text{if } x = 1\\\\\n\\frac{1}{6} & \\text{if } x = 2\\\\\n\\frac{1}{6} & \\text{if } x = 3\\\\\n\\frac{1}{6} & \\text{if } x = 4\\\\\n\\frac{1}{6} & \\text{if } x = 5\\\\\n\\frac{1}{6} & \\text{if } x = 6\n\\end{cases}\n\\tag{5.9}\\]\nThis PMF is visualized in Figure 5.1. We can sample from this PMF with\n\nsample(x = 1:6, size = 1)\n\n[1] 3\n\n\nWe can also sample with probabilities that differ for each event:\n\nsample(\n  x = c(\"rain\", \"sunshine\"), \n  size = 1, \n  prob = c(0.1, 0.9)\n)\n\n[1] \"sunshine\"\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nSample 1,000 observations from a Poisson distribution with \\(\\lambda = 20\\).\nSample 1,000 observations from a normal distribution with \\(\\mu = 20\\) and \\(\\sigma = \\sqrt{20}\\).\nVisualize and compare both distribution.\n\n\n\nWhen \\(\\lambda\\) is sufficiently large, the normal distribution is a reasonable approximation of the poisson distribution."
  },
  {
    "objectID": "05_simulation-and-sampling.html#continuous-random-variables",
    "href": "05_simulation-and-sampling.html#continuous-random-variables",
    "title": "5  Simulation and Sampling",
    "section": "5.5 Continuous Random Variables",
    "text": "5.5 Continuous Random Variables\n\n\n\n\n\n\nProbability Density Function (PDF)\n\n\n\nA probability density function is a non-negative, integrable function for each real value \\(x\\) that shows the relative probability of values of \\(x\\) for an absolutely continuous random variable \\(X\\).\nWe note PDF with \\(f_X(x)\\).\n\n\n\n\n\n\n\n\nCumulative Distribution Function (CDF)\n\n\n\nA cumulative distribution function (cdf) shows the probability of a random variable \\(X\\) taking on any value less than or equal to \\(x\\).\nWe note CDF with \\(F_X(x) = P(X \\le x)\\)\n\n\nHere is the PDF for a standard normal random variable:\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"PDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\n\n\n\nIf we integrate the entire function we get the CDF.\nCumulative Density Function (CDF): A function of a random variable \\(X\\) that returns the probability that the value \\(X &lt; x\\).\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = pnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"CDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n5.5.1 Uniform Distribution\nUniform random variables have equal probability for every value in the sample space. The distribution has two parameters: minimum and maximum. A standard uniform random has minimum = 0 and maximum = 1.\nWe show that a random variable is uniform distributed with\n\\[\nX \\sim U(a, b)\n\\tag{5.10}\\]\nThe PDF of a uniform random variable is\n\\[\nf(x) =\n\\begin{cases}\n\\frac{1}{b - a} & \\text{if } x \\in [a, b] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\tag{5.11}\\]\nStandard uniform random variables are useful for generating other random processes and imputation.\n\n\nCode\nset.seed(20200904)\n\ntibble(\n  x = runif(1000)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dunif, n = 101, color = \"red\") + \n  labs(\n    title = \"1,000 samples of a standard uniform RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n5.5.2 Normal Distribution\nThe normal distribution is the backbone of statistical inference because of the central limit theorem.\nWe show that a random variable is normally distributed with\n\\[\nX \\sim N(\\mu, \\sigma)\n\\tag{5.12}\\]\nThe PDF of a normally distributed random variable is\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right) ^ 2\\right]\n\\tag{5.13}\\]\n\n\n\n\n\n\nFundamental Probability Formula for Intervals\n\n\n\nThe probability that an absolutely continuous random variable takes on any specific value is always zero because the sample space is uncountable. Accordingly, we express the probability of observing events within a region for absolutely continuous random variables.\nIf \\(X\\) has a PDF and \\(a &lt; b\\), then\n\\[\nP(a \\le X \\le b) = P(a \\le X &lt; b) = P(a &lt; X \\le b) = P(a &lt; X &lt; b) = \\int_a^bf(x)dx = F_X(b) - F_X(a)\n\\tag{5.14}\\]\n\n\nThe last portion of this inequality is fundamental to working with continuous probability distributions and is the backbone of much of any intro to statistics course. For example, the probability, \\(P(X &lt; 0)\\) is represented by the blue region below.\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  geom_area(stat = \"function\", fun = dnorm, fill = \"blue\", xlim = c(-4, 0)) +\n  labs(\n    title = \"PDF for a standard normal random variable\",\n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nStudent’s t-distribution and the normal distribution are closely related.\n\nUse pnorm() to calculate \\(P(X &lt; -1)\\) for a standard normal distribution.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 10.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 100.\n\n\n\nObserve how the normal distribution becomes a better approximation for Student’s t-distribution when the degrees of freedom increases.\n\n\n5.5.3 Exponential Distribution\nAn exponential random variable is the wait time between events for a poisson random variable. It is useful for modeling wait time. For example, an exponential distribution can be used to model the wait time between arrivals in an emergency room between 1AM and 2AM. It has one parameter: rate (\\(\\lambda\\)).\nWe show that a random variable is exponentially distributed with\n\\[\nX \\sim Exp(\\lambda)\n\\tag{5.15}\\]\nThe PDF of an exponential random variable is\n\\[\nf(x) = \\lambda\\exp(-\\lambda x)\n\\tag{5.16}\\]\n\n\nCode\ntibble(\n  x = rexp(n = 1000, rate = 1)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_density() +\n  stat_function(fun = dexp, n = 101, args = list(rate = 1), color = \"red\") + \n  labs(\n    title = \"1,000 samples of an exponential RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n5.5.4 Other Distributions\n\nGeometric RV: Number of Bernoulli trials up to and including the \\(1^{st}\\) event\nNegative Binomial RV: Number of Bernoulli trials up to and including the \\(r^{th}\\) event\nGamma RV: Time until the \\(\\alpha\\) person arrives"
  },
  {
    "objectID": "05_simulation-and-sampling.html#parametric-density-estimation",
    "href": "05_simulation-and-sampling.html#parametric-density-estimation",
    "title": "5  Simulation and Sampling",
    "section": "5.6 Parametric Density Estimation",
    "text": "5.6 Parametric Density Estimation\nA key exercise in statistics is selecting a probability distribution to represent data and then learning the parameters of probability distributions from the data. The process is often called model fitting.\nWe are focused on parametric density estimation. Later, we will focus on nonparameteric density estimation. This section will focus on frequentist inference of population parameters from observed data. Later, we will adopt a Bayesian approach to inference.\n\n5.6.1 Maximum Likelihood Estimation\nAll of the probability distributions we have observed have a finite number of parameters. Maximum likelihood estimation is a common method for estimating these parameters.\nThe general process is\n\nPick the probability distribution that fits the observed data.\nIdentify the finite number of parameters associated with the probability distribution.\nCalculate the parameters that maximize the probability of the observed data.\n\n\n\n\n\n\n\nLikelihood\n\n\n\nLet \\(\\vec{x}\\) be observed data and \\(\\theta\\) be a parameter or parameters from a chosen probability distribution. The likelihood is the joint probability of the observed data conditional on values of the parameters.\nThe likelihood of discrete data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n p(x_i|\\theta)\n\\tag{5.17}\\]\nThe likelihood of continuous data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n f(x_i|\\theta)\n\\tag{5.18}\\]\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\n\nMaximum likelihood estimation is a process for estimating parameters for a given distribution that maximizes the log likelihood.\nIn other words, MLEs find the estimated parameters that maximize the probability of observing the observed set of data.\n\n\nWe won’t unpack how to derive the maximum likelihood estimators1 but it is easy to look up most MLEs.\n\nBinomial distribution MLEs\nSuppose we have a sample of data \\(x_1, ..., x_m\\). If the number of trials \\(n\\) is already known, then \\(p\\) is the only parameter for the binomial distribution that needs to be estimated. The MLE for \\(p\\) is \\(\\hat{p} = \\frac{\\sum_{i = 1}^n x_i}{mn}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat{p}\\).\n\nset.seed(20230909)\nx &lt;- rbinom(n = 8, size = 10, prob = 0.3)\n\nx\n\n[1] 4 3 6 3 4 3 3 2\n\n\n\nmle_binom &lt;- sum(x) / (10 * 8)\n\nmle_binom\n\n[1] 0.35\n\n\n\n\nNormal distribution MLEs\n\\(\\mu\\) and \\(\\sigma\\) are the parameters of a normal distribution. The MLEs for a normal distribution are \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i = \\bar{x}\\) and \\(s^2 = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^2\\).2\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i\\) and \\(\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\).\n\nset.seed(20230909)\nx &lt;- rnorm(n = 200, mean = 0, sd = 1)\n\n\nmean_hat &lt;- mean(x)\n\nmean_hat\n\n[1] 0.02825125\n\nsigma2_hat &lt;- mean((x - mean(x)) ^ 2)\n\nsigma2_hat\n\n[1] 0.8119682\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = mean_hat, sd = sqrt(sigma2_hat)))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nExponential distribution MLEs\n\\(\\lambda\\) is the only parameter of an exponential distribution. The MLE for an exponential distribution is \\(\\hat\\lambda = \\frac{1}{\\bar{x}}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\frac{1}{\\bar{x}}\\).\n\nset.seed(20230909)\nx &lt;- rexp(n = 200, rate = 10)\n\nmle_exp &lt;- 1 / mean(x)\n\nmle_exp\n\n[1] 10.58221\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dexp, color = \"red\", args = list(rate = mle_exp))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCreate the vector in the code chunk below:\n\n\n\nCode\nx &lt;- c(\n  30970.787, 10901.544, 15070.015, 10445.772, 8972.258, \n  15759.614, 13341.328, 18498.858, 134462.066, 17498.930, \n  7112.306, 27336.795, 75526.381, 110123.606, 32910.618, \n  16764.452, 21244.380, 18952.455, 954373.470, 4219.635,\n  7078.766, 27657.996, 18337.097, 14566.525, 14220.000, \n  21457.202, 9322.311, 26018.018, 96325.728, 26780.329, \n  25833.356, 10719.360, 8642.935, 29302.623, 10517.174,\n  33831.547, 339077.456, 5805.707, 141505.710, 28168.790, \n  10446.378, 4993.349, 27502.949, 35519.162, 45761.505, \n  26163.096, 72163.668, 15515.435, 69396.895, 84972.590, \n  67248.460, 26966.374, 24624.339, 4779.110, 23330.279,\n  196311.913, 20517.739, 80257.587, 32108.466, 9735.061, \n  20502.579, 2544.004, 165909.040, 20949.512, 16643.695, \n  30267.741, 8359.024, 13355.154, 8425.988, 4491.550,\n  32071.872, 61648.149, 75074.135, 62842.985, 26040.648, \n  68733.979, 63368.710, 11157.211, 5782.610, 3629.674, \n  44399.230, 2852.381, 8200.453, 41249.003, 15006.791,\n  808974.653, 30705.915, 6341.954, 28208.144, 5409.821,\n  54566.805, 10894.864, 4583.550, 31110.875, 43474.872, \n  69059.161, 33054.574, 8789.910, 218887.477, 11051.292, \n  3366.743, 63853.329, 68756.561, 48031.259, 11707.191,\n  26593.634, 8868.942, 19225.309, 27704.670, 10666.549, \n  47151.963, 20343.604, 123932.502, 33030.986, 5412.023, \n  23540.382, 9894.513, 52742.541, 21397.990, 25100.143,\n  23757.882, 48347.300, 4325.134, 23816.776, 11907.656, \n  24179.849, 25967.574, 7531.294, 15131.240, 21595.781, \n  40473.936, 35390.849, 4060.563, 55334.157, 37058.771, \n  34050.456, 17351.500, 7453.829, 48131.565, 10576.746,\n  26450.754, 33592.986, 21425.018, 34729.337, 77370.078, \n  5819.325, 9067.356, 19829.998, 20120.706, 3637.042, \n  44812.638, 22930.229, 29683.776, 76366.822, 15464.594, \n  1273.101, 53036.266, 2846.294, 114076.200, 14492.680, \n  55071.554, 31597.849, 199724.125, 52332.510, 98411.129, \n  43108.506, 6580.620, 12833.836, 8846.348, 7599.796, \n  6952.447, 30022.143, 24829.739, 40784.581, 8997.219,\n  3786.354, 11515.298, 116515.617, 137873.967, 3282.185,\n  107886.676, 13184.850, 51083.235, 2907.886, 51827.538, \n  37564.196, 23196.399, 20169.037, 9020.364, 11118.250, \n  56930.060, 11657.302, 84642.584, 44948.450, 16610.166, \n  5509.231, 4770.262, 15614.233, 5993.999, 22628.114\n)\n\n\n\nVisualize the data with a relative frequency histogram.\nCalculate the MLEs for a normal distribution and add a normal distribution to the visualization in red.\nCalculate the MLEs for a log-normal distribution and add a log-normal distribution to the visualization in blue."
  },
  {
    "objectID": "05_simulation-and-sampling.html#multivariate-random-variables",
    "href": "05_simulation-and-sampling.html#multivariate-random-variables",
    "title": "5  Simulation and Sampling",
    "section": "5.7 Multivariate Random Variables",
    "text": "5.7 Multivariate Random Variables\nWe’ve explored univariate or marginal distributions thus far. Next, we will focus on multivariate distributions.\n\n\n\n\n\n\nMultivariate Distribution\n\n\n\nA multivariate distribution is a probability distribution that shows the probability (discrete) or relative probability (continuous) of more than one random variable.\nMultivariate distributions require describing characteristics of random variables and the relationships between random variables.\n\n\n\n5.7.1 Multivariate Normal Distribution\nThe multivariate normal distribution is a higher-dimensional version of the normal distribution.\nInstead of a single mean and a single variance, the \\(k\\)-dimensional multivariate normal distribution has a vector of means of length \\(k\\) and a \\(k\\)-by-\\(k\\) variance-covariance matrix3. The vector describes the central tendencies of each dimension of the multivariate distribution and the matrix describe the variance of the distributions and relationships between the distributions.\nWe show that a random vector is multivariate normally distributed with\n\\[\n\\vec{X} \\sim \\mathcal{N}(\\vec\\mu, \\boldsymbol\\Sigma)\n\\tag{5.19}\\]\nThe PDF of a multivariate normally distributed random variable is\n\\[\nf(x) = (2\\pi)^{-k/2}det(\\boldsymbol\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\vec{x} - \\vec\\mu)^T\\boldsymbol\\Sigma^{-1}(\\vec{x} - \\vec\\mu)\\right)\n\\tag{5.20}\\]\nFunctions for working with multi-variate normal distributions from library(MASS). Figure 5.4 shows three different random samples from 2-dimensional multivariate normal distributions.\n\nSigma1 &lt;- matrix(c(1, 0.8, \n                   0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n  \nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma1) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nSigma2 &lt;- matrix(c(1, 0.2, \n                   0.2, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma2) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nSigma3 &lt;- matrix(c(1, -0.8, \n                   -0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma3) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nFigure 5.4: Samples from Multivariate Normal Distributions\n\n\n\n\n\n(a) Strong Positive Covariance\n\n\n\n\n\n\n\n(b) Weak Covariance\n\n\n\n\n\n\n\n(c) Strong Negative Covariance"
  },
  {
    "objectID": "05_simulation-and-sampling.html#monte-carlo-methods",
    "href": "05_simulation-and-sampling.html#monte-carlo-methods",
    "title": "5  Simulation and Sampling",
    "section": "5.8 Monte Carlo Methods",
    "text": "5.8 Monte Carlo Methods\nSimulation methods, including Monte Carlo simulation, are used for policy analysis:\n\nFiveThirtyEight and the New York Times use Monte Carlo simulation to predict the outcomes of elections.\nThe Social Security Administration uses microsimulation to evaluate the distributional impact of Social Security reforms.\nThe Census Bureau uses simulation to understand the impact of statistical disclosure control on released data.\nEconometricians and statisticians use Monte Carlo simulation to demonstrate the properties of estimators.\n\nWe can make probabilistic statements about common continuous random variables because their PDFs are integrable or at least easy enough to approximate with lookup tables. We can make probabilistic statements about common discrete random variables with summation.\nBut we often want to make probabilistic statements about uncommon or complex probability distributions. Maybe the probability distribution of the random variable doesn’t have a tractable integral (i.e. the area under the curve can’t practically be computed). Or maybe there are too many potential outcomes (e.g. rays of light emitting from a light bulb in the Marble Science video linked at the top).\nMonte Carlo: A Monte Carlo method estimates a deterministic quantity using stochastic (random) sampling.\nMonte Carlo but easier this time: A Monte Carlo method takes hundreds or thousands of independent samples from a random variable or variables and then approximates fixed population quantities with summaries of those draws. The quantities could be population parameters like a population mean or probabilities.\nMonte Carlo methods have three major applications:\n\nSampling – Monte Carlo simulation allows for sampling from complex probability distributions. The samples can be used to model real-world events (queues), to model outcomes with uncertain model inputs (election modeling), to generate fake data with known parameters to evaluate statistical methods (model selection when assumptions fail), and to draw from the posteriors of Bayesian models.\nNumerical integration – Integration, as noted above, is important to calculating probabilities and ultimately calculating quantities like expected value or the intervals. Monte Carlo methods can approximate multidimensional integrals that will never be directly solved by computers or simplify estimating probabilities when there are uncountably many potential outcomes (Solitaire).\nOptimization – Monte Carlo methods can be used for complex optimization. We will not focus on optimization.\n\nLet’s explore some examples:\n\n5.8.1 Example 1: Coin Tossing\nWe can calculate the proportion of tosses of a fair coin that we expect to turn up heads by finding the expected value of the binomial distribution and dividing by the number of tosses. But suppose we can’t… Or maybe we wish to confirm our calculations with simulations…\nLet’s try repeated sampling from a binomial distribution to approximate this process:\n\n#' Count the proportion of n tosses that turn up heads\n#'\n#' @param n An integer for the number of tosses\n#'\n#' @return The proportion of n tosses that turn up heads\n#' \ncount_heads &lt;- function(n) {\n  \n  # toss the fair coin n times\n  coin_tosses &lt;- rbinom(n = n, size = 1, prob = 0.5)\n    \n  coin_tosses &lt;- if_else(coin_tosses == 1, \"heads\", \"tails\")\n  \n  # calculate the proportion of heads\n  prop_heads &lt;- mean(coin_tosses == \"heads\")\n  \n  return(prop_heads)\n  \n}\n\nLet’s toss the coin ten times.\n\nset.seed(11)\ncount_heads(n = 10)\n\n[1] 0.3\n\n\nOk, we got 0.3, which we know isn’t close to the expected proportion of 0.5. What if we toss the coin 1 million times.\n\nset.seed(20)\ncount_heads(n = 1000000)\n\n[1] 0.499872\n\n\nOk, that’s more like it.\n\\[\\cdot\\cdot\\cdot\\]\nMonte Carlo simulation works because of the law of large numbers. The law of large numbers states that the probability that the average of trials differs from the expected value converges to zero as the number of trials goes to infinity.\nMonte Carlo simulation basically repeats the ideas behind frequentist inferential statistics. If we can’t measure every unit in a population then we can sample a representative population and estimate parameters about that population.\nThe keys to Monte Carlo simulation are randomness and independent and identically distributed sampling (i.i.d.).\n\n\n5.8.2 Example 2: Bootstrap Sampling\nOn average, a bootstrap sample includes about 63% of the observations from the data that are sampled. This means that an individual bootstrap sample excludes 37% of the observations from the source data!\nSo if we bootstrap sample from a vector of length 100, then \\(\\frac{63}{100}\\) values will end up in the bootstrap sample on average and \\(\\frac{37}{100}\\) of the values will be repeats on average.\nWe can explore this fact empirically with Monte Carlo simulation using repeated samples from a categorical distribution. We will use sample().\n\n#' Calculate the proportion of unique values from a vector of integers included \n#' in a bootstrap sample\n#'\n#' @param integers A vector of integers\n#'\n#' @return The proportion of integers included in the bootstrap sample\n#' \ncount_uniques &lt;- function(integers) {\n  \n  # generate a bootstrap sample\n  samples &lt;- sample(integers, size = length(integers), replace = TRUE)\n  \n  # calculate the proportion of unique values from the original vector\n  prop_unique &lt;- length(unique(samples)) / length(integers)\n  \n  return(prop_unique)\n  \n}\n\nLet’s bootstrap sample 100,000 times.\n\n# pre-allocate the output vector for efficient computation\nprop_unique &lt;- vector(mode = \"numeric\", length = 100000)\nfor (i in seq_along(prop_unique)) {\n  \n  prop_unique[i] &lt;- count_uniques(integers = 1:100)\n  \n}\n\nFinally, calculate the mean proportion and estimate the expected value.\n\nmean(prop_unique)\n\n[1] 0.6337935\n\n\nWe can also calculate a 95% confidence interval using the bootstrap samples.\n\nquantile(prop_unique, probs = c(0.025, 0.975))\n\n 2.5% 97.5% \n 0.57  0.69 \n\n\n\n\n5.8.3 Example 3: \\(\\pi\\)\nConsider one of the examples from Marble Science: Monte Carlo Simulation. Imagine we don’t know \\(\\pi\\) but we know that the equation for the area of a square is \\(r ^ 2\\) and the equation for the area of a circle is \\(\\pi r ^ 2\\). If we know the ratio of the areas of the circle and the square, then we can solve for \\(\\pi\\).\n\\[\n\\frac{\\text{Area of Cirle}}{\\text{Area of Square}} = \\frac{\\pi r ^ 2}{r ^ 2} = \\pi\n\\tag{5.21}\\]\nThis is simply solved with Monte Carlo simulation. Randomly sample a bivariate uniform random variables and count how frequently the values are inside of the square or inside the circle.\n\nexpand_grid(\n  x = seq(0, 4, 0.1),\n  y = seq(0, 2, 0.1)\n) |&gt;\nggplot() +\n  ggforce::geom_circle(aes(x0 = 2, y0 = 1, r = 1), fill = \"blue\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), fill = \"red\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 3, ymin = 0, ymax = 2), fill = NA, color = \"black\") +\n  coord_fixed()\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\nnumber_of_samples &lt;- 2000000\n\n# sample points in a rectangle with x in [0, 3] and y in [0, 2]\nset.seed(20210907)\nsamples &lt;- tibble(\n  x = runif(number_of_samples, min = 0, max = 3),\n  y = runif(number_of_samples, min = 0, max = 2)\n)\n\n# calculate if (x, y) is in the circle, the square, or neither\nsamples &lt;- samples |&gt;\n  mutate(\n    in_square = between(x, 0, 1) & between(y, 0, 1),\n    in_circle = (x - 2) ^ 2 + (y - 1) ^ 2 &lt; 1\n  ) \n\n# calculate the proportion of samples in each shape\nprop_in_shapes &lt;- samples |&gt;\n  summarize(\n    prop_in_square = mean(in_square), \n    prop_in_circle = mean(in_circle)\n  ) \n\n# calculate the ratio\nprop_in_shapes |&gt;\n  mutate(prop_in_circle / prop_in_square) |&gt;\n  print(digits = 3)\n\n# A tibble: 1 × 3\n  prop_in_square prop_in_circle `prop_in_circle/prop_in_square`\n           &lt;dbl&gt;          &lt;dbl&gt;                           &lt;dbl&gt;\n1          0.166          0.524                            3.15\n\n\nThe answer approximates \\(\\pi\\)!\n\n\n5.8.4 Example 4: Simple Linear Regression\nThe goal of statistical inference is to use data, statistics, and assumptions to infer parameters and probabilities about a population. Typically we engage in point estimation and interval estimation.\nSometimes it is useful to reverse this process to understand and confirm the properties of estimators. That means starting with known population parameters, simulating hundreds or thousands of samples from that population, and then observing point estimates and interval estimates over those samples.\n\nLinear Regression Assumptions\n\nThe population model is of the linear form \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\nThe estimation data come from a random sample or experiment\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) independently and identically distributed (i.i.d.)\n\\(x\\) has variance and there is no perfect collinearity in \\(x\\)\n\n\n\nStatistics\nIf we have one sample of data, we can estimate points and intervals with the following estimators:\nThe residual standard error is\n\\[\n\\hat\\sigma = \\frac{\\sum e_i^2}{(n - 2)}\n\\tag{5.22}\\]\nThe estimate of the slope is\n\\[\n\\hat\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\n\\tag{5.23}\\]\nThe standard error of the estimate of the slope, which can be used to calculate t-statistics and confidence intervals, is\n\\[\n\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2}\n\\tag{5.24}\\]\nThe estimate of the intercept term is\n\\[\n\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1\\bar{x}\n\\tag{5.25}\\]\nThe standard error of the intercept is\n\\[\n\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]}\n\\tag{5.26}\\]\n\n\nMonte Carlo Simulation\nConsider a simple linear regression model with the following population model:\n\\[y = 5 + 15x + \\epsilon\\]\nWe can calculate the above statistics over repeated sampling and confirm their asymptotic properties with Monte Carlo simulation.\nFirst, create 1,000 random samples from the population.\n\nset.seed(20210906)\n\ndata &lt;- map(\n  .x = 1:1000,\n  .f = ~ tibble(\n    x = rnorm(n = 10000, mean = 0, sd = 2),\n    epsilon = rnorm(n = 10000, mean = 0, sd = 10),\n    y = 5 + 15 * x + epsilon\n  )\n)\n\nNext, estimate a simple linear regression model for each draw of the population. This step includes calculating \\(\\hat\\sigma\\), \\(\\hat\\beta_1\\), \\(\\hat\\beta_0\\), \\(\\hat{SE}(\\hat\\beta_1)\\), and \\(\\hat{SE}(\\hat\\beta_0)\\).\n\nestimated_models &lt;- map(\n  .x = data,\n  .f = ~ lm(y ~ x, data = .x)\n)\n\nNext, we extract the coefficients and confidence intervals.\n\ncoefficients &lt;- map_df(\n  .x = estimated_models,\n  .f = tidy,\n  conf.int = TRUE\n)\n\nLet’s look at estimates of the residual standard error. The center of the distribution closely matches the population standard deviation of the error term.\n\nmodel_metrics &lt;- map_df(\n  .x = estimated_models,\n  .f = glance\n) \n\nmodel_metrics |&gt;\n  ggplot(aes(sigma)) +\n  geom_histogram() +\n  labs(title = \"Plot of the estimated residual standard errors\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLet’s plot the coefficients. The centers approximately match the population intercept of 5 and slope of 15.\n\ncoefficients |&gt;\n  ggplot(aes(estimate)) +\n  geom_histogram() +\n  facet_wrap(~term, scales = \"free_x\") +\n  labs(title = \"Coefficients estimates across 10,000 samples\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe standard deviation of the coefficients also matches the standard errors.\n\\[\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2} = \\sqrt\\frac{10^2}{40,000} = 0.05\\]\n\\[\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]} = \\sqrt{10^2\\left[\\frac{1}{10,000} + 0\\right]} = 0.1\\]\n\ncoefficients |&gt;\n  group_by(term) |&gt;\n  summarize(\n    mean(estimate), \n    sd(estimate)\n  )\n\n# A tibble: 2 × 3\n  term        `mean(estimate)` `sd(estimate)`\n  &lt;chr&gt;                  &lt;dbl&gt;          &lt;dbl&gt;\n1 (Intercept)             5.00         0.100 \n2 x                      15.0          0.0482\n\n\nLet’s look at how often the true parameter is inside the 95% confidence interval. It’s close although not exactly 95%.\n\ncoefficients |&gt;\n  filter(term == \"x\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 15 & conf.high &gt;= 15))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1           0.959\n\ncoefficients |&gt;\n  filter(term == \"(Intercept)\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 5 & conf.high &gt;= 5))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1            0.95\n\n\n\n\n\n5.8.5 Example 5: Queuing Example\nSuppose we have a queue at a Social Security field office. Let \\(t\\) be time. When the office opens, \\(t = 0\\) and the queue is empty.\nLet, \\(T_i\\) be the interarrival time and \\(T_i \\sim exp(\\lambda_1)\\)\nLet, \\(S_i\\) be the service time time and \\(S_I \\sim exp(\\lambda_2)\\)\nFrom these two random variables, we can calculate the arrival times, departure times, and wait times for each customer.\n\nThe arrival times are the cumulative sum of the interarrival times.\nThe wait times are zero if a person arrives after the person before them and the difference between the prior person’s departure and the current person’s arrival otherwise.\nThe departure time is arrival time plus the wait time plus the service time.\n\n\nset.seed(19920401)\nqueue &lt;- generate_queue(t = 100, lambda = 1, mu = 1)\n\nqueue\n\n# A tibble: 100 × 5\n   interarrival_time arrival_time service_time wait_time departure_time\n               &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1             0.467        0.467       5.80        0              6.27\n 2             1.97         2.43        0.0892      3.83           6.36\n 3             2.70         5.13        1.26        1.22           7.61\n 4             0.335        5.47        4.85        2.14          12.5 \n 5             0.372        5.84        1.89        6.62          14.3 \n 6             1.72         7.56        0.507       6.78          14.9 \n 7             2.28         9.84        0.932       5.01          15.8 \n 8             0.339       10.2         1.18        5.61          17.0 \n 9             2.54        12.7         1.42        4.25          18.4 \n10             0.572       13.3         0.157       5.10          18.5 \n# ℹ 90 more rows\n\n\n\nflow &lt;- tibble::tibble(\n  time = c(queue$arrival_time, queue$departure_time),\n  type = c(rep(\"arrival\", length(queue$arrival_time)), \n           rep(\"departure\", length(queue$departure_time))), \n  change = c(rep(1, length(queue$arrival_time)), rep(-1, length(queue$departure_time))),\n) |&gt;\n  arrange(time) |&gt; \n  filter(time &lt; 100) |&gt; \n  mutate(queue = cumsum(change) - 1)\n\nflow |&gt;\n  ggplot(aes(time, queue)) +\n  geom_step() +\n  labs(title = \"Simulated queue at the Social Security office\")\n\n\n\n\nThis is interesting, but it’s still only one draw from a Monte Carlo simulation. What if we are interested in the distribution of wait times for the fifth customer?\n\n#' Generate wait times at the queue\n#'\n#' @param person_number An integer for the person of interest\n#' @param iterations An integer for the number of Monte Carlo iterations\n#' @param t A t for the maximum time\n#'\n#' @return A vector of wait times\n#' \ngenerate_waits &lt;- function(person_number, iterations, t) {\n  \n  wait_time &lt;- vector(mode = \"numeric\", length = iterations)\n  for (i in seq_along(wait_time)) {\n    \n    wait_time[i] &lt;- generate_queue(t = t, lambda = 1, mu = 1)$wait_time[person_number]\n    \n  }\n  \n  return(wait_time)\n  \n}\n\nset.seed(20200908)\nwait_time &lt;- generate_waits(person_number = 5, iterations = 10000, t = 50)\n\nmean(wait_time)\n\n[1] 1.464371\n\nquantile(wait_time, probs = c(0.025, 0.5, 0.975))\n\n     2.5%       50%     97.5% \n0.0000000 0.9222193 5.8742015 \n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCreate a Monte Carlo simulation of an unfair coin toss where p = 0.6.\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nSuppose we have three independent normally-distributed random variables.\n\\[X_1 \\sim N(\\mu = 0, \\sigma = 1)\\]\n\\[X_2 \\sim N(\\mu = 1, \\sigma = 1)\\]\n\\[X_3 \\sim N(\\mu = 2, \\sigma = 1)\\]\n\nUse Monte Carlo simulation with 10,000 repetitions to estimate how often \\(X_{i1} &lt; X_{i2} &lt; X_{i3}\\).\n\n\n\n\n\n5.8.6 More examples of Monte Carlo simulation\n\nfivethirtyeight 2020 election forecast use\nU.S. Census Bureau simulation of data collection operations\n\n\nMarkov Chain Monte Carlo\nBayesian statisticians estimate posterior distributions of parameters that are combinations of prior distributions and sampling distributions. Outside of special cases, posterior distributions are difficult to identify. Accordingly, most Bayesian estimation uses an extension of Monte Carlo simulation called Markov Chain Monte Carlo or MCMC.\n\n\n\n5.8.7 One Final Note\nMonte Carlo simulations likely underestimate uncertainty. Monte Carlo simulations only capture aleatoric uncertainty and they don’t capture epistemic uncertainty.\nAleatoric uncertainty: Uncertainty due to probabilistic variety\nEpistemic uncertainty: Uncertainty due to a lack of knowledge\nIn other words, Monte Carlo simulations estimates assume the model is correct, which is almost certainly never fully true. Be transparent. Be humble."
  },
  {
    "objectID": "05_simulation-and-sampling.html#sampling-from-observed-data",
    "href": "05_simulation-and-sampling.html#sampling-from-observed-data",
    "title": "5  Simulation and Sampling",
    "section": "5.9 Sampling from Observed Data",
    "text": "5.9 Sampling from Observed Data\nUntil now, we’ve only discussed sampling from closed-form theoretical distributions. We also called this process simulation. There are many applications where we may want to sample from observed data.\nWe can break these methods into two general approaches:\n\nSampling\nResampling\n\n\n5.9.1 Sampling\n\n\n\n\n\n\nSampling\n\n\n\nSampling is the process of selecting a subset of data. Probability sampling is the process of selecting a sample when the selection uses randomization.\n\n\nSampling has many applications:\n\nReducing costs for the collection of data\nImplementing machine learning algorithms\nResampling\n\n\n\n5.9.2 Resampling\n\n\n\n\n\n\nResampling\n\n\n\nResampling is the process of repeatedly sampling from observed data to approximate the generation of new data.\n\n\nThere are at least three popular resampling methods:\n\nCross Validation: Partitioning the data and shuffling the partitions to understand the accuracy of predictive models.\nBootstrap sampling: Repeated sampling with replacement to estimate sampling distributions from observed data.\nJackknife: Leave-one-out sampling to estimate the bias and standard error of a statistic.\n\nWe focused on cross-validation for machine learning and predictive modeling in data science for public policy. We will use this approach again for predictive modeling.\nWe will also learn about bootstrap sampling when we discuss nonparametric statistics.\n\n\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical Inference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning."
  },
  {
    "objectID": "05_simulation-and-sampling.html#footnotes",
    "href": "05_simulation-and-sampling.html#footnotes",
    "title": "5  Simulation and Sampling",
    "section": "",
    "text": "(Casella and Berger 2002) offers a robust introduction to deriving maximum likelihood estimators.↩︎\nNote that the MLE for variance is biased.↩︎\nCorrelation may be more familiar than covariance. Sample correlation is standardized sample covariance. \\(Corr(\\vec{x}, \\vec{y}) = \\frac{Cov(\\vec{x}, \\vec{y})}{S_{\\vec{x}}S_{\\vec{y}}}\\). Correlation is also between -1 and 1 inclusive. Covariance can take on any real value.↩︎"
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#sec-review6a",
    "href": "06_advanced-unsupervised-ml.html#sec-review6a",
    "title": "6  Mixture Distributions and Mixture Modeling",
    "section": "6.1 Review 1",
    "text": "6.1 Review 1\nWe’ll start with a review of multivariate normal distributions. In particular, this exercise demonstrates the impact of the variance-covariance matrix on the shape of multivariate normal distributions.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nLoad library(mvtnorm).\nCopy and paste the following code. This code will not obviously not run as is. We will add tibbles to independent, sigma1 and sigma2 in the steps below.\n\n\nbind_rows(\n  independent = ,\n  var_covar1 = ,\n  var_covar2 = ,\n  .id = \"source\"\n)\n\n\nCreate a tibble with V1 and V2. For both variables, use rnorm() to sample 1,000 observations from a standard normal distribution. Add the results to independent.\nUsing the following variance-covariance matrix, sample 1,000 observations from a multivariate-normal distribution. Add the results for sigma1 and use as_tibble().\n\n\nsigma1 &lt;- matrix(\n  c(1, 0,\n    0, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\n\nUsing the following variance-covariance matrix, sample 1,000 observations from a multivariate-normal distribution. Add the results for sigma2 and use as_tibble().\n\n\nsigma2 &lt;- matrix(\n  c(1, 0.8,\n    0.8, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\n\nCreate a scatter plot with V1 on the x-axis and V2 on the y-axis. Facet based on source.\n\n\n\n\nlibrary(mvtnorm)\n\nsigma1 &lt;- matrix(\n  c(1, 0,\n    0, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\nsigma2 &lt;- matrix(\n  c(1, 0.8,\n    0.8, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\nbind_rows(\n  independent = tibble(\n    V1 = rnorm(n = 1000),\n    V2 = rnorm(n = 1000)\n  ),\n  sigma1 = rmvnorm(\n    n = 1000, \n    sigma = sigma1\n  ) |&gt;\n    as_tibble(),\n  sigma2 = rmvnorm(\n    n = 1000, \n    sigma = sigma2\n  ) |&gt;\n    as_tibble(),\n  .id = \"source\"\n) |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point() +\n  facet_wrap(~ source)"
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#a-new-type-of-random-variable",
    "href": "06_advanced-unsupervised-ml.html#a-new-type-of-random-variable",
    "title": "6  Mixture Distributions and Mixture Modeling",
    "section": "6.2 A New Type of Random Variable",
    "text": "6.2 A New Type of Random Variable\nWe learned about common univariate and multivariate distributions. For each of the distributions, there are well-defined and straightforward ways to sample values from the distribution. We can also manipulate these distributions to calculate probabilities.\nThe real world is complicated, and we will quickly come across data where we struggle to find a common probability distributions.\nFigure Figure 6.1 shows a relative frequency histogram for the duration of eruptions at Old Faithful in Yellowstone National Park.\n\n# faithful is a data set built into R\nfaithful |&gt;\n  ggplot(aes(eruptions, y = after_stat(density))) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 6.1: Distribution of waiting times between eruptions at the Old Faithful geyser in Yellowstone National Park.\n\n\n\n\nThis distribution looks very complicated. But what if we break this distribution into pieces? In this case, what if we think of the distribution as a combination of two normal distributions?\n\n\nCode\n# show geyser as two normal distribution\nlibrary(mclust)\n\ngmm_geyser &lt;- Mclust(\n  data = dplyr::select(faithful, eruptions), \n  G = 2\n)\n\n\nbind_cols(\n  faithful,\n  cluster = gmm_geyser$classification\n) |&gt;\n  ggplot(aes(eruptions, y = after_stat(density), \n             fill = factor(cluster))) +\n  geom_histogram() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nLatent Variable\n\n\n\nA latent variable is a variable that isn’t directly observed but can be inferred through other variables and modeling. Sometimes the latent variable is meaningful but unobserved. Sometimes it isn’t meaningful.\nLatent variables are sometimes called hidden variables.\n\n\nBreaking complex problems into smaller pieces is good. These latent variables will allow us to do some cools things:\n\nSimply express complicated probability distributions\nMake inferences about complex populations\nCluster data\n\nIn this set of notes, we’ll use latent variables to\n\nConstruct mixture distributions\nCluster data\n\nLet’s consider a “data generation story” different than anything we considered in Chapter 5. Instead of sampling directly from one known probability distribution, we will sample in two stages (Hastie, Tibshirani, and Friedman 2009).\n\nSample from a discrete probability distribution with \\(k\\) unique values (i.e. Bernoulli distribution when \\(k = 2\\) and categorical distribution when \\(k &gt; 2\\)).\nSample from one of \\(k\\) different distributions conditional on the outcome of step 1.\n\nThis new sampling procedure aligns closely with the idea of hierarchical sampling and hierarchical models. It is also sometimes called ancestral sampling (Bishop 2006, 430).\nThis two-step approach dramatically increases the types of distributions at our disposal because we are no longer limited to individual common univariate distributions like a single normal distribution or a single uniform distribution. The two-step approach is also the foundation of two related tools:\n\nMixture distributions: Distributions expressed as the linear combination of other distributions. Mixture distributions can be very complicated distributions expressed in terms of simple distributions with known properties.\nMixture modeling: Statistical inference about sub-populations made only with pooled data without labels for the sub populations.\n\nWith mixture distributions, we care about the overall distribution and don’t care about the latent variables.\nWith mixture modeling, we use the overall distribution to learn about the latent variables/sub populations/clusters in the data."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#mixture-distributions",
    "href": "06_advanced-unsupervised-ml.html#mixture-distributions",
    "title": "6  Mixture Distributions and Mixture Modeling",
    "section": "6.3 Mixture Distributions",
    "text": "6.3 Mixture Distributions\n\n\n\n\n\n\nMixture Distribution\n\n\n\nA mixture distribution is a probabilistic model that is a linear combination of common probability distributions.\nA discrete mixture distribution can be expressed as\n\\[\np_{mixture}(x) = \\sum_{k = 1}^K \\pi_kp(x)\n\\]\nwhere \\(K\\) is the number of mixtures and \\(\\pi_k\\) is the weight of each PMF included in the mixture distribution.\nA continuous mixture distribution can be expressed as\n\\[\np_{mixture}(x) = \\sum_{k = 1}^K \\pi_kf(x)\n\\]\nwhere \\(K\\) is the number of mixtures and \\(\\pi_k\\) is the weight of each PDF included in the mixture distribution.\n\n\n\n6.3.1 Example 1\nLet’s consider a concrete example with a Bernoulli distribution and two normal distributions.\n\nSample \\(X \\sim Bern(p = 0.25)\\)\nSample from \\(Y \\sim N(\\mu = 0, \\sigma = 2)\\) if \\(X = 0\\) and \\(Y \\sim (\\mu = 4, \\sigma = 2)\\) if \\(X = 1\\).\n\nNow, let’s sample from a Bernoulli distribution and then sample from one of two normal distributions using R code.\n\ngenerate_data &lt;- function(n) {\n  \n  step1 &lt;- sample(x = c(0, 1), size = n, replace = TRUE, prob = c(0.75, 0.25))\n  \n  step1 &lt;- sort(step1)\n  \n  step2 &lt;- c(\n    rnorm(n = sum(step1 == 0), mean = 0, sd = 2),\n    rnorm(n = sum(step1 == 1), mean = 5, sd = 1)\n  )\n  \n  tibble::tibble(\n    x = step1,\n    y = step2\n  )\n\n}\n\nset.seed(1)\n\ngenerate_data(n = 1000) |&gt;\n  ggplot(aes(x = y, y = after_stat(density))) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis marginal distribution looks complex but the process of creating the marginal distribution is simple.\nIn fact, consider this quote from Bishop (2006) (Page 111):\n\nBy using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.\n\n\n\n\n\n\n\nComponent\n\n\n\nA component is each common probability distribution that is combined to create a mixture distribution. For example, a mixture of two Gaussian distributions has two components.\n\n\n\n\n\n\n\n\nMixing Coefficient\n\n\n\nA mixing coefficient is the probability associated with a component with a component in a mixture distribution. Mixing coefficients must sum to 1.\nWe’ll use \\(\\pi_k\\) for population mixing coefficients and \\(p_k\\) for sample mixing coefficients. Mixing coefficients are also called mixing weights and mixing probabilities.\n\n\nMixture distributions are often overparameterized, which means they have an excessive number of parameters. For a univariate mixture of normals with \\(k\\) components, we have \\(k\\) means, \\(k\\) standard deviations, and \\(k\\) mixing coefficients.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nSample 1,000 observations from a mixture of three normal distributions with the following parameters:\n\n\n\\(p_1 = p_2 = p_3\\)\n\\(\\mu_1 = -3\\), \\(\\mu_2 = 0\\), \\(\\mu_3 = 3\\)\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3 = 1\\)\n\n\nCreate a relative frequency histogram of the values.\n\n\n\n\n\n6.3.2 Example 2\nSuppose we used statistical inference to infer some parameters for the geysers example above. We will describe how to estimate these paramaters later.\n\n\\(p_1 =\\) 0.3485696 and \\(p_2 =\\) 0.6514304\n\\(\\bar{x_1} =\\) 2.0189927 and \\(\\bar{x_2} =\\) 4.2737083\n\\(s_1 =\\) 0.2362355and \\(s_2 =\\) 0.4365146\n\nThe mixture density is\n\\[\nf_{mixture}(x) = p_1f(x|\\mu = \\bar{x_1}, \\sigma = s_1) + p_2f(x|\\mu = \\bar{x_2},\\sigma=s_2)\n\\tag{6.1}\\]\n\ngeyser_density &lt;- function(x, model) {\n  \n  probs &lt;- model$parameters$pro\n  \n  d1 &lt;- dnorm(\n    x, \n    mean =  model$parameters$mean[1], \n    sd = sqrt(model$parameters$variance$sigmasq[1])\n  )\n  \n  d2 &lt;- dnorm(\n    x, \n    mean =  model$parameters$mean[2], \n    sd = sqrt(model$parameters$variance$sigmasq[2])\n  )\n  \n  probs[1] * d1 + probs[2] * d2\n  \n}\n\nmm &lt;- tibble(\n  x = seq(0, 5, 0.01),\n  f_x = map_dbl(x, geyser_density, model = gmm_geyser)\n) \n\nggplot() +\n  geom_histogram(data = faithful, mapping = aes(x = eruptions, y = after_stat(density))) +\n  geom_line(data = mm, mapping = aes(x, f_x), color = \"red\") + \n  labs(\n    title = \"\",\n    subtitles = \"Observed data in black, inferred distribution in red\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#sec-review6b",
    "href": "06_advanced-unsupervised-ml.html#sec-review6b",
    "title": "6  Mixture Distributions and Mixture Modeling",
    "section": "6.4 Review #2",
    "text": "6.4 Review #2\n\n6.4.1 Multivariate Normal Distribution\nThe multivariate normal distribution is a higher-dimensional version of the univariate normal distribution. The MVN distribution has a vector of means of length \\(k\\) and a \\(k\\)-by-\\(k\\) variance-covariance matrix.\nWe show that a random vector is multivariate normally distributed with\n\\[\n\\vec{X} \\sim \\mathcal{N}(\\vec\\mu, \\boldsymbol\\Sigma)\n\\tag{6.2}\\]\nThe PDF of a multivariate normally distributed random variable is\n\\[\nf(x) = (2\\pi)^{-k/2}det(\\boldsymbol\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\vec{x} - \\vec\\mu)^T\\boldsymbol\\Sigma^{-1}(\\vec{x} - \\vec\\mu)\\right)\n\\tag{6.3}\\]\n\n\n6.4.2 K-Means Clustering\nK-Means Clustering is a heuristic-based approach to finding latent groups in data. The algorithm assigns each observation to one and only one group through a two step iteration that minimizes the Euclidean distance between observations and centroids for each group.\n\nSetupStep 1Step 2Step 3Step 4Step 5\n\n\nConsider the following data set.\n\n\nCode\ndata &lt;- tibble(x = c(1, 2, 1, 4, 7, 10, 8),\n               y = c(5, 4, 4, 3, 7, 8, 5))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 1: Randomly place K centroids in your n-dimensional vector space\n\n\nCode\ncentroids &lt;- tibble(x = c(2, 5),\n                  y = c(5, 5),\n                  cluster = c(\"a\", \"b\"))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 2: Calculate the nearest centroid for each point using a distance measure\n\n\nCode\ncentroids &lt;- tibble(x = c(2, 5),\n                  y = c(5, 5),\n                  cluster = c(\"a\", \"b\"))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  geom_line(aes(x = c(4, 2), y = c(3, 5)), linetype = \"dashed\") +  \n  geom_line(aes(x = c(4, 5), y = c(3, 5)), linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 3: Assign each point to the nearest centroid\n\n\nCode\ndata$cluster &lt;- c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\")\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 4: Recalculate the position of the centroids based on the means of the assigned points\n\n\nCode\ncentroids2 &lt;- data %&gt;%\n  group_by(cluster) %&gt;%\n  summarize(x = mean(x), y = mean(y))\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids, aes(x, y), size = 4, alpha = 0.25) +\n  geom_point(data = centroids2, aes(x, y, color = cluster), size = 4) +  \n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\nStep 5: Repeat steps 2-4 until no points change cluster assignments\n\n\nCode\ndata$cluster &lt;- c(\"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\")\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids2, aes(x, y, color = cluster), size = 4) +  \n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse library(tidyclust) to cluster the faithful data into three clusters."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#mixture-modelingmodel-based-clustering",
    "href": "06_advanced-unsupervised-ml.html#mixture-modelingmodel-based-clustering",
    "title": "6  Mixture Distributions and Mixture Modeling",
    "section": "6.5 Mixture Modeling/Model-Based Clustering",
    "text": "6.5 Mixture Modeling/Model-Based Clustering\nUntil now, we’ve assumed that we’ve known all parameters when working with mixture distributions. What if we want to learn these parameters/make inferences about these parameters?\nThe process of making inferences about latent groups is related to K-Means Clustering. While K-Means Clustering is heuristic based, mixture modeling formalize the process of making inferences about latent groups using probability models. Gaussian mixture models (GMM) are a popular mixture model.\n\n\n\n\n\n\nMixture Modeling\n\n\n\nMixture modeling is the process of making inferences about sub populations using data that contain sub population but no labels for the sub populations."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#gaussian-mixture-modeling-gmm",
    "href": "06_advanced-unsupervised-ml.html#gaussian-mixture-modeling-gmm",
    "title": "6  Mixture Distributions and Mixture Modeling",
    "section": "6.6 Gaussian Mixture Modeling (GMM)",
    "text": "6.6 Gaussian Mixture Modeling (GMM)\n\n\n\n\n\n\nGaussian Mixture Modeling (GMM)\n\n\n\nGaussian mixture modeling (GMM) is mixture modeling that uses normal and multivariate normal distributions.\n\n\n\n\n\n\n\n\nHard Assignment\n\n\n\nHard assignment assigns an observation in a clustering model to one and only one group.\n\n\n\n\n\n\n\n\nSoft Assignment\n\n\n\nSoft assignment assigns an observation in a clustering model to all groups with varying weights or probabilities.\n\n\n\n\n\n\n\n\nResponsibilities\n\n\n\nSoft assignments are quantified with responsibilities. Responsibilities are the probability that a given observation belongs to a given group. The soft assignments for an observation sum to 1.\nWe quantified responsibilities with \\(\\pi_k\\) for mixture distributions. Responsibilities are parameters we will infer during mixture modeling.\n\n\nThere are two main differences between K-Means Clustering and GMM.\n\nInstead of calculating Euclidean distance from each observation to each group centroid, we use multivariate normal distributions to calculate the probability that an observation belongs to each group.\n\nObservations close to the means of a mixture will have a high relative probability of belonging to that mixture.\nObservations far from the means of a mixture will have a low relative probability of belonging to that mixture.\n\nInstead of simply updating \\(k\\) group centroids, we must update \\(k\\) multivariate normal distributions. This requires calculating a vector of means and a variance-covariance matrix for each of the \\(k\\) groups.\n\n\n6.6.1 Example 3\nThe parameters in example 2 were estimated using GMM. Let’s repeat a similar exercise with the faithful using eruptions and waiting instead of just eruptions. We’ll assume there are three groups.\n\n# fit GMM\ngmm2_geyser &lt;- Mclust(faithful, G = 3)\n\nLet’s plot the multivariate normal distributions. Figure 6.2 shows the centroids (stars) and shapes (ellipses) of the distributions in black. The colors represent hard assignments to groups and the size of the points represent the uncertainty of the assignments with larger points having more uncertainty.\n\n# plot fitted model\nplot(gmm2_geyser, what = \"uncertainty\")\n\n\n\n\nFigure 6.2: Uncertainty plot from a GMM\n\n\n\n\nWe can also summarize the model with library(broom).\n\nlibrary(broom)\n\naugment(gmm2_geyser)\n\n# A tibble: 272 × 4\n   eruptions waiting .class .uncertainty\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1      3.6       79 1          2.82e- 2\n 2      1.8       54 2          8.60e-13\n 3      3.33      74 1          3.26e- 3\n 4      2.28      62 2          3.14e- 7\n 5      4.53      85 3          1.17e- 2\n 6      2.88      55 2          3.09e- 3\n 7      4.7       88 3          2.99e- 3\n 8      3.6       85 1          2.39e- 2\n 9      1.95      51 2          5.23e-12\n10      4.35      85 3          5.52e- 2\n# ℹ 262 more rows\n\ntidy(gmm2_geyser)\n\n# A tibble: 3 × 5\n  component  size proportion mean.eruptions mean.waiting\n      &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1         1    40      0.166           3.79         77.5\n2         2    97      0.356           2.04         54.5\n3         3   135      0.478           4.46         80.8\n\nglance(gmm2_geyser)\n\n# A tibble: 1 × 7\n  model     G    BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 EEE       3 -2314. -1126.    11     NA   272\n\n\n\n\n6.6.2 mclust\nThe previous example uses library(mclust)1 and library(broom).\nMclust() is the main function for fitting Gaussian Mixture Models. The function contains several different types of models for the variances of the multivariate normal distributions. The defaults are sensible. G is the number of groups. If G isn’t specified, then Mclust() will try 1:9 and pick the G with the lowest BIC (defined below).\nplot() with what = \"uncertainty\" creates a very useful data visualization for seeing the multivariate normal distributions and classifications for low-dimensional GMM.\nglance(), tidy(), and augment() from library(broom) return important information about the assignments, groups, and model diagnostics.\n\n\n6.6.3 Estimation\nSuppose we have \\(n\\) observations, \\(k\\) groups, and \\(p\\) variables. A single GMM will have\n\nan \\(n\\) by \\(k\\) matrix of responsibilities\n\\(k\\) vectors of means of length \\(p\\)\n\\(k\\) \\(p\\) by \\(p\\) variance-covariance matrices\n\nWe want the maximum likelihood estimates for all of the parameters in the model. For technical reasons, it is very difficult to get these estimates using popular methods like stochastic gradient descent.\nInstead, we will use expectations maximization (EM) to find the parameters. We also used EM for K-Means clustering.\n\n\n\n\n\n\n\nRandomly initialize all of the parameters. Calculate the log-likelihood.\nE-Step: Update the responsibilities assuming the means and variance-covariance matrices are known.\nM-Step: Estimate new means and variance-covariance matrices assuming the responsibilities are known. The means and variance-covariance matrices are calculated using weighted MLE where the responsibilities are the weights.\nCalculate the log-likelihood. Go back to step 2 if the log-likelihood improves by at least as much as the stopping threshold.\n\n\n\n\nThis algorithm is computationally efficient, but it is possible for it to find a local maximum log-likelihood without finding the global maximum log-likelihood.\nFor a more mathematical description of this process, see Elements of Statistical Learning Section 6.8 (Hastie, Tibshirani, and Friedman 2009). A highly descriptive comparison to kmeans (with Python code) can be seen here.\n\n\n6.6.4 Example 4\nLet’s consider a policy-relevant example using data from the Small Area Health Insurance Estimates (SAHIE) Program.\nFirst, we pull the 2016 county-level estimates of the uninsured rate. We label a state as an expansion state if it expanded data before 2015-01-01. We use this date with 2016 data because of policy lags.\n\nlibrary(censusapi)\n\nsahie &lt;- getCensus(\n  name = \"timeseries/healthins/sahie\",\n  vars = c(\"GEOID\", \"PCTUI_PT\"),\n  region = \"county:*\",\n  time = 2016\n) |&gt;\n  as_tibble()\n\nNext, we pull data from the Kaiser Family Foundation about the expansion dates of Medicaid under the Patient Protection and Affordable Care Act.\n\nstates &lt;- tribble(\n  ~state, ~state_fips, ~implementation_date,\n  \"Alabama\", \"01\", NA,\n  \"Alaska\", \"02\", \"2015-09-15\",\n  \"Arizona\", \"04\", \"2014-01-01\",\n  \"Arkansas\", \"05\", \"2014-01-01\",\n  \"California\", \"06\", \"2014-01-01\",\n  \"Colorado\", \"08\", \"2014-01-01\",\n  \"Connecticut\", \"09\", \"2014-01-01\",\n  \"Delaware\", \"10\", \"2014-01-01\",\n  \"District of Columbia\", \"11\", \"2014-01-01\",\n  \"Florida\", \"12\", NA,\n  \"Georgia\", \"13\", NA,\n  \"Hawaii\", \"15\", \"2014-01-01\",\n  \"Idaho\", \"16\", \"2020-01-01\",\n  \"Illinois\", \"17\", \"2014-01-01\",\n  \"Indiana\", \"18\", \"2015-02-01\",\n  \"Iowa\", \"19\", \"2014-01-01\",\n  \"Kansas\", \"20\", NA,\n  \"Kentucky\", \"21\", \"2014-01-01\", \n  \"Louisiana\", \"22\", \"2016-07-01\",\n  \"Maine\", \"23\", \"2018-07-02\",\n  \"Maryland\", \"24\", \"2014-01-01\",\n  \"Massachusetts\", \"25\", \"2014-01-01\",\n  \"Michigan\", \"26\", \"2014-04-01\",\n  \"Minnesota\", \"27\", \"2014-01-01\",\n  \"Mississippi\", \"28\", NA,\n  \"Missouri\", \"29\", \"2021-07-01\",\n  \"Montana\", \"30\", \"2016-01-01\",\n  \"Nebraska\", \"31\", \"2020-10-01\",\n  \"Nevada\", \"32\", \"2014-01-01\", \n  \"New Hampshire\", \"33\", \"2014-08-15\",\n  \"New Jersey\", \"34\", \"2014-01-01\",\n  \"New Mexico\", \"35\", \"2014-01-01\",\n  \"New York\", \"36\", \"2014-01-01\", \n  \"North Carolina\", \"37\", NA,\n  \"North Dakota\", \"38\", \"2014-01-01\", \n  \"Ohio\", \"39\", \"2014-01-01\",\n  \"Oklahoma\", \"40\", \"2021-07-01\", \n  \"Oregon\", \"41\", \"2014-01-01\", \n  \"Pennsylvania\", \"42\", \"2015-01-01\", \n  \"Rhode Island\", \"44\", \"2014-01-01\", \n  \"South Carolina\", \"45\", NA,\n  \"South Dakota\", \"46\", \"2023-07-01\", \n  \"Tennessee\", \"47\", NA,\n  \"Texas\", \"48\", NA,\n  \"Utah\", \"49\", \"2020-01-01\",\n  \"Vermont\", \"50\", \"2014-01-01\",\n  \"Virginia\", \"51\", \"2019-01-01\", \n  \"Washington\", \"53\", \"2014-01-01\",\n  \"West Virginia\", \"54\", \"2014-01-01\",\n  \"Wisconsin\", \"55\", NA,\n  \"Wyoming\", \"56\", NA\n) %&gt;%\n  mutate(implementation_date = ymd(implementation_date))\n\nsahie &lt;- left_join(\n  sahie, \n  states,\n  by = c(\"state\" = \"state_fips\")\n) |&gt;\n  filter(!is.na(PCTUI_PT)) |&gt; \n  mutate(expanded = implementation_date &lt; \"2015-01-01\") %&gt;%\n  mutate(expanded = replace_na(expanded, FALSE))\n\nWe use GMM to cluster the data.\n\nuni &lt;- select(sahie, PCTUI_PT)\n\nset.seed(1)\nuni_mc &lt;- Mclust(uni, G = 2)\n\nglance(uni_mc)\n\n# A tibble: 1 × 7\n  model     G     BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 V         2 -18542. -9251.     5     NA  3141\n\n\nWhen we compare .class to expansion, we see that the model does an good job of labeling counties’ expansion status without observing counties’ expansion status.\n\nbind_cols(\n  sahie,\n  augment(uni_mc)\n) |&gt;\n  count(expanded, .class)\n\nNew names:\n• `PCTUI_PT` -&gt; `PCTUI_PT...5`\n• `PCTUI_PT` -&gt; `PCTUI_PT...9`\n\n\n# A tibble: 4 × 3\n  expanded .class     n\n  &lt;lgl&gt;    &lt;fct&gt;  &lt;int&gt;\n1 FALSE    1        465\n2 FALSE    2       1486\n3 TRUE     1       1042\n4 TRUE     2        148\n\n\n\n\n6.6.5 BIC\nLikelihood quantifies how likely observed data are given a set of parameters. If \\(\\theta\\) is a vector of parameters, then \\(L(\\theta |x) = f(x |\\theta)\\) is the likelihood function.\nWe often don’t know the exact number of latent groups in the data. We need a way to compare models with varying numbers of groups. Simply picking the model with the maximum likelihood will lead to models with too many groups.\nThe Bayesian information criterion (BIC) is an alternative to likelihoods that penalizes models for having many parameters. Let \\(L\\) be the likelihood, \\(m\\) the number of free parameters, and \\(n\\) the number of observations.\n\\[\nBIC = -2log(L) + mlog(n)\n\\tag{6.4}\\]\nWe will choose models that minimize BIC. Ideally, we will use v-fold cross validation for this process.\n\n\n6.6.6 Example 5\nThe Mclust() function will try G = 1:9 when G isn’t specified. Mclust() will also try 14 different variance models for the mixture models.\n\n\n\n\n\n\nImportant\n\n\n\nWe want to minimize BIC but library(mclust) is missing a negative sign. So we want to maximize the BIC plotted by library(mclust). You can read more here.\n\n\nWe can plot the BICs with plot() and view the optimal model with glance().\n\nfaithful_gmm &lt;- Mclust(faithful)\n\nplot(faithful_gmm, what = \"BIC\")\n\n\n\nglance(faithful_gmm)\n\n# A tibble: 1 × 7\n  model     G    BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 EEE       3 -2314. -1126.    11     NA   272"
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#bernoulli-mixture-modeling-bmm",
    "href": "06_advanced-unsupervised-ml.html#bernoulli-mixture-modeling-bmm",
    "title": "6  Mixture Distributions and Mixture Modeling",
    "section": "6.7 Bernoulli Mixture Modeling (BMM)",
    "text": "6.7 Bernoulli Mixture Modeling (BMM)\nLet’s consider a data generation story based on the Bernoulli distribution. Now, each variable, \\(X_1, X_2, ..., X_D\\), is draw from a mixture of \\(K\\) Bernoulli distributions.\n\\[\nX_d  = \\begin{cases}\nBern(p_1) \\text{ with probability }\\pi_1 \\\\\nBern(p_2) \\text{ with probability }\\pi_2 \\\\\n\\vdots \\\\\nBern(p_K) \\text{ with probability }\\pi_K\n\\end{cases}\n\\tag{6.5}\\]\nLet \\(i\\) be an index for each mixture that contributes to the random variable. The probability mass function of the random variable is written as\n\\[\nP(X_d) = \\Pi_{i = 1}^Kp_i^{x_i} (1 - p_i)^{1 - x_i}\n\\tag{6.6}\\]\nLet’s consider a classic example from Bishop (2006) and Murphy (2022). The example uses the MNIST database, which contains 70,000 handwritten digits. The digits are stored in 784 variables, from a 28 by 28 grid, with values ranging from 0 to 255, which indicate the darkness of the pixel.\nTo prepare the data, we divide each pixel by 255 and then turn the pixels into indicators with values under 0.5 as 0 and values over 0.5 as 1. Figure Figure 6.3 visualizes the first four digits after reading in the data and applying pre-processing.\n\nsource(here::here(\"R\", \"visualize_digit.R\"))\n\nmnist &lt;- read_csv(here::here(\"data\", \"mnist_binary.csv\"))\n\nglimpse(dplyr::select(mnist, 1:10))\n\nRows: 60,000\nColumns: 10\n$ label    &lt;dbl&gt; 5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4…\n$ pix_28_1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_5 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_9 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nvisualize_digit(mnist, 1)\nvisualize_digit(mnist, 2)\nvisualize_digit(mnist, 3)\nvisualize_digit(mnist, 4)\n\n\n\n\n\n\n\n(a) 5\n\n\n\n\n\n\n\n(b) 0\n\n\n\n\n\n\n\n(c) 4\n\n\n\n\n\n\n\n(d) 1\n\n\n\n\nFigure 6.3: First Four Digits\n\n\n\nThe digits are labelled in the MNIST data set but we will ignore the labels and use Bernoulli Mixture Modeling to learn the latent labels or groups. We will treat each pixel as its own Bernoulli distribution and cluster observations using mixtures of 784 Bernoulli distributions. This means each cluster will contain \\(784\\) parameters.\n\n6.7.1 Two Digit Example\nLet’s start with a simple example using just the digits “1” and “8”. We’ll use library(flexmix) by Leisch (2004). library(flexmix) is powerful but uses different syntax than we are used to.\n\nThe function flexmix() expects a matrix.\nThe formula expects the entire matrix on the left side of the ~.\nWe specify the distribution used during the maximization (M) step with model = FLXMCmvbinary().\n\n\nlibrary(flexmix)\n\nLoading required package: lattice\n\nmnist_18 &lt;- mnist |&gt;\n  filter(label %in% c(\"1\", \"8\")) |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nThe starting assignments are random, so we set a seed.\n\nset.seed(20230612)\nmnist_18_clust &lt;- flexmix(\n  formula = mnist_18 ~ 1, \n  k = 2, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 100)\n)\n\nThe MNIST data are already labelled, so we can compare our assignments to the labels if we convert the “soft assignments” to “hard assignments”. Note that most applications won’t have labels.\n\nmnist |&gt;\n  filter(label %in% c(\"1\", \"8\")) |&gt;  \n  bind_cols(cluster = mnist_18_clust@cluster) |&gt;\n  count(label, cluster)\n\n# A tibble: 4 × 3\n  label cluster     n\n  &lt;dbl&gt;   &lt;int&gt; &lt;int&gt;\n1     1       1   482\n2     1       2  6260\n3     8       1  5610\n4     8       2   241\n\n\nFigure 6.4 shows the estimated \\(p_i\\) for each pixel for each cluster. The figure shows 784 \\(p_i\\) for \\(k = 1\\) and 784 \\(p_i\\) for \\(k = 2\\). We see that the estimated parameters closely resemble the digits.\nOf course, each digit can differ from these images because everyone writes differently. In some ways, these are average digits across many version of the digits.\n\nmeans_18 &lt;- rbind(\n  t(parameters(mnist_18_clust, component = 1)),\n  t(parameters(mnist_18_clust, component = 2))\n) |&gt;\n  as_tibble() |&gt;\n  mutate(label = NA)\n\n\nvisualize_digit(means_18, 1)\nvisualize_digit(means_18, 2)\n\n\n\n\n\n\n\n(a) 8\n\n\n\n\n\n\n\n(b) 1\n\n\n\n\nFigure 6.4: Estimated Parameters for Each Cluster\n\n\n\nThe BMM does a good job of labeling the digits and recovering the average shape of the digits.\n\n\n6.7.2 Ten Digit Example\nLet’s now consider an example that uses all 10 digits.\nIn most applications, we won’t know the number of latent variables. First, we sample 1,0002 digits and run the model with \\(k = 2, 3, ..., 12\\). We’ll calculate the BIC for each hyperparameter and pick the \\(k\\) with lowest BIC.\n\nset.seed(20230613)\nmnist_sample &lt;- mnist |&gt;\n  slice_sample(n = 1000) |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nsteps &lt;- stepFlexmix(\n  formula = mnist_sample ~ 1, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 100, minprior = 0),\n  k = 2:12, \n  nrep = 1\n)\n\n\\(k = 7\\) provides the lowest BIC. This is probably because digits like 3 and 8 are very similar.\n\nsteps\n\n\nCall:\nstepFlexmix(formula = mnist_sample ~ 1, model = FLXMCmvbinary(), \n    control = list(iter.max = 100, minprior = 0), k = 2:12, nrep = 1)\n\n   iter converged  k k0    logLik      AIC      BIC      ICL\n2    43      TRUE  2  2 -196191.7 395521.4 403221.6 403227.9\n3    30      TRUE  3  3 -188722.8 382153.6 393706.4 393713.9\n4    32      TRUE  4  4 -182949.0 372176.0 387581.4 387585.1\n5    27      TRUE  5  5 -178955.2 365758.4 385016.4 385019.7\n6    35      TRUE  6  6 -175448.7 360315.5 383426.1 383428.5\n7    37      TRUE  7  7 -171697.0 354381.9 381345.1 381347.8\n8    37      TRUE  8  8 -171282.5 355123.1 385938.8 385941.1\n9    38      TRUE  9  9 -169213.3 352554.6 387223.0 387224.9\n10   25      TRUE 10 10 -165521.6 346741.2 385262.2 385263.7\n11   34      TRUE 11 11 -162919.3 343106.5 385480.1 385481.8\n12   26      TRUE 12 12 -162253.5 343345.0 389571.1 389572.7\n\n\nNext, we run the BMM on the full data with \\(k = 7\\).\n\nmnist_full &lt;- mnist |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nmnist_clust &lt;- flexmix(\n  formula = mnist_full ~ 1, \n  k = 7, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 200, minprior = 0)\n)\n\nThe MNIST data are already labelled, so we can compare our assignments to the labels if we convert the “soft assignments” to “hard assignments”. Note that most applications won’t have labels. The rows of the table are the digits. The columns of the table are the clusters. We can see, for example, that most of the 0’s are clustered in cluster 5.\n\nlabels &lt;- mnist |&gt;\n  bind_cols(cluster = mnist_clust@cluster)\n\ntable(labels$label, labels$cluster)\n\n   \n       1    2    3    4    5    6    7\n  0    5  357  282  289 4875    1  114\n  1   36  288   40   35    0 6319   24\n  2  114  166  652   73   50  163 4740\n  3  263  473 4786   80   41  260  228\n  4 3384 1779    4  139    7   53  476\n  5  325 2315 2367  173  109   59   73\n  6    9   86   41 4365   42  128 1247\n  7 3560 2395   21    0   25  234   30\n  8  257 2582 2369   57   32  445  109\n  9 3739 1797  109    5   26  136  137\n\n\nFigure 6.5 shows the estimated \\(p_i\\) for each pixel for each cluster. The following visualize the \\(784K\\) parameters that we estimated. It shows 784 \\(p_i\\) for \\(k = 1, 2, ..., 7\\) clusters. We see that the estimated parameters closely resemble the digits.\n\nmeans &lt;- rbind(\n  t(parameters(mnist_clust, component = 1)),\n  t(parameters(mnist_clust, component = 2)),\n  t(parameters(mnist_clust, component = 3)),\n  t(parameters(mnist_clust, component = 4)),\n  t(parameters(mnist_clust, component = 5)),\n  t(parameters(mnist_clust, component = 6)),\n  t(parameters(mnist_clust, component = 7))\n) |&gt;\n  as_tibble() |&gt;\n  mutate(label = NA)\n\nvisualize_digit(means, 1)\nvisualize_digit(means, 2)\nvisualize_digit(means, 3)\nvisualize_digit(means, 4)\nvisualize_digit(means, 5)\nvisualize_digit(means, 6)\nvisualize_digit(means, 7)\n\n\n\n\n\n\n\n(a) 3, 7, and 9\n\n\n\n\n\n\n\n(b) 5, 7, and 8\n\n\n\n\n\n\n\n(c) 3\n\n\n\n\n\n\n\n(d) 6\n\n\n\n\n\n\n\n\n\n(e) 0\n\n\n\n\n\n\n\n(f) 1\n\n\n\n\n\n\n\n(g) 2\n\n\n\n\nFigure 6.5: Estimated Parameters for Each Cluster\n\n\n\nThe example with all digits doesn’t result in 10 distinct mixtures but it does a fairly good job of structuring finding structure in the data. Without labels and considering the variety of messy handwriting, this is a useful model."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#considerations",
    "href": "06_advanced-unsupervised-ml.html#considerations",
    "title": "6  Mixture Distributions and Mixture Modeling",
    "section": "6.8 Considerations",
    "text": "6.8 Considerations\nMixture modeling is difficult for a couple of reasons:\n\nWe need to assume a model. It can be difficult to assume a multivariate distribution that fits the data in all dimensions of interest.\nThe models are overparameterized and can take a very long time to fit.\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n\n\nLeisch, Friedrich. 2004. “FlexMix: A General Framework for Finite Mixture Models and Latent Class Regression in R.” Journal of Statistical Software 11 (8). https://doi.org/10.18637/jss.v011.i08.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press."
  },
  {
    "objectID": "06_advanced-unsupervised-ml.html#footnotes",
    "href": "06_advanced-unsupervised-ml.html#footnotes",
    "title": "6  Mixture Distributions and Mixture Modeling",
    "section": "",
    "text": "library(tidyclust) currently doesn’t support mixture modeling. I hope this will change in the future.↩︎\nThis is solely to save computation time.↩︎"
  },
  {
    "objectID": "07_microsimulation.html#motivation",
    "href": "07_microsimulation.html#motivation",
    "title": "7  Microsimulation",
    "section": "7.1 Motivation",
    "text": "7.1 Motivation\nIt is often important to ask “what would be the policy impact of…” on a population or subpopulation.\nOne common approach is to look at representative units. For example, we could construct one observation that is representative (e.g. median family structure, median income, etc.) and pass its values into a calculator. Then, we could extrapolate this experience to other observations.\nAnother common approach is to look at aggregated data. For example, we could look at county-level insurance coverage in Medicaid expansion and non-expansion states and then extrapolate to other health care expansions.\nOrcutt (1957) suggested a radically different approach. Instead of using a representative unit or aggregated data to project outcomes, model outcomes for individual units and aggregate the results. Potential units-of-analysis include people, households, and firms. Models include anything from simple accounting rules to complex behavioral and demographic models.\nIt took decades for this approach to see wide adoption because of data and computational limitations, but it is now commonplace in policy evaluation."
  },
  {
    "objectID": "07_microsimulation.html#calculators",
    "href": "07_microsimulation.html#calculators",
    "title": "7  Microsimulation",
    "section": "7.2 Calculators",
    "text": "7.2 Calculators\nWe’ll first look at calculators, which are an important tool for representative unit methods and microsimulation.\n\nThe Tax Policy Center’s Marriage Calculator can be used to calculate tax marriage penalties and benefits.\nPolicyEngine contains scores of calculators for taxes and benefits. For example, this calculator evaluates marginal tax rates in California accounting for taxes and benefits.\n\nThese two examples show the value of using calculators to explore potentially harmful marriage disincentives like marriage penalties, benefits cliffs, and extremely high marginal tax rates.\n\n7.2.1 Example 1\nSuppose we are interested in creating a new tax credit that is very simple. Its only parameters are number of children and total family income. It has the following characteristics:\n\nIgnore the first $20,000 of family income\nCreate a maximum benefit of $3,000 for one child, $4,500 for two children, and $6,000 for three or more children\nReduce the benefit by $0.10 for every dollar of income in excess of $20,000.\n\nWe first create an R function that implements this policy proposal.\n\n#' Calculate the benefit from the new tax credit\n#'\n#' @param num_children A numeric for the number of children\n#' @param family_income A numeric for family income\n#'\n#' @return Numeric benefit in dollars\n#' \nnew_tax_credit &lt;- function(num_children, family_income) {\n  \n  modified_income &lt;- pmax(family_income - 20000, 0)\n  \n  benefit &lt;- dplyr::case_when(\n    num_children &gt;= 3 ~ pmax(0, 6000 - 0.1 * modified_income),\n    num_children == 2 ~ pmax(0, 4500 - 0.1 * modified_income),\n    num_children == 1 ~ pmax(0, 3000 - 0.1 * modified_income),\n    TRUE ~ 0\n  )\n\n  return(benefit)\n  \n}\n\nWe can apply the calculator to representative cases:\n\nnew_tax_credit(num_children = 1, family_income = 34000)\n\n[1] 1600\n\nnew_tax_credit(num_children = 4, family_income = 12000)\n\n[1] 6000\n\n\nWe can also apply the calculator to many potential values and generate benefit plots:\n\n\nCode\nexpand_grid(\n  family_income = seq(0, 100000, 100),\n  num_children = 1:3\n) |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = num_children, \n      .y = family_income, \n      .f = new_tax_credit\n    )\n  ) |&gt;\n  ggplot(aes(family_income, benefit, color = factor(num_children))) +\n  geom_line() +\n  scale_x_continuous(labels = scales::label_dollar()) +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  labs(\n    x = \"Family income\",\n    y = \"Benefit\",\n    color = \"Number of children\"\n  )\n\n\n\n\n\nFigure 7.1: Benefits for different income levels and numbers of children\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCreate a new, well-documented function called alternative_tax_credit() based on new_tax_credit() that models the following proposed tax credit:\n\nIgnore the first $20,000 if family income.\nCreate a maximum benefits of $4,000 for one child, $4,500 for two children, and $5,000 for three children.\nReduces the benefit by $.05 for every dollar of income in excess of $20,000.\n\nUse library(ggplot2), expand_grid(), and map2_dbl() to create a chart like the one in the notes for this alternative tax credit."
  },
  {
    "objectID": "07_microsimulation.html#microsimulation",
    "href": "07_microsimulation.html#microsimulation",
    "title": "7  Microsimulation",
    "section": "7.3 Microsimulation",
    "text": "7.3 Microsimulation\n\n\n\n\n\n\nMicrosimulation\n\n\n\nMicrosimulation is a tool for projection that starts with individual observations (i.e. people or households) and then aggregates those individuals.\n\n\nMicrosimulation requires many assumptions and significant investment, but is useful for a few reasons:\n\nIt can be used to project heterogeneous outcomes and to look at the distribution of outcomes instead of just typical outcomes.\nIt can be used to evaluate “what-if” situations by comparing baseline projections with counterfactual projections.\n\nMicrosimulation is widely used in government, not-for-profits, and academia:\n\nThe Congressional Budget Office’s Long-Term (CBOLT) model is used for long-term fiscal forecasts.\nThe CBO uses HISIM2 to forecast health insurance coverage and premiums for people under age 65.\nThe Urban-Brookings Tax Policy Center evaluates most major tax proposals with its tax microsimulation model.\n\nMicrosimulation is also widely cited in popular publications around important debates:\n\nThe Urban Institute’s HIPSM was cited in the majority opinion of the Supreme Court case King v. Burwell, which upheld the Patient Protection and Affordable Care Act.\nTPC’s tax model is regularly cited in the New York Times, Wall Street Journal, and Washington Post when TPC evaluates candidates’ tax plans.\n\nMicrosimulation models range from simple calculators applied to representative microdata to very complex dynamic models.\n\n7.3.1 Basic Microsimulation\n\n\n\n\n\n\nAccounting Rules\n\n\n\nAccounting rules are the basic calculations associated with government law and programs like taxes, Social Security benefits, and Medicare.\nAccounting rules are sometimes called arithmetic rules because they are typically limited to addition, subtraction, multiplication, division, and simple if-else logic.\n\n\nLet’s start with a very simple algorithm for microsimulation modeling:\n\nConstruct a sample that represents the population of interest.\nApply accounting rules.\nAggregate.\n\nThe simplest microsimulation models essentially apply tax models similar to TurboTax to a representative set of microdata.\nWe can summarize output from microsimulation models with summary statistics that demonstrate the distribution of outcomes. For example, it is common to look at deciles or key percentiles to understand the heterogeneity of changes.\nWe can construct a baseline simulation by applying current law for step 2. Next, we can construct an alternative or counterfactual simulation by changing step 2 to a proposed policy. Finally, we can difference current law and the counterfactual to estimate the impact of a policy.\n\n\n7.3.2 Example 2\nLet’s consider a simple example where we apply the benefit calculator from earlier to families from the 2022 Annual Social and Economic Supplement to the Current Population Survey.\nTo keep things simple, we only consider families related to the head of household (and ignore other families in the household). Furthermore, we will ignore observations weights.1\n\n\nCode\n# if file doesn't exist in data, then download\nif (!file.exists(here(\"data\", \"cps_microsim.csv\"))) {\n  \n  cps_extract_request &lt;- define_extract_cps(\n    description = \"2018-2019 CPS Data\",\n    samples = \"cps2022_03s\",\n    variables = c(\"YEAR\", \"NCHILD\", \"FTOTVAL\")\n  )\n  \n  submitted_extract &lt;- submit_extract(cps_extract_request)\n  \n  downloadable_extract &lt;- wait_for_extract(submitted_extract)\n  \n  data_files &lt;- download_extract(\n    downloadable_extract,\n    download_dir = here(\"data\")\n  )\n  \n  cps_data &lt;- read_ipums_micro(data_files)\n  \n  cps_data |&gt;\n    filter(PERNUM == 1) |&gt;\n    mutate(\n      FTOTVAL = zap_labels(FTOTVAL),\n      NCHILD = zap_labels(NCHILD)\n    ) |&gt;\n    select(SERIAL, YEAR, ASECWT, NCHILD, FTOTVAL) |&gt;\n    rename_with(tolower) |&gt;\n    write_csv(here(\"data\", \"cps_microsim.csv\"))\n  \n}\n\n\n\nasec &lt;- read_csv(here(\"data\", \"cps_microsim.csv\"))\n\nRows: 59148 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): serial, year, asecwt, nchild, ftotval\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproposal1 &lt;- asec |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    )\n  )\n\narrange(proposal1, desc(benefit))\n\n# A tibble: 59,148 × 6\n   serial  year asecwt nchild ftotval benefit\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1    118  2022   865.      4    1552    6000\n 2    159  2022  1098.      3       1    6000\n 3    405  2022   933.      3   16478    6000\n 4   1914  2022   614.      4    9300    6000\n 5   3269  2022   391.      3    5400    6000\n 6   4632  2022  2624.      3       0    6000\n 7   4692  2022  1746.      3   15500    6000\n 8   4812  2022  1468.      4   12000    6000\n 9   5494  2022   662.      3   18000    6000\n10   5596  2022  1708.      3   10002    6000\n# ℹ 59,138 more rows\n\n\n\n\n7.3.3 Distributional Analysis\nAggregate analysis and representative unit analysis often mask important heterogeneity. The first major advantage of microsimulation is the ability to apply distributional analysis.\n\n\n\n\n\n\nDistributional Analysis\n\n\n\nDistributional analysis is the calculation and interpretation of statistics outside of the mean, median, and total. The objective is to understand a range of outcomes instead of typical outcomes.\n\n\nHere, we expand step 3 from the basic microsimulation algorithm to include a range of statistics. The most common statistics are percentiles or outcomes for ntiles.\n\n\n7.3.4 Example 3\nConsider the previous example. Let’s summarize the mean benefit by family income decile. We can use the ntile() function to construct ntiles(). We can use min() and max() in summarize() to define the bounds of the ntiles.\n\ndistributional_table &lt;- proposal1 |&gt;\n  mutate(ftotval_decile = ntile(ftotval, n = 10)) |&gt;\n  group_by(ftotval_decile) |&gt;\n  summarize(\n    min_income = min(ftotval),\n    max_income = max(ftotval),\n    mean_benefit = mean(benefit)\n  )\n\ndistributional_table\n\n# A tibble: 10 × 4\n   ftotval_decile min_income max_income mean_benefit\n            &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1              1     -19935      14160       1019. \n 2              2      14160      25200        921. \n 3              3      25200      37432        863. \n 4              4      37433      50100        622. \n 5              5      50100      65122        266. \n 6              6      65124      84000         59.6\n 7              7      84000     108006          0  \n 8              8     108010     143210          0  \n 9              9     143221     205154          0  \n10             10     205163    2320191          0  \n\n\n\n\nCode\ndistributional_table |&gt;\n  ggplot(aes(ftotval_decile, mean_benefit)) +\n  geom_col() +\n  geom_text(aes(label = scales::label_dollar()(mean_benefit)), vjust = -1) +\n  scale_x_continuous(breaks = 1:10) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1)),\n    labels = scales::label_dollar() \n  ) +\n  labs(\n    title = \"Distribution of Average Benefits Under the Benefit Proposal\",\n    x = \"Familiy Income Decile\",\n    y = \"Average Benefit\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nApply the calculator from the earlier exercise to the CPS data.\nAggregate outcomes for the 5th, 15th, and 25th percentiles.\nAggregate outcomes for the bottom 10 vintiles (a vintile is one twentieth of the population).\n\n\n\n\n\n7.3.5 Counterfactual Analysis\n\n\n\n\n\n\nCounterfactual\n\n\n\nA counterfactual is a situation that would be true under different circumstances.\n\n\nThe second major advantage of microsimulation is the ability to implement counterfactuals and evaluate “what-if” situations. Consider a few examples:\n\nWhat could happen to the distribution of post-tax income if the top marginal tax rate is increased by 5 percentage points?\nWhat could happen to median Social Security benefits in 2030 if the retirement age is increased by 2 months every year beginning in 2024?\nWhat could happen to total student loan balances if federal student loan interest accumulation is paused for 6 more months.\n\nWe update our microsimulation algorithm to include counterfactual analysis.\n\nConstruct a sample that represents the population of interest.\nApply accounting rules that reflect current circumstances. This is the baseline microsimulation.\nApply accounting rules that reflect counterfactual circumstances. This is the counterfactual microsimulation.\nAggregate results with a focus on the difference between the baseline microsimulation and the counterfactual simulation.\n\n\n\n7.3.6 Example 4\nLet’s pretend the new_tax_credit() is current law. It is our baseline. Suppose a legislator proposes reforms to the law. This is our counterfactual. Here are the proposed changes:\n\nEliminate benefits for families with zero income to promote work.\nEliminate the $20,000 income exclusion to reduce benefits for higher earners.\n\n\n#' Calculate the benefit from the new tax credit\n#'\n#' @param num_children A numeric for the number of children\n#' @param family_income A numeric for family income\n#'\n#' @return Numeric benefit in dollars\nnewer_tax_credit &lt;- function(num_children, family_income) {\n  \n  dplyr::case_when(\n    family_income == 0 ~ 0,\n    num_children &gt;= 3 ~ pmax(0, 6000 - 0.1 * family_income),\n    num_children == 2 ~ pmax(0, 4500 - 0.1 * family_income),\n    num_children == 1 ~ pmax(0, 3000 - 0.1 * family_income),\n    TRUE ~ 0\n  )\n\n}\n\n\nproposal2 &lt;- asec |&gt;\n  mutate(\n    benefit_baseline = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    ),\n    benefit_counteractual = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = newer_tax_credit\n    )\n  )\n\nproposal2 |&gt;\n  mutate(benefit_change = benefit_counteractual - benefit_baseline) |&gt;\n    mutate(ftotval_decile = ntile(ftotval, n = 10)) |&gt;\n  group_by(ftotval_decile) |&gt;\n  summarize(\n    min_income = min(ftotval),\n    max_income = max(ftotval),\n    mean_benefit = mean(benefit_change)\n  )\n\n# A tibble: 10 × 4\n   ftotval_decile min_income max_income mean_benefit\n            &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1              1     -19935      14160       -374. \n 2              2      14160      25200       -436. \n 3              3      25200      37432       -557. \n 4              4      37433      50100       -467. \n 5              5      50100      65122       -242. \n 6              6      65124      84000        -59.6\n 7              7      84000     108006          0  \n 8              8     108010     143210          0  \n 9              9     143221     205154          0  \n10             10     205163    2320191          0  \n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nRun the baseline and counterfactual simulations.\nCreate a bar chart representing the change in benefits.\n\n\n\n\n\n7.3.7 Extrapolation\n\n\n\n\n\n\nExtrapolation\n\n\n\nExtrapolation is the extension of microsimulation models to unobserved time periods. Most often, microsimulation models are extrapolated into the future.\n\n\nMost microsimulation models incorporate time. For example, many models look at a few or many years. We now add a new step 2 to the microsimulation algorithm, where we we can project demographic and economic outcomes into the future or past before applying accounting rules and aggregating the results.\n\nConstruct a sample that represents the population of interest.\nExtrapolate the population of interest over multiple time periods.\nApply accounting rules.\nAggregate.\n\nSome microsimulation models treat time as continuous. More often, microsimulation treats time as discrete. For example, models represent time every month or every year.\nExtrapolation adds uncertainty and assumptions to microsimulation. It is important to be clear about what microsimulation is and isn’t.\n\n\n\n\n\n\nProjection\n\n\n\nA projection explores what could happen under a given set of assumptions. It is always correct under the assumptions.\nStats Canada\n\n\n\n\n\n\n\n\nForecast\n\n\n\nA forecast attempts to predict the most-likely future.\nStats Canada\n\n\nMost microsimulation models are projections, not forecasts.\n\n\n7.3.8 Transitions\n\n\n\n\n\n\nStatic Microsimulation\n\n\n\nStatic microsimulation models do not subject units to individual transitions between time periods \\(t - 1\\) and \\(t\\) or to behavioral responses.\n\n\nStatic models typically only deal with one time period or they deal with multiple time periods but reweight the data to match expected totals and characteristics over time. Basically, individual decisions affect the distribution of outcomes but have little impact on overall outcomes.\n\n\n\n\n\n\nDynamic Microsimulation\n\n\n\nDynamic microsimulation models subject individual units to transitions between time periods \\(t - 1\\) and \\(t\\). This is sometimes referred to as “aging” the population. Dynamic microsimulation models sometimes subject individual units to behavioral responses.\n\n\nTransitions from period \\(t - 1\\) to period \\(t\\) are key to dynamic microsimulation. Transitions can be deterministic or stochastic. An example of a deterministic transition is an individual always joining Medicare at age 65. An example of a stochastic transition is an unmarried individual marrying at age 30 with probability \\(p_1\\) and remaining unmarried with probability \\(p_2\\). Stochastic transitions are connected to the idea of Monte Carlo simulation.\nTransition models are fundamental to stochastic transitions. Transition models include transition probability models (categorical variables) and processes based on probability distributions (continuous variables).\nHere are are few examples of transitions that could be modeled:\n\nIf an individual will acquire more education.\nIf an individual will work.\nIf an individual will marry, divorce, or widow.\nIf an individual will have children.\nIf an individual will retire.\nIf an individual will die.\n\nTransition models are often taken from existing literature or estimated on panel data. The data used to estimate these models must have at least two time periods.\nIt is common to extrapolate backwards, which is also known as backcasting, to evaluate microsimulation models against history. Backcasting can add important longitudinal information to records like detailed earnings histories for calculating Social Security benefits. Backcasting also offers opportunities to benchmark model projections against observed history.\n\n\n7.3.9 Example 5\nLet’s extrapolate our 2022 CPS data to 2023 and 2024 using transition models for number of children and family total income.\nThe transition model for number of children is very simple. 15% of families lose a child, 80% of families observe no change, and 5% of families gain one child.\n\n#' Extrapolate the number of children\n#'\n#' @param num_children A numeric for the number of children in time t\n#'\n#' @return A numeric for the number in time t + 1\n#'\nchildren_hazard &lt;- function(num_children) {\n  \n  change &lt;- sample(x = c(-1, 0, 1), size = 1, prob = c(0.15, 0.8, 0.05))\n  \n  pmax(num_children + change, 0)\n  \n}\n\nThe transition model for family income is very simple. The proportion change is drawn from a normal distribution with \\(\\mu = 0.02\\) and \\(\\sigma = 0.03\\).\n\n#' Extrapolate family income\n#'\n#' @param num_children A numeric for family income in time t\n#'\n#' @return A numeric for family income in time t + 1\n#'\nincome_hazard &lt;- function(family_income) {\n  \n  change &lt;- rnorm(n = 1, mean = 0.02, sd = 0.03)\n  \n  family_income + family_income * change\n  \n}\n\nFor simplicity, we combine both transition models into one function.\n\n#' Extrapolate the simple CPS\n#'\n#' @param data A tibble with nchild and ftotval\n#'\n#' @return A data frame in time t + 1\n#'\nextrapolate &lt;- function(data) {\n  \n  data |&gt;\n    mutate(\n      nchild = map_dbl(.x = nchild, .f = children_hazard),\n      ftotval = map_dbl(.x = ftotval, .f = income_hazard),\n      year = year + 1\n    )\n\n}\n\nFinally, we extrapolate.\n\n# make the stochastic results reproducible\nset.seed(20230812)\n\n# extrapolate using t to create t + 1\nasec2023 &lt;- extrapolate(asec)\nasec2024 &lt;- extrapolate(asec2023)\n\n# combine\nasec_extrapolated &lt;- bind_rows(\n  asec,\n  asec2023,\n  asec2024\n)\n\nThe benefit and income amounts in new_tax_credit() are not indexed for inflation. Let’s see how benefits change over time with the extrapolated data.\n\nasec_extrapolated |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    )\n  ) |&gt;\n  group_by(year) |&gt;\n  summarize(\n    mean_ftotval = mean(ftotval),\n    total_nchild = sum(nchild),\n    mean_benefit = mean(benefit)\n  )\n\n# A tibble: 3 × 4\n   year mean_ftotval total_nchild mean_benefit\n  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1  2022       97768.        46296         375.\n2  2023       99705.        45615         379.\n3  2024      101700.        44848         379.\n\n\nEven though incomes grew and the number of children declined, it turns out that enough families went from zero children to one child under the transition probability model for children that average benefits remained about the same.\n\n\n7.3.10 Beyond Accounting Rules\n\n\n\n\n\n\nBehavioral Responses\n\n\n\nA behavioral response is an assumed reaction to changing circumstances in a microsimulation model.\nFor example, increasing Social Security benefits may crowd out retirement savings (Chetty et al. 2014). In other words, the existence of Social Security may induce a person to save less for retirement.\n\n\nUntil now, we’ve only considered accounting rules within each time period. Accounting rules are appealing because they don’t require major assumptions, but they are typically insufficient. It is often necessary to consider how units will respond to a changing environment.\nBehavioral responses are often elasticities estimated with econometric analysis. Behavioral responses are also a huge source of assumptions and uncertainty for microsimulation.\nConsider a microsimulation model that models retirement savings. When modeling a proposal to expand Social Security benefits, an analyst may use parameters estimated in (Chetty et al. 2014) to model reductions in savings in accounts like 401(k) accounts. These are behavioral responses. These estimates are uncertain, so the analyst could add sensitivity analysis to model low, medium, and high rates of crowding out."
  },
  {
    "objectID": "07_microsimulation.html#building-a-representative-population",
    "href": "07_microsimulation.html#building-a-representative-population",
    "title": "7  Microsimulation",
    "section": "7.4 Building a Representative Population",
    "text": "7.4 Building a Representative Population\nBuilding a starting sample and constructing data for estimating transition models is difficult. We briefly outline a few techniques of interest.\n\n7.4.1 Synthetic Starting Data\nWe’ve adopted a cross-sectional starting population. Some microsimulation models adopt a synthetic2 approach where every observation is simulated from birth.\n\n\n7.4.2 Data Linkage and Imputation\nMany microsimulation models need more variables than are included in any one source of information. For example, retirement models often need demographic information, longitudinal earnings information, and tax information. Many microsimulation models rely on data linkage and data imputation techniques to augment their starting data.\n\n\n\n\n\n\nData Linkage\n\n\n\nData linkage is the process of using distance-based rules or probabilistic models to connect an observation in one source of data to an observation in another source of data.\n\n\n\n\n\n\n\n\nData imputation\n\n\n\nData imputation is the process of using models to predict values where data is unobserved. The data could be missing because of nonresponse or because the information was not gathered in the data collection process.\n\n\n\n\n7.4.3 Validation\n\n\n\n\n\n\nValidation\n\n\n\nValidation is the process of reviewing results to determine their validity. Techniques include comparing statistics, visual comparisons, and statistical comparisons like the Kolmogorov-Smirnov test for the equivalence of two distributions.\n\n\nVisual and statistical validation are essential to evaluating the quality of a microsimulation model. If validation looks poor, then a modeler can redo other parts of the microsimulation workflow or they can reweight or align the data.\n\n\n7.4.4 Reweighting\n\n\n\n\n\n\nReweighting\n\n\n\nReweighting is the process of adjusting observation weights in a data set so aggregate weighted statistics from the data set hit specified targets.\n\n\nSuppose a well-regarded source of information says mean income is $50,000 but a microsimulation model estimates mean income of $45,000. We can use reweighting to plausibly adjust the weights in the microsimulation model so mean income is $50,000.\nTechniques include post-stratification and calibration. Kolenikov (2016) offers a good introduction.\n\n\n7.4.5 Alignment\n\n\n\n\n\n\nAlignment\n\n\n\nAlignment is the process of adjusting model coefficients or predicted values so aggregated outputs align with specified aggregate targets. These targets are aggregate outcomes like total income or state-level tax revenue.\n\n\nSuppose a well-regarded source of information says mean income is $50,000 but a microsimulation model estimates mean income of $45,000. We can adjust the intercept in linear regression models or adjust predicted values so mean income is $50,000. Li and O’Donoghue (2014) offers a good introduction."
  },
  {
    "objectID": "07_microsimulation.html#assumptions",
    "href": "07_microsimulation.html#assumptions",
    "title": "7  Microsimulation",
    "section": "7.5 Assumptions",
    "text": "7.5 Assumptions\nMicrosimulation is only as useful as its assumptions. We will review a few key assumptions present in many microsimulation models.\n\n7.5.1 Open vs. Closed\n\n\n\n\n\n\nClosed Model\n\n\n\nA closed microsimulation model models the life cycle of all units in the model. For example, two existing units marry.\n\n\n\n\n\n\n\n\nOpen Model\n\n\n\nAn open microsimulation model allows for the on-demand generation of new, but mature, units in the model.\n\n\n\n\n7.5.2 Independence\nAnother important assumption deals with the relationship between units in the model. Should observations be treated as wholly independent or do they interact? For example, if someone takes a job is it more difficult for another individual to take the job?\nInteractions can be explicit or implicit.\n\n\n\n\n\n\nExplicit Interaction\n\n\n\nExplicit interaction allows the actions of one unit to affect other units during extrapolation or behavioral models.\nFor example, models of marriage markets may account for changing economic circumstances among potential matches.\n\n\n\n\n\n\n\n\nImplicit Interaction\n\n\n\nImplicit interaction allows the actions of one unit to affect other units in post-processing.\nFor example, reweighting and alignment techniques allow outcomes for one unit to affect other units through intercepts in models and new weights.\n\n\n\n\n7.5.3 Markov Assumption\n\n\n\n\n\n\nMarkov Assumption\n\n\n\nThe Markov assumptions states that the only factors affecting a transition from period \\(t - 1\\) to period \\(t\\) are observable in \\(t - 1\\).\nThe Markov assumption only considers memory or history to the extent that it is observable in period \\(t - 1\\). For example, \\(t - 2\\) may affect educational attainment or college savings in \\(t - 1\\), which can affect the transition from \\(t - 1\\) to \\(t\\), but \\(t - 2\\) will never be explicitly included."
  },
  {
    "objectID": "07_microsimulation.html#uncertainty",
    "href": "07_microsimulation.html#uncertainty",
    "title": "7  Microsimulation",
    "section": "7.6 Uncertainty",
    "text": "7.6 Uncertainty\n\n\n\n\n\n\nAletoric Uncertainty\n\n\n\nAleatoric uncertainty is uncertainty due to probabilistic randomness.\n\n\n\n\n\n\n\n\nEpistemic Uncertainty\n\n\n\nEpistemic uncertainty is uncertainty due to lack of knowledge of the underlying system.\n\n\nThe microsimulation field has a poor track record of quantifying uncertainty. Most estimates do not contain standard errors or even crude distributions of outcomes.\nMicrosimulation models have many sources of uncertainty. The starting data and data used for model estimation are often samples with sampling error. Transition models fail to capture all sources of variation. The models often intentionally include Monte Carlo error.\nMcClelland, Khitatrakun, and Lu (2020) explored adding confidence intervals to microsimulation models using normal approximations and bootstrapping methods3. They find that normal approximations work well in most cases unless policy changes affect a small number of returns.\nAcross microsimulations for five policy approaches, they estimate modest confidence intervals. This makes sense. First, microsimulation aggregates many units that have their own sources of uncertainty. This is different than earlier examples of Monte Carlo simulation that focused on one observation at a time. Aggregation reduces variance.4\nSecond, the authors only consider uncertainty because of sampling variation. This fails to capture aleatoric uncertainty from statistical matching, imputation, and projection. Furthermore, these methods fail to capture epistemic uncertainty. The confidence intervals, in other words, assume the model is correct.\nGiven these shortcomings, it is important to be clear about assumptions, transparent about implementations, and humble about conclusions. All useful models are wrong. The hope is to be as little wrong as possible."
  },
  {
    "objectID": "07_microsimulation.html#microsimulation-model-examples",
    "href": "07_microsimulation.html#microsimulation-model-examples",
    "title": "7  Microsimulation",
    "section": "7.7 Microsimulation Model Examples",
    "text": "7.7 Microsimulation Model Examples\nFirst, let’s outline a few characteristics of microsimulation models.\n\n\n\n\n\n\nCompiled Programming Languages\n\n\n\nCompiled programming languages have an explicit compiling step where code is converted to assembly language and ultimately binary code.\nC++, Fortran, and Java are examples of compiled programming languages.\n\n\n\n\n\n\n\n\nScripting Programming Languages\n\n\n\nScripting programming languages, also known as interpreted programming languages, do not have a compiling step.\nR, Python, and Julia are examples of scripting programming languages.\n\n\nAruoba and Fernndez-Villaverde (2018) benchmark several programming languages on the same computing task. Lower-level, compiled programming languages dominate higher-level, scripting programming languages like R and Python. Julia is the lone bright spot that blends usability and performance.\nMany microsimulation models are written in lower-level, compiled programming languages. Fortran may seem old, but there is a reason it has stuck around for microsimulation.\n\n\n\n\n\n\nGeneral Microsimulation Models\n\n\n\nGeneral microsimulation models contain a wide range of behaviors and population segments.\n\n\n\n\n\n\n\n\nSpecialized Microsimulation Models\n\n\n\nSpecialized microsimulation models focus on a limit set of behaviors or population segments. ~ Stats Canada\n\n\n\n7.7.1 Simulating the 2020 Census\n\nName: Simulating the 2020 Census\nAuthors: Diana Elliott, Steven Martin, Jessica Shakesprere, Jessica Kelly\nGeneral or specific: Specific\nLanguage: Python\nPurpose: The authors simulate various Decennial Census response factors and evaluate the distribution of responses to the Decennial Census.\n\n\n\n7.7.2 TPC Microsimulation in the Cloud\n\nName: TPC Microsimulation in the Cloud\nAuthors: The TPC microsimulation team, Jessica Kelly, Kyle Ueyama, Alyssa Harris\nGeneral or specific: Specific\nLanguage: Fortran\nPurpose: The authors take TPC’s tax microsimulation model and move it to the cloud. This allows TPC to reverse the typical microsimulation process. Instead of describing policies and observing the outcomes, they can describe desirable outcomes and then use grid search to back out policies that achieve those outcomes.\n\n\n\n7.7.3 Modeling Income in the Near Term (MINT)5\n\nName: Modeling Income in the Near Term (MINT)\nAuthors: Karen E. Smith and many other people\nGeneral or specific: General\nLanguage: SAS6\nPurpose: The Social Security Administration uses MINT to evaluate the distributional impact of various Social Security policy proposals.\n\n\n\n\n\nAruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A Comparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien Nielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and Crowd-Out in Retirement Savings Accounts: Evidence from Denmark*.” The Quarterly Journal of Economics 129 (3): 1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response Adjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary Alignment Methods in Microsimulation Models.” Journal of Artificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020. “Estimating Confidence Intervals in a Tax Microsimulation Model.” International Journal of Microsimulation 13 (2): 2–20. https://doi.org/10.34196/IJM.00216.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.” The Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528."
  },
  {
    "objectID": "07_microsimulation.html#footnotes",
    "href": "07_microsimulation.html#footnotes",
    "title": "7  Microsimulation",
    "section": "",
    "text": "Most data for microsimulation models are collected through complex surveys. Accordingly, most microsimulation models need to account for weights to calculate estimates that represent the entire population of interest.↩︎\nSynthetic will carry many meanings this semester.↩︎\nTheir implementation of bootstrapping is clever and uses replicate weights to simplify computation and manage memory.↩︎\nThe standard error of the mean reduces at a rate of \\(\\sqrt{n}\\) as the sample size increases.↩︎\nMINT comically models income and many other variables for at least 75 years into the future.↩︎\nThe Dynamic Simulation of Income Model (DYNASIM) is a related to MINT and is written in Fortran.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A\nComparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nBarrientos, Andrés F., Aaron R. Williams, Joshua Snoke, and Claire McKay\nBowen. 2021. “A Feasibility Study of Differentially Private\nSummary Statistics and Regression Analyses with Evaluations on\nAdministrative and Survey Data.” https://doi.org/10.48550/ARXIV.2110.12055.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical\nInference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien\nNielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and\nCrowd-Out in Retirement Savings Accounts: Evidence from\nDenmark*.” The Quarterly Journal of Economics 129 (3):\n1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nFellegi, I. P. 1972. “On the Question of Statistical\nConfidentiality.” Journal of the American Statistical\nAssociation 67 (337): 7–18. https://www.jstor.org/stable/2284695?seq=1#metadata_info_tab_contents.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. Springer Series in Statistics. New York, NY:\nSpringer.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response\nAdjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLeisch, Friedrich. 2004. “FlexMix: A General Framework for Finite\nMixture Models and Latent Class Regression in\nR.” Journal of Statistical\nSoftware 11 (8). https://doi.org/10.18637/jss.v011.i08.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary\nAlignment Methods in Microsimulation Models.” Journal of\nArtificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020.\n“Estimating Confidence Intervals in a Tax Microsimulation\nModel.” International Journal of Microsimulation 13 (2):\n2–20. https://doi.org/10.34196/IJM.00216.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An\nIntroduction. Adaptive Computation and Machine Learning Series.\nCambridge, Massachusetts: The MIT Press.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.”\nThe Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528.\n\n\nRavn, Signe, Ashley Barnwell, and Barbara Barbosa Neves. 2020.\n“What Is “Publicly Available Data”?\nExploring Blurred PublicPrivate Boundaries and Ethical\nPractices Through a Case Study on Instagram.” Journal of\nEmpirical Research on Human Research Ethics 15 (1-2): 40–45. https://doi.org/10.1177/1556264619850736.\n\n\nSomepalli, Gowthami, Singla, Micah Goldblum, Jonas Geiping, and Tom\nGoldstein. 2023. “Diffusion Art or Digital Forgery? Investigating\nData Replication in Diffusion Models.” Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 6048–58. https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualie, and Model\nData. 2nd edition. Sebastopol, CA: O’Reilly."
  }
]