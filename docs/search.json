[
  {
    "objectID": "27_advanced-unsupervised-ml.html",
    "href": "27_advanced-unsupervised-ml.html",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "",
    "text": "17.1 Review 1\nWe’ll start with a review of multivariate normal distributions. In particular, this exercise demonstrates the impact of the variance-covariance matrix on the shape of multivariate normal distributions.\nlibrary(mvtnorm)\n\nsigma1 &lt;- matrix(\n  c(1, 0,\n    0, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\nsigma2 &lt;- matrix(\n  c(1, 0.8,\n    0.8, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\nbind_rows(\n  independent = tibble(\n    V1 = rnorm(n = 1000),\n    V2 = rnorm(n = 1000)\n  ),\n  sigma1 = rmvnorm(\n    n = 1000, \n    sigma = sigma1\n  ) |&gt;\n    as_tibble(),\n  sigma2 = rmvnorm(\n    n = 1000, \n    sigma = sigma2\n  ) |&gt;\n    as_tibble(),\n  .id = \"source\"\n) |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point() +\n  facet_wrap(~ source)",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#sec-review6a",
    "href": "27_advanced-unsupervised-ml.html#sec-review6a",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "",
    "text": "Exercise 1\n\n\n\n\nLoad library(mvtnorm).\nCopy and paste the following code. This code will not obviously not run as is. We will add tibbles to independent, sigma1 and sigma2 in the steps below.\n\n\nbind_rows(\n  independent = ,\n  var_covar1 = ,\n  var_covar2 = ,\n  .id = \"source\"\n)\n\n\nCreate a tibble with V1 and V2. For both variables, use rnorm() to sample 1,000 observations from a standard normal distribution. Add the results to independent.\nUsing the following variance-covariance matrix, sample 1,000 observations from a multivariate-normal distribution. Add the results for sigma1 and use as_tibble().\n\n\nsigma1 &lt;- matrix(\n  c(1, 0,\n    0, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\n\nUsing the following variance-covariance matrix, sample 1,000 observations from a multivariate-normal distribution. Add the results for sigma2 and use as_tibble().\n\n\nsigma2 &lt;- matrix(\n  c(1, 0.8,\n    0.8, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\n\nCreate a scatter plot with V1 on the x-axis and V2 on the y-axis. Facet based on source.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#a-new-type-of-random-variable",
    "href": "27_advanced-unsupervised-ml.html#a-new-type-of-random-variable",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "17.2 A New Type of Random Variable",
    "text": "17.2 A New Type of Random Variable\nWe learned about common univariate and multivariate distributions. For each of the distributions, there are well-defined and straightforward ways to sample values from the distribution. We can also manipulate these distributions to calculate probabilities.\nThe real world is complicated, and we will quickly come across data where we struggle to find a common probability distributions.\nFigure Figure 17.1 shows a relative frequency histogram for the duration of eruptions at Old Faithful in Yellowstone National Park.\n\n# faithful is a data set built into R\nfaithful |&gt;\n  ggplot(aes(eruptions, y = after_stat(density))) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 17.1: Distribution of waiting times between eruptions at the Old Faithful geyser in Yellowstone National Park.\n\n\n\n\n\nThis distribution looks very complicated. But what if we break this distribution into pieces? In this case, what if we think of the distribution as a combination of two normal distributions?\n\n\nCode\n# show geyser as two normal distribution\nlibrary(mclust)\n\ngmm_geyser &lt;- Mclust(\n  data = dplyr::select(faithful, eruptions), \n  G = 2\n)\n\n\nbind_cols(\n  faithful,\n  cluster = gmm_geyser$classification\n) |&gt;\n  ggplot(aes(eruptions, y = after_stat(density), \n             fill = factor(cluster))) +\n  geom_histogram() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Variable\n\n\n\nA latent variable is a variable that isn’t directly observed but can be inferred through other variables and modeling. Sometimes the latent variable is meaningful but unobserved. Sometimes it isn’t meaningful.\nLatent variables are sometimes called hidden variables.\n\n\nBreaking complex problems into smaller pieces is good. These latent variables will allow us to do some cools things:\n\nSimply express complicated probability distributions\nMake inferences about complex populations\nCluster data\n\nIn this set of notes, we’ll use latent variables to\n\nConstruct mixture distributions\nCluster data\n\nLet’s consider a “data generation story” different than anything we considered in Chapter 5. Instead of sampling directly from one known probability distribution, we will sample in two stages (Hastie, Tibshirani, and Friedman 2009).\n\nSample from a discrete probability distribution with \\(k\\) unique values (i.e. Bernoulli distribution when \\(k = 2\\) and categorical distribution when \\(k &gt; 2\\)).\nSample from one of \\(k\\) different distributions conditional on the outcome of step 1.\n\nThis new sampling procedure aligns closely with the idea of hierarchical sampling and hierarchical models. It is also sometimes called ancestral sampling (Bishop 2006, 430).\nThis two-step approach dramatically increases the types of distributions at our disposal because we are no longer limited to individual common univariate distributions like a single normal distribution or a single uniform distribution. The two-step approach is also the foundation of two related tools:\n\nMixture distributions: Distributions expressed as the linear combination of other distributions. Mixture distributions can be very complicated distributions expressed in terms of simple distributions with known properties.\nMixture modeling: Statistical inference about sub-populations made only with pooled data without labels for the sub populations.\n\nWith mixture distributions, we care about the overall distribution and don’t care about the latent variables.\nWith mixture modeling, we use the overall distribution to learn about the latent variables/sub populations/clusters in the data.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#mixture-distributions",
    "href": "27_advanced-unsupervised-ml.html#mixture-distributions",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "17.3 Mixture Distributions",
    "text": "17.3 Mixture Distributions\n\n\n\n\n\n\nMixture Distribution\n\n\n\nA mixture distribution is a probabilistic model that is a linear combination of common probability distributions.\nA discrete mixture distribution can be expressed as\n\\[\np_{mixture}(x) = \\sum_{k = 1}^K \\pi_kp(x)\n\\]\nwhere \\(K\\) is the number of mixtures and \\(\\pi_k\\) is the weight of each PMF included in the mixture distribution.\nA continuous mixture distribution can be expressed as\n\\[\np_{mixture}(x) = \\sum_{k = 1}^K \\pi_kf(x)\n\\]\nwhere \\(K\\) is the number of mixtures and \\(\\pi_k\\) is the weight of each PDF included in the mixture distribution.\n\n\n\n17.3.1 Example 1\nLet’s consider a concrete example with a Bernoulli distribution and two normal distributions.\n\nSample \\(X \\sim Bern(p = 0.25)\\)\nSample from \\(Y \\sim N(\\mu = 0, \\sigma = 2)\\) if \\(X = 0\\) and \\(Y \\sim (\\mu = 4, \\sigma = 2)\\) if \\(X = 1\\).\n\nNow, let’s sample from a Bernoulli distribution and then sample from one of two normal distributions using R code.\n\ngenerate_data &lt;- function(n) {\n  \n  step1 &lt;- sample(x = c(0, 1), size = n, replace = TRUE, prob = c(0.75, 0.25))\n  \n  step1 &lt;- sort(step1)\n  \n  step2 &lt;- c(\n    rnorm(n = sum(step1 == 0), mean = 0, sd = 2),\n    rnorm(n = sum(step1 == 1), mean = 5, sd = 1)\n  )\n  \n  tibble::tibble(\n    x = step1,\n    y = step2\n  )\n\n}\n\nset.seed(1)\n\ngenerate_data(n = 1000) |&gt;\n  ggplot(aes(x = y, y = after_stat(density))) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThis marginal distribution looks complex but the process of creating the marginal distribution is simple.\nIn fact, consider this quote from Bishop (2006) (Page 111):\n\nBy using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.\n\n\n\n\n\n\n\nComponent\n\n\n\nA component is each common probability distribution that is combined to create a mixture distribution. For example, a mixture of two Gaussian distributions has two components.\n\n\n\n\n\n\n\n\nMixing Coefficient\n\n\n\nA mixing coefficient is the probability associated with a component with a component in a mixture distribution. Mixing coefficients must sum to 1.\nWe’ll use \\(\\pi_k\\) for population mixing coefficients and \\(p_k\\) for sample mixing coefficients. Mixing coefficients are also called mixing weights and mixing probabilities.\n\n\nMixture distributions are often overparameterized, which means they have an excessive number of parameters. For a univariate mixture of normals with \\(k\\) components, we have \\(k\\) means, \\(k\\) standard deviations, and \\(k\\) mixing coefficients.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nSample 1,000 observations from a mixture of three normal distributions with the following parameters:\n\n\n\\(p_1 = p_2 = p_3\\)\n\\(\\mu_1 = -3\\), \\(\\mu_2 = 0\\), \\(\\mu_3 = 3\\)\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3 = 1\\)\n\n\nCreate a relative frequency histogram of the values.\n\n\n\n\n\n17.3.2 Example 2\nSuppose we used statistical inference to infer some parameters for the geysers example above. We will describe how to estimate these paramaters later.\n\n\\(p_1 =\\) 0.3485696 and \\(p_2 =\\) 0.6514304\n\\(\\bar{x_1} =\\) 2.0189927 and \\(\\bar{x_2} =\\) 4.2737083\n\\(s_1 =\\) 0.2362355and \\(s_2 =\\) 0.4365146\n\nThe mixture density is\n\\[\nf_{mixture}(x) = p_1f(x|\\mu = \\bar{x_1}, \\sigma = s_1) + p_2f(x|\\mu = \\bar{x_2},\\sigma=s_2)\n\\tag{17.1}\\]\n\ngeyser_density &lt;- function(x, model) {\n  \n  probs &lt;- model$parameters$pro\n  \n  d1 &lt;- dnorm(\n    x, \n    mean =  model$parameters$mean[1], \n    sd = sqrt(model$parameters$variance$sigmasq[1])\n  )\n  \n  d2 &lt;- dnorm(\n    x, \n    mean =  model$parameters$mean[2], \n    sd = sqrt(model$parameters$variance$sigmasq[2])\n  )\n  \n  probs[1] * d1 + probs[2] * d2\n  \n}\n\nmm &lt;- tibble(\n  x = seq(0, 5, 0.01),\n  f_x = map_dbl(x, geyser_density, model = gmm_geyser)\n) \n\nggplot() +\n  geom_histogram(data = faithful, mapping = aes(x = eruptions, y = after_stat(density))) +\n  geom_line(data = mm, mapping = aes(x, f_x), color = \"red\") + \n  labs(\n    title = \"\",\n    subtitles = \"Observed data in black, inferred distribution in red\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#sec-review6b",
    "href": "27_advanced-unsupervised-ml.html#sec-review6b",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "17.4 Review #2",
    "text": "17.4 Review #2\n\n17.4.1 Multivariate Normal Distribution\nThe multivariate normal distribution is a higher-dimensional version of the univariate normal distribution. The MVN distribution has a vector of means of length \\(k\\) and a \\(k\\)-by-\\(k\\) variance-covariance matrix.\nWe show that a random vector is multivariate normally distributed with\n\\[\n\\vec{X} \\sim \\mathcal{N}(\\vec\\mu, \\boldsymbol\\Sigma)\n\\tag{17.2}\\]\nThe PDF of a multivariate normally distributed random variable is\n\\[\nf(x) = (2\\pi)^{-k/2}det(\\boldsymbol\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\vec{x} - \\vec\\mu)^T\\boldsymbol\\Sigma^{-1}(\\vec{x} - \\vec\\mu)\\right)\n\\tag{17.3}\\]\n\n\n17.4.2 K-Means Clustering\nK-Means Clustering is a heuristic-based approach to finding latent groups in data. The algorithm assigns each observation to one and only one group through a two step iteration that minimizes the Euclidean distance between observations and centroids for each group.\n\nSetupStep 1Step 2Step 3Step 4Step 5\n\n\nConsider the following data set.\n\n\nCode\ndata &lt;- tibble(x = c(1, 2, 1, 4, 7, 10, 8),\n               y = c(5, 4, 4, 3, 7, 8, 5))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Randomly place K centroids in your n-dimensional vector space\n\n\nCode\ncentroids &lt;- tibble(x = c(2, 5),\n                  y = c(5, 5),\n                  cluster = c(\"a\", \"b\"))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Calculate the nearest centroid for each point using a distance measure\n\n\nCode\ncentroids &lt;- tibble(x = c(2, 5),\n                  y = c(5, 5),\n                  cluster = c(\"a\", \"b\"))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  geom_line(aes(x = c(4, 2), y = c(3, 5)), linetype = \"dashed\") +  \n  geom_line(aes(x = c(4, 5), y = c(3, 5)), linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Assign each point to the nearest centroid\n\n\nCode\ndata$cluster &lt;- c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\")\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Recalculate the position of the centroids based on the means of the assigned points\n\n\nCode\ncentroids2 &lt;- data %&gt;%\n  group_by(cluster) %&gt;%\n  summarize(x = mean(x), y = mean(y))\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids, aes(x, y), size = 4, alpha = 0.25) +\n  geom_point(data = centroids2, aes(x, y, color = cluster), size = 4) +  \n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Repeat steps 2-4 until no points change cluster assignments\n\n\nCode\ndata$cluster &lt;- c(\"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\")\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids2, aes(x, y, color = cluster), size = 4) +  \n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse library(tidyclust) to cluster the faithful data into three clusters.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#mixture-modelingmodel-based-clustering",
    "href": "27_advanced-unsupervised-ml.html#mixture-modelingmodel-based-clustering",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "17.5 Mixture Modeling/Model-Based Clustering",
    "text": "17.5 Mixture Modeling/Model-Based Clustering\nUntil now, we’ve assumed that we’ve known all parameters when working with mixture distributions. What if we want to learn these parameters/make inferences about these parameters?\nThe process of making inferences about latent groups is related to K-Means Clustering. While K-Means Clustering is heuristic based, mixture modeling formalize the process of making inferences about latent groups using probability models. Gaussian mixture models (GMM) are a popular mixture model.\n\n\n\n\n\n\nMixture Modeling\n\n\n\nMixture modeling is the process of making inferences about sub populations using data that contain sub population but no labels for the sub populations.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#gaussian-mixture-modeling-gmm",
    "href": "27_advanced-unsupervised-ml.html#gaussian-mixture-modeling-gmm",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "17.6 Gaussian Mixture Modeling (GMM)",
    "text": "17.6 Gaussian Mixture Modeling (GMM)\n\n\n\n\n\n\nGaussian Mixture Modeling (GMM)\n\n\n\nGaussian mixture modeling (GMM) is mixture modeling that uses normal and multivariate normal distributions.\n\n\n\n\n\n\n\n\nHard Assignment\n\n\n\nHard assignment assigns an observation in a clustering model to one and only one group.\n\n\n\n\n\n\n\n\nSoft Assignment\n\n\n\nSoft assignment assigns an observation in a clustering model to all groups with varying weights or probabilities.\n\n\n\n\n\n\n\n\nResponsibilities\n\n\n\nSoft assignments are quantified with responsibilities. Responsibilities are the probability that a given observation belongs to a given group. The soft assignments for an observation sum to 1.\nWe quantified responsibilities with \\(\\pi_k\\) for mixture distributions. Responsibilities are parameters we will infer during mixture modeling.\n\n\nThere are two main differences between K-Means Clustering and GMM.\n\nInstead of calculating Euclidean distance from each observation to each group centroid, we use multivariate normal distributions to calculate the probability that an observation belongs to each group.\n\nObservations close to the means of a mixture will have a high relative probability of belonging to that mixture.\nObservations far from the means of a mixture will have a low relative probability of belonging to that mixture.\n\nInstead of simply updating \\(k\\) group centroids, we must update \\(k\\) multivariate normal distributions. This requires calculating a vector of means and a variance-covariance matrix for each of the \\(k\\) groups.\n\n\n17.6.1 Example 3\nThe parameters in example 2 were estimated using GMM. Let’s repeat a similar exercise with the faithful using eruptions and waiting instead of just eruptions. We’ll assume there are three groups.\n\n# fit GMM\ngmm2_geyser &lt;- Mclust(faithful, G = 3)\n\nLet’s plot the multivariate normal distributions. Figure 17.2 shows the centroids (stars) and shapes (ellipses) of the distributions in black. The colors represent hard assignments to groups and the size of the points represent the uncertainty of the assignments with larger points having more uncertainty.\n\n# plot fitted model\nplot(gmm2_geyser, what = \"uncertainty\")\n\n\n\n\n\n\n\nFigure 17.2: Uncertainty plot from a GMM\n\n\n\n\n\nWe can also summarize the model with library(broom).\n\nlibrary(broom)\n\naugment(gmm2_geyser)\n\n# A tibble: 272 × 4\n   eruptions waiting .class .uncertainty\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1      3.6       79 1          2.82e- 2\n 2      1.8       54 2          8.60e-13\n 3      3.33      74 1          3.26e- 3\n 4      2.28      62 2          3.14e- 7\n 5      4.53      85 3          1.17e- 2\n 6      2.88      55 2          3.09e- 3\n 7      4.7       88 3          2.99e- 3\n 8      3.6       85 1          2.39e- 2\n 9      1.95      51 2          5.23e-12\n10      4.35      85 3          5.52e- 2\n# ℹ 262 more rows\n\ntidy(gmm2_geyser)\n\n# A tibble: 3 × 5\n  component  size proportion mean.eruptions mean.waiting\n      &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1         1    40      0.166           3.79         77.5\n2         2    97      0.356           2.04         54.5\n3         3   135      0.478           4.46         80.8\n\nglance(gmm2_geyser)\n\n# A tibble: 1 × 7\n  model     G    BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 EEE       3 -2314. -1126.    11     NA   272\n\n\n\n\n17.6.2 mclust\nThe previous example uses library(mclust)1 and library(broom).\nMclust() is the main function for fitting Gaussian Mixture Models. The function contains several different types of models for the variances of the multivariate normal distributions. The defaults are sensible. G is the number of groups. If G isn’t specified, then Mclust() will try 1:9 and pick the G with the lowest BIC (defined below).\nplot() with what = \"uncertainty\" creates a very useful data visualization for seeing the multivariate normal distributions and classifications for low-dimensional GMM.\nglance(), tidy(), and augment() from library(broom) return important information about the assignments, groups, and model diagnostics.\n\n\n17.6.3 Estimation\nSuppose we have \\(n\\) observations, \\(k\\) groups, and \\(p\\) variables. A single GMM will have\n\nan \\(n\\) by \\(k\\) matrix of responsibilities\n\\(k\\) vectors of means of length \\(p\\)\n\\(k\\) \\(p\\) by \\(p\\) variance-covariance matrices\n\nWe want the maximum likelihood estimates for all of the parameters in the model. For technical reasons, it is very difficult to get these estimates using popular methods like stochastic gradient descent.\nInstead, we will use expectations maximization (EM) to find the parameters. We also used EM for K-Means clustering.\n\n\n\n\n\n\n\nRandomly initialize all of the parameters. Calculate the log-likelihood.\nE-Step: Update the responsibilities assuming the means and variance-covariance matrices are known.\nM-Step: Estimate new means and variance-covariance matrices assuming the responsibilities are known. The means and variance-covariance matrices are calculated using weighted MLE where the responsibilities are the weights.\nCalculate the log-likelihood. Go back to step 2 if the log-likelihood improves by at least as much as the stopping threshold.\n\n\n\n\nThis algorithm is computationally efficient, but it is possible for it to find a local maximum log-likelihood without finding the global maximum log-likelihood.\nFor a more mathematical description of this process, see Elements of Statistical Learning Section 6.8 (Hastie, Tibshirani, and Friedman 2009). A highly descriptive comparison to kmeans (with Python code) can be seen here.\n\n\n17.6.4 Example 4\nLet’s consider a policy-relevant example using data from the Small Area Health Insurance Estimates (SAHIE) Program.\nFirst, we pull the 2016 county-level estimates of the uninsured rate. We label a state as an expansion state if it expanded data before 2015-01-01. We use this date with 2016 data because of policy lags.\n\nlibrary(censusapi)\n\nsahie &lt;- getCensus(\n  name = \"timeseries/healthins/sahie\",\n  key = Sys.getenv(\"CENSUS_KEY\"),\n  vars = c(\"GEOID\", \"PCTUI_PT\"),\n  region = \"county:*\",\n  time = 2016\n) |&gt;\n  as_tibble()\n\nNext, we pull data from the Kaiser Family Foundation about the expansion dates of Medicaid under the Patient Protection and Affordable Care Act.\n\nstates &lt;- tribble(\n  ~state, ~state_fips, ~implementation_date,\n  \"Alabama\", \"01\", NA,\n  \"Alaska\", \"02\", \"2015-09-15\",\n  \"Arizona\", \"04\", \"2014-01-01\",\n  \"Arkansas\", \"05\", \"2014-01-01\",\n  \"California\", \"06\", \"2014-01-01\",\n  \"Colorado\", \"08\", \"2014-01-01\",\n  \"Connecticut\", \"09\", \"2014-01-01\",\n  \"Delaware\", \"10\", \"2014-01-01\",\n  \"District of Columbia\", \"11\", \"2014-01-01\",\n  \"Florida\", \"12\", NA,\n  \"Georgia\", \"13\", NA,\n  \"Hawaii\", \"15\", \"2014-01-01\",\n  \"Idaho\", \"16\", \"2020-01-01\",\n  \"Illinois\", \"17\", \"2014-01-01\",\n  \"Indiana\", \"18\", \"2015-02-01\",\n  \"Iowa\", \"19\", \"2014-01-01\",\n  \"Kansas\", \"20\", NA,\n  \"Kentucky\", \"21\", \"2014-01-01\", \n  \"Louisiana\", \"22\", \"2016-07-01\",\n  \"Maine\", \"23\", \"2018-07-02\",\n  \"Maryland\", \"24\", \"2014-01-01\",\n  \"Massachusetts\", \"25\", \"2014-01-01\",\n  \"Michigan\", \"26\", \"2014-04-01\",\n  \"Minnesota\", \"27\", \"2014-01-01\",\n  \"Mississippi\", \"28\", NA,\n  \"Missouri\", \"29\", \"2021-07-01\",\n  \"Montana\", \"30\", \"2016-01-01\",\n  \"Nebraska\", \"31\", \"2020-10-01\",\n  \"Nevada\", \"32\", \"2014-01-01\", \n  \"New Hampshire\", \"33\", \"2014-08-15\",\n  \"New Jersey\", \"34\", \"2014-01-01\",\n  \"New Mexico\", \"35\", \"2014-01-01\",\n  \"New York\", \"36\", \"2014-01-01\", \n  \"North Carolina\", \"37\", NA,\n  \"North Dakota\", \"38\", \"2014-01-01\", \n  \"Ohio\", \"39\", \"2014-01-01\",\n  \"Oklahoma\", \"40\", \"2021-07-01\", \n  \"Oregon\", \"41\", \"2014-01-01\", \n  \"Pennsylvania\", \"42\", \"2015-01-01\", \n  \"Rhode Island\", \"44\", \"2014-01-01\", \n  \"South Carolina\", \"45\", NA,\n  \"South Dakota\", \"46\", \"2023-07-01\", \n  \"Tennessee\", \"47\", NA,\n  \"Texas\", \"48\", NA,\n  \"Utah\", \"49\", \"2020-01-01\",\n  \"Vermont\", \"50\", \"2014-01-01\",\n  \"Virginia\", \"51\", \"2019-01-01\", \n  \"Washington\", \"53\", \"2014-01-01\",\n  \"West Virginia\", \"54\", \"2014-01-01\",\n  \"Wisconsin\", \"55\", NA,\n  \"Wyoming\", \"56\", NA\n) %&gt;%\n  mutate(implementation_date = ymd(implementation_date))\n\nsahie &lt;- left_join(\n  sahie, \n  states,\n  by = c(\"state\" = \"state_fips\")\n) |&gt;\n  filter(!is.na(PCTUI_PT)) |&gt; \n  mutate(expanded = implementation_date &lt; \"2015-01-01\") %&gt;%\n  mutate(expanded = replace_na(expanded, FALSE))\n\nWe use GMM to cluster the data.\n\nuni &lt;- select(sahie, PCTUI_PT)\n\nset.seed(1)\nuni_mc &lt;- Mclust(uni, G = 2)\n\nglance(uni_mc)\n\n# A tibble: 1 × 7\n  model     G     BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 V         2 -18542. -9251.     5     NA  3141\n\n\nWhen we compare .class to expansion, we see that the model does an good job of labeling counties’ expansion status without observing counties’ expansion status.\n\nbind_cols(\n  sahie,\n  augment(uni_mc)\n) |&gt;\n  count(expanded, .class)\n\nNew names:\n• `PCTUI_PT` -&gt; `PCTUI_PT...5`\n• `PCTUI_PT` -&gt; `PCTUI_PT...9`\n\n\n# A tibble: 4 × 3\n  expanded .class     n\n  &lt;lgl&gt;    &lt;fct&gt;  &lt;int&gt;\n1 FALSE    1        465\n2 FALSE    2       1486\n3 TRUE     1       1042\n4 TRUE     2        148\n\n\n\n\n17.6.5 BIC\nLikelihood quantifies how likely observed data are given a set of parameters. If \\(\\theta\\) is a vector of parameters, then \\(L(\\theta |x) = f(x |\\theta)\\) is the likelihood function.\nWe often don’t know the exact number of latent groups in the data. We need a way to compare models with varying numbers of groups. Simply picking the model with the maximum likelihood will lead to models with too many groups.\nThe Bayesian information criterion (BIC) is an alternative to likelihoods that penalizes models for having many parameters. Let \\(L\\) be the likelihood, \\(m\\) the number of free parameters, and \\(n\\) the number of observations.\n\\[\nBIC = -2log(L) + mlog(n)\n\\tag{17.4}\\]\nWe will choose models that minimize BIC. Ideally, we will use v-fold cross validation for this process.\n\n\n17.6.6 Example 5\nThe Mclust() function will try G = 1:9 when G isn’t specified. Mclust() will also try 14 different variance models for the mixture models.\n\n\n\n\n\n\nImportant\n\n\n\nWe want to minimize BIC but library(mclust) is missing a negative sign. So we want to maximize the BIC plotted by library(mclust). You can read more here.\n\n\nWe can plot the BICs with plot() and view the optimal model with glance().\n\nfaithful_gmm &lt;- Mclust(faithful)\n\nplot(faithful_gmm, what = \"BIC\")\n\n\n\n\n\n\n\nglance(faithful_gmm)\n\n# A tibble: 1 × 7\n  model     G    BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 EEE       3 -2314. -1126.    11     NA   272",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#bernoulli-mixture-modeling-bmm",
    "href": "27_advanced-unsupervised-ml.html#bernoulli-mixture-modeling-bmm",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "17.7 Bernoulli Mixture Modeling (BMM)",
    "text": "17.7 Bernoulli Mixture Modeling (BMM)\nLet’s consider a data generation story based on the Bernoulli distribution. Now, each variable, \\(X_1, X_2, ..., X_D\\), is draw from a mixture of \\(K\\) Bernoulli distributions.\n\\[\nX_d  = \\begin{cases}\nBern(p_1) \\text{ with probability }\\pi_1 \\\\\nBern(p_2) \\text{ with probability }\\pi_2 \\\\\n\\vdots \\\\\nBern(p_K) \\text{ with probability }\\pi_K\n\\end{cases}\n\\tag{17.5}\\]\nLet \\(i\\) be an index for each mixture that contributes to the random variable. The probability mass function of the random variable is written as\n\\[\nP(X_d) = \\Pi_{i = 1}^Kp_i^{x_i} (1 - p_i)^{1 - x_i}\n\\tag{17.6}\\]\nLet’s consider a classic example from Bishop (2006) and Murphy (2022). The example uses the MNIST database, which contains 70,000 handwritten digits. The digits are stored in 784 variables, from a 28 by 28 grid, with values ranging from 0 to 255, which indicate the darkness of the pixel.\nTo prepare the data, we divide each pixel by 255 and then turn the pixels into indicators with values under 0.5 as 0 and values over 0.5 as 1. Figure Figure 17.3 visualizes the first four digits after reading in the data and applying pre-processing.\n\nsource(here::here(\"R\", \"visualize_digit.R\"))\n\nmnist &lt;- read_csv(here::here(\"data\", \"mnist_binary.csv\"))\n\nglimpse(dplyr::select(mnist, 1:10))\n\nRows: 60,000\nColumns: 10\n$ label    &lt;dbl&gt; 5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4…\n$ pix_28_1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_5 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_9 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nvisualize_digit(mnist, 1)\nvisualize_digit(mnist, 2)\nvisualize_digit(mnist, 3)\nvisualize_digit(mnist, 4)\n\n\n\n\n\n\n\n\n\n\n\n(a) 5\n\n\n\n\n\n\n\n\n\n\n\n(b) 0\n\n\n\n\n\n\n\n\n\n\n\n(c) 4\n\n\n\n\n\n\n\n\n\n\n\n(d) 1\n\n\n\n\n\n\n\nFigure 17.3: First Four Digits\n\n\n\nThe digits are labelled in the MNIST data set but we will ignore the labels and use Bernoulli Mixture Modeling to learn the latent labels or groups. We will treat each pixel as its own Bernoulli distribution and cluster observations using mixtures of 784 Bernoulli distributions. This means each cluster will contain \\(784\\) parameters.\n\n17.7.1 Two Digit Example\nLet’s start with a simple example using just the digits “1” and “8”. We’ll use library(flexmix) by Leisch (2004). library(flexmix) is powerful but uses different syntax than we are used to.\n\nThe function flexmix() expects a matrix.\nThe formula expects the entire matrix on the left side of the ~.\nWe specify the distribution used during the maximization (M) step with model = FLXMCmvbinary().\n\n\nlibrary(flexmix)\n\nLoading required package: lattice\n\nmnist_18 &lt;- mnist |&gt;\n  filter(label %in% c(\"1\", \"8\")) |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nThe starting assignments are random, so we set a seed.\n\nset.seed(20230612)\nmnist_18_clust &lt;- flexmix(\n  formula = mnist_18 ~ 1, \n  k = 2, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 100)\n)\n\nThe MNIST data are already labelled, so we can compare our assignments to the labels if we convert the “soft assignments” to “hard assignments”. Note that most applications won’t have labels.\n\nmnist |&gt;\n  filter(label %in% c(\"1\", \"8\")) |&gt;  \n  bind_cols(cluster = mnist_18_clust@cluster) |&gt;\n  count(label, cluster)\n\n# A tibble: 4 × 3\n  label cluster     n\n  &lt;dbl&gt;   &lt;int&gt; &lt;int&gt;\n1     1       1   482\n2     1       2  6260\n3     8       1  5610\n4     8       2   241\n\n\nFigure 17.4 shows the estimated \\(p_i\\) for each pixel for each cluster. The figure shows 784 \\(p_i\\) for \\(k = 1\\) and 784 \\(p_i\\) for \\(k = 2\\). We see that the estimated parameters closely resemble the digits.\nOf course, each digit can differ from these images because everyone writes differently. In some ways, these are average digits across many version of the digits.\n\nmeans_18 &lt;- rbind(\n  t(parameters(mnist_18_clust, component = 1)),\n  t(parameters(mnist_18_clust, component = 2))\n) |&gt;\n  as_tibble() |&gt;\n  mutate(label = NA)\n\nvisualize_digit(means_18, 1)\nvisualize_digit(means_18, 2)\n\n\n\n\n\n\n\n\n\n\n\n(a) 8\n\n\n\n\n\n\n\n\n\n\n\n(b) 1\n\n\n\n\n\n\n\nFigure 17.4: Estimated Parameters for Each Cluster\n\n\n\nThe BMM does a good job of labeling the digits and recovering the average shape of the digits.\n\n\n17.7.2 Ten Digit Example\nLet’s now consider an example that uses all 10 digits.\nIn most applications, we won’t know the number of latent variables. First, we sample 1,0002 digits and run the model with \\(k = 2, 3, ..., 12\\). We’ll calculate the BIC for each hyperparameter and pick the \\(k\\) with lowest BIC.\n\nset.seed(20230613)\nmnist_sample &lt;- mnist |&gt;\n  slice_sample(n = 1000) |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nsteps &lt;- stepFlexmix(\n  formula = mnist_sample ~ 1, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 100, minprior = 0),\n  k = 2:12, \n  nrep = 1\n)\n\n\\(k = 7\\) provides the lowest BIC. This is probably because digits like 3 and 8 are very similar.\n\nsteps\n\n\nCall:\nstepFlexmix(formula = mnist_sample ~ 1, model = FLXMCmvbinary(), \n    control = list(iter.max = 100, minprior = 0), k = 2:12, nrep = 1)\n\n   iter converged  k k0    logLik      AIC      BIC      ICL\n2    43      TRUE  2  2 -196191.7 395521.4 403221.6 403227.9\n3    30      TRUE  3  3 -188722.8 382153.6 393706.4 393713.9\n4    32      TRUE  4  4 -182949.0 372176.0 387581.4 387585.1\n5    27      TRUE  5  5 -178955.2 365758.4 385016.4 385019.7\n6    35      TRUE  6  6 -175448.7 360315.5 383426.1 383428.5\n7    37      TRUE  7  7 -171697.0 354381.9 381345.1 381347.8\n8    37      TRUE  8  8 -171282.5 355123.1 385938.8 385941.1\n9    38      TRUE  9  9 -169213.3 352554.6 387223.0 387224.9\n10   25      TRUE 10 10 -165521.6 346741.2 385262.2 385263.7\n11   34      TRUE 11 11 -162919.3 343106.5 385480.1 385481.8\n12   26      TRUE 12 12 -162253.5 343345.0 389571.1 389572.7\n\n\nNext, we run the BMM on the full data with \\(k = 7\\).\n\nmnist_full &lt;- mnist |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nmnist_clust &lt;- flexmix(\n  formula = mnist_full ~ 1, \n  k = 7, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 200, minprior = 0)\n)\n\nThe MNIST data are already labelled, so we can compare our assignments to the labels if we convert the “soft assignments” to “hard assignments”. Note that most applications won’t have labels. The rows of the table are the digits. The columns of the table are the clusters. We can see, for example, that most of the 0’s are clustered in cluster 5.\n\nlabels &lt;- mnist |&gt;\n  bind_cols(cluster = mnist_clust@cluster)\n\ntable(labels$label, labels$cluster)\n\n   \n       1    2    3    4    5    6    7\n  0    5  357  282  289 4875    1  114\n  1   36  288   40   35    0 6319   24\n  2  114  166  652   73   50  163 4740\n  3  263  473 4786   80   41  260  228\n  4 3384 1779    4  139    7   53  476\n  5  325 2315 2367  173  109   59   73\n  6    9   86   41 4365   42  128 1247\n  7 3560 2395   21    0   25  234   30\n  8  257 2582 2369   57   32  445  109\n  9 3739 1797  109    5   26  136  137\n\n\nFigure 17.5 shows the estimated \\(p_i\\) for each pixel for each cluster. The following visualize the \\(784K\\) parameters that we estimated. It shows 784 \\(p_i\\) for \\(k = 1, 2, ..., 7\\) clusters. We see that the estimated parameters closely resemble the digits.\nmeans &lt;- rbind(\n  t(parameters(mnist_clust, component = 1)),\n  t(parameters(mnist_clust, component = 2)),\n  t(parameters(mnist_clust, component = 3)),\n  t(parameters(mnist_clust, component = 4)),\n  t(parameters(mnist_clust, component = 5)),\n  t(parameters(mnist_clust, component = 6)),\n  t(parameters(mnist_clust, component = 7))\n) |&gt;\n  as_tibble() |&gt;\n  mutate(label = NA)\n\nvisualize_digit(means, 1)\nvisualize_digit(means, 2)\nvisualize_digit(means, 3)\nvisualize_digit(means, 4)\nvisualize_digit(means, 5)\nvisualize_digit(means, 6)\nvisualize_digit(means, 7)\n\n\n\n\n\n\n\n\n\n\n\n(a) 3, 7, and 9\n\n\n\n\n\n\n\n\n\n\n\n(b) 5, 7, and 8\n\n\n\n\n\n\n\n\n\n\n\n(c) 3\n\n\n\n\n\n\n\n\n\n\n\n(d) 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) 0\n\n\n\n\n\n\n\n\n\n\n\n(f) 1\n\n\n\n\n\n\n\n\n\n\n\n(g) 2\n\n\n\n\n\n\n\nFigure 17.5: Estimated Parameters for Each Cluster\n\n\n\nThe example with all digits doesn’t result in 10 distinct mixtures but it does a fairly good job of structuring finding structure in the data. Without labels and considering the variety of messy handwriting, this is a useful model.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#considerations",
    "href": "27_advanced-unsupervised-ml.html#considerations",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "17.8 Considerations",
    "text": "17.8 Considerations\nMixture modeling is difficult for a couple of reasons:\n\nWe need to assume a model. It can be difficult to assume a multivariate distribution that fits the data in all dimensions of interest.\nThe models are overparameterized and can take a very long time to fit.\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n\n\nLeisch, Friedrich. 2004. “FlexMix: A General Framework for Finite Mixture Models and Latent Class Regression in R.” Journal of Statistical Software 11 (8). https://doi.org/10.18637/jss.v011.i08.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#footnotes",
    "href": "27_advanced-unsupervised-ml.html#footnotes",
    "title": "17  Mixture Distributions and Mixture Modeling",
    "section": "",
    "text": "library(tidyclust) currently doesn’t support mixture modeling. I hope this will change in the future.↩︎\nThis is solely to save computation time.↩︎",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A\nComparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nAshraf, N., D. Karlan, and W. Yin. 2006. “Tying Odysseus to the\nMast: Evidence From a Commitment Savings Product in the\nPhilippines.” The Quarterly Journal of Economics 121\n(2): 635–72. https://doi.org/10.1162/qjec.2006.121.2.635.\n\n\nBarrientos, Andrés F., Aaron R. Williams, Joshua Snoke, and Claire McKay\nBowen. 2021. “A Feasibility Study of Differentially Private\nSummary Statistics and Regression Analyses with Evaluations on\nAdministrative and Survey Data.” https://doi.org/10.48550/ARXIV.2110.12055.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nBlumenstock, Joshua. n.d. “Calling for Better Measurement:\nEstimating an Individual’s Wealth and Well-Being from\nMobile Phone Transaction Records.” Center for Effective\nGlobal Action. https://escholarship.org/uc/item/8zs63942.\n\n\nBlumenstock, Joshua, Gabriel Cadamuro, and Robert On. 2015.\n“Predicting Poverty and Wealth from Mobile Phone Metadata.”\nScience 350 (6264): 1073–76. https://doi.org/10.1126/science.aac4420.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial Proportion.”\nStatistical Science 16 (2). https://doi.org/10.1214/ss/1009213286.\n\n\nBruce, Anotnio, and Gregory Robinson. 2003. “The Planning\nDatabase: Its Development and Use as an Effective Tool in Census\n2000.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b2edb90180a9132f64f8287af2db92c031b5d40b.\n\n\nBruce, Antonio, Gregory Robinson, and Monique V. Sanders. 2001.\n“Hard-to-Count Scores and Broad Demographic Groups Associated with\nPatterns of Response Rates in Census 2000.” Proceedings of\nthe Social Statistics Section, American Statistical Association.\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical\nInference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.\n\n\nChernick, Michael R., and Robert A. LaBudde. 2011. An Introduction\nto Bootstrap Methods with Applications to r. Hoboken, N.J: Wiley.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien\nNielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and\nCrowd-Out in Retirement Savings Accounts: Evidence from\nDenmark*.” The Quarterly Journal of Economics 129 (3):\n1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nErdman, Chandra, and Nancy Bates. 2014. “The u.s. Census Bureau\nMail Return Rate Challenge: Crowdsourcing to Develop a Hard-to-Count\nScore.” https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/rrs2014-08.pdf.\n\n\n———. 2017. “The Low Response Score (LRS).” Public\nOpinion Quarterly 81 (1): 144–56. https://doi.org/10.1093/poq/nfw040.\n\n\nFellegi, I. P. 1972. “On the Question of Statistical\nConfidentiality.” Journal of the American Statistical\nAssociation 67 (337): 7–18. https://www.jstor.org/stable/2284695?seq=1#metadata_info_tab_contents.\n\n\nGinsberg, Jeremy, Matthew H. Mohebbi, Rajan S. Patel, Lynnette Brammer,\nMark S. Smolinski, and Larry Brilliant. 2009. “Detecting Influenza\nEpidemics Using Search Engine Query Data.” Nature 457\n(7232): 1012–14. https://doi.org/10.1038/nature07634.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. Springer Series in Statistics. New York, NY:\nSpringer.\n\n\nHiggins, James J. 2004a. An Introduction to Modern Nonparametric\nStatistics. Pacific Grove, CA: Brooks/Cole.\n\n\n———. 2004b. An Introduction to Modern Nonparametric Statistics.\nPacific Grove, CA: Brooks/Cole.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2017. An introduction to statistical learning: with applications in\nR. Corrected at 8th printing. Springer texts in statistics. New\nYork Heidelberg Dordrecht London: Springer. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response\nAdjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLeisch, Friedrich. 2004. “FlexMix: A General Framework for Finite\nMixture Models and Latent Class Regression in\nR.” Journal of Statistical\nSoftware 11 (8). https://doi.org/10.18637/jss.v011.i08.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary\nAlignment Methods in Microsimulation Models.” Journal of\nArtificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020.\n“Estimating Confidence Intervals in a Tax Microsimulation\nModel.” International Journal of Microsimulation 13 (2):\n2–20. https://doi.org/10.34196/IJM.00216.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An\nIntroduction. Adaptive Computation and Machine Learning Series.\nCambridge, Massachusetts: The MIT Press.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.”\nThe Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528.\n\n\nPotash, Eric, Joe Brew, Alexander Loewi, Subhabrata Majumdar, Andrew\nReece, Joe Walsh, Eric Rozier, Emile Jorgenson, Raed Mansour, and Rayid\nGhani. 2015. “KDD ’15: The 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining.” In, 2039–47.\nSydney NSW Australia: ACM. https://doi.org/10.1145/2783258.2788629.\n\n\nRavn, Signe, Ashley Barnwell, and Barbara Barbosa Neves. 2020.\n“What Is “Publicly Available Data”?\nExploring Blurred PublicPrivate Boundaries and Ethical\nPractices Through a Case Study on Instagram.” Journal of\nEmpirical Research on Human Research Ethics 15 (1-2): 40–45. https://doi.org/10.1177/1556264619850736.\n\n\nRizzo, Maria L. 2008. Statistical Computing with r. Chapman\n& Hall/CRC Computer Science and Data Analysis Series. Boca Raton:\nChapman & Hall/CRC.\n\n\nSalganik, Matthew J. 2018. Bit by Bit: Social Research in the\nDigital Age. Princeton: Princeton University Press.\n\n\nScott, David W., and Stephan R. Sain. 2005. “Multidimensional\nDensity Estimation.” In, 24:229–61. Elsevier. https://doi.org/10.1016/S0169-7161(04)24009-3.\n\n\nSomepalli, Gowthami, Singla, Micah Goldblum, Jonas Geiping, and Tom\nGoldstein. 2023. “Diffusion Art or Digital Forgery? Investigating\nData Replication in Diffusion Models.” Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 6048–58. https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.”\nJournal of Computational and Graphical Statistics 19 (1): 3–28.\nhttps://doi.org/10.1198/jcgs.2009.07098.\n\n\n———. 2014. “Tidy Data.” https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualie, and Model\nData. 2nd edition. Sebastopol, CA: O’Reilly.\n\n\nZheng, Vivian. 2020. “How Urban Piloted Data Science Techniques to\nCollect Land-Use Reform Data.” https://urban-institute.medium.com/how-urban-piloted-data-science-techniques-to-collect-land-use-reform-data-475409903b88.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html",
    "href": "19_predictive-modeling-motivation.html",
    "title": "13  Predictive Modeling Motivation",
    "section": "",
    "text": "13.1 Problem Structure\nThere are many policy applications where it is useful to make accurate predictions on “unseen” data. Unseen data means any data not used to develop a model for predictions.\nThe broad process of developing models to make predictions of a pre-defined variable is predictive modeling or supervised machine learning. The narrower process of developing a specific model with data is called fitting a model, learning a model, or estimating a model. The pre-defined variable the model is intended to predict is the outcome variable and the remaining variables used to predict the outcome variable are predictors.1\nIn most applications we have modeling data that include the outcome of interest and implementation data that do not include the outcome of interest. Our objective is to fit, or estimate, some \\(\\hat{f}(\\vec{x})\\) using the modeling data to predict the missing outcome in the implementation data.\nOur fitted models never perfectly predict the outcome, \\(y\\). Therefore, we end up with \\(y = \\hat{f}(\\vec{x}) + \\epsilon\\). A major effort in predictive modeling is improving \\(\\hat{f}(\\vec{x})\\) to reduce \\(\\epsilon\\).",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#problem-structure",
    "href": "19_predictive-modeling-motivation.html#problem-structure",
    "title": "13  Predictive Modeling Motivation",
    "section": "",
    "text": "Figure 13.1: Modeling Data and Implementation Data for predicting the presence of lead in homes\n\n\n\n\n\n\n\n\n\nModeling Data\n\n\n\nModeling data contain the outcome variable of interest, and predictors for fitting models to predict the outcome variable of interest.\nModeling data are often historical data or high-cost data.\n\n\n\n\n\n\n\n\nImplementation Data\n\n\n\nImplementation data contain predictors for predicting the outcome of interest but don’t necessarily contain the outcome of interest.\nThe implementation data needs to contain all of the predictors used in the modeling data, and the implementation data can’t be too different from the modeling data.\n\n\n\n\n\n\n\n\nFigure 13.2: Using modeling data to fit a model and make predctions on the implementation data",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#example-marbles",
    "href": "19_predictive-modeling-motivation.html#example-marbles",
    "title": "13  Predictive Modeling Motivation",
    "section": "13.2 Example: Marbles",
    "text": "13.2 Example: Marbles\n\n13.2.1 Implementation Data\n\n\n\nSize\nColor\n\n\n\n\nSmall\n?\n\n\nBig\n?\n\n\nBig\n?\n\n\n\n\n\n13.2.2 Modeling Data\n\n\n\nSize\nColor\n\n\n\n\nSmall\nRed\n\n\nSmall\nRed\n\n\nSmall\nRed\n\n\nSmall\nRed\n\n\nSmall\nBlue\n\n\nBig\nRed\n\n\nBig\nBlue\n\n\nBig\nBlue\n\n\nBig\nBlue\n\n\nBig\nBlue\n\n\n\n\n\n13.2.3 Predictive Model\n\nChallengeSolution\n\n\nHow can we predict color in the implementation data?\n\n\nA simple decision tree works well here! Predictive modeling! Machine learning!\n\nIf a marble is small, then predict Red\nIf a marble is big, then predict Blue\n\nMost predictive modeling is repeating this type of learning in higher-dimensional applications where it is tougher to see the model with a glance.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#applications",
    "href": "19_predictive-modeling-motivation.html#applications",
    "title": "13  Predictive Modeling Motivation",
    "section": "13.3 Applications",
    "text": "13.3 Applications\nPredictive modeling has many applications in public policy. Predictive models can be used to improve the allocation of scarce resources, scale survey responses or hand-labelled data sets, and forecast or nowcast.\n\n13.3.1 Targeting Interventions\n\n\n\n\n\n\nTargeting Interventions\n\n\n\nUsing predictive modeling to target the person, household, or place most in need of a policy intervention. Effective targeting can maximize the impact of a limited intervention.\n\n\n\n\n13.3.2 Amplified Asking\n\n\n\n\n\n\nAmplified Asking\n\n\n\nAmplified asking (Salganik 2018) uses predictive modeling trained on a small amount of survey data from one data source and applies it to a larger data source to produce estimates at a scale or geographic granularity not possible with either data set on its own.\n\n\n\n\n13.3.3 Scaled Labeling\n\n\n\n\n\n\nScaled Labeling\n\n\n\nThe process of labeling a subset of data and then using predictive models to label the full data.\nThis is most effective when labeling the data is time consuming or expensive.\n\n\n\n\n13.3.4 Forecasting and Nowcasting\n\n\n\n\n\n\nForecasting and Nowcasting\n\n\n\nA forecast attempts to predict the most-likely future. A nowcast attempts to predict the most-likely present in situations where information is collected and reported with a lag.\n\n\n\n\n13.3.5 Imputation\nBroadly, imputation is the task of using models to assign, or impute, missing values in some dataset. Broadly, imputation can be used for a variety of applications including trying to reduce the consequences of omitted variable bias, generating synthetic data to minimize disclosure risks, and data fusion. Amplified asking and scaled labeling are also examples of imputation tasks.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#case-studies",
    "href": "19_predictive-modeling-motivation.html#case-studies",
    "title": "13  Predictive Modeling Motivation",
    "section": "13.4 Case Studies",
    "text": "13.4 Case Studies\n\n13.4.1 Preventing Lead Poisoning\nConsider “Predictive Modeling for Public Health: Preventing Childhood Lead Poisoning” by Potash et al. (2015).\nThe authors use historical data in Chicago from home lead inspectors and observable characteristics about homes and neighborhoods to predict if homes contain lead.\n\nMotivation: Lead poisoning is terrible for children, but Chicago has limited lead inspectors.\nImplementation data: Observed characteristics about homes and neighborhoods in Chicago.\nModeling data: Historical characteristics about homes and neighborhoods in Chicago and results from home lead inspections.\nObjective: Predict the probability of a home containing lead. 1. Prioritize sending inspectors to the homes of the most at-risk children before the child suffers from elevated blood lead level (BLL). 2. Create a risk score for children that can be used by parents and health providers.\nTools: Cross validation and regularized logistic regression, linear SVC, and Random Forests.\nResults: The precision of the model was two to five times better than the baseline scenario of randomly sending inspectors. The advantage faded over time and in situations where more inspectors were available.\n\n\n\n13.4.2 Measuring Wealth in Rwanda\nConsider “Calling for Better Measurement: Estimating an Individual’s Wealth and Well-Being from Mobile Phone Transaction Records” by Blumenstock (n.d.) and “Predicting poverty and wealth from mobile phone metadata” by Blumenstock, Cadamuro, and On (2015).\n\nMotivation: Measuring wealth and poverty in developing nations is challenging and expensive.\nImplementation data: 2005-2009 phone metadata from about 1.5 million phone customers in Rwanda.\nModeling data: A follow-up phone stratified survey of 856 phone customers with questions about asset ownership, home ownership, and basic welfare indicators linked to the phone metadata.\nObjective: Use the phone metadata to accurately predict variables about asset ownership, home ownership, and basic welfare indicators. One of their outcomes variables is the first principal component of asset variables.\nTools: 5-fold cross-validation, elastic net regression and regularized logistic regression, deterministic finite automation.\nResults: The results in the first paper were poor, but follow-up work resulted in models with AUC as high as 0.88 and \\(R^2\\) as high as 0.46.\n\n\n\n13.4.3 Collecting Land-Use Reform Data\nConsider “How Urban Piloted Data Science Techniques to Collect Land-Use Reform Data” by Zheng (2020).\n\nMotivation: Land-use reform and zoning are poorly understood at the national level. The Urban Institute set out to create a data set from newspaper articles that reflects land-use reforms.\nImplementation data: About 76,000 newspaper articles\nModeling data: 568 hand-labelled newspaper articles\nObjective: Label the nature of zoning reforms on roughly 76,000 newspaper articles.\nTools: Hand coding, natural language processing, cross-validation, and random forests\nResults: The results were not accurate enough to treat as a data set but could be used to target subsequent hand coding and investigation.\n\n\n\n13.4.4 Google Flu Trends\nConsider the high-profile Google Flu Trends model by Ginsberg et al. (2009).\n\nMotivation: The CDC tracks flu-like illnesses and releases data on about a two-week lag. Understanding flu-like illnesses on a one-day lag can improve public health responses.\nImplementation data: Real-time counts of Google search queries.\nModeling data: 2003-2008 CDC data about flu-like illnesses and counts for 50 million of the most common Google search queries in each state.\nObjective: Predict regional-level flu outbreaks on a one-day lag using Google searches to make predictions.\nTools: Logistic regression and the top 45 influenza related illness query terms.\nResults: The in-sample results were very promising. Unfortunately, the model performed poorly when implemented. Google implemented Google Flu Trends but later discontinued the model. The two major issues were:\n\nDrift: The model decayed over time because of changes to search\nAlgorithmic confounding: The model overestimated the 2009 flu season because of fear-induced search traffic during the Swine flu outbreak\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nWhat other policy applications do you know for making accurate predictions on unseen data?\n\n\n\n\n\n\n\nBlumenstock, Joshua. n.d. “Calling for Better Measurement: Estimating an Individual’s Wealth and Well-Being from Mobile Phone Transaction Records.” Center for Effective Global Action. https://escholarship.org/uc/item/8zs63942.\n\n\nBlumenstock, Joshua, Gabriel Cadamuro, and Robert On. 2015. “Predicting Poverty and Wealth from Mobile Phone Metadata.” Science 350 (6264): 1073–76. https://doi.org/10.1126/science.aac4420.\n\n\nGinsberg, Jeremy, Matthew H. Mohebbi, Rajan S. Patel, Lynnette Brammer, Mark S. Smolinski, and Larry Brilliant. 2009. “Detecting Influenza Epidemics Using Search Engine Query Data.” Nature 457 (7232): 1012–14. https://doi.org/10.1038/nature07634.\n\n\nPotash, Eric, Joe Brew, Alexander Loewi, Subhabrata Majumdar, Andrew Reece, Joe Walsh, Eric Rozier, Emile Jorgenson, Raed Mansour, and Rayid Ghani. 2015. “KDD ’15: The 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.” In, 2039–47. Sydney NSW Australia: ACM. https://doi.org/10.1145/2783258.2788629.\n\n\nSalganik, Matthew J. 2018. Bit by Bit: Social Research in the Digital Age. Princeton: Princeton University Press.\n\n\nZheng, Vivian. 2020. “How Urban Piloted Data Science Techniques to Collect Land-Use Reform Data.” https://urban-institute.medium.com/how-urban-piloted-data-science-techniques-to-collect-land-use-reform-data-475409903b88.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#footnotes",
    "href": "19_predictive-modeling-motivation.html#footnotes",
    "title": "13  Predictive Modeling Motivation",
    "section": "",
    "text": "The are many names for outcome variables including target and dependent variable. There are many names for predictors including features and independent variables.↩︎",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html",
    "href": "15_microsimulation.html",
    "title": "10  Microsimulation",
    "section": "",
    "text": "10.1 Motivation\nIt is often important to ask “what would be the policy impact of…” on a population or subpopulation.\nOne common approach is to look at representative units. For example, we could construct one observation that is representative (e.g. median family structure, median income, etc.) and pass its values into a calculator. Then, we could extrapolate this experience to other observations.\nAnother common approach is to look at aggregated data. For example, we could look at county-level insurance coverage in Medicaid expansion and non-expansion states and then extrapolate to other health care expansions.\nOrcutt (1957) suggested a radically different approach. Instead of using a representative unit or aggregated data to project outcomes, model outcomes for individual units and aggregate the results. Potential units-of-analysis include people, households, and firms. Models include anything from simple accounting rules to complex behavioral and demographic models.\nIt took decades for this approach to see wide adoption because of data and computational limitations, but it is now commonplace in policy evaluation.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#calculators",
    "href": "15_microsimulation.html#calculators",
    "title": "10  Microsimulation",
    "section": "10.2 Calculators",
    "text": "10.2 Calculators\nWe’ll first look at calculators, which are an important tool for representative unit methods and microsimulation.\n\nThe Tax Policy Center’s Marriage Calculator can be used to calculate tax marriage penalties and benefits.\nPolicyEngine contains scores of calculators for taxes and benefits. For example, this calculator evaluates marginal tax rates in California accounting for taxes and benefits.\n\nThese two examples show the value of using calculators to explore potentially harmful marriage disincentives like marriage penalties, benefits cliffs, and extremely high marginal tax rates.\n\n10.2.1 Example 1\nSuppose we are interested in creating a new tax credit that is very simple. Its only parameters are number of children and total family income. It has the following characteristics:\n\nIgnore the first $20,000 of family income\nCreate a maximum benefit of $3,000 for one child, $4,500 for two children, and $6,000 for three or more children\nReduce the benefit by $0.10 for every dollar of income in excess of $20,000.\n\nWe first create an R function that implements this policy proposal.\n\n#' Calculate the benefit from the new tax credit\n#'\n#' @param num_children A numeric for the number of children\n#' @param family_income A numeric for family income\n#'\n#' @return Numeric benefit in dollars\n#' \nnew_tax_credit &lt;- function(num_children, family_income) {\n  \n  modified_income &lt;- pmax(family_income - 20000, 0)\n  \n  benefit &lt;- dplyr::case_when(\n    num_children &gt;= 3 ~ pmax(0, 6000 - 0.1 * modified_income),\n    num_children == 2 ~ pmax(0, 4500 - 0.1 * modified_income),\n    num_children == 1 ~ pmax(0, 3000 - 0.1 * modified_income),\n    TRUE ~ 0\n  )\n\n  return(benefit)\n  \n}\n\nWe can apply the calculator to representative cases:\n\nnew_tax_credit(num_children = 1, family_income = 34000)\n\n[1] 1600\n\nnew_tax_credit(num_children = 4, family_income = 12000)\n\n[1] 6000\n\n\nWe can also apply the calculator to many potential values and generate benefit plots:\n\n\nCode\nexpand_grid(\n  family_income = seq(0, 100000, 100),\n  num_children = 1:3\n) |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = num_children, \n      .y = family_income, \n      .f = new_tax_credit\n    )\n  ) |&gt;\n  ggplot(aes(family_income, benefit, color = factor(num_children))) +\n  geom_line() +\n  scale_x_continuous(labels = scales::label_dollar()) +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  labs(\n    x = \"Family income\",\n    y = \"Benefit\",\n    color = \"Number of children\"\n  )\n\n\n\n\n\n\n\n\nFigure 10.1: Benefits for different income levels and numbers of children\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCreate a new, well-documented function called alternative_tax_credit() based on new_tax_credit() that models the following proposed tax credit:\n\nIgnore the first $20,000 if family income.\nCreate a maximum benefits of $4,000 for one child, $4,500 for two children, and $5,000 for three children.\nReduces the benefit by $.05 for every dollar of income in excess of $20,000.\n\nUse library(ggplot2), expand_grid(), and map2_dbl() to create a chart like the one in the notes for this alternative tax credit.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#microsimulation",
    "href": "15_microsimulation.html#microsimulation",
    "title": "10  Microsimulation",
    "section": "10.3 Microsimulation",
    "text": "10.3 Microsimulation\n\n\n\n\n\n\nMicrosimulation\n\n\n\nMicrosimulation is a tool for projection that starts with individual observations (i.e. people or households) and then aggregates those individuals.\n\n\nMicrosimulation requires many assumptions and significant investment, but is useful for a few reasons:\n\nIt can be used to project heterogeneous outcomes and to look at the distribution of outcomes instead of just typical outcomes.\nIt can be used to evaluate “what-if” situations by comparing baseline projections with counterfactual projections.\n\nMicrosimulation is widely used in government, not-for-profits, and academia:\n\nThe Congressional Budget Office’s Long-Term (CBOLT) model is used for long-term fiscal forecasts.\nThe CBO uses HISIM2 to forecast health insurance coverage and premiums for people under age 65.\nThe Urban-Brookings Tax Policy Center evaluates most major tax proposals with its tax microsimulation model.\n\nMicrosimulation is also widely cited in popular publications around important debates:\n\nThe Urban Institute’s HIPSM was cited in the majority opinion of the Supreme Court case King v. Burwell, which upheld the Patient Protection and Affordable Care Act.\nTPC’s tax model is regularly cited in the New York Times, Wall Street Journal, and Washington Post when TPC evaluates candidates’ tax plans.\n\nMicrosimulation models range from simple calculators applied to representative microdata to very complex dynamic models.\n\n10.3.1 Basic Microsimulation\n\n\n\n\n\n\nAccounting Rules\n\n\n\nAccounting rules are the basic calculations associated with government law and programs like taxes, Social Security benefits, and Medicare.\nAccounting rules are sometimes called arithmetic rules because they are typically limited to addition, subtraction, multiplication, division, and simple if-else logic.\n\n\nLet’s start with a very simple algorithm for microsimulation modeling:\n\nConstruct a sample that represents the population of interest.\nApply accounting rules.\nAggregate.\n\nThe simplest microsimulation models essentially apply tax models similar to TurboTax to a representative set of microdata.\nWe can summarize output from microsimulation models with summary statistics that demonstrate the distribution of outcomes. For example, it is common to look at deciles or key percentiles to understand the heterogeneity of changes.\nWe can construct a baseline simulation by applying current law for step 2. Next, we can construct an alternative or counterfactual simulation by changing step 2 to a proposed policy. Finally, we can difference current law and the counterfactual to estimate the impact of a policy.\n\n\n10.3.2 Example 2\nLet’s consider a simple example where we apply the benefit calculator from earlier to families from the 2022 Annual Social and Economic Supplement to the Current Population Survey.\nTo keep things simple, we only consider families related to the head of household (and ignore other families in the household). Furthermore, we will ignore observations weights.1\n\n\nCode\n# if file doesn't exist in data, then download\nif (!file.exists(here(\"data\", \"cps_microsim.csv\"))) {\n  \n  cps_extract_request &lt;- define_extract_cps(\n    description = \"2018-2019 CPS Data\",\n    samples = \"cps2022_03s\",\n    variables = c(\"YEAR\", \"NCHILD\", \"FTOTVAL\")\n  )\n  \n  submitted_extract &lt;- submit_extract(cps_extract_request)\n  \n  downloadable_extract &lt;- wait_for_extract(submitted_extract)\n  \n  data_files &lt;- download_extract(\n    downloadable_extract,\n    download_dir = here(\"data\")\n  )\n  \n  cps_data &lt;- read_ipums_micro(data_files)\n  \n  cps_data |&gt;\n    filter(PERNUM == 1) |&gt;\n    mutate(\n      FTOTVAL = zap_labels(FTOTVAL),\n      NCHILD = zap_labels(NCHILD)\n    ) |&gt;\n    select(SERIAL, YEAR, ASECWT, NCHILD, FTOTVAL) |&gt;\n    rename_with(tolower) |&gt;\n    write_csv(here(\"data\", \"cps_microsim.csv\"))\n  \n}\n\n\n\nasec &lt;- read_csv(here(\"data\", \"cps_microsim.csv\"))\n\nRows: 59148 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): serial, year, asecwt, nchild, ftotval\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproposal1 &lt;- asec |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    )\n  )\n\narrange(proposal1, desc(benefit))\n\n# A tibble: 59,148 × 6\n   serial  year asecwt nchild ftotval benefit\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1    118  2022   865.      4    1552    6000\n 2    159  2022  1098.      3       1    6000\n 3    405  2022   933.      3   16478    6000\n 4   1914  2022   614.      4    9300    6000\n 5   3269  2022   391.      3    5400    6000\n 6   4632  2022  2624.      3       0    6000\n 7   4692  2022  1746.      3   15500    6000\n 8   4812  2022  1468.      4   12000    6000\n 9   5494  2022   662.      3   18000    6000\n10   5596  2022  1708.      3   10002    6000\n# ℹ 59,138 more rows\n\n\n\n\n10.3.3 Distributional Analysis\nAggregate analysis and representative unit analysis often mask important heterogeneity. The first major advantage of microsimulation is the ability to apply distributional analysis.\n\n\n\n\n\n\nDistributional Analysis\n\n\n\nDistributional analysis is the calculation and interpretation of statistics outside of the mean, median, and total. The objective is to understand a range of outcomes instead of typical outcomes.\n\n\nHere, we expand step 3 from the basic microsimulation algorithm to include a range of statistics. The most common statistics are percentiles or outcomes for ntiles.\n\n\n10.3.4 Example 3\nConsider the previous example. Let’s summarize the mean benefit by family income decile. We can use the ntile() function to construct ntiles(). We can use min() and max() in summarize() to define the bounds of the ntiles.\n\ndistributional_table &lt;- proposal1 |&gt;\n  mutate(ftotval_decile = ntile(ftotval, n = 10)) |&gt;\n  group_by(ftotval_decile) |&gt;\n  summarize(\n    min_income = min(ftotval),\n    max_income = max(ftotval),\n    mean_benefit = mean(benefit)\n  )\n\ndistributional_table\n\n# A tibble: 10 × 4\n   ftotval_decile min_income max_income mean_benefit\n            &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1              1     -19935      14160       1019. \n 2              2      14160      25200        921. \n 3              3      25200      37432        863. \n 4              4      37433      50100        622. \n 5              5      50100      65122        266. \n 6              6      65124      84000         59.6\n 7              7      84000     108006          0  \n 8              8     108010     143210          0  \n 9              9     143221     205154          0  \n10             10     205163    2320191          0  \n\n\n\n\nCode\ndistributional_table |&gt;\n  ggplot(aes(ftotval_decile, mean_benefit)) +\n  geom_col() +\n  geom_text(aes(label = scales::label_dollar()(mean_benefit)), vjust = -1) +\n  scale_x_continuous(breaks = 1:10) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1)),\n    labels = scales::label_dollar() \n  ) +\n  labs(\n    title = \"Distribution of Average Benefits Under the Benefit Proposal\",\n    x = \"Familiy Income Decile\",\n    y = \"Average Benefit\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nApply the calculator from the earlier exercise to the CPS data.\nAggregate outcomes for the 5th, 15th, and 25th percentiles.\nAggregate outcomes for the bottom 10 vintiles (a vintile is one twentieth of the population).\n\n\n\n\n\n10.3.5 Counterfactual Analysis\n\n\n\n\n\n\nCounterfactual\n\n\n\nA counterfactual is a situation that would be true under different circumstances.\n\n\nThe second major advantage of microsimulation is the ability to implement counterfactuals and evaluate “what-if” situations. Consider a few examples:\n\nWhat could happen to the distribution of post-tax income if the top marginal tax rate is increased by 5 percentage points?\nWhat could happen to median Social Security benefits in 2030 if the retirement age is increased by 2 months every year beginning in 2024?\nWhat could happen to total student loan balances if federal student loan interest accumulation is paused for 6 more months.\n\nWe update our microsimulation algorithm to include counterfactual analysis.\n\nConstruct a sample that represents the population of interest.\nApply accounting rules that reflect current circumstances. This is the baseline microsimulation.\nApply accounting rules that reflect counterfactual circumstances. This is the counterfactual microsimulation.\nAggregate results with a focus on the difference between the baseline microsimulation and the counterfactual simulation.\n\n\n\n10.3.6 Example 4\nLet’s pretend the new_tax_credit() is current law. It is our baseline. Suppose a legislator proposes reforms to the law. This is our counterfactual. Here are the proposed changes:\n\nEliminate benefits for families with zero income to promote work.\nEliminate the $20,000 income exclusion to reduce benefits for higher earners.\n\n\n#' Calculate the benefit from the new tax credit\n#'\n#' @param num_children A numeric for the number of children\n#' @param family_income A numeric for family income\n#'\n#' @return Numeric benefit in dollars\nnewer_tax_credit &lt;- function(num_children, family_income) {\n  \n  dplyr::case_when(\n    family_income == 0 ~ 0,\n    num_children &gt;= 3 ~ pmax(0, 6000 - 0.1 * family_income),\n    num_children == 2 ~ pmax(0, 4500 - 0.1 * family_income),\n    num_children == 1 ~ pmax(0, 3000 - 0.1 * family_income),\n    TRUE ~ 0\n  )\n\n}\n\n\nproposal2 &lt;- asec |&gt;\n  mutate(\n    benefit_baseline = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    ),\n    benefit_counteractual = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = newer_tax_credit\n    )\n  )\n\nproposal2 |&gt;\n  mutate(benefit_change = benefit_counteractual - benefit_baseline) |&gt;\n    mutate(ftotval_decile = ntile(ftotval, n = 10)) |&gt;\n  group_by(ftotval_decile) |&gt;\n  summarize(\n    min_income = min(ftotval),\n    max_income = max(ftotval),\n    mean_benefit = mean(benefit_change)\n  )\n\n# A tibble: 10 × 4\n   ftotval_decile min_income max_income mean_benefit\n            &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1              1     -19935      14160       -374. \n 2              2      14160      25200       -436. \n 3              3      25200      37432       -557. \n 4              4      37433      50100       -467. \n 5              5      50100      65122       -242. \n 6              6      65124      84000        -59.6\n 7              7      84000     108006          0  \n 8              8     108010     143210          0  \n 9              9     143221     205154          0  \n10             10     205163    2320191          0  \n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nRun the baseline and counterfactual simulations.\nCreate a bar chart representing the change in benefits.\n\n\n\n\n\n10.3.7 Extrapolation\n\n\n\n\n\n\nExtrapolation\n\n\n\nExtrapolation is the extension of microsimulation models to unobserved time periods. Most often, microsimulation models are extrapolated into the future.\n\n\nMost microsimulation models incorporate time. For example, many models look at a few or many years. We now add a new step 2 to the microsimulation algorithm, where we we can project demographic and economic outcomes into the future or past before applying accounting rules and aggregating the results.\n\nConstruct a sample that represents the population of interest.\nExtrapolate the population of interest over multiple time periods.\nApply accounting rules.\nAggregate.\n\nSome microsimulation models treat time as continuous. More often, microsimulation treats time as discrete. For example, models represent time every month or every year.\nExtrapolation adds uncertainty and assumptions to microsimulation. It is important to be clear about what microsimulation is and isn’t.\n\n\n\n\n\n\nProjection\n\n\n\nA projection explores what could happen under a given set of assumptions. It is always correct under the assumptions.\nStats Canada\n\n\n\n\n\n\n\n\nForecast\n\n\n\nA forecast attempts to predict the most-likely future.\nStats Canada\n\n\nMost microsimulation models are projections, not forecasts.\n\n\n10.3.8 Transitions\n\n\n\n\n\n\nStatic Microsimulation\n\n\n\nStatic microsimulation models do not subject units to individual transitions between time periods \\(t - 1\\) and \\(t\\) or to behavioral responses.\n\n\nStatic models typically only deal with one time period or they deal with multiple time periods but reweight the data to match expected totals and characteristics over time. Basically, individual decisions affect the distribution of outcomes but have little impact on overall outcomes.\n\n\n\n\n\n\nDynamic Microsimulation\n\n\n\nDynamic microsimulation models subject individual units to transitions between time periods \\(t - 1\\) and \\(t\\). This is sometimes referred to as “aging” the population. Dynamic microsimulation models sometimes subject individual units to behavioral responses.\n\n\nTransitions from period \\(t - 1\\) to period \\(t\\) are key to dynamic microsimulation. Transitions can be deterministic or stochastic. An example of a deterministic transition is an individual always joining Medicare at age 65. An example of a stochastic transition is an unmarried individual marrying at age 30 with probability \\(p_1\\) and remaining unmarried with probability \\(p_2\\). Stochastic transitions are connected to the idea of Monte Carlo simulation.\nTransition models are fundamental to stochastic transitions. Transition models include transition probability models (categorical variables) and processes based on probability distributions (continuous variables).\nHere are are few examples of transitions that could be modeled:\n\nIf an individual will acquire more education.\nIf an individual will work.\nIf an individual will marry, divorce, or widow.\nIf an individual will have children.\nIf an individual will retire.\nIf an individual will die.\n\nTransition models are often taken from existing literature or estimated on panel data. The data used to estimate these models must have at least two time periods.\nIt is common to extrapolate backwards, which is also known as backcasting, to evaluate microsimulation models against history. Backcasting can add important longitudinal information to records like detailed earnings histories for calculating Social Security benefits. Backcasting also offers opportunities to benchmark model projections against observed history.\n\n\n10.3.9 Example 5\nLet’s extrapolate our 2022 CPS data to 2023 and 2024 using transition models for number of children and family total income.\nThe transition model for number of children is very simple. 15% of families lose a child, 80% of families observe no change, and 5% of families gain one child.\n\n#' Extrapolate the number of children\n#'\n#' @param num_children A numeric for the number of children in time t\n#'\n#' @return A numeric for the number in time t + 1\n#'\nchildren_hazard &lt;- function(num_children) {\n  \n  change &lt;- sample(x = c(-1, 0, 1), size = 1, prob = c(0.15, 0.8, 0.05))\n  \n  pmax(num_children + change, 0)\n  \n}\n\nThe transition model for family income is very simple. The proportion change is drawn from a normal distribution with \\(\\mu = 0.02\\) and \\(\\sigma = 0.03\\).\n\n#' Extrapolate family income\n#'\n#' @param num_children A numeric for family income in time t\n#'\n#' @return A numeric for family income in time t + 1\n#'\nincome_hazard &lt;- function(family_income) {\n  \n  change &lt;- rnorm(n = 1, mean = 0.02, sd = 0.03)\n  \n  family_income + family_income * change\n  \n}\n\nFor simplicity, we combine both transition models into one function.\n\n#' Extrapolate the simple CPS\n#'\n#' @param data A tibble with nchild and ftotval\n#'\n#' @return A data frame in time t + 1\n#'\nextrapolate &lt;- function(data) {\n  \n  data |&gt;\n    mutate(\n      nchild = map_dbl(.x = nchild, .f = children_hazard),\n      ftotval = map_dbl(.x = ftotval, .f = income_hazard),\n      year = year + 1\n    )\n\n}\n\nFinally, we extrapolate.\n\n# make the stochastic results reproducible\nset.seed(20230812)\n\n# extrapolate using t to create t + 1\nasec2023 &lt;- extrapolate(asec)\nasec2024 &lt;- extrapolate(asec2023)\n\n# combine\nasec_extrapolated &lt;- bind_rows(\n  asec,\n  asec2023,\n  asec2024\n)\n\nThe benefit and income amounts in new_tax_credit() are not indexed for inflation. Let’s see how benefits change over time with the extrapolated data.\n\nasec_extrapolated |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    )\n  ) |&gt;\n  group_by(year) |&gt;\n  summarize(\n    mean_ftotval = mean(ftotval),\n    total_nchild = sum(nchild),\n    mean_benefit = mean(benefit)\n  )\n\n# A tibble: 3 × 4\n   year mean_ftotval total_nchild mean_benefit\n  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1  2022       97768.        46296         375.\n2  2023       99705.        45615         379.\n3  2024      101700.        44848         379.\n\n\nEven though incomes grew and the number of children declined, it turns out that enough families went from zero children to one child under the transition probability model for children that average benefits remained about the same.\n\n\n10.3.10 Beyond Accounting Rules\n\n\n\n\n\n\nBehavioral Responses\n\n\n\nA behavioral response is an assumed reaction to changing circumstances in a microsimulation model.\nFor example, increasing Social Security benefits may crowd out retirement savings (Chetty et al. 2014). In other words, the existence of Social Security may induce a person to save less for retirement.\n\n\nUntil now, we’ve only considered accounting rules within each time period. Accounting rules are appealing because they don’t require major assumptions, but they are typically insufficient. It is often necessary to consider how units will respond to a changing environment.\nBehavioral responses are often elasticities estimated with econometric analysis. Behavioral responses are also a huge source of assumptions and uncertainty for microsimulation.\nConsider a microsimulation model that models retirement savings. When modeling a proposal to expand Social Security benefits, an analyst may use parameters estimated in (Chetty et al. 2014) to model reductions in savings in accounts like 401(k) accounts. These are behavioral responses. These estimates are uncertain, so the analyst could add sensitivity analysis to model low, medium, and high rates of crowding out.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#building-a-representative-population",
    "href": "15_microsimulation.html#building-a-representative-population",
    "title": "10  Microsimulation",
    "section": "10.4 Building a Representative Population",
    "text": "10.4 Building a Representative Population\nBuilding a starting sample and constructing data for estimating transition models is difficult. We briefly outline a few techniques of interest.\n\n10.4.1 Synthetic Starting Data\nWe’ve adopted a cross-sectional starting population. Some microsimulation models adopt a synthetic2 approach where every observation is simulated from birth.\n\n\n10.4.2 Data Linkage and Imputation\nMany microsimulation models need more variables than are included in any one source of information. For example, retirement models often need demographic information, longitudinal earnings information, and tax information. Many microsimulation models rely on data linkage and data imputation techniques to augment their starting data.\n\n\n\n\n\n\nData Linkage\n\n\n\nData linkage is the process of using distance-based rules or probabilistic models to connect an observation in one source of data to an observation in another source of data.\n\n\n\n\n\n\n\n\nData imputation\n\n\n\nData imputation is the process of using models to predict values where data is unobserved. The data could be missing because of nonresponse or because the information was not gathered in the data collection process.\n\n\n\n\n10.4.3 Validation\n\n\n\n\n\n\nValidation\n\n\n\nValidation is the process of reviewing results to determine their validity. Techniques include comparing statistics, visual comparisons, and statistical comparisons like the Kolmogorov-Smirnov test for the equivalence of two distributions.\n\n\nVisual and statistical validation are essential to evaluating the quality of a microsimulation model. If validation looks poor, then a modeler can redo other parts of the microsimulation workflow or they can reweight or align the data.\n\n\n10.4.4 Reweighting\n\n\n\n\n\n\nReweighting\n\n\n\nReweighting is the process of adjusting observation weights in a data set so aggregate weighted statistics from the data set hit specified targets.\n\n\nSuppose a well-regarded source of information says mean income is $50,000 but a microsimulation model estimates mean income of $45,000. We can use reweighting to plausibly adjust the weights in the microsimulation model so mean income is $50,000.\nTechniques include post-stratification and calibration. Kolenikov (2016) offers a good introduction.\n\n\n10.4.5 Alignment\n\n\n\n\n\n\nAlignment\n\n\n\nAlignment is the process of adjusting model coefficients or predicted values so aggregated outputs align with specified aggregate targets. These targets are aggregate outcomes like total income or state-level tax revenue.\n\n\nSuppose a well-regarded source of information says mean income is $50,000 but a microsimulation model estimates mean income of $45,000. We can adjust the intercept in linear regression models or adjust predicted values so mean income is $50,000. Li and O’Donoghue (2014) offers a good introduction.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#assumptions",
    "href": "15_microsimulation.html#assumptions",
    "title": "10  Microsimulation",
    "section": "10.5 Assumptions",
    "text": "10.5 Assumptions\nMicrosimulation is only as useful as its assumptions. We will review a few key assumptions present in many microsimulation models.\n\n10.5.1 Open vs. Closed\n\n\n\n\n\n\nClosed Model\n\n\n\nA closed microsimulation model models the life cycle of all units in the model. For example, two existing units marry.\n\n\n\n\n\n\n\n\nOpen Model\n\n\n\nAn open microsimulation model allows for the on-demand generation of new, but mature, units in the model.\n\n\n\n\n10.5.2 Independence\nAnother important assumption deals with the relationship between units in the model. Should observations be treated as wholly independent or do they interact? For example, if someone takes a job is it more difficult for another individual to take the job?\nInteractions can be explicit or implicit.\n\n\n\n\n\n\nExplicit Interaction\n\n\n\nExplicit interaction allows the actions of one unit to affect other units during extrapolation or behavioral models.\nFor example, models of marriage markets may account for changing economic circumstances among potential matches.\n\n\n\n\n\n\n\n\nImplicit Interaction\n\n\n\nImplicit interaction allows the actions of one unit to affect other units in post-processing.\nFor example, reweighting and alignment techniques allow outcomes for one unit to affect other units through intercepts in models and new weights.\n\n\n\n\n10.5.3 Markov Assumption\n\n\n\n\n\n\nMarkov Assumption\n\n\n\nThe Markov assumptions states that the only factors affecting a transition from period \\(t - 1\\) to period \\(t\\) are observable in \\(t - 1\\).\nThe Markov assumption only considers memory or history to the extent that it is observable in period \\(t - 1\\). For example, \\(t - 2\\) may affect educational attainment or college savings in \\(t - 1\\), which can affect the transition from \\(t - 1\\) to \\(t\\), but \\(t - 2\\) will never be explicitly included.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#uncertainty",
    "href": "15_microsimulation.html#uncertainty",
    "title": "10  Microsimulation",
    "section": "10.6 Uncertainty",
    "text": "10.6 Uncertainty\n\n\n\n\n\n\nAletoric Uncertainty\n\n\n\nAleatoric uncertainty is uncertainty due to probabilistic randomness.\n\n\n\n\n\n\n\n\nEpistemic Uncertainty\n\n\n\nEpistemic uncertainty is uncertainty due to lack of knowledge of the underlying system.\n\n\nThe microsimulation field has a poor track record of quantifying uncertainty. Most estimates do not contain standard errors or even crude distributions of outcomes.\nMicrosimulation models have many sources of uncertainty. The starting data and data used for model estimation are often samples with sampling error. Transition models fail to capture all sources of variation. The models often intentionally include Monte Carlo error.\nMcClelland, Khitatrakun, and Lu (2020) explored adding confidence intervals to microsimulation models using normal approximations and bootstrapping methods3. They find that normal approximations work well in most cases unless policy changes affect a small number of returns.\nAcross microsimulations for five policy approaches, they estimate modest confidence intervals. This makes sense. First, microsimulation aggregates many units that have their own sources of uncertainty. This is different than earlier examples of Monte Carlo simulation that focused on one observation at a time. Aggregation reduces variance.4\nSecond, the authors only consider uncertainty because of sampling variation. This fails to capture aleatoric uncertainty from statistical matching, imputation, and projection. Furthermore, these methods fail to capture epistemic uncertainty. The confidence intervals, in other words, assume the model is correct.\nGiven these shortcomings, it is important to be clear about assumptions, transparent about implementations, and humble about conclusions. All useful models are wrong. The hope is to be as little wrong as possible.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#microsimulation-model-examples",
    "href": "15_microsimulation.html#microsimulation-model-examples",
    "title": "10  Microsimulation",
    "section": "10.7 Microsimulation Model Examples",
    "text": "10.7 Microsimulation Model Examples\nFirst, let’s outline a few characteristics of microsimulation models.\n\n\n\n\n\n\nCompiled Programming Languages\n\n\n\nCompiled programming languages have an explicit compiling step where code is converted to assembly language and ultimately binary code.\nC++, Fortran, and Java are examples of compiled programming languages.\n\n\n\n\n\n\n\n\nScripting Programming Languages\n\n\n\nScripting programming languages, also known as interpreted programming languages, do not have a compiling step.\nR, Python, and Julia are examples of scripting programming languages.\n\n\nAruoba and Fernndez-Villaverde (2018) benchmark several programming languages on the same computing task. Lower-level, compiled programming languages dominate higher-level, scripting programming languages like R and Python. Julia is the lone bright spot that blends usability and performance.\nMany microsimulation models are written in lower-level, compiled programming languages. Fortran may seem old, but there is a reason it has stuck around for microsimulation.\n\n\n\n\n\n\nGeneral Microsimulation Models\n\n\n\nGeneral microsimulation models contain a wide range of behaviors and population segments.\n\n\n\n\n\n\n\n\nSpecialized Microsimulation Models\n\n\n\nSpecialized microsimulation models focus on a limit set of behaviors or population segments. ~ Stats Canada\n\n\n\n10.7.1 Simulating the 2020 Census\n\nName: Simulating the 2020 Census\nAuthors: Diana Elliott, Steven Martin, Jessica Shakesprere, Jessica Kelly\nGeneral or specific: Specific\nLanguage: Python\nPurpose: The authors simulate various Decennial Census response factors and evaluate the distribution of responses to the Decennial Census.\n\n\n\n10.7.2 TPC Microsimulation in the Cloud\n\nName: TPC Microsimulation in the Cloud\nAuthors: The TPC microsimulation team, Jessica Kelly, Kyle Ueyama, Alyssa Harris\nGeneral or specific: Specific\nLanguage: Fortran\nPurpose: The authors take TPC’s tax microsimulation model and move it to the cloud. This allows TPC to reverse the typical microsimulation process. Instead of describing policies and observing the outcomes, they can describe desirable outcomes and then use grid search to back out policies that achieve those outcomes.\n\n\n\n10.7.3 Modeling Income in the Near Term (MINT)5\n\nName: Modeling Income in the Near Term (MINT)\nAuthors: Karen E. Smith and many other people\nGeneral or specific: General\nLanguage: SAS6\nPurpose: The Social Security Administration uses MINT to evaluate the distributional impact of various Social Security policy proposals.\n\n\n\n\n\nAruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A Comparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien Nielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and Crowd-Out in Retirement Savings Accounts: Evidence from Denmark*.” The Quarterly Journal of Economics 129 (3): 1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response Adjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary Alignment Methods in Microsimulation Models.” Journal of Artificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020. “Estimating Confidence Intervals in a Tax Microsimulation Model.” International Journal of Microsimulation 13 (2): 2–20. https://doi.org/10.34196/IJM.00216.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.” The Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#footnotes",
    "href": "15_microsimulation.html#footnotes",
    "title": "10  Microsimulation",
    "section": "",
    "text": "Most data for microsimulation models are collected through complex surveys. Accordingly, most microsimulation models need to account for weights to calculate estimates that represent the entire population of interest.↩︎\nSynthetic will carry many meanings this semester.↩︎\nTheir implementation of bootstrapping is clever and uses replicate weights to simplify computation and manage memory.↩︎\nThe standard error of the mean reduces at a rate of \\(\\sqrt{n}\\) as the sample size increases.↩︎\nMINT comically models income and many other variables for at least 75 years into the future.↩︎\nThe Dynamic Simulation of Income Model (DYNASIM) is a related to MINT and is written in Fortran.↩︎",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html",
    "href": "20_predictive-modeling-concepts.html",
    "title": "14  Predictive Modeling Concepts",
    "section": "",
    "text": "14.1 Regression\nPredictive modeling is split into two approaches: regression and classification.\nThis chapter will focus on regression. A later chapter focuses on classification.\nWith regression, our general goal is to fit \\(\\hat{f}(\\vec{x})\\) using modeling data that will minimize predictive errors on unseen data. There are many algorithms for fitting \\(\\hat{f}(\\vec{x})\\). For regression, most of these algorithms fit conditional means; however, some use conditional quantiles like the conditional median.\nFamilies of predictive models:",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Predictive Modeling Concepts</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#regression",
    "href": "20_predictive-modeling-concepts.html#regression",
    "title": "14  Predictive Modeling Concepts",
    "section": "",
    "text": "Regression\n\n\n\nPredictive modeling with a numeric outcome.\nNote: Regression and ordinary least squares linear regression (which we often informally refer to simply as “linear regression”) are different ideas. We will use many algorithms for regression applications that are not linear regression.\n\n\n\n\n\n\n\n\nClassification\n\n\n\nPredictive modeling with a categorical outcome. The output of these models can be predicted classes of a categorical variable or predicted probabilities (e.g. 0.75 for “A” and 0.25 for “B”).\n\n\n\n\n\n\nlinear\ntrees\nnaive\nkernel\nNN\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nName all of the algorithms you know for each family of predictive model.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Predictive Modeling Concepts</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#regression-algorithms",
    "href": "20_predictive-modeling-concepts.html#regression-algorithms",
    "title": "14  Predictive Modeling Concepts",
    "section": "14.2 Regression Algorithms",
    "text": "14.2 Regression Algorithms\nLet’s explore a few algorithms for generating predictions in regression applications. Consider some simulated data with 1,000 observations, one predictor, and one outcome.\n\nlibrary(tidymodels)\n\nset.seed(20201004)\n\nx &lt;- runif(n = 1000, min = 0, max = 10)\n\ndata1 &lt;- bind_cols(\n  x = x,\n  y = 10 * sin(x) + x + 20 + rnorm(n = length(x), mean = 0, sd = 2)\n)\n\nFor reasons explained later, we split the data into a training data set with 750 observations and a testing data set with 250 observations.\n\nset.seed(20201007)\n\n# create a split object\ndata1_split &lt;- initial_split(data = data1, prop = 0.75)\n\n# create the training and testing data\ndata1_train &lt;- training(x = data1_split)\ndata1_test  &lt;- testing(x = data1_split)\n\nFigure 14.1 visualizes the training data.\n\n# visualize the data\ndata1_train |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = 0.25) +\n  labs(title = \"Example 1 Data\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 14.1: Simulated training data\n\n\n\n\n\n\n14.2.1 Linear Regression\n\nFind the line that minimizes the sum of squared errors between the line and the observed data.\n\nNote: Linear regression is linear in its coefficients but can fit non-linear patterns in the data with the inclusion of higher order terms (for example, \\(x^2\\)).\nFigure 14.2 shows four different linear regression models fit to the training data. Degree is the magnitude of the highest order term included in the model.\nFor example, “Degree = 1” means \\(\\hat{y}_i = b_0 + b_1x_i\\) and “Degree = 3” means \\(\\hat{y}_i = b_0 + b_1x_i + b_2x_i^2 + b_3x_i^3\\).\n\n\nCode\nlin_reg_model1 &lt;- linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nlin_reg_model2 &lt;- linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ poly(x, degree = 2, raw = TRUE), data = data1_train)\n\nlin_reg_model3 &lt;- linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ poly(x, degree = 3, raw = TRUE), data = data1_train)\n\nlin_reg_model4 &lt;- linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ poly(x, degree = 4, raw = TRUE), data = data1_train)\n\n# create a grid of predictions\nnew_data &lt;- tibble(x = seq(0, 10, 0.1))\n\npredictions_grid &lt;- tibble(\n  x = seq(0, 10, 0.1),\n  `Degree = 1` = predict(object = lin_reg_model1, new_data = new_data)$.pred,\n  `Degree = 2` = predict(object = lin_reg_model2, new_data = new_data)$.pred,\n  `Degree = 3` = predict(object = lin_reg_model3, new_data = new_data)$.pred,\n  `Degree = 4` = predict(object = lin_reg_model4, new_data = new_data)$.pred\n) |&gt;\n  pivot_longer(-x, names_to = \"model\", values_to = \".pred\")\n\nggplot() +\n  geom_point(data = data1_train, aes(x = x, y = y), alpha = 0.25) +\n  geom_path(data = predictions_grid, aes(x = x, y = .pred), color = \"red\") +\n  facet_wrap(~model) +\n  labs(\n    title = \"Example 1: Data with Predictions (Linear Regression)\",\n    subtitle = \"Prediction in red\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 14.2: Fitting the non-linear pattern in the data requires higher order terms\n\n\n\n\n\n\n\n14.2.2 KNN\n\nFind the \\(k\\) closest observations using only the predictors. Closeness is typically measured with Euclidean distance.\nTake the mean of the outcome variables for the \\(k\\) closest observations.\n\n\n\nCode\n# create a knn model specification\nknn_mod1 &lt;- \n  nearest_neighbor(neighbors = 1) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nknn_mod5 &lt;- \n  nearest_neighbor(neighbors = 5) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nknn_mod50 &lt;- \n  nearest_neighbor(neighbors = 50) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nknn_mod500 &lt;- \n  nearest_neighbor(neighbors = 500) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\n# create a grid of predictions\nnew_data &lt;- tibble(x = seq(0, 10, 0.1))\n\npredictions_grid &lt;- tibble(\n  x = seq(0, 10, 0.1),\n  `KNN with k=1` = predict(object = knn_mod1, new_data = new_data)$.pred,\n  `KNN with k=5` = predict(object = knn_mod5, new_data = new_data)$.pred,\n  `KNN with k=50` = predict(object = knn_mod50, new_data = new_data)$.pred,\n  `KNN with k=500` = predict(object = knn_mod500, new_data = new_data)$.pred\n) |&gt;\n  pivot_longer(-x, names_to = \"model\", values_to = \".pred\")\n\n# visualize the data\nggplot() +\n  geom_point(data = data1_train, aes(x = x, y = y), alpha = 0.25) +\n  geom_path(data = predictions_grid, aes(x = x, y = .pred), color = \"red\") +\n  facet_wrap(~model) +\n  labs(\n    title = \"Example 1: Data with Predictions (KNN)\",\n    subtitle = \"Prediction in red\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nChanging k leads to models that under and over fit to the data\n\n\n\n\n\n\n14.2.3 Regression Trees\n\nConsider all binary splits of all predictors to split the data into two groups. Choose the split that results in groups with the lowest MSE for the outcome variable within each group.\nRepeat step 1 recursively until a stopping parameter is reached (cost complexity, minimum group size, tree depth).\n\nDecision Tree: The Obama-Clinton Divide from the New York Times is a clear example of a regression tree.\n\n\nCode\nreg_tree_mod1 &lt;- \n  decision_tree(cost_complexity = 0.1) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nreg_tree_mod2 &lt;- \n  decision_tree(cost_complexity = 0.01) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nreg_tree_mod3 &lt;- \n  decision_tree(cost_complexity = 0.001) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nreg_tree_mod4 &lt;- \n  decision_tree(cost_complexity = 0.0001) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\n# create a grid of predictions\nnew_data &lt;- tibble(x = seq(0, 10, 0.1))\n\npredictions_grid &lt;- tibble(\n  x = seq(0, 10, 0.1),\n  `Regression Tree with cp=0.1` = predict(object = reg_tree_mod1, new_data = new_data)$.pred,\n  `Regression Tree with cp=0.01` = predict(object = reg_tree_mod2, new_data = new_data)$.pred,\n  `Regression Tree with cp=0.001` = predict(object = reg_tree_mod3, new_data = new_data)$.pred,\n  `Regression Tree with cp=0.0001` = predict(object = reg_tree_mod4, new_data = new_data)$.pred\n) |&gt;\n  pivot_longer(-x, names_to = \"model\", values_to = \".pred\")\n\n# visualize the data\nggplot() +\n  geom_point(data = data1_train, aes(x = x, y = y), alpha = 0.25) +\n  geom_path(data = predictions_grid, aes(x = x, y = .pred), color = \"red\") +\n  facet_wrap(~model) +\n  labs(\n    title = \"Example 1: Data with Predictions (Regression Trees)\",\n    subtitle = \"Prediction in red\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 14.3: Changing the cost complexity parameter leads to models that are under of over fit to the data\n\n\n\n\n\nRegression trees generate a series of binary splits that can be visualized. Consider the simple tree from Figure 14.3 where cp=0.1.\n\n\nCode\nlibrary(rpart.plot)\n\nrpart.plot(reg_tree_mod1$fit, roundint = FALSE)\n\n\n\n\n\n\n\n\n\nLinear regression is a parameteric approach. It requires making assumptions about the functional form of the data and reducing the model to a finite number of parameters.\nKNN and regression trees are nonparametric approaches to predictive modeling because they do not require an assumption about the functional form of the model. The data-driven approach of nonparametric models is appealing because it requires fewer assumptions, but it often requires more data and care to not overfit the modeling data.\nRegression trees are the simplest tree-based algorithms. Other tree-based algorithms, like gradient-boosted trees and random forests, often have far better performance than regression trees.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nUse library(tidymodels) to fit a KNN model to the data1_train. Pick any plausible value of \\(k\\).\nMake predictions on the testing data.\nEvaluate the model using rmse().",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Predictive Modeling Concepts</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#error",
    "href": "20_predictive-modeling-concepts.html#error",
    "title": "14  Predictive Modeling Concepts",
    "section": "14.3 Error",
    "text": "14.3 Error\nDifferent applications call for different error metrics. Root mean squared error (RMSE) and mean squared error (MSE) are popular metrics for regression.\n\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}\n\\tag{14.1}\\]\n\\[\nMSE = \\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\n\\tag{14.2}\\]\nLet’s consider the MSE for a single observation \\((x_0, y_0)\\) and think about the model that generates \\(\\hat{y_0}\\), which we will denote \\(\\hat{f}(x_0)\\).\n\\[\nMSE = (y_0 - \\hat{f}(x_0))^2\n\\tag{14.3}\\]\nAssuming \\(y_i\\) independent and \\(y_i - \\hat{y}_i \\sim N(0, \\sigma^2)\\), we can decompose the expected value of MSE into three types of error: irreducible error, bias error, and variance error. Understanding the types of error will inform modeling decisions.\n\\[\nE[MSE] = E[y_0 - \\hat{f}(x_0)]^2 = Var(\\epsilon) + [Bias(\\hat{f}(x_0))]^2 + Var(\\hat{f}(x_0))\n\\tag{14.4}\\]\n\\[\nE[MSE] = E[y_0 - \\hat{f}(x_0)]^2 = \\sigma^2 + (\\text{model bias})^2 + \\text{model variance}\n\\tag{14.5}\\]\n\n\n\n\n\n\nIrreducible error (\\(\\sigma^2\\))\n\n\n\nError that can’t be reduced regardless of model quality. This is often caused by factors that affect the outcome of interest that aren’t measured or included in the data set.\n\n\n\n\n\n\n\n\nBias error\n\n\n\nDifference between the expected prediction of a predictive model and the correct value. Bias error is generally the error that comes from approximating complicated real-world problems with relatively simple models.\n\n\nHere, the model on the left does a poor job fitting the data (high bias) and the model on the right does a decent job fitting the data (low bias).\n\n\nCode\nset.seed(20200225)\n\nsample1 &lt;- tibble(\n  x = runif(100, min = -10, max = 10),\n  noise = rnorm(100, mean = 10, sd = 10),\n  y = -(x ^ 2) + noise\n)\n\ngrid.arrange(\n  sample1 |&gt;\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\",\n                se = FALSE) +\n    theme_void() +\n    labs(title = \"High Bias\") +\n    theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\")),\n  \n  sample1 |&gt;\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(se = FALSE,\n                span = 0.08) +\n    theme_void() +\n    labs(title = \"Low Bias\") +\n    theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\")),\n  ncol = 2\n)\n\n\n\n\n\nBias\n\n\n\n\n\n\n\n\n\n\nVariance error\n\n\n\nHow much the predicted value (\\(\\hat{y}\\)) and fitted model (\\(\\hat{f}\\)) change for a given data point when using a different sample of data to fit the model.\n\n\nHere, the model on the left does not change much as the training data change (low variance) and the model on the right changes a lot when the training data change (high variance).\n\n\nCode\nset.seed(20200226)\n\nsample2 &lt;- tibble(\n  sample_number = rep(c(\"Sample 1\", \"Sample 2\", \"Sample 3\", \"Sample 4\"), 100),\n  x = runif(400, min = -10, max = 10),\n  noise = rnorm(400, mean = 10, sd = 10),\n  y = -(x ^ 2) + noise\n)\n\ngrid.arrange(\n  sample2 |&gt;\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\",\n                se = FALSE,\n                alpha = 0.5) +\n    facet_wrap(~sample_number) +\n    theme_void() +\n    labs(title = \"Low Variance\") +\n    theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\")),\n  \n  sample2 |&gt;\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(se = FALSE,\n                span = 0.08,\n                alpha = 0.5) +\n    facet_wrap(~sample_number) +\n    theme_void() +\n    labs(title = \"High Variance\") +\n    theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\")),\n  ncol = 2\n)\n\n\n\n\n\nVariance\n\n\n\n\nAs can be seen in the error decomposition and plots above, for any given amount of total prediction error, there is a trade-off between bias error and variance error. When one type of error is reduced, the other type of error increases.\nThe bias-variance trade-off can generally be reconceived as\n\nunderfitting-overfitting tradeoff\nsimplicity-complexity tradeoff\n\n\n\n\n\n\n\n\n\n\nOur objective is to make accurate predictions on unseen data. It is important to make sure that models make accurate predictions on the modeling data and the implementation data.\n\n\n\n\n\n\nIn-sample error\n\n\n\nThe predictive error of a model measured on the data used to estimate the predictive model.\n\n\n\n\n\n\n\n\nOut-of-sample error\n\n\n\nThe predictive error of a model measured on the data not used to estimate the predictive model. Out-of-sample error is generally greater than the in-sample error.\n\n\n\n\n\n\n\n\nGeneralizability\n\n\n\nHow well a model makes predictions on unseen data relative to how well it makes predictions on the data used to estimate the model.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Predictive Modeling Concepts</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#spending-data",
    "href": "20_predictive-modeling-concepts.html#spending-data",
    "title": "14  Predictive Modeling Concepts",
    "section": "14.4 Spending Data",
    "text": "14.4 Spending Data\nPredictive modelers strategically “spend data” to create accurate models that generalize to the implementation data. We will focus on two strategies: the training-testing split and v-fold cross validation.\n\n\n\n\n\n\nTraining data\n\n\n\nA subset of data used to develop a predictive model. The share of data committed to a training set depends on the number of observations, the number of predictors in a model, and heterogeneity in the data. 0.8 is a common share.\n\n\n\n\n\n\n\n\nTesting data\n\n\n\nA subset of data used to estimate model performance. The testing set usually includes all observations not included in the testing set. Do not look at these data until the very end and only estimate the out-of-sample error rate on the testing set once. If the error rate is estimated more than once on the testing data, it will underestimate the error rate.\n\n\n\n\n\n\n\n\nFigure 14.4\n\n\n\n\n\n\n\n\n\n\\(v\\)-fold Cross-Validation\n\n\n\nBreak the data into \\(v\\) equal-sized, exclusive groups. Train the model on data from \\(v - 1\\) folds (analysis data) and measure the error metric (i.e. RMSE) on the left-out fold (assessment data). Repeat this process \\(v\\) times, each time leaving out a different fold. Average the \\(v\\) error metrics.\n\\(v\\)-fold cross-validation is sometimes called \\(k\\)-fold cross-validation.\n\n\nStrategies for spending data differ based on the application and the size and nature of the data. Ideally, the training-testing split is made at the beginning of the modeling process. The \\(v\\)-fold cross-validation is used on the testing data for comparing\n\napproaches for feature/target engineering\nalgorithms\nhyperparameter tuning\n\n\n\n\n\n\n\nData Leakage\n\n\n\nWhen information that won’t be available when the model makes out-of-sample predictions is used when estimating a model. Looking at data from the testing set creates data leakage. Data leakage leads to an underestimate of out-of-sample error.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFeature and target engineering is a common source of data leakage.\nFor example, regularized regression models expect variables to be normalized (divide each predictor by the sample mean and divide by the sample standard deviation). A naive approach would be to calculate the means and standard deviations once one the full data set and use them on the testing data or use them in cross validation. This causes data leakage and biased underestimates of the out-of-sample error rate.\nNew means and standard deviations need to be estimated every time a model is fit including within each iteration of a resampling method! This is a lot of work, but library(tidymodels) makes this simple.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Predictive Modeling Concepts</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#predictive-modeling-pipelines",
    "href": "20_predictive-modeling-concepts.html#predictive-modeling-pipelines",
    "title": "14  Predictive Modeling Concepts",
    "section": "14.5 Predictive Modeling Pipelines",
    "text": "14.5 Predictive Modeling Pipelines\nThe rest of this chapter focuses on a predictive modeling pipeline applied to the simulated data. This pipeline is a general approach to creating predictive models and can change based on the specifics of an application. We will demonstrate the pipeline using KNN and regression trees.\n\nProblem Formulation\nSplit data into training and testing data\nExploratory Data Analysis\nSet up Resampling for Model Selection\nCreate Candidate Models\nTest and Choose the “Best” Model\nOptional: Expand the Model Fit\nEvaluate the Final Model\nOptional: Implement the Final Model\n\n\n14.5.1 Example: KNN\n\n1. Split data into training and testing data\n\nset.seed(20201007)\n\n# create a split object\ndata1_split &lt;- initial_split(data = data1, prop = 0.75)\n\n# create the training and testing data\ndata1_train &lt;- training(x = data1_split)\ndata1_test  &lt;- testing(x = data1_split)\n\n\n\n2. Exploratory Data Analysis\n\n\n\n\n\n\nWarning\n\n\n\nOnly perform EDA on the training data.\nExploring the testing data will lead to data leakage.\n\n\n\ndata1_train |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = 0.25) +\n  labs(title = \"Example 1 Data\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3. Set up Resampling for Model Selection\nWe use 10-fold cross validation for hyperparameter tuning and model selection. The training data are small (750 observations and one predictor), so we will repeat the entire cross validation process five times.\nEstimating out-of-sample error rates with cross-validation is an uncertain process. Repeated cross-validation improves the accuracy of our estimated error rates.\n\ndata1_folds &lt;- vfold_cv(data = data1_train, v = 10, repeats = 5)\n\n\n\n4. Create Candidate Models\nWe have three main levers for creating candidate models:\n\nFeature and target engineering. Feature and target engineering is generally specified when creating a recipe with recipe() and step_*() functions.\nSwitching algorithms. Algorithms are specified using functions from library(parsnip).\nHyperparameter tuning. Hyperparameter tuning is typically handled by using tune() when picking an algorithm and then creating a hyperparameter tuning grid.\n\nIt is common to normalize (subtract the sample mean and divide by the sample standard deviation) predictors when using KNN.\n\nknn_recipe &lt;-\n  recipe(formula = y ~ ., data = data1_train) |&gt;\n  step_normalize(all_predictors())\n\nWe use nearest_neighbor() to create a KNN model and we use tune() as a placeholder for k. Note that we can tune many hyperparameters for a given model. The Regression Tree example later in this chapter illustrates this concept.\n\nknn_mod &lt;-\n  nearest_neighbor(neighbors = tune()) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\")\n\nWe use grid_regular() to specify a hyperparameter tuning grid for potential values of \\(k\\).\n\nknn_grid &lt;- grid_regular(neighbors(range = c(1, 99)), levels = 10)\n\nknn_grid\n\n# A tibble: 10 × 1\n   neighbors\n       &lt;int&gt;\n 1         1\n 2        11\n 3        22\n 4        33\n 5        44\n 6        55\n 7        66\n 8        77\n 9        88\n10        99\n\n\nFinally, we combine the recipe and model objects into a workflow().\n\nknn_workflow &lt;-\n  workflow() |&gt;\n  add_recipe(recipe = knn_recipe) |&gt;\n  add_model(spec = knn_mod)\n\n\n\n5. Test and Choose the “Best” Model\ntune_grid() fits the models to the repeated cross validation with differing hyperparameters and captures RMSE against each evaluation data set.\n\nknn_res &lt;-\n  knn_workflow |&gt;\n  tune_grid(\n    resamples = data1_folds,\n    grid = knn_grid,\n    metrics = metric_set(rmse)\n  )\n\nWe can quickly extract information from the tuned results object with functions like collect_metrics(), show_best(), and autoplot().\n\nknn_res |&gt;\n  collect_metrics()\n\n# A tibble: 10 × 7\n   neighbors .metric .estimator  mean     n std_err .config              \n       &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1         1 rmse    standard    2.86    50  0.0325 Preprocessor1_Model01\n 2        11 rmse    standard    2.06    50  0.0209 Preprocessor1_Model02\n 3        22 rmse    standard    2.01    50  0.0200 Preprocessor1_Model03\n 4        33 rmse    standard    2.00    50  0.0193 Preprocessor1_Model04\n 5        44 rmse    standard    2.01    50  0.0197 Preprocessor1_Model05\n 6        55 rmse    standard    2.03    50  0.0205 Preprocessor1_Model06\n 7        66 rmse    standard    2.07    50  0.0221 Preprocessor1_Model07\n 8        77 rmse    standard    2.11    50  0.0242 Preprocessor1_Model08\n 9        88 rmse    standard    2.17    50  0.0266 Preprocessor1_Model09\n10        99 rmse    standard    2.24    50  0.0291 Preprocessor1_Model10\n\nknn_res |&gt;\n  show_best()\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        33 rmse    standard    2.00    50  0.0193 Preprocessor1_Model04\n2        22 rmse    standard    2.01    50  0.0200 Preprocessor1_Model03\n3        44 rmse    standard    2.01    50  0.0197 Preprocessor1_Model05\n4        55 rmse    standard    2.03    50  0.0205 Preprocessor1_Model06\n5        11 rmse    standard    2.06    50  0.0209 Preprocessor1_Model02\n\nknn_res |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n6. Optional: Expand the Model Fit\nSometimes we will pick the best predictive model specification (feature and target engineering + algorithm + hyperparameters) and fit the model on all of the training data.\nFirst, finalize the workflow with the “best” hyperparameters from the tuned results.\n\nfinal_wf &lt;- \n  knn_workflow |&gt; \n  finalize_workflow(select_best(knn_res))\n\nSecond, use last_fit() to fit the model on all of the data.\n\nfinal_fit &lt;- \n  final_wf |&gt;\n  last_fit(data1_split) \n\n\n\n\n\n\n\nNote\n\n\n\nselect_best() takes a narrow view of model selection and picks the model with the lowest error rate.\nIt is important to consider other elements when picking a “best.” Other considerations include parsimony, cost, and equity.\n\n\n\n\n7. Evaluate the Final Model\nThe final fit object contains the out-of-sample error metric from the testing data. This metric is our best estimate of model performance on unseen data.\nIn this case, the testing data error is slightly higher than the training data error, which makes sense.\n\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2.04  Preprocessor1_Model1\n2 rsq     standard       0.922 Preprocessor1_Model1\n\n\n\n\n8. Optional: Implement the Final Model\nfinal_fit and pedict() can be used to apply the model to new, unseen data.\nOnly implement the model if it achieves the objectives of the final model.\n\n\n\n14.5.2 Example: Regression Trees\nWe’ll next work with a subset of data from the Chicago data set from library(tidymodels).\n\nchicago_small &lt;- Chicago |&gt;\n  select(\n    ridership,\n    Clark_Lake,\n    Quincy_Wells,\n    Irving_Park,\n    Monroe,\n    Polk,\n    temp,\n    percip,\n    Bulls_Home,\n    Bears_Home,\n    WhiteSox_Home,\n    Cubs_Home,\n    date\n  ) |&gt;\n  mutate(weekend = wday(date, label = TRUE) %in% c(\"Sat\", \"Sun\")) |&gt;\n  glimpse()\n\nRows: 5,698\nColumns: 14\n$ ridership     &lt;dbl&gt; 15.732, 15.762, 15.872, 15.874, 15.423, 2.425, 1.467, 15…\n$ Clark_Lake    &lt;dbl&gt; 15.561, 15.720, 15.558, 15.745, 15.602, 2.413, 1.374, 9.…\n$ Quincy_Wells  &lt;dbl&gt; 8.371, 8.351, 8.359, 7.852, 7.621, 0.911, 0.414, 4.807, …\n$ Irving_Park   &lt;dbl&gt; 3.744, 3.853, 3.861, 3.843, 3.878, 1.735, 1.164, 2.903, …\n$ Monroe        &lt;dbl&gt; 5.672, 6.013, 5.786, 5.959, 5.769, 1.044, 0.530, 3.165, …\n$ Polk          &lt;dbl&gt; 2.481, 2.436, 2.526, 2.450, 2.573, 0.006, 0.000, 1.065, …\n$ temp          &lt;dbl&gt; 19.45, 30.45, 25.00, 22.45, 27.00, 24.80, 18.00, 32.00, …\n$ percip        &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.…\n$ Bulls_Home    &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Bears_Home    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ WhiteSox_Home &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Cubs_Home     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ date          &lt;date&gt; 2001-01-22, 2001-01-23, 2001-01-24, 2001-01-25, 2001-01…\n$ weekend       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FA…\n\n\n\n0. Problem Formulation\nThe variable ridership is the outcome variable. It measures ridership at the Clark/Lake L station in Chicago. The variables Clark_Lake, Quincy_Wells, Irving_Park, Monroe, and Polk measure ridership at several stations 14 days before the ridership variable.\nThe objective is predict ridership to better allocate staff and resources to the Clark/Lake L station.\n\n\n1. Split data into training and testing data\n\nset.seed(20231106)\n\n# create a split object\nchicago_split &lt;- initial_split(data = chicago_small, prop = 0.8)\n\n# create the training and testing data\nchicago_train &lt;- training(x = chicago_split)\nchicago_test  &lt;- testing(x = chicago_split)\n\n\n\n2. Exploratory Data Analysis\nThis chapter in “Feature and Target Engineering” contains EDA for the data set of interest.\n\n\n3. Set up Resampling for Model Selection\nWe use 10-fold cross validation for hyperparameter tuning and model selection. The training data are larger in the previous example, so we skip repeats.\n\nchicago_folds &lt;- vfold_cv(data = chicago_train, v = 10)\n\n\n\n4. Create Candidate Models\nWe will use regression trees for this example. For feature and target engineering, we will simply remove the date column.\n\nrpart_recipe &lt;-\n  recipe(formula = ridership ~ ., data = chicago_train) |&gt;\n  step_rm(date)\n\nWe specify a regression tree with the rpart engine.\n\nrpart_mod &lt;-\n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune(),\n    min_n = tune()\n  ) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\")\n\nWe use grid_regular() to specify a hyperparameter tuning grid for potential values of the cost-complexity parameter, tree depth parameter, and minimum n.\n\nrpart_grid &lt;- grid_regular(\n  cost_complexity(range = c(-5, -1)), \n  tree_depth(range = c(3, 15)), \n  min_n(),\n  levels = 10\n)\n\nrpart_grid\n\n# A tibble: 1,000 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1       0.00001            3     2\n 2       0.0000278          3     2\n 3       0.0000774          3     2\n 4       0.000215           3     2\n 5       0.000599           3     2\n 6       0.00167            3     2\n 7       0.00464            3     2\n 8       0.0129             3     2\n 9       0.0359             3     2\n10       0.1                3     2\n# ℹ 990 more rows\n\n\nFinally, we combine the recipe and model objects into a workflow().\n\nrpart_workflow &lt;-\n  workflow() |&gt;\n  add_recipe(recipe = rpart_recipe) |&gt;\n  add_model(spec = rpart_mod)\n\n\n\n5. Test and Choose the “Best” Model\n\nrpart_res &lt;-\n  rpart_workflow |&gt;\n  tune_grid(resamples = chicago_folds,\n            grid = rpart_grid,\n            metrics = metric_set(rmse))\n\nrpart_res |&gt;\n  collect_metrics()\n\n# A tibble: 1,000 × 9\n   cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1       0.00001            3     2 rmse    standard    2.44    10  0.0414\n 2       0.0000278          3     2 rmse    standard    2.44    10  0.0414\n 3       0.0000774          3     2 rmse    standard    2.44    10  0.0414\n 4       0.000215           3     2 rmse    standard    2.44    10  0.0414\n 5       0.000599           3     2 rmse    standard    2.44    10  0.0414\n 6       0.00167            3     2 rmse    standard    2.45    10  0.0418\n 7       0.00464            3     2 rmse    standard    2.47    10  0.0456\n 8       0.0129             3     2 rmse    standard    2.67    10  0.0392\n 9       0.0359             3     2 rmse    standard    2.67    10  0.0392\n10       0.1                3     2 rmse    standard    3.02    10  0.0383\n# ℹ 990 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\nrpart_res |&gt;\n  show_best()\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1       0.00001            5    40 rmse    standard    2.37    10  0.0374\n2       0.0000278          5    40 rmse    standard    2.37    10  0.0374\n3       0.00001            5    35 rmse    standard    2.37    10  0.0374\n4       0.0000278          5    35 rmse    standard    2.37    10  0.0374\n5       0.0000774          5    35 rmse    standard    2.37    10  0.0369\n# ℹ 1 more variable: .config &lt;chr&gt;\n\nrpart_res |&gt;\n  select_best()\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config                \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                  \n1         0.00001          5    40 Preprocessor1_Model0921\n\nrpart_res |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n6. Optional: Expand the Model Fit\nSometimes we will pick the best predictive model specification (feature and target engineering + algorithm + hyperparameters) and fit the model on all of the training data.\nFirst, finalize the workflow with the “best” hyperparameters from the tuned results.\n\nfinal_wf &lt;- \n  rpart_workflow |&gt; \n  finalize_workflow(select_best(rpart_res))\n\nSecond, use last_fit() to fit the model on all of the data.\n\nfinal_fit &lt;- \n  final_wf |&gt;\n  last_fit(chicago_split) \n\n\n\n7. Evaluate the Final Model\nThe final fit object contains the out-of-sample error metric from the testing data. This metric is our best estimate of model performance on unseen data.\nIn this case, the testing data error is slightly higher than the training data error, which makes sense.\n\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2.34  Preprocessor1_Model1\n2 rsq     standard       0.869 Preprocessor1_Model1\n\n\n\n\n8. Optional: Implement the Final Model\nIf we think an expected error of around 2,340 people is appropriate, we can implement this model for the Chicago Transit Authority.\nInstead, we’re going back to the drawing board in our class lab to see if we can do better.1",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Predictive Modeling Concepts</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#footnotes",
    "href": "20_predictive-modeling-concepts.html#footnotes",
    "title": "14  Predictive Modeling Concepts",
    "section": "",
    "text": "Pretend we haven’t already touched the testing data!↩︎",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Predictive Modeling Concepts</span>"
    ]
  }
]