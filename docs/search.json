[
  {
    "objectID": "01_intro-r.html",
    "href": "01_intro-r.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 Six Principles for Data Analysis",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#accuracy",
    "href": "01_intro-r.html#accuracy",
    "title": "1  Introduction to R",
    "section": "2.1 1) Accuracy",
    "text": "2.1 1) Accuracy\nDeliberate steps should be taken to minimize the chance of making an error and maximize the chance of catching errors when errors inevitably occur.\nEmbrace Your Fallibility: Thoughts on Code Integrity",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#computational-reproducibility",
    "href": "01_intro-r.html#computational-reproducibility",
    "title": "1  Introduction to R",
    "section": "2.2 2) Computational reproducibility",
    "text": "2.2 2) Computational reproducibility\nComputational reproducibility should be embraced to improve accuracy, promote transparency, and prove the quality of analytic work.\nReplication: the recreation of findings across repeated studies, is a cornerstone of science.\nReproducibility: the ability to access data, source code, tools, and documentation and recreate all calculations, visualizations, and artifacts of an analysis.\nComputational reproducibility should be the minimum standard for computational social sciences and statistical programming.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#human-interpretability",
    "href": "01_intro-r.html#human-interpretability",
    "title": "1  Introduction to R",
    "section": "2.3 3) Human interpretability",
    "text": "2.3 3) Human interpretability\nCode should be written so humans can easily understand what’s happening—even if it occasionally sacrifices machine performance.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#portability",
    "href": "01_intro-r.html#portability",
    "title": "1  Introduction to R",
    "section": "2.4 4) Portability",
    "text": "2.4 4) Portability\nAnalyses should be designed so strangers can understand each and every step without additional instruction or inquiry from the original analyst.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#accessibility",
    "href": "01_intro-r.html#accessibility",
    "title": "1  Introduction to R",
    "section": "2.5 5) Accessibility",
    "text": "2.5 5) Accessibility\nResearch and data are non-rivalrous and can be non-excludable. They are public goods that should be widely and easily shared. Decisions about tools, methods, data, and language during the research process should be made in ways that promote the ability of anyone and everyone to access an analysis.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#efficiency",
    "href": "01_intro-r.html#efficiency",
    "title": "1  Introduction to R",
    "section": "2.6 6) Efficiency",
    "text": "2.6 6) Efficiency\nAnalysts should seek to make all parts of the research process more efficient with clear communication, by adopting best practices, and by managing computation.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#vectors",
    "href": "01_intro-r.html#vectors",
    "title": "1  Introduction to R",
    "section": "9.1 Vectors",
    "text": "9.1 Vectors\nVectors are one-dimensional arrays that contain one and only one type of data. Atomic vectors in R are homogeneous. There are six types of atomic vectors:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex (uncommon)\nraw (uncommon)\n\nFor now, the simplest way to create a vector is with c(), the combine function.\n\n# a logical vector\nc(TRUE, FALSE, FALSE)\n\n[1]  TRUE FALSE FALSE\n\n# an integer vector\nc(1, 2, 3)\n\n[1] 1 2 3\n\n# a double vector\nc(1.1, 2.2, 3.3)\n\n[1] 1.1 2.2 3.3\n\n# a character vector\nc(\"District of Columbia\", \"Virginia\", \"Maryland\")\n\n[1] \"District of Columbia\" \"Virginia\"             \"Maryland\"            \n\n\nThe class() function can be used to identify the type, or class, of an object. For example:\n\nclass(c(TRUE, FALSE, FALSE))\n\n[1] \"logical\"\n\nclass(c(1, 2, 3))\n\n[1] \"numeric\"\n\nclass(c(\"District of Columbia\", \"Virginia\", \"Maryland\"))\n\n[1] \"character\"\n\n\nIf you create a vector with mixed data types, R will coerce all of the values to a single type:\n\nc(TRUE, 1, \"District of Columbia\")\n\n[1] \"TRUE\"                 \"1\"                    \"District of Columbia\"\n\nclass(c(TRUE, 1, \"District of Columbia\"))\n\n[1] \"character\"\n\n\nLists are one- or multi-dimensional arrays that are made up of other lists. Lists are heterogeneous - they can contain many lists of different types and dimensions. A vector is a list but a list is not necessarily a vector.\nNULL is the null object in R. It means a value does not exist.\n![Source: R4DS\nNA is a missing value of lenth 1 in R. NAs are powerful representations in R with special properties. NA is a contagious value in R that will override all calculations.\n\n1 + 2 + 3 + NA\n\n[1] NA\n\n\nThis forces programmers to be deliberate about missing values. This is a feature, not a bug!\nR contains special functions and function arguments for handling NAs. For example, we can wrap a vector with missing values in is.na() to create a vector of Booleans where TRUE represents an element that is an NA and FALSE represents an element that is not an NA.\n\nis.na(c(1, 2, NA))\n\n[1] FALSE FALSE  TRUE\n\n\nNote: NA and NULL have different meanings! NULL means no value exists. NA means a value could exist but it is unknown.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#matrices",
    "href": "01_intro-r.html#matrices",
    "title": "1  Introduction to R",
    "section": "9.2 Matrices",
    "text": "9.2 Matrices\nMatrices are multi-dimensional arrays where every element is of the same type. Most data in data science contains at least numeric information and character information. Accordingly, we will not use matrices much in this course.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#data-frames",
    "href": "01_intro-r.html#data-frames",
    "title": "1  Introduction to R",
    "section": "9.3 Data frames",
    "text": "9.3 Data frames\nInstead, data frames, and their powerful cousins tibbles, are the backbone of data science and this course. Data frames are two-dimensional arrays where each column is a list (usually a vector). Most times, each column will be of one type while a given row will contain many different types. We usually refer to columns as variables and rows as observations.\nHere are the first six rows of a data frame with information about diamond prices and diamond characteristics:\n\nhead(ggplot2::diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#tibbles",
    "href": "01_intro-r.html#tibbles",
    "title": "1  Introduction to R",
    "section": "9.4 tibbles",
    "text": "9.4 tibbles\ntibbles are special data frames that have a few extra features:\n\nOnly the first ten rows of tibbles print by default\nExtra meta data are printed with tibbles\nThey have some convenient protections against partial subsetting\nThey are easier to create from scratch in a .R script\n\n\ntibble(\n  a = c(TRUE, FALSE, FALSE),\n  b = c(1, 2, 3),\n  c = c(1.1, 2.2, 3.3),\n  d = c(\"District of Columbia\", \"Virginia\", \"Maryland\")\n)\n\n# A tibble: 3 × 4\n  a         b     c d                   \n  &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;               \n1 TRUE      1   1.1 District of Columbia\n2 FALSE     2   2.2 Virginia            \n3 FALSE     3   3.3 Maryland            \n\n\nFrom this moment forward, I will use data frame to mean tibble.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#section",
    "href": "01_intro-r.html#section",
    "title": "1  Introduction to R",
    "section": "11.1 ?",
    "text": "11.1 ?\nDocumentation for functions can be easily accessed by prefacing the function name with ? and dropping the ().\n\n?mean\n\nThe documentation typically includes a description, a list of the arguments, references, a list of related functions, and examples. The examples are incredibly useful.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#arguments",
    "href": "01_intro-r.html#arguments",
    "title": "1  Introduction to R",
    "section": "11.2 Arguments",
    "text": "11.2 Arguments\nR functions typically contain many arguments. For example, mean() has x, trim, and na.rm. Many arguments have default values and don’t need to be included in function calls. Default values can be seen in the documentation. trim = 0 and na.rm = FALSE are the defaults for mean().\nArguments can be passed to functions implicitly by position or explicitly by name.\n\nnumeric_vector &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# by position (correctly)\nmean(numeric_vector, 0.2)\n\n[1] 5.5\n\n\n\n# by position (incorrectly)\nmean(0.2, numeric_vector)\n\n\n# by name\nmean(x = numeric_vector, trim = 0.2)\n\n[1] 5.5\n\n\nFunction calls can include arguments by position and by name. The first argument in most functions is data or x. It is custom to usually include the first argument by position and then all subsequent arguments by name.\n\nmean(numeric_vector, trim = 0.2)\n\n[1] 5.5",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#base-r",
    "href": "01_intro-r.html#base-r",
    "title": "1  Introduction to R",
    "section": "12.1 Base R",
    "text": "12.1 Base R\nOpening RStudio automatically loads “base R”, a fundamental collection of code and functions that handles simple operations like math and system management.\nFor years, R was only base R. New paradigms in R have developed over the last fifteen years that are more intuitive and more flexible than base R. Next week, we’ll discuss the “tidyverse”, the most popular paradigm for R programming.\nAll R programming will involve some base R, but much base R has been replaced with new tools that are more concise. Just know that at some point you may end up on a Stack Overflow page that looks like alphabet soup because it’s in a paradigm that you have not learned.\nOne other popular R paradigm is data.table. We will not discuss data.table in this class.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#extensibility",
    "href": "01_intro-r.html#extensibility",
    "title": "1  Introduction to R",
    "section": "12.2 Extensibility",
    "text": "12.2 Extensibility\nR is an extensible programming language. It was designed to allow for new capabilities and functionality.\nR is also open source. All of it’s source code is publicly available.\nThese two features have allowed R users to contribute millions of lines of code that can be used by other users without condition or compensation. The main mode of contribution are R packages. Packages are collections of functions and data that expand the power and usefulness of R.\nThe predecessor of R, the S programming language, was designed to call FORTRAN subroutines. Accordingly, many R packages used to call compiled FORTRAN code. Now, many R packaged call compiled C++ code. This gives users the intuition of R syntax with better performance. Here’s a brief history of R and S.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#cran",
    "href": "01_intro-r.html#cran",
    "title": "1  Introduction to R",
    "section": "12.3 CRAN",
    "text": "12.3 CRAN\nMost R packages are stored on the Comprehensive R Archive Network (CRAN). Packages must pass a modest number of tests for stability and design to be added to CRAN.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#install.packages",
    "href": "01_intro-r.html#install.packages",
    "title": "1  Introduction to R",
    "section": "12.4 install.packages()",
    "text": "12.4 install.packages()\nPackages can be directly installed from CRAN using install.packages(). Simply include the name of the desired package in quotes as the only argument to the function.\nInstallation need only happen once per computer per package version. It is customary to never include install.packages() in a .R script.\n\n12.4.1 Exercise\nFor practice, let’s install the RXKCD package so we can view some comics in R.\ninstall.packages(\"RXKCD\")\n\\[\\cdot\\cdot\\cdot\\]",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#library",
    "href": "01_intro-r.html#library",
    "title": "1  Introduction to R",
    "section": "12.5 library()",
    "text": "12.5 library()\nAfter installation, packages need to be loaded once per R session using the library() function. While install.packages() expects a quoted package name, it is best practice to use unquoted names in library().\nIt is a good idea to include library() statements at the top of scripts for each package used in the script. This way it is obvious at the top of the script which packages are necessary.\n\n12.5.1 Exercise\nFor practice let’s load the RXKCD package.\n\nlibrary(RXKCD)\n\nFinally, let’s look at some comics!\n\nmeta_data &lt;- getXKCD(which = 327)\n\n\n\n\n\n\n\n\n\nmeta_data &lt;- getXKCD(which = 303)\n\n\n\n\n\n\n\n\n\nmeta_data &lt;- getXKCD(which = 149)\n\n\n\n\n\n\n\n\n\\[\\cdot\\cdot\\cdot\\]",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#section-1",
    "href": "01_intro-r.html#section-1",
    "title": "1  Introduction to R",
    "section": "12.6 ::",
    "text": "12.6 ::\nSometimes two packages have functions with the same name. :: can be used to directly access an exported R object from a package’s namespace.\ndplyr::select()\nMASS::select()",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#r-projects",
    "href": "01_intro-r.html#r-projects",
    "title": "1  Introduction to R",
    "section": "13.1 R Projects",
    "text": "13.1 R Projects\nR Projects, proper noun, are the best way to organize an analysis. They have several advantages:\n\nThey make it possible to concurrently run multiple RStudio sessions.\nThey allow for project-specific RStudio settings.\nThey integrate well with Git version control.\nThey are the “node” of relative file paths. (more on this in a second) This makes code highly portable.\n\n\n13.1.1 Exercise\nBefore setting up an R Project, go to Tools &gt; Global Options and uncheck “Restore most recently opened project at startup”.\n\\[\\cdot\\cdot\\cdot\\]\nEvery new analysis in R should start with an R Project. First, create a directory that holds all data, scripts, and files for the analysis. You can do this right in RStudio by clicking the “New Folder” button at the top of the “Files” tab located in the top or bottom right of RStudio. Storing files and data in a sub-directories is encouraged. For example, data can be stored in a folder called data/.\nNext, click “New Project…” in the top right corner.\n\nWhen prompted, turn your recently created “Existing Directory” into a project.\n\nUpon completion, the name of the R Project should now be displayed in the top right corner of RStudio where it previously displayed “Project: (None)”. Once opened, .RProj files do not need to be saved. Double-clicking .Rproj files in the directory is now the best way to open RStudio. This will allow for the concurrent use of multiple R sessions and ensure the portability of file paths. Once an RStudio project is open, scripts can be opened by double-clicking individual files in the computer directory or clicking files in the “Files” tab.\n\n\n13.1.2 Exercise\nLet’s walk through this process and create an R project for this class.\n\\[\\cdot\\cdot\\cdot\\]",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#filepaths",
    "href": "01_intro-r.html#filepaths",
    "title": "1  Introduction to R",
    "section": "13.2 Filepaths",
    "text": "13.2 Filepaths\nWindows file paths are usually delimited with \\. *nix file paths are usually delimited with /. Never use \\ in file paths in R. \\ is an escape character in R and will complicate an analysis. Fortunately, RStudio understands / in file paths regardless of operating system.\nNever use setwd() in R. It is unnecessary, it makes code unreproducible across machines, and it is rude to collaborators. R Projects create a better framework for file paths. Simply treat the directory where the R Project lives as the working directory and directories inside of that directory as sub-directories.\nFor example, say there’s a .Rproj called starwars-analysis.Rproj in a directory called starwars-analysis/. If there is a .csv in that folder called jedi.csv, the file can be loaded with read_csv(\"jedi.csv\") instead of read_csv(\"H:/alena/analyses/starwars-analysis/jedi.csv\"). If that file is in a sub-directory of starwars-analysis called data, it can be loaded with read_csv(\"data/jedi.csv\"). The same concepts hold for writing data and graphics.\nThis simplifies code and makes it portable because all relative file paths will be identical on all computers. To share an analysis, simply send the entire directory to a collaborator or share it with GitHub.\nHere’s an example directory:",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#googling",
    "href": "01_intro-r.html#googling",
    "title": "1  Introduction to R",
    "section": "14.1 Googling",
    "text": "14.1 Googling\nWhen Googling for R or data science help, set the search range to the last year or less to avoid out-of-date solutions and to focus on up-to-date practices. The search window can be set by clicking Tools after a Google search.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#stack-overflow",
    "href": "01_intro-r.html#stack-overflow",
    "title": "1  Introduction to R",
    "section": "14.2 Stack Overflow",
    "text": "14.2 Stack Overflow\nStack Overflow contains numerous solutions. If a problem is particularly perplexing, it is simple to submit questions. Exercise caution when submitting questions because the Stack Overflow community has strict norms about questions and loose norms about respecting novices.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#rstudio-community",
    "href": "01_intro-r.html#rstudio-community",
    "title": "1  Introduction to R",
    "section": "14.3 RStudio community",
    "text": "14.3 RStudio community\nRStudio Community is a new forum for R Users. It has a smaller back catalog than Stack Overflow but users are friendlier than on Stack Overflow.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#cran-task-views",
    "href": "01_intro-r.html#cran-task-views",
    "title": "1  Introduction to R",
    "section": "14.4 CRAN task views",
    "text": "14.4 CRAN task views\nCRAN task views contains thorough introductions to packages and techniques organized by subject matter. The Econometrics, Reproducible Research, and and Social Sciences task views are good starting places.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#twitter",
    "href": "01_intro-r.html#twitter",
    "title": "1  Introduction to R",
    "section": "14.5 Twitter",
    "text": "14.5 Twitter\nTwitter is mostly bad. But the #rstats hashtag and #rstats community are mostly good. They are also generally inclusive and civil. In particular, open sources developers like Hadley Wickham (@hadleywickham), Jenny Bryan (@JennyBryan), and Joe Cheng (@jcheng) are active.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#data-science-for-public-policy-slack",
    "href": "01_intro-r.html#data-science-for-public-policy-slack",
    "title": "1  Introduction to R",
    "section": "14.6 Data Science for Public Policy Slack",
    "text": "14.6 Data Science for Public Policy Slack\nWe’ve created a Slack workspace for this class (which will be shared across both sections) and encourage you to ask questions in Slack. In general, we ask that you try to answer questions on your own using the sources above before posting in Slack. Practicing finding and applying the relevant information to answer your questions is an important data science skill! The teaching staff will be checking the Slack to help answer questions in a reasonable time frame and we also encourage you to answer each other’s questions - it’s a great way to improve your R skills!\nQuestions on Slack must be asked using reproducible examples. Simply copying-and-pasting questions or answers in the Slack channel is not allowed. If you’re unsure how to share a reproducible example without sharing your answers in a public channel, you can DM the teaching staff to be safe.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#chatgpt",
    "href": "01_intro-r.html#chatgpt",
    "title": "1  Introduction to R",
    "section": "14.7 ChatGPT:",
    "text": "14.7 ChatGPT:\nSince there is R code on the internet, ChatGPT has been trained on R code and has the capability to answer R coding questions. Exercise extreme caution when using ChatGPT! ChatGPT saves and uses the queries you provide it. This means that asking a question about sensitive data or code could expose that data. If you decide to use ChatGPT, only ask queries of it using a reproducible example with non-sensitive data. The diamonds dataset, loaded with ggplot::diamonds() is a great candidate.\nThis warning aside, ChatGPT can be a powerful tool. Some helpful tips for using ChatGPT for coding questions are: - Provide it detailed questions - Give it reproducible example code - Refine queries when the initial responses are unsatisfactory",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A\nComparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nAshraf, N., D. Karlan, and W. Yin. 2006. “Tying Odysseus to the\nMast: Evidence From a Commitment Savings Product in the\nPhilippines.” The Quarterly Journal of Economics 121\n(2): 635–72. https://doi.org/10.1162/qjec.2006.121.2.635.\n\n\nBarrientos, Andrés F., Aaron R. Williams, Joshua Snoke, and Claire McKay\nBowen. 2021. “A Feasibility Study of Differentially Private\nSummary Statistics and Regression Analyses with Evaluations on\nAdministrative and Survey Data.” https://doi.org/10.48550/ARXIV.2110.12055.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nBlumenstock, Joshua. n.d. “Calling for Better Measurement:\nEstimating an Individual’s Wealth and Well-Being from\nMobile Phone Transaction Records.” Center for Effective\nGlobal Action. https://escholarship.org/uc/item/8zs63942.\n\n\nBlumenstock, Joshua, Gabriel Cadamuro, and Robert On. 2015.\n“Predicting Poverty and Wealth from Mobile Phone Metadata.”\nScience 350 (6264): 1073–76. https://doi.org/10.1126/science.aac4420.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial Proportion.”\nStatistical Science 16 (2). https://doi.org/10.1214/ss/1009213286.\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical\nInference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.\n\n\nChernick, Michael R., and Robert A. LaBudde. 2011. An Introduction\nto Bootstrap Methods with Applications to r. Hoboken, N.J: Wiley.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien\nNielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and\nCrowd-Out in Retirement Savings Accounts: Evidence from\nDenmark*.” The Quarterly Journal of Economics 129 (3):\n1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nFellegi, I. P. 1972. “On the Question of Statistical\nConfidentiality.” Journal of the American Statistical\nAssociation 67 (337): 7–18. https://www.jstor.org/stable/2284695?seq=1#metadata_info_tab_contents.\n\n\nGinsberg, Jeremy, Matthew H. Mohebbi, Rajan S. Patel, Lynnette Brammer,\nMark S. Smolinski, and Larry Brilliant. 2009. “Detecting Influenza\nEpidemics Using Search Engine Query Data.” Nature 457\n(7232): 1012–14. https://doi.org/10.1038/nature07634.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. Springer Series in Statistics. New York, NY:\nSpringer.\n\n\nHiggins, James J. 2004a. An Introduction to Modern Nonparametric\nStatistics. Pacific Grove, CA: Brooks/Cole.\n\n\n———. 2004b. An Introduction to Modern Nonparametric Statistics.\nPacific Grove, CA: Brooks/Cole.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response\nAdjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLeisch, Friedrich. 2004. “FlexMix: A General Framework for Finite\nMixture Models and Latent Class Regression in\nR.” Journal of Statistical\nSoftware 11 (8). https://doi.org/10.18637/jss.v011.i08.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary\nAlignment Methods in Microsimulation Models.” Journal of\nArtificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020.\n“Estimating Confidence Intervals in a Tax Microsimulation\nModel.” International Journal of Microsimulation 13 (2):\n2–20. https://doi.org/10.34196/IJM.00216.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An\nIntroduction. Adaptive Computation and Machine Learning Series.\nCambridge, Massachusetts: The MIT Press.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.”\nThe Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528.\n\n\nPotash, Eric, Joe Brew, Alexander Loewi, Subhabrata Majumdar, Andrew\nReece, Joe Walsh, Eric Rozier, Emile Jorgenson, Raed Mansour, and Rayid\nGhani. 2015. “KDD ’15: The 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining.” In, 2039–47.\nSydney NSW Australia: ACM. https://doi.org/10.1145/2783258.2788629.\n\n\nRavn, Signe, Ashley Barnwell, and Barbara Barbosa Neves. 2020.\n“What Is “Publicly Available Data”?\nExploring Blurred PublicPrivate Boundaries and Ethical\nPractices Through a Case Study on Instagram.” Journal of\nEmpirical Research on Human Research Ethics 15 (1-2): 40–45. https://doi.org/10.1177/1556264619850736.\n\n\nRizzo, Maria L. 2008. Statistical Computing with r. Chapman\n& Hall/CRC Computer Science and Data Analysis Series. Boca Raton:\nChapman & Hall/CRC.\n\n\nSalganik, Matthew J. 2018. Bit by Bit: Social Research in the\nDigital Age. Princeton: Princeton University Press.\n\n\nScott, David W., and Stephan R. Sain. 2005. “Multidimensional\nDensity Estimation.” In, 24:229–61. Elsevier. https://doi.org/10.1016/S0169-7161(04)24009-3.\n\n\nSomepalli, Gowthami, Singla, Micah Goldblum, Jonas Geiping, and Tom\nGoldstein. 2023. “Diffusion Art or Digital Forgery? Investigating\nData Replication in Diffusion Models.” Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 6048–58. https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualie, and Model\nData. 2nd edition. Sebastopol, CA: O’Reilly.\n\n\nZheng, Vivian. 2020. “How Urban Piloted Data Science Techniques to\nCollect Land-Use Reform Data.” https://urban-institute.medium.com/how-urban-piloted-data-science-techniques-to-collect-land-use-reform-data-475409903b88.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "02_tidyverse.html",
    "href": "02_tidyverse.html",
    "title": "2  Introduction to the Tidyverse",
    "section": "",
    "text": "2.1 Review",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#select",
    "href": "02_tidyverse.html#select",
    "title": "2  Introduction to the Tidyverse",
    "section": "2.5 1. select()",
    "text": "2.5 1. select()\nselect() drops columns from a dataframe and/or reorders the columns in a dataframe. The arguments after the name of the dataframe should be the names of columns you wish to keep, without quotes. All other columns not listed are dropped.\n\nselect(.data = asec, year, month, serial)\n\n# A tibble: 157,959 × 3\n    year month serial\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  2020 March      1\n 2  2020 March      1\n 3  2020 March      2\n 4  2020 March      2\n 5  2020 March      3\n 6  2020 March      4\n 7  2020 March      4\n 8  2020 March      5\n 9  2020 March      5\n10  2020 March      5\n# ℹ 157,949 more rows\n\n\nThis works great until the goal is to select 99 of 100 variables. Fortunately, - can be used to remove variables. You can also select all but multiple variables by listing them with the - symbol separated by commas.\n\nselect(.data = asec, -asecflag)\n\n# A tibble: 157,959 × 16\n    year serial month   cpsid asecwth pernum  cpsidp asecwt ftype ftotval inctot\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  2020      1 March 2.02e13   1553.      1 2.02e13  1553. Prim…  127449  52500\n 2  2020      1 March 2.02e13   1553.      2 2.02e13  1553. Prim…  127449  74949\n 3  2020      2 March 2.02e13    990.      1 2.02e13   990. Prim…   64680  44000\n 4  2020      2 March 2.02e13    990.      2 2.02e13   990. Prim…   64680  20680\n 5  2020      3 March 2.02e13   1505.      1 2.02e13  1505. Nonf…   40002  40002\n 6  2020      4 March 2.02e13   1431.      1 2.02e13  1431. Prim…    8424      0\n 7  2020      4 March 2.02e13   1431.      2 2.02e13  1197. Prim…    8424   8424\n 8  2020      5 March 2.02e13   1133.      1 2.02e13  1133. Prim…   59114    610\n 9  2020      5 March 2.02e13   1133.      2 2.02e13  1133. Prim…   59114  58001\n10  2020      5 March 2.02e13   1133.      3 2.02e13  1322. Prim…   59114    503\n# ℹ 157,949 more rows\n# ℹ 5 more variables: incwage &lt;dbl&gt;, offpov &lt;chr&gt;, offpovuniv &lt;chr&gt;,\n#   offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nTidy data generally results in longer, wider data sets than other programming languages so iteratively selecting by column names is less important than in Stata or SAS. Still, dplyr contains powerful helper functions that can select variables based on patterns in column names:\n\ncontains(): Contains a given string\nstarts_with(): Starts with a prefix\nends_with(): Ends with a suffix\nmatches(): Matches a regular expression\nnum_range(): Matches a numerical range\n\nThese are a subset of the tidyselect selection language and helpers which enable users to apply library(dplyr) functions to select variables.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nSelect pernum and inctot from asec.\npull() is related to select() but can only select one variable. What is the other difference with pull()?",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#rename",
    "href": "02_tidyverse.html#rename",
    "title": "2  Introduction to the Tidyverse",
    "section": "2.6 2. rename()",
    "text": "2.6 2. rename()\nrename() renames columns in a data frame. The pattern is new_name = old_name.\n\nrename(.data = asec, serial_number = serial)\n\n# A tibble: 157,959 × 17\n    year serial_number month   cpsid asecflag asecwth pernum  cpsidp asecwt\n   &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  2020             1 March 2.02e13        1   1553.      1 2.02e13  1553.\n 2  2020             1 March 2.02e13        1   1553.      2 2.02e13  1553.\n 3  2020             2 March 2.02e13        1    990.      1 2.02e13   990.\n 4  2020             2 March 2.02e13        1    990.      2 2.02e13   990.\n 5  2020             3 March 2.02e13        1   1505.      1 2.02e13  1505.\n 6  2020             4 March 2.02e13        1   1431.      1 2.02e13  1431.\n 7  2020             4 March 2.02e13        1   1431.      2 2.02e13  1197.\n 8  2020             5 March 2.02e13        1   1133.      1 2.02e13  1133.\n 9  2020             5 March 2.02e13        1   1133.      2 2.02e13  1133.\n10  2020             5 March 2.02e13        1   1133.      3 2.02e13  1322.\n# ℹ 157,949 more rows\n# ℹ 8 more variables: ftype &lt;chr&gt;, ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;,\n#   offpov &lt;chr&gt;, offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nYou can also rename a selection of variables using rename_with(). The .cols argument is used to select the columns to rename and takes a tidyselect statement like those we introduced above. Here, we’re using the where() selection helper which selects all columns where a given condition is TRUE. The default value for the .cols argument is everything() which selects all columns in the dataset.\n\nrename_with(.data = asec, .fn = toupper, .cols = where(is.numeric))\n\n# A tibble: 157,959 × 17\n    YEAR SERIAL month   CPSID ASECFLAG ASECWTH PERNUM  CPSIDP ASECWT ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: FTOTVAL &lt;dbl&gt;, INCTOT &lt;dbl&gt;, INCWAGE &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, OFFTOTVAL &lt;dbl&gt;, OFFCUTOFF &lt;dbl&gt;\n\n\nMost dplyr functions can rename columns simply by prefacing the operation with new_name =. For example, this can be done with select():\n\nselect(.data = asec, year, month, serial_number = serial)\n\n# A tibble: 157,959 × 3\n    year month serial_number\n   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1  2020 March             1\n 2  2020 March             1\n 3  2020 March             2\n 4  2020 March             2\n 5  2020 March             3\n 6  2020 March             4\n 7  2020 March             4\n 8  2020 March             5\n 9  2020 March             5\n10  2020 March             5\n# ℹ 157,949 more rows",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#filter",
    "href": "02_tidyverse.html#filter",
    "title": "2  Introduction to the Tidyverse",
    "section": "2.7 3. filter()",
    "text": "2.7 3. filter()\nfilter() reduces the number of observations in a dataframe. Every column in a dataframe has a name. Rows do not necessarily have names in a dataframe, so rows need to be filtered based on logical conditions.\n==, &lt;, &gt;, &lt;=, &gt;=, !=, %in%, and is.na() are all operators that can be used for logical conditions. ! can be used to negate a condition and & and | can be used to combine conditions. | means or.\n\n# return rows with pernum of 1 and incwage &gt; $100,000\nfilter(.data = asec, pernum == 1 & incwage &gt; 100000)\n\n# A tibble: 5,551 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020     28 March 2.02e13        1    678.      1 2.02e13   678. Primary fa…\n 2  2020    134 March 0              1    923.      1 0         923. Primary fa…\n 3  2020    136 March 2.02e13        1    906.      1 2.02e13   906. Primary fa…\n 4  2020    137 March 2.02e13        1   1493.      1 2.02e13  1493. Nonfamily …\n 5  2020    359 March 2.02e13        1    863.      1 2.02e13   863. Primary fa…\n 6  2020    372 March 2.02e13        1   1338.      1 2.02e13  1338. Primary fa…\n 7  2020    404 March 0              1    677.      1 0         677. Primary fa…\n 8  2020    420 March 2.02e13        1    747.      1 2.02e13   747. Primary fa…\n 9  2020    450 March 2.02e13        1   1309.      1 2.02e13  1309. Primary fa…\n10  2020    491 March 0              1   1130.      1 0        1130. Primary fa…\n# ℹ 5,541 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nIPUMS CPS contains full documentation with information about pernum and incwage.\n\nExercise\n\nFilter asec to rows with month equal to \"March\".\nFilter asec to rows with inctot less than 999999999.\nFilter asec to rows with pernum equal to 3 and inctot less than 999999999.\n\n\\[\\cdots\\]\n###4. arrange()\narrange() sorts the rows of a data frame in alpha-numeric order based on the values of a variable or variables. The dataframe is sorted by the first variable first and each subsequent variable is used to break ties. desc() is used to reverse the sort order for a given variable.\n\n# sort pernum is descending order because high pernums are interesting\narrange(.data = asec, desc(pernum))\n\n# A tibble: 157,959 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020  91430 March 0              1    505.     16 0         604. Secondary …\n 2  2020  91430 March 0              1    505.     15 0         465. Secondary …\n 3  2020  91430 March 0              1    505.     14 0         416. Secondary …\n 4  2020  15037 March 2.02e13        1   2272.     13 2.02e13  2633. Primary fa…\n 5  2020  78495 March 0              1   1279.     13 0        1424. Related su…\n 6  2020  91430 March 0              1    505.     13 0         465. Secondary …\n 7  2020  15037 March 2.02e13        1   2272.     12 2.02e13  1689. Primary fa…\n 8  2020  18102 March 0              1   2468.     12 0        2871. Primary fa…\n 9  2020  22282 March 0              1   2801.     12 0        3879. Related su…\n10  2020  30274 March 2.02e13        1    653.     12 2.02e13   858. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\n####Exercise\n\nSort asec in descending order by pernum and ascending order by inctot.\n\n\\[\\cdots\\]\n###5. mutate()\nmutate() creates new variables or edits existing variables. We can use arithmetic arguments, such as +, -, *, /, and ^. We can also custom functions and functions from packages. For example, we can use library(stringr) for string manipulation and library(lubridate) for date manipulation.\nVariables are created by adding a new column name, like inctot_adjusted, to the left of = in mutate().\n\n# adjust inctot for underreporting\nmutate(.data = asec, inctot_adjusted = inctot * 1.1)\n\n# A tibble: 157,959 × 18\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 8 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;, inctot_adjusted &lt;dbl&gt;\n\n\nVariables are edited by including an existing column name, like inctot, to the left of = in mutate().\n\n# adjust income because of underreporting\nmutate(.data = asec, inctot = inctot * 1.1)\n\n# A tibble: 157,959 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nConditional logic inside of mutate() with functions like if_else() and case_when() is key to mastering data munging in R.\n####Exercise\n\nCreate a new variable called in_poverty. If offtotval is less than offcutoff then use \"Below Poverty Line\". Otherwise, use \"Above Poverty Line\". Hint: if_else() is useful and works like the IF command in Microsoft Excel.\n\n\\[\\cdots\\]\n###|&gt;\nData munging is tiring when each operation needs to be assigned to a name with &lt;-. The pipe, |&gt;, allows lines of code to be chained together so the assignment operator only needs to be used once.\n|&gt; passes the output from function as the first argument in a subsequent function. For example, this line can be rewritten:\nLegacy R code may use %&gt;%, the pipe from the magrittr package. It was (and remains) wildly popular, particularly in the tidyverse framework. Due to this popularity, base R incorporated a similar concept in the base pipe. In many cases, these pipes work the same way, but there are some differences. Because |&gt; is new and continues to be developed, developers have increased |&gt;’s abilities over time. To see a list of key differences between %&gt;% and |&gt;, see this blog.\n\n# old way\nmutate(.data = asec, inctot_adjusted = inctot * 1.1)\n\n# new way\nasec %&gt;%\n  mutate(inctot_adjusted = inctot * 1.1)\n\nSee the power:\n\nnew_asec &lt;- asec %&gt;%\n  filter(pernum == 1) %&gt;%\n  select(year, month, pernum, inctot) %&gt;%\n  mutate(inctot_adjusted = inctot * 1.1) %&gt;%\n  select(-inctot)\n\nnew_asec\n\n# A tibble: 60,460 × 4\n    year month pernum inctot_adjusted\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;           &lt;dbl&gt;\n 1  2020 March      1          57750 \n 2  2020 March      1          48400 \n 3  2020 March      1          44002.\n 4  2020 March      1              0 \n 5  2020 March      1            671 \n 6  2020 March      1          19279.\n 7  2020 March      1          12349.\n 8  2020 March      1          21589.\n 9  2020 March      1          47306.\n10  2020 March      1          10949.\n# ℹ 60,450 more rows\n\n\n###6. summarize()\nsummarize() collapses many rows in a dataframe into fewer rows with summary statistics of the many rows. n(), mean(), and sum() are common summary statistics. Renaming is useful with summarize()!\n\n# summarize without renaming the statistics\nasec %&gt;%\n  summarize(mean(ftotval), mean(inctot))\n\n# A tibble: 1 × 2\n  `mean(ftotval)` `mean(inctot)`\n            &lt;dbl&gt;          &lt;dbl&gt;\n1         105254.     209921275.\n\n# summarize and rename the statistics\nasec %&gt;%\n  summarize(mean_ftotval = mean(ftotval), mean_inctot = mean(inctot))\n\n# A tibble: 1 × 2\n  mean_ftotval mean_inctot\n         &lt;dbl&gt;       &lt;dbl&gt;\n1      105254.  209921275.\n\n\nsummarize() returns a data frame. This means all dplyr functions can be used on the output of summarize(). This is powerful! Manipulating summary statistics in Stata and SAS can be a chore. Here, it’s just another dataframe that can be manipulated with a tool set optimized for dataframes: dplyr.\n###7. group_by()\ngroup_by() groups a dataframe based on specified variables. summarize() with grouped dataframes creates subgroup summary statistics. mutate() with group_by() calculates grouped summaries for each row.\n\nasec %&gt;%\n  group_by(pernum) %&gt;%\n  summarize(\n    n = n(),\n    mean_ftotval = mean(ftotval), \n    mean_inctot = mean(inctot)\n  )\n\n# A tibble: 16 × 4\n   pernum     n mean_ftotval mean_inctot\n    &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 60460       94094.      57508.\n 2      2 45151      108700.   77497357.\n 3      3 25650      117966.  473030618.\n 4      4 15797      121815.  634999933.\n 5      5  6752      108609.  691504650.\n 6      6  2582       89448.  682810446.\n 7      7   922       78889.  682218196.\n 8      8   353       72284.  682725646.\n 9      9   158       54599.  632917559.\n10     10    73       58145.  657543632.\n11     11    37       61847.  702708584 \n12     12    18       50249.  777780725.\n13     13     3       25152   666666666 \n14     14     1       18000       18000 \n15     15     1       25000       25000 \n16     16     1       15000       15000 \n\n\nDataframes can be grouped by multiple variables.\nGrouped tibbles include metadata about groups. For example, Groups:   pernum, offpov [40]. One grouping is dropped each time summarize() is used. It is easy to forget if a dataframe is grouped, so it is safe to include ungroup() at the end of a section of functions.\n\nasec %&gt;%\n  group_by(pernum, offpov) %&gt;%\n  summarize(\n    n = n(),\n    mean_ftotval = mean(ftotval), \n    mean_inctot = mean(inctot)\n  ) %&gt;%\n  arrange(offpov) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'pernum'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 40 × 5\n   pernum offpov                 n mean_ftotval mean_inctot\n    &lt;dbl&gt; &lt;chr&gt;              &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 Above Poverty Line 53872      104451.      63642.\n 2      2 Above Poverty Line 40978      118691.   59082162.\n 3      3 Above Poverty Line 23052      129891.  463440562.\n 4      4 Above Poverty Line 14076      135039.  631720097.\n 5      5 Above Poverty Line  5805      123937.  688206447.\n 6      6 Above Poverty Line  2118      105867.  683199297.\n 7      7 Above Poverty Line   724       96817.  697520661.\n 8      8 Above Poverty Line   269       90328.  672870019.\n 9      9 Above Poverty Line   114       70438.  622815186.\n10     10 Above Poverty Line    57       71483.  666678408.\n# ℹ 30 more rows\n\n\n####Exercise\n\nfilter() to only include observations with \"In Poverty Universe\" in offpovuniv.\ngroup_by() offpov.\nUse summarize() and n() to count the number of observations in poverty.\n\n####Exercise\n\nfilter() to only include observations with \"In Poverty Universe\".\ngroup_by() cpsid.\nUse mutate(family_size = n()) to calculate the family size for each observation in asec.\nungroup()\nCreate a new variable called in_poverty. If offtotval is less than offcutoff then use \"Below Poverty Line\". Otherwise, use \"Above Poverty Line\".\ngroup_by() family_size, offpov, and in_poverty\nUse summarize() and n() to see if you get the same result for offpov and in_poverty. You should only get two rows per family size if your poverty calculation is correct.\n\n\noffcutoff comes from Census Bureau poverty tables with 48 unique thresholds based on family composition. Do not confuse the tables with HHS poverty tables.\nThese data come from IPUMS CPS. IPUMS has cleaned and pre-processed the data to include variables like offcutoff.\n\nAre the estimates from the previous two exercises correct?\nLet’s look at a Census Report to see how many people were in poverty in 2019. We estimated about 16,500 people. The Census Bureau says 34.0 million people.\nNo! We did not account for sampling weights, so our estimates are incorrect. Assignment 3 will demonstrate how to incorporate sampling weights into an analysis.\n\\[\\cdots\\]\n###BONUS: count()\ncount() is a shortcut to df %&gt;% group_by(var) %&gt;% summarize(n()). count() counts the number of observations with a level of a variable or levels of several variables. It is too useful to skip:\n\ncount(asec, pernum)\n\n# A tibble: 16 × 2\n   pernum     n\n    &lt;dbl&gt; &lt;int&gt;\n 1      1 60460\n 2      2 45151\n 3      3 25650\n 4      4 15797\n 5      5  6752\n 6      6  2582\n 7      7   922\n 8      8   353\n 9      9   158\n10     10    73\n11     11    37\n12     12    18\n13     13     3\n14     14     1\n15     15     1\n16     16     1\n\n\n\ncount(x = asec, pernum, offpov)\n\n# A tibble: 40 × 3\n   pernum offpov                 n\n    &lt;dbl&gt; &lt;chr&gt;              &lt;int&gt;\n 1      1 Above Poverty Line 53872\n 2      1 Below Poverty Line  6588\n 3      2 Above Poverty Line 40978\n 4      2 Below Poverty Line  4156\n 5      2 NIU                   17\n 6      3 Above Poverty Line 23052\n 7      3 Below Poverty Line  2527\n 8      3 NIU                   71\n 9      4 Above Poverty Line 14076\n10      4 Below Poverty Line  1648\n# ℹ 30 more rows\n\n\n\n##Mutating joins\nMutating joins join one dataframe to columns from another dataframe by matching values common in both dataframes. The syntax is derived from Structured Query Language (SQL).\nEach function requires an x (or left) dataframe, a y (or right) data frame, and by variables that exist in both dataframes. Note that below we’re creating dataframes using the tribble() function, which creates a tibble using a row-wise layout.\n\nmath_scores &lt;- tribble(\n  ~name, ~math_score,\n  \"Alec\", 95,\n  \"Bart\", 97,\n  \"Carrie\", 100\n)\n\nreading_scores &lt;- tribble(\n  ~name, ~reading_score,\n  \"Alec\", 88,\n  \"Bart\", 67,\n  \"Carrie\", 100,\n  \"Zeta\", 100\n)\n\n###left_join()\nleft_join() matches observations from the y dataframe to the x dataframe. It only keeps observations from the y data frame that have a match in the x dataframe.\n\nleft_join(x = math_scores, y = reading_scores, by = \"name\")\n\n# A tibble: 3 × 3\n  name   math_score reading_score\n  &lt;chr&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1 Alec           95            88\n2 Bart           97            67\n3 Carrie        100           100\n\n\nObservations that exist in the x (left) dataframe but not in the y (right) dataframe result in NAs.\n\nleft_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 4 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n4 Zeta             100         NA\n\n\n###inner_join()\ninner_join() matches observations from the y dataframe to the x dataframe. It only keeps observations from either data frame that have a match.\n\ninner_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 3 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n\n\n###full_join()\nfull_join() matches observations from the y dataframe to the x dataframe. It keeps observations from both dataframes.\n\nfull_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 4 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n4 Zeta             100         NA\n\n\n###anti_join()\nanti_join() returns all rows from x where there are not matching values in y. anti_join() complements inner_join(). Together, they should exhaust the x dataframe.\n\nanti_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 1 × 2\n  name  reading_score\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Zeta            100\n\n\nThe Combine Tables column in the Data Transformation Cheat Sheet is an invaluable resource for navigating joins. The “column matching for joins” section of that cheat sheet outlines how to join tables by matching on multiple columns or match on columns with different names in each table.\n##readr\nreadr is a core tidyverse package for reading and parsing rectangular data from text files (.csv, .tsv, etc.). read_csv() reads .csv files and has a bevy of advantages versus read.csv(). We recommend never using read.csv().\nMany .csvs can be read without issue with simple syntax read_csv(file = \"relative/path/to/data\").\nreadr and read_csv() have powerful tools for resolving parsing issues. More can be learned in the data import section in R4DS.\n##readxl\nreadxl is a tidyverse package for reading data from Microsoft Excel files. It is not a core tidyverse package so it needs to be explicitly loaded in each R session.\nThe tidyverse website has a good tutorial on readxl.\n##Next Skills\n\nacross() can be used with library(dplyr) functions such as summarise() and mutate() to apply the same transformations to multiple columns. For example, it can be used to calculate the mean of many columns with summarize(). across() uses the same tidyselect select language and helpers discussed earlier to select the columns to transform.\npivot_wider() and pivot_longer() can be used to switch between wide and long formats of the data. This is important for tidying data and data visualization.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#arrange",
    "href": "02_tidyverse.html#arrange",
    "title": "2  Introduction to the tidyverse",
    "section": "6.4 4. arrange()",
    "text": "6.4 4. arrange()\narrange() sorts the rows of a data frame in alpha-numeric order based on the values of a variable or variables. The dataframe is sorted by the first variable first and each subsequent variable is used to break ties. desc() is used to reverse the sort order for a given variable.\n\n# sort pernum is descending order because high pernums are interesting\narrange(.data = asec, desc(pernum))\n\n# A tibble: 157,959 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020  91430 March 0              1    505.     16 0         604. Secondary …\n 2  2020  91430 March 0              1    505.     15 0         465. Secondary …\n 3  2020  91430 March 0              1    505.     14 0         416. Secondary …\n 4  2020  15037 March 2.02e13        1   2272.     13 2.02e13  2633. Primary fa…\n 5  2020  78495 March 0              1   1279.     13 0        1424. Related su…\n 6  2020  91430 March 0              1    505.     13 0         465. Secondary …\n 7  2020  15037 March 2.02e13        1   2272.     12 2.02e13  1689. Primary fa…\n 8  2020  18102 March 0              1   2468.     12 0        2871. Primary fa…\n 9  2020  22282 March 0              1   2801.     12 0        3879. Related su…\n10  2020  30274 March 2.02e13        1    653.     12 2.02e13   858. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\n\n6.4.1 Exercise\n\nSort asec in descending order by pernum and ascending order by inctot.\n\n\\[\\cdots\\]",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#mutate",
    "href": "02_tidyverse.html#mutate",
    "title": "2  Introduction to the tidyverse",
    "section": "6.5 5. mutate()",
    "text": "6.5 5. mutate()\nmutate() creates new variables or edits existing variables. We can use arithmetic arguments, such as +, -, *, /, and ^. We can also custom functions and functions from packages. For example, we can use library(stringr) for string manipulation and library(lubridate) for date manipulation.\nVariables are created by adding a new column name, like inctot_adjusted, to the left of = in mutate().\n\n# adjust inctot for underreporting\nmutate(.data = asec, inctot_adjusted = inctot * 1.1)\n\n# A tibble: 157,959 × 18\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 8 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;, inctot_adjusted &lt;dbl&gt;\n\n\nVariables are edited by including an existing column name, like inctot, to the left of = in mutate().\n\n# adjust income because of underreporting\nmutate(.data = asec, inctot = inctot * 1.1)\n\n# A tibble: 157,959 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nConditional logic inside of mutate() with functions like if_else() and case_when() is key to mastering data munging in R.\n\n6.5.1 Exercise\n\nCreate a new variable called in_poverty. If offtotval is less than offcutoff then use \"Below Poverty Line\". Otherwise, use \"Above Poverty Line\". Hint: if_else() is useful and works like the IF command in Microsoft Excel.\n\n\\[\\cdots\\]",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#section",
    "href": "02_tidyverse.html#section",
    "title": "2  Introduction to the tidyverse",
    "section": "6.6 |>",
    "text": "6.6 |&gt;\nData munging is tiring when each operation needs to be assigned to a name with &lt;-. The pipe, |&gt;, allows lines of code to be chained together so the assignment operator only needs to be used once.\n|&gt; passes the output from function as the first argument in a subsequent function. For example, this line can be rewritten:\nLegacy R code may use %&gt;%, the pipe from the magrittr package. It was (and remains) wildly popular, particularly in the tidyverse framework. Due to this popularity, base R incorporated a similar concept in the base pipe. In many cases, these pipes work the same way, but there are some differences. Because |&gt; is new and continues to be developed, developers have increased |&gt;’s abilities over time. To see a list of key differences between %&gt;% and |&gt;, see this blog.\n\n# old way\nmutate(.data = asec, inctot_adjusted = inctot * 1.1)\n\n# new way\nasec %&gt;%\n  mutate(inctot_adjusted = inctot * 1.1)\n\nSee the power:\n\nnew_asec &lt;- asec %&gt;%\n  filter(pernum == 1) %&gt;%\n  select(year, month, pernum, inctot) %&gt;%\n  mutate(inctot_adjusted = inctot * 1.1) %&gt;%\n  select(-inctot)\n\nnew_asec\n\n# A tibble: 60,460 × 4\n    year month pernum inctot_adjusted\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;           &lt;dbl&gt;\n 1  2020 March      1          57750 \n 2  2020 March      1          48400 \n 3  2020 March      1          44002.\n 4  2020 March      1              0 \n 5  2020 March      1            671 \n 6  2020 March      1          19279.\n 7  2020 March      1          12349.\n 8  2020 March      1          21589.\n 9  2020 March      1          47306.\n10  2020 March      1          10949.\n# ℹ 60,450 more rows",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#summarize",
    "href": "02_tidyverse.html#summarize",
    "title": "2  Introduction to the tidyverse",
    "section": "6.7 6. summarize()",
    "text": "6.7 6. summarize()\nsummarize() collapses many rows in a dataframe into fewer rows with summary statistics of the many rows. n(), mean(), and sum() are common summary statistics. Renaming is useful with summarize()!\n\n# summarize without renaming the statistics\nasec %&gt;%\n  summarize(mean(ftotval), mean(inctot))\n\n# A tibble: 1 × 2\n  `mean(ftotval)` `mean(inctot)`\n            &lt;dbl&gt;          &lt;dbl&gt;\n1         105254.     209921275.\n\n# summarize and rename the statistics\nasec %&gt;%\n  summarize(mean_ftotval = mean(ftotval), mean_inctot = mean(inctot))\n\n# A tibble: 1 × 2\n  mean_ftotval mean_inctot\n         &lt;dbl&gt;       &lt;dbl&gt;\n1      105254.  209921275.\n\n\nsummarize() returns a data frame. This means all dplyr functions can be used on the output of summarize(). This is powerful! Manipulating summary statistics in Stata and SAS can be a chore. Here, it’s just another dataframe that can be manipulated with a tool set optimized for dataframes: dplyr.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#group_by",
    "href": "02_tidyverse.html#group_by",
    "title": "2  Introduction to the tidyverse",
    "section": "6.8 7. group_by()",
    "text": "6.8 7. group_by()\ngroup_by() groups a dataframe based on specified variables. summarize() with grouped dataframes creates subgroup summary statistics. mutate() with group_by() calculates grouped summaries for each row.\n\nasec %&gt;%\n  group_by(pernum) %&gt;%\n  summarize(\n    n = n(),\n    mean_ftotval = mean(ftotval), \n    mean_inctot = mean(inctot)\n  )\n\n# A tibble: 16 × 4\n   pernum     n mean_ftotval mean_inctot\n    &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 60460       94094.      57508.\n 2      2 45151      108700.   77497357.\n 3      3 25650      117966.  473030618.\n 4      4 15797      121815.  634999933.\n 5      5  6752      108609.  691504650.\n 6      6  2582       89448.  682810446.\n 7      7   922       78889.  682218196.\n 8      8   353       72284.  682725646.\n 9      9   158       54599.  632917559.\n10     10    73       58145.  657543632.\n11     11    37       61847.  702708584 \n12     12    18       50249.  777780725.\n13     13     3       25152   666666666 \n14     14     1       18000       18000 \n15     15     1       25000       25000 \n16     16     1       15000       15000 \n\n\nDataframes can be grouped by multiple variables.\nGrouped tibbles include metadata about groups. For example, Groups:   pernum, offpov [40]. One grouping is dropped each time summarize() is used. It is easy to forget if a dataframe is grouped, so it is safe to include ungroup() at the end of a section of functions.\n\nasec %&gt;%\n  group_by(pernum, offpov) %&gt;%\n  summarize(\n    n = n(),\n    mean_ftotval = mean(ftotval), \n    mean_inctot = mean(inctot)\n  ) %&gt;%\n  arrange(offpov) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'pernum'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 40 × 5\n   pernum offpov                 n mean_ftotval mean_inctot\n    &lt;dbl&gt; &lt;chr&gt;              &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 Above Poverty Line 53872      104451.      63642.\n 2      2 Above Poverty Line 40978      118691.   59082162.\n 3      3 Above Poverty Line 23052      129891.  463440562.\n 4      4 Above Poverty Line 14076      135039.  631720097.\n 5      5 Above Poverty Line  5805      123937.  688206447.\n 6      6 Above Poverty Line  2118      105867.  683199297.\n 7      7 Above Poverty Line   724       96817.  697520661.\n 8      8 Above Poverty Line   269       90328.  672870019.\n 9      9 Above Poverty Line   114       70438.  622815186.\n10     10 Above Poverty Line    57       71483.  666678408.\n# ℹ 30 more rows\n\n\n\n6.8.1 Exercise\n\nfilter() to only include observations with \"In Poverty Universe\" in offpovuniv.\ngroup_by() offpov.\nUse summarize() and n() to count the number of observations in poverty.\n\n\n\n6.8.2 Exercise\n\nfilter() to only include observations with \"In Poverty Universe\".\ngroup_by() cpsid.\nUse mutate(family_size = n()) to calculate the family size for each observation in asec.\nungroup()\nCreate a new variable called in_poverty. If offtotval is less than offcutoff then use \"Below Poverty Line\". Otherwise, use \"Above Poverty Line\".\ngroup_by() family_size, offpov, and in_poverty\nUse summarize() and n() to see if you get the same result for offpov and in_poverty. You should only get two rows per family size if your poverty calculation is correct.\n\n\noffcutoff comes from Census Bureau poverty tables with 48 unique thresholds based on family composition. Do not confuse the tables with HHS poverty tables.\nThese data come from IPUMS CPS. IPUMS has cleaned and pre-processed the data to include variables like offcutoff.\n\nAre the estimates from the previous two exercises correct?\nLet’s look at a Census Report to see how many people were in poverty in 2019. We estimated about 16,500 people. The Census Bureau says 34.0 million people.\nNo! We did not account for sampling weights, so our estimates are incorrect. Assignment 3 will demonstrate how to incorporate sampling weights into an analysis.\n\\[\\cdots\\]",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#bonus-count",
    "href": "02_tidyverse.html#bonus-count",
    "title": "2  Introduction to the tidyverse",
    "section": "6.9 BONUS: count()",
    "text": "6.9 BONUS: count()\ncount() is a shortcut to df %&gt;% group_by(var) %&gt;% summarize(n()). count() counts the number of observations with a level of a variable or levels of several variables. It is too useful to skip:\n\ncount(asec, pernum)\n\n# A tibble: 16 × 2\n   pernum     n\n    &lt;dbl&gt; &lt;int&gt;\n 1      1 60460\n 2      2 45151\n 3      3 25650\n 4      4 15797\n 5      5  6752\n 6      6  2582\n 7      7   922\n 8      8   353\n 9      9   158\n10     10    73\n11     11    37\n12     12    18\n13     13     3\n14     14     1\n15     15     1\n16     16     1\n\n\n\ncount(x = asec, pernum, offpov)\n\n# A tibble: 40 × 3\n   pernum offpov                 n\n    &lt;dbl&gt; &lt;chr&gt;              &lt;int&gt;\n 1      1 Above Poverty Line 53872\n 2      1 Below Poverty Line  6588\n 3      2 Above Poverty Line 40978\n 4      2 Below Poverty Line  4156\n 5      2 NIU                   17\n 6      3 Above Poverty Line 23052\n 7      3 Below Poverty Line  2527\n 8      3 NIU                   71\n 9      4 Above Poverty Line 14076\n10      4 Below Poverty Line  1648\n# ℹ 30 more rows",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#left_join",
    "href": "02_tidyverse.html#left_join",
    "title": "2  Introduction to the tidyverse",
    "section": "7.1 left_join()",
    "text": "7.1 left_join()\nleft_join() matches observations from the y dataframe to the x dataframe. It only keeps observations from the y data frame that have a match in the x dataframe.\n\nleft_join(x = math_scores, y = reading_scores, by = \"name\")\n\n# A tibble: 3 × 3\n  name   math_score reading_score\n  &lt;chr&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1 Alec           95            88\n2 Bart           97            67\n3 Carrie        100           100\n\n\nObservations that exist in the x (left) dataframe but not in the y (right) dataframe result in NAs.\n\nleft_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 4 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n4 Zeta             100         NA",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#inner_join",
    "href": "02_tidyverse.html#inner_join",
    "title": "2  Introduction to the tidyverse",
    "section": "7.2 inner_join()",
    "text": "7.2 inner_join()\ninner_join() matches observations from the y dataframe to the x dataframe. It only keeps observations from either data frame that have a match.\n\ninner_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 3 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#full_join",
    "href": "02_tidyverse.html#full_join",
    "title": "2  Introduction to the tidyverse",
    "section": "7.3 full_join()",
    "text": "7.3 full_join()\nfull_join() matches observations from the y dataframe to the x dataframe. It keeps observations from both dataframes.\n\nfull_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 4 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n4 Zeta             100         NA",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#anti_join",
    "href": "02_tidyverse.html#anti_join",
    "title": "2  Introduction to the tidyverse",
    "section": "7.4 anti_join()",
    "text": "7.4 anti_join()\nanti_join() returns all rows from x where there are not matching values in y. anti_join() complements inner_join(). Together, they should exhaust the x dataframe.\n\nanti_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 1 × 2\n  name  reading_score\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Zeta            100\n\n\nThe Combine Tables column in the Data Transformation Cheat Sheet is an invaluable resource for navigating joins. The “column matching for joins” section of that cheat sheet outlines how to join tables by matching on multiple columns or match on columns with different names in each table.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html",
    "href": "03_advanced-data-cleaning.html",
    "title": "3  Advanced Data Cleaning",
    "section": "",
    "text": "3.1 Review\nR for Data Science (2e) displays the first steps of the data science process as “Import”, “Tidy”, and “Transform”. DSPP1 introduced important techniques for importing data like read_csv() and querying web APIs, for tidying data like pivot_longer(), and for transforming data like mutate().",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#sec-review2",
    "href": "03_advanced-data-cleaning.html#sec-review2",
    "title": "3  Advanced Data Cleaning",
    "section": "",
    "text": "Exercise 1\n\n\n\n\nUse mutate() and case_when() to add a new variable called speed_cat to cars where the values are \"slow\" when speed &lt; 10, \"moderate\" when speed &lt; 20, and \"fast\" otherwise.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#sec-import",
    "href": "03_advanced-data-cleaning.html#sec-import",
    "title": "3  Advanced Data Cleaning",
    "section": "3.2 Import",
    "text": "3.2 Import\n\n3.2.1 library(here)\nDeveloping Quarto documents in subdirectories is a pain. When interactively running code in the console, file paths are read as if the .qmd file is in the same folder as the .Rproj. When clicking render, paths are treated as if they are in the subdirectory where the .qmd file is.\nlibrary(here) resolves headaches around file referencing in project-oriented workflows.\nLoading library(here) will print your working directory.\n\nlibrary(here)\n\nhere() starts at /Users/gabemorrison/Documents/Github/data-science-for-public-policy2\n\n\nAfter this, here() will use reasonable heuristics to find project files using relative file paths. When placing Quarto documents in a directory below the top-level directory, use here() and treat each folder and file as a different string.\nBefore\n\nread_csv(\"data/raw/important-data.csv\")\n\nAfter\n\nread_csv(here(\"data\", \"raw\", \"important-data.csv\"))\n\n\n\n3.2.2 library(readxl)\nWe will focus on reading data from Excel workbooks. Excel is a bad tool with bad design that has led to many analytical errors. Unfortunately, it’s a dominant tool for storing data and often enters the data science workflow.\nlibrary(readxl) is the premier package for reading data from .xls and .xlsx files. read_excel(), which works like read_csv(), loads data from .xls and .xlsx files. Consider data from the Urban Institute’s Debt in America feature accessed through the Urban Institute Data Catalog.\n\nlibrary(readxl)\n\nread_excel(here(\"data\", \"state_dia_delinquency_ 7 Jun 2022.xlsx\"))\n\n# A tibble: 51 × 28\n   fips  state_name          state Share with Any Debt …¹ Share with Any Debt …²\n   &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;                  &lt;chr&gt;                 \n 1 01    Alabama             AL    .3372881               .5016544              \n 2 02    Alaska              AK    .1672429               .221573               \n 3 04    Arizona             AZ    .2666938               .3900013              \n 4 05    Arkansas            AR    .3465793               .5426918              \n 5 06    California          CA    .2087713               .2462195              \n 6 08    Colorado            CO    .213803                .3554938              \n 7 09    Connecticut         CT    .2194708               .3829038              \n 8 10    Delaware            DE    .2866829               .469117               \n 9 11    District of Columb… DC    .2232908               .3485817              \n10 12    Florida             FL    .2893825               .3439322              \n# ℹ 41 more rows\n# ℹ abbreviated names: ¹​`Share with Any Debt in Collections, All`,\n#   ²​`Share with Any Debt in Collections, Communities of Color`\n# ℹ 23 more variables:\n#   `Share with Any Debt in Collections, Majority White Communities` &lt;chr&gt;,\n#   `Median Debt in Collections, All` &lt;chr&gt;,\n#   `Median Debt in Collections, Communities of Color` &lt;chr&gt;, …\n\n\nread_excel() has several useful arguments:\n\nsheet selects the sheet to read.\nrange selects the cells to read and can use Excel-style ranges like “C34:D50”.\nskip skips the selected number of rows.\nn_max selects the maximum number of rows to read.\n\nExcel encourages bad habits and untidy data, so these arguments are useful for extracting data from messy Excel workbooks.\nreadxl_example() contains a perfect example. The workbook contains two sheets, which we can see with excel_sheets().\n\nreadxl_example(\"clippy.xlsx\") |&gt;\n  excel_sheets()\n\n[1] \"list-column\"    \"two-row-header\"\n\n\nAs is common with many Excel workbooks, the second sheet contains a second row of column names with parenthetical comments about each column.1\n\nreadxl_example(\"clippy.xlsx\") |&gt;  \n  read_excel(sheet = \"two-row-header\")\n\n# A tibble: 2 × 4\n  name       species              death                 weight    \n  &lt;chr&gt;      &lt;chr&gt;                &lt;chr&gt;                 &lt;chr&gt;     \n1 (at birth) (office supply type) (date is approximate) (in grams)\n2 Clippy     paperclip            39083                 0.9       \n\n\nThis vignette suggests a simple solution to this problem.\n\n# extract the column names\ncol_names &lt;- readxl_example(\"clippy.xlsx\") |&gt;  \n  read_excel(sheet = \"two-row-header\", n_max = 0) |&gt;\n  names()\n\n# load the data and add the column names\nreadxl_example(\"clippy.xlsx\") |&gt;  \n    read_excel(\n      sheet = \"two-row-header\", \n      skip = 2,\n      col_names = col_names\n    )\n\n# A tibble: 1 × 4\n  name   species   death               weight\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dttm&gt;               &lt;dbl&gt;\n1 Clippy paperclip 2007-01-01 00:00:00    0.9\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nThe IRS Statistics of Income Division is one of the US’s 13 principal statistical agencies. They publish rich information derived from tax returns. We will focus on Table 1, Adjusted Gross Income (AGI) percentiles by state.\n\nRead in the 52 cells in the first column that contain “United States”, all 50 states, and the “District of Columbia”.\nIdentify the cells containing data for “Adjusted gross income floor on percentiles”. Read in the data with read_excel(). Either programmatically read in the column names (i.e. “Top 1 Percent”, …) or assign them with col_names().\nUse bind_cols() to combine the data from step 1 and step 2.\n\n\n\nlibrary(tidyxl) contains tools for working with messy Excel workbooks, library(openxlsx) contains tools for creating Excel workbooks with R, and library(googlesheets4) contains tools for working with Google Sheets.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#sec-tidy",
    "href": "03_advanced-data-cleaning.html#sec-tidy",
    "title": "3  Advanced Data Cleaning",
    "section": "3.3 Tidy",
    "text": "3.3 Tidy\nThe defining opinion of the tidyverse is its wholehearted adoption of tidy data. Tidy data has three features:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a dataframe.\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. ~ Hadley Wickham\n\nlibrary(tidyr) is the main package for tidying untidy data. We’ll practice some skills using examples from three workbooks from the IRS SOI.\npivot_longer() is commonly used for tidying data and for making data longer for library(ggplot2). pivot_longer() reorients data so that key-value pairs expressed as column name-column value are column value-column value in adjacent columns. pivot_longer() has three essential arguments:\n\ncols is a vector of columns to pivot (or not pivot).\nnames_to is a string for the name of the column where the old column names will go (i.e. “series” in the figure).\nvalues_to is a string for the name of the column where the values will go (i.e. “rate” in the figure).\n\n\n\n\n\n\n\n\n\n\npivot_wider() is the inverse of pivot_longer().\n\nTidying Example 1\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable1 &lt;- tribble(\n  ~state, ~agi2006, ~agi2016, ~agi2020,\n  \"Alabama\", 95067, 114510, 138244,\n  \"Alaska\", 17458, 23645, 26445,\n  \"Arizona\", 146307, 181691, 245258\n)\n\ntable1\n\n# A tibble: 3 × 4\n  state   agi2006 agi2016 agi2020\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Alabama   95067  114510  138244\n2 Alaska    17458   23645   26445\n3 Arizona  146307  181691  245258\n\n\n\n\nYear is a variable. This data is untidy because year is included in the column names.\n\ntable1 &lt;- tribble(\n  ~state, ~agi2006, ~agi2016, ~agi2020,\n  \"Alabama\", 95067, 114510, 138244,\n  \"Alaska\", 17458, 23645, 26445,\n  \"Arizona\", 146307, 181691, 245258\n)\n\ntable1\n\n# A tibble: 3 × 4\n  state   agi2006 agi2016 agi2020\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Alabama   95067  114510  138244\n2 Alaska    17458   23645   26445\n3 Arizona  146307  181691  245258\n\npivot_longer(\n  data = table1, \n  cols = -state, \n  names_to = \"year\", \n  values_to = \"agi\"\n)\n\n# A tibble: 9 × 3\n  state   year       agi\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama agi2006  95067\n2 Alabama agi2016 114510\n3 Alabama agi2020 138244\n4 Alaska  agi2006  17458\n5 Alaska  agi2016  23645\n6 Alaska  agi2020  26445\n7 Arizona agi2006 146307\n8 Arizona agi2016 181691\n9 Arizona agi2020 245258\n\n\nThe year column isn’t useful yet. We’ll fix that later.\n\n\n\n\nlibrary(tidyr) contains several functions to split values into multiple cells.\n\nseparate_wider_delim() separates a value based on a delimeter and creates wider data.\nseparate_wider_position() separates a value based on position and creates wider data.\nseparate_longer_delim() separates a value based on a delimeter and creates longer data.\nseparate_longer_position() separates a value based on position and creates longer data.\n\n\n\nTidying Example 2\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable2 &lt;- tribble(\n  ~state, ~`agi2006|2016|2020`,\n  \"Alabama\", \"95067|114510|138244\",\n  \"Alaska\", \"17458|23645|26445\",\n  \"Arizona\", \"146307|181691|245258\"\n)\n\ntable2\n\n# A tibble: 3 × 2\n  state   `agi2006|2016|2020` \n  &lt;chr&gt;   &lt;chr&gt;               \n1 Alabama 95067|114510|138244 \n2 Alaska  17458|23645|26445   \n3 Arizona 146307|181691|245258\n\n\n\n\nThe values for 2006, 2016, and 2020 are all squished into one cell.\n\ntable2 &lt;- tribble(\n  ~state, ~`agi2006|2016|2020`,\n  \"Alabama\", \"95067|114510|138244\",\n  \"Alaska\", \"17458|23645|26445\",\n  \"Arizona\", \"146307|181691|245258\"\n)\n\ntable2\n\n# A tibble: 3 × 2\n  state   `agi2006|2016|2020` \n  &lt;chr&gt;   &lt;chr&gt;               \n1 Alabama 95067|114510|138244 \n2 Alaska  17458|23645|26445   \n3 Arizona 146307|181691|245258\n\nseparate_wider_delim(\n  data = table2, \n  cols = `agi2006|2016|2020`, \n  delim = \"|\",\n  names = c(\"2006\", \"2016\", \"2020\")\n) |&gt;\n  pivot_longer(\n    cols = -state,\n    names_to = \"year\", \n    values_to = \"agi\"\n  )\n\n# A tibble: 9 × 3\n  state   year  agi   \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; \n1 Alabama 2006  95067 \n2 Alabama 2016  114510\n3 Alabama 2020  138244\n4 Alaska  2006  17458 \n5 Alaska  2016  23645 \n6 Alaska  2020  26445 \n7 Arizona 2006  146307\n8 Arizona 2016  181691\n9 Arizona 2020  245258\n\n\n\n\n\n\nbind_rows() combines data frames by stacking the rows.\n\none &lt;- tribble(\n  ~id, ~var,\n  \"1\", 3.14,\n  \"2\", 3.15,\n)\n\ntwo &lt;- tribble(\n  ~id, ~var,\n  \"3\", 3.16,\n  \"4\", 3.17,\n)\n\nbind_rows(one, two)\n\n# A tibble: 4 × 2\n  id      var\n  &lt;chr&gt; &lt;dbl&gt;\n1 1      3.14\n2 2      3.15\n3 3      3.16\n4 4      3.17\n\n\nbind_cols() combines data frames by appending columns.\n\nthree &lt;- tribble(\n  ~id, ~var1,\n  \"1\", 3.14,\n  \"2\", 3.15,\n)\n\nfour &lt;- tribble(\n  ~id, ~var2,\n  \"1\", 3.16,\n  \"2\", 3.17,\n)\n\nbind_cols(three, four)\n\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...3`\n\n\n# A tibble: 2 × 4\n  id...1  var1 id...3  var2\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 1       3.14 1       3.16\n2 2       3.15 2       3.17\n\n\nWhen possible, we recommend using relational joins like left_join() to combine by columns because it is easy to miss-align rows with bind_cols().\n\nleft_join(\n  x = three,\n  y = four,\n  by = \"id\"\n)\n\n# A tibble: 2 × 3\n  id     var1  var2\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1      3.14  3.16\n2 2      3.15  3.17\n\n\n\n\nTidying Example 3\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable3_2006 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", \"95067\",\n  \"Alaska\", \"17458\",\n  \"Arizona\", \"146307\"\n)\n\ntable3_2006\n\n# A tibble: 3 × 2\n  state   agi   \n  &lt;chr&gt;   &lt;chr&gt; \n1 Alabama 95067 \n2 Alaska  17458 \n3 Arizona 146307\n\ntable3_2016 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", \"114510\",\n  \"Alaska\", \"23645\",\n  \"Arizona\", \"181691\"\n)\n\ntable3_2016\n\n# A tibble: 3 × 2\n  state   agi   \n  &lt;chr&gt;   &lt;chr&gt; \n1 Alabama 114510\n2 Alaska  23645 \n3 Arizona 181691\n\ntable3_2020 &lt;- tribble(\n  ~state, ~`agi`,\n  \"Alabama\", \"138244\",\n  \"Alaska\", \"26445\",\n  \"Arizona\", \"245258\"\n)\n\ntable3_2020\n\n# A tibble: 3 × 2\n  state   agi   \n  &lt;chr&gt;   &lt;chr&gt; \n1 Alabama 138244\n2 Alaska  26445 \n3 Arizona 245258\n\n\n\n\nThe variable year is contained in the data set names. The .id argument in bind_rows() allows us to create the year variable.\n\ntable3_2006 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 95067,\n  \"Alaska\", 17458,\n  \"Arizona\", 146307\n)\n\ntable3_2006\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama  95067\n2 Alaska   17458\n3 Arizona 146307\n\ntable3_2016 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 114510,\n  \"Alaska\", 23645,\n  \"Arizona\", 181691\n)\n\ntable3_2016\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama 114510\n2 Alaska   23645\n3 Arizona 181691\n\ntable3_2020 &lt;- tribble(\n  ~state, ~`agi`,\n  \"Alabama\", 138244,\n  \"Alaska\", 26445,\n  \"Arizona\", 245258\n)\n\ntable3_2020\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama 138244\n2 Alaska   26445\n3 Arizona 245258\n\nbind_rows(\n  `2006` = table3_2006,\n  `2016` = table3_2016,\n  `2020` = table3_2020,\n  .id = \"year\"\n)\n\n# A tibble: 9 × 3\n  year  state      agi\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 2006  Alabama  95067\n2 2006  Alaska   17458\n3 2006  Arizona 146307\n4 2016  Alabama 114510\n5 2016  Alaska   23645\n6 2016  Arizona 181691\n7 2020  Alabama 138244\n8 2020  Alaska   26445\n9 2020  Arizona 245258\n\n\n\n\n\n\nRelational joins are fundamental to working with tidy data. Tidy data can only contain one unit of observation (e.g. county or state not county and state). When data exist on multiple levels, they must be stored in separate tables that can later be combined.\n\n\n\n\n\n\nMutating Joins\n\n\n\nMutating joins add new variables to a data frame by matching observations from one data frame to observations in another data frame.\n\n\n\n\n\n\n\n\nFiltering Joins\n\n\n\nFiltering joins drop observations based on the presence of their key (identifier) in another data frame.\nFor example, we may have a list of students in detention and a list of all students. We can use a filtering join to create a list of student not in detention.\n\n\nFor now, we will focus on mutating joins. Let their be two data frames x and y and let both data frames have a key variable that uniquely identifies rows.\n\nleft_join(x, y) appends variables from y on to x but only keeps observations from x.\nfull_join(x, y) appends variables from y on to x and keeps all observations from x and y.\nanti_join(x, y) returns all observations from x without a match in y. anti_join() is traditionally only used for filtering joins, but it is useful for writing tests for mutating joins.\n\nTo learn more, read the Joins chapter of R for Data Science (2e). library(tidylog) is a useful function for monitoring the behavior of joins.\n\n\nTidying Example 4\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable4a &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 95067,\n  \"Alaska\", 17458,\n  \"Arizona\", 146307\n)\n\ntable4a\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama  95067\n2 Alaska   17458\n3 Arizona 146307\n\ntable4b &lt;- tribble(\n  ~state, ~returns,\n  \"Alabama\", 1929941,\n  \"Alaska\", 322369,\n  \"Arizona\", 2454951\n)\n\ntable4b\n\n# A tibble: 3 × 2\n  state   returns\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Alabama 1929941\n2 Alaska   322369\n3 Arizona 2454951\n\n\n\n\nThese data are tidy! But keeping the data in two separate data frames may not make sense. Let’s use full_join() to combine the data and anti_join() to see if there are mismatches.\n\ntable4a &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 95067,\n  \"Alaska\", 17458,\n  \"Arizona\", 146307\n)\n\ntable4a\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama  95067\n2 Alaska   17458\n3 Arizona 146307\n\ntable4b &lt;- tribble(\n  ~state, ~returns,\n  \"Alabama\", 1929941,\n  \"Alaska\", 322369,\n  \"Arizona\", 2454951\n)\n\ntable4b\n\n# A tibble: 3 × 2\n  state   returns\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Alabama 1929941\n2 Alaska   322369\n3 Arizona 2454951\n\nfull_join(table4a, table4b, by = \"state\")\n\n# A tibble: 3 × 3\n  state      agi returns\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 Alabama  95067 1929941\n2 Alaska   17458  322369\n3 Arizona 146307 2454951\n\nanti_join(table4a, table4b, by = \"state\")\n\n# A tibble: 0 × 2\n# ℹ 2 variables: state &lt;chr&gt;, agi &lt;dbl&gt;\n\nanti_join(table4b, table4a, by = \"state\")\n\n# A tibble: 0 × 2\n# ℹ 2 variables: state &lt;chr&gt;, returns &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse pivot_longer() to make the SOI percentile data from the earlier exercise longer. After the transformation, there should be one row per percentile per state.\n\n\n\nTo see more examples, read the tidy data section in R for Data Science (2e)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#sec-transform",
    "href": "03_advanced-data-cleaning.html#sec-transform",
    "title": "3  Advanced Data Cleaning",
    "section": "3.4 Transform",
    "text": "3.4 Transform\n\n3.4.1 Strings\nCheck out the stringr cheat sheet.\nlibrary(stringr) contains powerful functions for working with strings in R. In data analysis, we may need to detect matches, subset strings, work with the lengths of strings, modify strings, and join and split strings.\n\nDetecting Matches\nstr_detect() is useful for detecting matches in strings, which can be useful with filter(). Consider the executive orders data set and suppose we want to return executive orders that contain the word \"Virginia\".\n\neos &lt;- read_csv(here(\"data\", \"executive-orders.csv\")) |&gt;\n  filter(!is.na(text)) |&gt;\n  group_by(executive_order_number) |&gt;\n  summarize(text = list(text)) |&gt;\n  mutate(text = map_chr(text, ~paste(.x, collapse = \" \")))\n\nRows: 196537 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): text, president\ndbl  (1): executive_order_number\ndate (1): signing_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\neos\n\n# A tibble: 1,126 × 2\n   executive_order_number text                                                  \n                    &lt;dbl&gt; &lt;chr&gt;                                                 \n 1                  12890 \"Executive Order 12890 of December 30, 1993 Amendment…\n 2                  12944 \"Executive Order 12944 of December 28, 1994 Adjustmen…\n 3                  12945 \"Executive Order 12945 of January 20, 1995 Amendment …\n 4                  12946 \"Executive Order 12946 of January 20, 1995 President'…\n 5                  12947 \"Executive Order 12947 of January 23, 1995 Prohibitin…\n 6                  12948 \"Executive Order 12948 of January 30, 1995 Amendment …\n 7                  12949 \"Executive Order 12949 of February 9, 1995 Foreign In…\n 8                  12950 \"Executive Order 12950 of February 22, 1995 Establish…\n 9                  12951 \"Executive Order 12951 of February 22, 1995 Release o…\n10                  12952 \"Executive Order 12952 of February 24, 1995 Amendment…\n# ℹ 1,116 more rows\n\neos |&gt;\n  filter(str_detect(string = text, pattern = \"Virginia\"))\n\n# A tibble: 6 × 2\n  executive_order_number text                                                   \n                   &lt;dbl&gt; &lt;chr&gt;                                                  \n1                  13150 Executive Order 13150 of April 21, 2000 Federal Workfo…\n2                  13508 Executive Order 13508 of May 12, 2009 Chesapeake Bay P…\n3                  13557 Executive Order 13557 of November 4, 2010 Providing an…\n4                  13775 Executive Order 13775 of February 9, 2017 Providing an…\n5                  13787 Executive Order 13787 of March 31, 2017 Providing an O…\n6                  13934 Executive Order 13934 of July 3, 2020 Building and Reb…\n\n\n\n\nSubsetting Strings\nstr_sub() can subset strings based on positions within the string. Consider an example where we want to extract state FIPS codes from county FIPS codes.\n\ntibble(fips = c(\"01001\", \"02013\", \"04001\")) |&gt;\n  mutate(state_fips = str_sub(fips, start = 1, end = 2))\n\n# A tibble: 3 × 2\n  fips  state_fips\n  &lt;chr&gt; &lt;chr&gt;     \n1 01001 01        \n2 02013 02        \n3 04001 04        \n\n\n\n\nManaging Lengths\nstr_pad() is useful for managing lengths. Consider the common situation when zeros are dropped from the beginning of FIPS codes.\n\ntibble(fips = c(1, 2, 4)) |&gt;\n  mutate(fips = str_pad(fips, side = \"left\", pad = \"0\", width = 2))\n\n# A tibble: 3 × 1\n  fips \n  &lt;chr&gt;\n1 01   \n2 02   \n3 04   \n\n\n\n\nModifying Strings\nstr_replace(), str_replace_all(), str_remove(), and str_remove_all() can delete or modify parts of strings. Consider an example where we have course names and we want to delete everything except numeric digits.2\n\ntibble(course = c(\"PPOL 670\", \"GOVT 8009\", \"PPOL 6819\")) |&gt;\n  mutate(course = str_remove(course, pattern = \"[:alpha:]*\\\\s\"))\n\n# A tibble: 3 × 1\n  course\n  &lt;chr&gt; \n1 670   \n2 8009  \n3 6819  \n\n\nstr_c() and str_glue() are useful for joining strings. Consider an example where we want to “fill in the blank” with a variable in a data frame.\n\ntibble(fruit = c(\"apple\", \"banana\", \"cantelope\")) |&gt;\n  mutate(sentence = str_glue(\"my favorite fruit is {fruit}\"))\n\n# A tibble: 3 × 2\n  fruit     sentence                      \n  &lt;chr&gt;     &lt;glue&gt;                        \n1 apple     my favorite fruit is apple    \n2 banana    my favorite fruit is banana   \n3 cantelope my favorite fruit is cantelope\n\n\n\ntibble(fruit = c(\"apple\", \"banana\", \"cantelope\")) |&gt;\n  mutate(\n    another_sentence = \n      str_c(\"Who doesn't like a good \", fruit, \".\")\n    )\n\n# A tibble: 3 × 2\n  fruit     another_sentence                  \n  &lt;chr&gt;     &lt;chr&gt;                             \n1 apple     Who doesn't like a good apple.    \n2 banana    Who doesn't like a good banana.   \n3 cantelope Who doesn't like a good cantelope.\n\n\nThis workflow is useful for building up URLs when accessing APIs, scraping information from the Internet, and downloading many files.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nUse mutate() and library(stringr) to create a variable for year from the earlier SOI exercise. For instance, \"agi2006\" should be \"2006\".\nUse as.numeric() to convert the string from step 1 into a numeric value.\nCreate a data visualization with year on the x-axis.\n\n\n\n\n\n\n3.4.2 Factors\nCheck out the forcats cheat sheet.\nMuch of our work focuses on four of the six types of atomic vectors: logical, integer, double, and character. R also contains augmented vectors like factors.\nFactors are categorical data stored as integers with a levels attribute. Character vectors often work well for categorical data and many of R’s functions convert character vectors to factors. This happens with lm().\nFactors have many applications:\n\nGiving the levels of a categorical variable non-alpha numeric order in a ggplot2 data visualization.\nRunning calculations on data with empty groups.\nRepresenting categorical outcome variables in classification models.\n\n\nFactor Basics\n\nx1 &lt;- factor(c(\"a\", \"a\", \"b\", \"c\"), levels = c(\"d\", \"c\", \"b\", \"a\"))\n\nx1\n\n[1] a a b c\nLevels: d c b a\n\nattributes(x1)\n\n$levels\n[1] \"d\" \"c\" \"b\" \"a\"\n\n$class\n[1] \"factor\"\n\nlevels(x1)\n\n[1] \"d\" \"c\" \"b\" \"a\"\n\n\nx1 has order but it isn’t ordinal. Sometimes we’ll come across ordinal factor variables, like with the diamonds data set. Unintentional ordinal variables can cause unexpected errors. For example, including ordinal data as predictors in regression models will lead to different estimated coefficients than other variable types.\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\nx2 &lt;- factor(\n  c(\"a\", \"a\", \"b\", \"c\"), \n  levels = c(\"d\", \"c\", \"b\", \"a\"),\n  ordered = TRUE\n)\n\nx2\n\n[1] a a b c\nLevels: d &lt; c &lt; b &lt; a\n\nattributes(x2)\n\n$levels\n[1] \"d\" \"c\" \"b\" \"a\"\n\n$class\n[1] \"ordered\" \"factor\" \n\nlevels(x2)\n\n[1] \"d\" \"c\" \"b\" \"a\"\n\n\nFigure 3.1 shows how we can use a factor to give a variable a non-alpha numeric order and preserve empty levels. In this case, February and March have zero tropical depressions, tropical storms, and hurricanes and we want to demonstrate that emptiness.\n# use case_match to convert integers into month names\nstorms &lt;- storms |&gt;\n  mutate(\n    month = case_match(\n      month,\n      1 ~ \"Jan\",\n      4 ~ \"Apr\",\n      5 ~ \"May\",\n      6 ~ \"Jun\",\n      7 ~ \"Jul\",\n      8 ~ \"Aug\",\n      9 ~ \"Sep\",\n      10 ~ \"Oct\",\n      11 ~ \"Nov\",\n      12 ~ \"Dec\"\n    )\n  )\n\n# create data viz without factors\nstorms |&gt;\n  count(month) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n# add factor variable\nmonths &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n            \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nstorms &lt;- storms |&gt;\n  mutate(month = factor(month, levels = months)) \n\n# create data viz with factors\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n(a) Figure without a factor\n\n\n\n\n\n\n\n\n\n\n\n(b) Figure with a factor\n\n\n\n\n\n\n\nFigure 3.1: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\nFactors also change the behavior of summary functions like count().\n\nstorms |&gt;\n  count(month)\n\n# A tibble: 10 × 2\n   month     n\n   &lt;fct&gt; &lt;int&gt;\n 1 Jan      70\n 2 Apr      66\n 3 May     201\n 4 Jun     809\n 5 Jul    1651\n 6 Aug    4442\n 7 Sep    7778\n 8 Oct    3138\n 9 Nov    1170\n10 Dec     212\n\nstorms |&gt;\n  count(month, .drop = FALSE)\n\n# A tibble: 12 × 2\n   month     n\n   &lt;fct&gt; &lt;int&gt;\n 1 Jan      70\n 2 Feb       0\n 3 Mar       0\n 4 Apr      66\n 5 May     201\n 6 Jun     809\n 7 Jul    1651\n 8 Aug    4442\n 9 Sep    7778\n10 Oct    3138\n11 Nov    1170\n12 Dec     212\n\n\nlibrary(forcats) simplifies many common operations on factor vectors.\n\n\nChanging Order\nfct_relevel(), fct_rev(), and fct_reorder() are useful functions for modifying the order of factor variables. Figure 3.2 demonstrates using fct_rev() to flip the order of a categorical axis in ggplot2.\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\nstorms |&gt;\n  mutate(month = fct_rev(month)) |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n(a) Descending\n\n\n\n\n\n\n\n\n\n\n\n(b) Ascending\n\n\n\n\n\n\n\nFigure 3.2: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\nFigure 3.3 orders the factor variable based on the number of observations in each category using fct_reorder(). fct_reorder() can order variables based on more sophisticated summaries than just magnitude. For example, it can order box-and-whisker plots based on the median or even something as arbitrary at the 60th percentile.\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  mutate(month = fct_reorder(.f = month, .x = n, .fun = median)) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n(a) Alpha-numeric\n\n\n\n\n\n\n\n\n\n\n\n(b) Magnitude\n\n\n\n\n\n\n\nFigure 3.3: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\n\n\nChanging Values\nFunctions like fct_recode() and fct_lump_min() are useful for changing factor variables. Figure 3.4 combines categories with fewer than 1,000 observations into an \"Other\" group.\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\nstorms |&gt;\n  mutate(month = fct_lump_min(month, min = 1000)) |&gt;  \n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n(a) All\n\n\n\n\n\n\n\n\n\n\n\n(b) Lumped\n\n\n\n\n\n\n\nFigure 3.4: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\n\n\n\n3.4.3 Dates and Date-Times\nCheck out the lubridate cheat sheet.\nThere are many ways to store dates.\n\nMarch 14, 1992\n03/14/1992\n14/03/1992\n14th of March ’92\n\nOne way of storing dates is the best. The ISO 8601 date format is an international standard with appealing properties like fixed lengths and self ordering. The format is YYYY-MM-DD.\nlibrary(lubridate) has useful functions that will take dates of any format and convert them to the ISO 8601 standard.\n\nlibrary(lubridate)\n\nmdy(\"March 14, 1992\")\n\n[1] \"1992-03-14\"\n\nmdy(\"03/14/1992\")\n\n[1] \"1992-03-14\"\n\ndmy(\"14/03/1992\")\n\n[1] \"1992-03-14\"\n\ndmy(\"14th of March '92\")\n\n[1] \"1992-03-14\"\n\n\nThese functions return variables of class \"Date\".\n\nclass(mdy(\"March 14, 1992\"))\n\n[1] \"Date\"\n\n\nlibrary(lubridate) also contains functions for parsing date times into ISO 8601 standard. Times are slightly trickier because of time zones.\n\nmdy_hms(\"12/02/2021 1:00:00\")\n\n[1] \"2021-12-02 01:00:00 UTC\"\n\nmdy_hms(\"12/02/2021 1:00:00\", tz = \"EST\")\n\n[1] \"2021-12-02 01:00:00 EST\"\n\nmdy_hms(\"12/02/2021 1:00:00\", tz = \"America/Chicago\")\n\n[1] \"2021-12-02 01:00:00 CST\"\n\n\nBy default, library(lubridate) will put the date times in Coordinated Universal Time (UTC), which is the successor to Greenwich Mean Time (GMT). I recommend carefully reading the data dictionary if time zones are important for your analysis or if your data cross time zones. This is especially important during time changes (e.g. “spring forward” and “fall back”).\nFortunately, if you encode your dates or date-times correctly, then library(lubridate) will automatically account for time changes, time zones, leap years, leap seconds, and all of the quirks of dates and times.\n\n\n\n\n\n\nExercise 5\n\n\n\n\ndates &lt;- tribble(\n  ~date,\n  \"12/01/1987\",\n  \"12/02/1987\",\n  \"12/03/1987\"\n)\n\n\nCreate the dates data from above with tribble().\nUse mutate() to convert the date column to the ISO 8601 standard (YYYY-MM-DD).\n\n\n\n\nExtracting Components\nlibrary(lubridate) contains functions for extracting components from dates like the year, month, day, and weekday. Conisder the follow data set about full moons in Washington, DC in 2023.\n\nfull_moons &lt;- tribble(\n  ~full_moon,\n  \"2023-01-06\",\n  \"2023-02-05\",\n  \"2023-03-07\",\n  \"2023-04-06\",\n  \"2023-05-05\",\n  \"2023-06-03\",\n  \"2023-07-03\",\n  \"2023-08-01\",\n  \"2023-08-30\",\n  \"2023-09-29\",\n  \"2023-10-28\",\n  \"2023-11-27\",\n  \"2023-12-26\"\n) |&gt;\n  mutate(full_moon = as_date(full_moon))\n\nSuppose we want to know the weekday of each full moon.\n\nfull_moons |&gt;\n  mutate(week_day = wday(full_moon, label = TRUE))\n\n# A tibble: 13 × 2\n   full_moon  week_day\n   &lt;date&gt;     &lt;ord&gt;   \n 1 2023-01-06 Fri     \n 2 2023-02-05 Sun     \n 3 2023-03-07 Tue     \n 4 2023-04-06 Thu     \n 5 2023-05-05 Fri     \n 6 2023-06-03 Sat     \n 7 2023-07-03 Mon     \n 8 2023-08-01 Tue     \n 9 2023-08-30 Wed     \n10 2023-09-29 Fri     \n11 2023-10-28 Sat     \n12 2023-11-27 Mon     \n13 2023-12-26 Tue     \n\n\n\n\nMath\nlibrary(lubridate) easily handles math with dates and date-times. Suppose we want to calculate the number of days since American Independence Day:\n\ntoday() - as_date(\"1776-07-04\")\n\nTime difference of 90464 days\n\n\nIn this case, subtraction creates an object of class difftime represented in days. We can use the difftimes() function to calculate differences in other units.\n\ndifftime(today(), as_date(\"1776-07-04\"), units = \"mins\")\n\nTime difference of 130268160 mins\n\n\n\n\nPeriods\nPeriods track clock time or a calendar time. We use periods when we set a recurring meetings on a calendar and when we set an alarm to wake up in the morning.\nThis can lead to some interesting results. Do we always add 365 days when we add 1 year to a date? With periods, this isn’t true. Sometimes we add 366 days during leap years. For example,\n\nstart &lt;- as_date(\"1999-03-14\")\nend &lt;- start + years(1)\n\nend\n\n[1] \"2000-03-14\"\n\nend - start\n\nTime difference of 366 days\n\n\n\n\nDurations\nDurations track the passage of physical time in exact seconds. Durations are like sand falling into an hourglass. Duration functions start with d like dyears() and dminutes().\n\nstart &lt;- as_date(\"1999-03-14\")\nend &lt;- start + dyears(1)\n\nend\n\n[1] \"2000-03-13 06:00:00 UTC\"\n\n\nNow we always add 365 days, but we see that March 13th is one year after March 14th.\n\n\nIntervals\nUntil now, we’ve focused on points in time. Intervals have length and have a starting point and an ending point.\nSuppose classes start on August 23rd and proceed every week for a while. Do any of these dates conflict with Georgetown’s fall break?\n\nclasses &lt;- as_date(\"2023-08-23\") + weeks(0:15)\n\nfall_break &lt;- interval(as_date(\"2023-11-22\"), as_date(\"2023-11-26\"))\n\nclasses %within% fall_break\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE  TRUE FALSE FALSE\n\n\nWe focused on dates, but many of the same principles hold for date-times.\n\n\n\n\n\n\nExercise 6\n\n\n\n\nCreate a date object for your birth date.\nCalculate the number of days since your birth date.\nCreate a vector of your birthdays from your birth date for the next 120 years. Do you use periods or durations?\n\n\n\n\n\n\n3.4.4 Missing Data\nMissing data are ever present in data analysis. R stores missing values as NA, which are contagious and are fortunately difficult to ignore.\nreplace_na() is the quickest function to replace missing values. It is a shortcut for a specific instance of if_else().\n\nx &lt;- c(1, NA, 3)\n\nif_else(condition = is.na(x), true = 2, false = x)\n\n[1] 1 2 3\n\nreplace_na(x, replace = 2)\n\n[1] 1 2 3\n\n\nWe recommend avoiding arguments like na.rm and using filter() for structurally missing values and replace_na() or imputation for nonresponse.\n\n\n\n\n\n\nExercise 7\n\n\n\nLet’s focus on different data shared by SOI. Now we’ll focus on individual income and tax data by state.\nThis Excel workbook is a beast. For instance, it isn’t clear how the hierarchy works. I expected all of the rows nested under “Number of returns” to sum up to the number of returns. Unfortunately, the rows are not disjoint. Also, the merged cells for column headers are very difficult to use with programming languages.\n\nStart with 20in01al.xlsx.\nCreate a tidy data frame with rows 10 through 12 (“Number of single returns”, “Number of joint returns”, and “Number of head of household returns”) disaggregated by “size of adjusted gross income”.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#footnotes",
    "href": "03_advanced-data-cleaning.html#footnotes",
    "title": "3  Advanced Data Cleaning",
    "section": "",
    "text": "The instinct to include these comments is good. The execution is poor because it creates big headaches for people using programming languages. I suggest using a data dictionary instead.↩︎\nThis example uses regular expressions (regex). Visit R4DS (2e) for a review of regex.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#review",
    "href": "02_tidyverse.html#review",
    "title": "2  Introduction to the Tidyverse",
    "section": "",
    "text": "2.1.1 Assignment operator\n&lt;- is the assignment operator. An object created on the right side of an assignment operator is assigned to a name on the left side of an assignment operator. Assignment operators are important for saving the consequences of operations and functions. Without assignment, the result of a calculation is not saved for use in future calculations. Operations without assignment operators will typically be printed to the console but not saved for future use.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#tidy-data",
    "href": "02_tidyverse.html#tidy-data",
    "title": "2  Introduction to the Tidyverse",
    "section": "2.3 Tidy data",
    "text": "2.3 Tidy data\n\n2.3.1 tidyverse\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. ~ tidyverse.org\n\nlibrary(tidyverse) contains:\n\nggplot2, for data visualization.\ndplyr, for data manipulation.\ntidyr, for data tidying.\nreadr, for data import.\npurrr, for functional programming.\ntibble, for tibbles, a modern re-imagining of data frames.\nstringr, for strings.\nforcats, for factors.\n\n\n\n2.3.2 Opinionated software\n\nOpinionated software is a software product that believes a certain way of approaching a business process is inherently better and provides software crafted around that approach. ~ Stuart Eccles\n\n\n\n2.3.3 Tidy data\nThe defining opinion of the tidyverse is its wholehearted adoption of tidy data. Tidy data has three features:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a dataframe. (This is from the paper, not the book)\n\n\n\n\nSource: R4DS\n\n\nTidy data was formalized by Hadley Wickham (2014) in the Journal of Statistical Software It is equivalent to Codd’s 3rd normal form (Codd, 1990) for relational databases.\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. ~ Hadley Wickham\n\nThe tidy approach to data science is powerful because it breaks data work into two distinct parts. First, get the data into a tidy format. Second, use tools optimized for tidy data. By standardizing the data structure for most community-created tools, the framework oriented diffuse development and reduced the friction of data work.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#dplyr",
    "href": "02_tidyverse.html#dplyr",
    "title": "2  Introduction to the Tidyverse",
    "section": "2.4 dplyr",
    "text": "2.4 dplyr\nlibrary(dplyr) contains workhorse functions for manipulating and summarizing data once it is in a tidy format. library(tidyr) contains functions for getting data into a tidy format.\ndplyr can be explicitly loaded with library(dplyr) or loaded with library(tidyverse):\n\nlibrary(tidyverse)\n\nWe’ll focus on the key dplyr syntax using the March 2020 Annual Social and Economic Supplement (ASEC) to the Current Population Survey (CPS). Run the following code to load the data.\n\nasec &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/cps/cps-asec.csv\"\n  )\n)\n\nRows: 157959 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): month, ftype, offpov, offpovuniv\ndbl (13): year, serial, cpsid, asecflag, asecwth, pernum, cpsidp, asecwt, ft...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can use glimpse(asec) to quickly view the data. We can also use View(asec) to open up asec in RStudio.\n\nglimpse(x = asec)\n\nRows: 157,959\nColumns: 17\n$ year       &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020,…\n$ serial     &lt;dbl&gt; 1, 1, 2, 2, 3, 4, 4, 5, 5, 5, 5, 7, 8, 9, 10, 10, 10, 12, 1…\n$ month      &lt;chr&gt; \"March\", \"March\", \"March\", \"March\", \"March\", \"March\", \"Marc…\n$ cpsid      &lt;dbl&gt; 2.01903e+13, 2.01903e+13, 2.01812e+13, 2.01812e+13, 2.01902…\n$ asecflag   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ asecwth    &lt;dbl&gt; 1552.90, 1552.90, 990.49, 990.49, 1505.27, 1430.70, 1430.70…\n$ pernum     &lt;dbl&gt; 1, 2, 1, 2, 1, 1, 2, 1, 2, 3, 4, 1, 1, 1, 1, 2, 3, 1, 2, 3,…\n$ cpsidp     &lt;dbl&gt; 2.01903e+13, 2.01903e+13, 2.01812e+13, 2.01812e+13, 2.01902…\n$ asecwt     &lt;dbl&gt; 1552.90, 1552.90, 990.49, 990.49, 1505.27, 1430.70, 1196.57…\n$ ftype      &lt;chr&gt; \"Primary family\", \"Primary family\", \"Primary family\", \"Prim…\n$ ftotval    &lt;dbl&gt; 127449, 127449, 64680, 64680, 40002, 8424, 8424, 59114, 591…\n$ inctot     &lt;dbl&gt; 52500, 74949, 44000, 20680, 40002, 0, 8424, 610, 58001, 503…\n$ incwage    &lt;dbl&gt; 52500, 56000, 34000, 0, 40000, 0, 8424, 0, 58000, 0, 0, 0, …\n$ offpov     &lt;chr&gt; \"Above Poverty Line\", \"Above Poverty Line\", \"Above Poverty …\n$ offpovuniv &lt;chr&gt; \"In Poverty Universe\", \"In Poverty Universe\", \"In Poverty U…\n$ offtotval  &lt;dbl&gt; 127449, 127449, 64680, 64680, 40002, 8424, 8424, 59114, 591…\n$ offcutoff  &lt;dbl&gt; 17120, 17120, 17120, 17120, 13300, 15453, 15453, 26370, 263…\n\n\nWe’re going to learn seven functions and one new piece of syntax from library(dplyr) that will be our main tools for manipulating tidy frames. These functions and a few extensions outlined in the Data Transformation Cheat Sheet are the core of data analysis in the Tidyverse.\n\n2.4.1 select()\nselect() drops columns from a dataframe and/or reorders the columns in a dataframe. The arguments after the name of the dataframe should be the names of columns you wish to keep, without quotes. All other columns not listed are dropped.\n\nselect(.data = asec, year, month, serial)\n\n# A tibble: 157,959 × 3\n    year month serial\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  2020 March      1\n 2  2020 March      1\n 3  2020 March      2\n 4  2020 March      2\n 5  2020 March      3\n 6  2020 March      4\n 7  2020 March      4\n 8  2020 March      5\n 9  2020 March      5\n10  2020 March      5\n# ℹ 157,949 more rows\n\n\nThis works great until the goal is to select 99 of 100 variables. Fortunately, - can be used to remove variables. You can also select all but multiple variables by listing them with the - symbol separated by commas.\n\nselect(.data = asec, -asecflag)\n\n# A tibble: 157,959 × 16\n    year serial month   cpsid asecwth pernum  cpsidp asecwt ftype ftotval inctot\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  2020      1 March 2.02e13   1553.      1 2.02e13  1553. Prim…  127449  52500\n 2  2020      1 March 2.02e13   1553.      2 2.02e13  1553. Prim…  127449  74949\n 3  2020      2 March 2.02e13    990.      1 2.02e13   990. Prim…   64680  44000\n 4  2020      2 March 2.02e13    990.      2 2.02e13   990. Prim…   64680  20680\n 5  2020      3 March 2.02e13   1505.      1 2.02e13  1505. Nonf…   40002  40002\n 6  2020      4 March 2.02e13   1431.      1 2.02e13  1431. Prim…    8424      0\n 7  2020      4 March 2.02e13   1431.      2 2.02e13  1197. Prim…    8424   8424\n 8  2020      5 March 2.02e13   1133.      1 2.02e13  1133. Prim…   59114    610\n 9  2020      5 March 2.02e13   1133.      2 2.02e13  1133. Prim…   59114  58001\n10  2020      5 March 2.02e13   1133.      3 2.02e13  1322. Prim…   59114    503\n# ℹ 157,949 more rows\n# ℹ 5 more variables: incwage &lt;dbl&gt;, offpov &lt;chr&gt;, offpovuniv &lt;chr&gt;,\n#   offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nTidy data generally results in longer, wider data sets than other programming languages so iteratively selecting by column names is less important than in Stata or SAS. Still, dplyr contains powerful helper functions that can select variables based on patterns in column names:\n\ncontains(): Contains a given string\nstarts_with(): Starts with a prefix\nends_with(): Ends with a suffix\nmatches(): Matches a regular expression\nnum_range(): Matches a numerical range\n\nThese are a subset of the tidyselect selection language and helpers which enable users to apply library(dplyr) functions to select variables.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nSelect pernum and inctot from asec.\npull() is related to select() but can only select one variable. What is the other difference with pull()?\n\n\n\n\n\n2.4.2 rename()\nrename() renames columns in a data frame. The pattern is new_name = old_name.\n\nrename(.data = asec, serial_number = serial)\n\n# A tibble: 157,959 × 17\n    year serial_number month   cpsid asecflag asecwth pernum  cpsidp asecwt\n   &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  2020             1 March 2.02e13        1   1553.      1 2.02e13  1553.\n 2  2020             1 March 2.02e13        1   1553.      2 2.02e13  1553.\n 3  2020             2 March 2.02e13        1    990.      1 2.02e13   990.\n 4  2020             2 March 2.02e13        1    990.      2 2.02e13   990.\n 5  2020             3 March 2.02e13        1   1505.      1 2.02e13  1505.\n 6  2020             4 March 2.02e13        1   1431.      1 2.02e13  1431.\n 7  2020             4 March 2.02e13        1   1431.      2 2.02e13  1197.\n 8  2020             5 March 2.02e13        1   1133.      1 2.02e13  1133.\n 9  2020             5 March 2.02e13        1   1133.      2 2.02e13  1133.\n10  2020             5 March 2.02e13        1   1133.      3 2.02e13  1322.\n# ℹ 157,949 more rows\n# ℹ 8 more variables: ftype &lt;chr&gt;, ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;,\n#   offpov &lt;chr&gt;, offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nYou can also rename a selection of variables using rename_with(). The .cols argument is used to select the columns to rename and takes a tidyselect statement like those we introduced above. Here, we’re using the where() selection helper which selects all columns where a given condition is TRUE. The default value for the .cols argument is everything() which selects all columns in the dataset.\n\nrename_with(.data = asec, .fn = toupper, .cols = where(is.numeric))\n\n# A tibble: 157,959 × 17\n    YEAR SERIAL month   CPSID ASECFLAG ASECWTH PERNUM  CPSIDP ASECWT ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: FTOTVAL &lt;dbl&gt;, INCTOT &lt;dbl&gt;, INCWAGE &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, OFFTOTVAL &lt;dbl&gt;, OFFCUTOFF &lt;dbl&gt;\n\n\nMost dplyr functions can rename columns simply by prefacing the operation with new_name =. For example, this can be done with select():\n\nselect(.data = asec, year, month, serial_number = serial)\n\n# A tibble: 157,959 × 3\n    year month serial_number\n   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1  2020 March             1\n 2  2020 March             1\n 3  2020 March             2\n 4  2020 March             2\n 5  2020 March             3\n 6  2020 March             4\n 7  2020 March             4\n 8  2020 March             5\n 9  2020 March             5\n10  2020 March             5\n# ℹ 157,949 more rows\n\n\n\n\n2.4.3 filter()\nfilter() reduces the number of observations in a dataframe. Every column in a dataframe has a name. Rows do not necessarily have names in a dataframe, so rows need to be filtered based on logical conditions.\n==, &lt;, &gt;, &lt;=, &gt;=, !=, %in%, and is.na() are all operators that can be used for logical conditions. ! can be used to negate a condition and & and | can be used to combine conditions. | means or.\n\n# return rows with pernum of 1 and incwage &gt; $100,000\nfilter(.data = asec, pernum == 1 & incwage &gt; 100000)\n\n# A tibble: 5,551 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020     28 March 2.02e13        1    678.      1 2.02e13   678. Primary fa…\n 2  2020    134 March 0              1    923.      1 0         923. Primary fa…\n 3  2020    136 March 2.02e13        1    906.      1 2.02e13   906. Primary fa…\n 4  2020    137 March 2.02e13        1   1493.      1 2.02e13  1493. Nonfamily …\n 5  2020    359 March 2.02e13        1    863.      1 2.02e13   863. Primary fa…\n 6  2020    372 March 2.02e13        1   1338.      1 2.02e13  1338. Primary fa…\n 7  2020    404 March 0              1    677.      1 0         677. Primary fa…\n 8  2020    420 March 2.02e13        1    747.      1 2.02e13   747. Primary fa…\n 9  2020    450 March 2.02e13        1   1309.      1 2.02e13  1309. Primary fa…\n10  2020    491 March 0              1   1130.      1 0        1130. Primary fa…\n# ℹ 5,541 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nIPUMS CPS contains full documentation with information about pernum and incwage.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nFilter asec to rows with month equal to \"March\".\nFilter asec to rows with inctot less than 999999999.\nFilter asec to rows with pernum equal to 3 and inctot less than 999999999.\n\n\n\n\n\n2.4.4 arrange()\narrange() sorts the rows of a data frame in alpha-numeric order based on the values of a variable or variables. The dataframe is sorted by the first variable first and each subsequent variable is used to break ties. desc() is used to reverse the sort order for a given variable.\n\n# sort pernum is descending order because high pernums are interesting\narrange(.data = asec, desc(pernum))\n\n# A tibble: 157,959 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020  91430 March 0              1    505.     16 0         604. Secondary …\n 2  2020  91430 March 0              1    505.     15 0         465. Secondary …\n 3  2020  91430 March 0              1    505.     14 0         416. Secondary …\n 4  2020  15037 March 2.02e13        1   2272.     13 2.02e13  2633. Primary fa…\n 5  2020  78495 March 0              1   1279.     13 0        1424. Related su…\n 6  2020  91430 March 0              1    505.     13 0         465. Secondary …\n 7  2020  15037 March 2.02e13        1   2272.     12 2.02e13  1689. Primary fa…\n 8  2020  18102 March 0              1   2468.     12 0        2871. Primary fa…\n 9  2020  22282 March 0              1   2801.     12 0        3879. Related su…\n10  2020  30274 March 2.02e13        1    653.     12 2.02e13   858. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nSort asec in descending order by pernum and ascending order by inctot.\n\n\n\n\n\n2.4.5 mutate()\nmutate() creates new variables or edits existing variables. We can use arithmetic arguments, such as +, -, *, /, and ^. We can also custom functions and functions from packages. For example, we can use library(stringr) for string manipulation and library(lubridate) for date manipulation.\nVariables are created by adding a new column name, like inctot_adjusted, to the left of = in mutate().\n\n# adjust inctot for underreporting\nmutate(.data = asec, inctot_adjusted = inctot * 1.1)\n\n# A tibble: 157,959 × 18\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 8 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;, inctot_adjusted &lt;dbl&gt;\n\n\nVariables are edited by including an existing column name, like inctot, to the left of = in mutate().\n\n# adjust income because of underreporting\nmutate(.data = asec, inctot = inctot * 1.1)\n\n# A tibble: 157,959 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nConditional logic inside of mutate() with functions like if_else() and case_when() is key to mastering data munging in R.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCreate a new variable called in_poverty. If offtotval is less than offcutoff then use \"Below Poverty Line\". Otherwise, use \"Above Poverty Line\". Hint: if_else() is useful and works like the IF command in Microsoft Excel.\n\n\n\n\n\n2.4.6 |&gt;\nData munging is tiring when each operation needs to be assigned to a name with &lt;-. The pipe, |&gt;, allows lines of code to be chained together so the assignment operator only needs to be used once.\n|&gt; passes the output from function as the first argument in a subsequent function. For example, this line can be rewritten:\nLegacy R code may use %&gt;%, the pipe from the magrittr package. It was (and remains) wildly popular, particularly in the tidyverse framework. Due to this popularity, base R incorporated a similar concept in the base pipe. In many cases, these pipes work the same way, but there are some differences. Because |&gt; is new and continues to be developed, developers have increased |&gt;’s abilities over time. To see a list of key differences between %&gt;% and |&gt;, see this blog.\n\n# old way\nmutate(.data = asec, inctot_adjusted = inctot * 1.1)\n\n# new way\nasec %&gt;%\n  mutate(inctot_adjusted = inctot * 1.1)\n\nSee the power:\n\nnew_asec &lt;- asec %&gt;%\n  filter(pernum == 1) %&gt;%\n  select(year, month, pernum, inctot) %&gt;%\n  mutate(inctot_adjusted = inctot * 1.1) %&gt;%\n  select(-inctot)\n\nnew_asec\n\n# A tibble: 60,460 × 4\n    year month pernum inctot_adjusted\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;           &lt;dbl&gt;\n 1  2020 March      1          57750 \n 2  2020 March      1          48400 \n 3  2020 March      1          44002.\n 4  2020 March      1              0 \n 5  2020 March      1            671 \n 6  2020 March      1          19279.\n 7  2020 March      1          12349.\n 8  2020 March      1          21589.\n 9  2020 March      1          47306.\n10  2020 March      1          10949.\n# ℹ 60,450 more rows\n\n\n\n\n2.4.7 summarize()\nsummarize() collapses many rows in a dataframe into fewer rows with summary statistics of the many rows. n(), mean(), and sum() are common summary statistics. Renaming is useful with summarize()!\n\n# summarize without renaming the statistics\nasec %&gt;%\n  summarize(mean(ftotval), mean(inctot))\n\n# A tibble: 1 × 2\n  `mean(ftotval)` `mean(inctot)`\n            &lt;dbl&gt;          &lt;dbl&gt;\n1         105254.     209921275.\n\n# summarize and rename the statistics\nasec %&gt;%\n  summarize(mean_ftotval = mean(ftotval), mean_inctot = mean(inctot))\n\n# A tibble: 1 × 2\n  mean_ftotval mean_inctot\n         &lt;dbl&gt;       &lt;dbl&gt;\n1      105254.  209921275.\n\n\nsummarize() returns a data frame. This means all dplyr functions can be used on the output of summarize(). This is powerful! Manipulating summary statistics in Stata and SAS can be a chore. Here, it’s just another dataframe that can be manipulated with a tool set optimized for dataframes: dplyr.\n\n\n2.4.8 group_by()\ngroup_by() groups a dataframe based on specified variables. summarize() with grouped dataframes creates subgroup summary statistics. mutate() with group_by() calculates grouped summaries for each row.\n\nasec %&gt;%\n  group_by(pernum) %&gt;%\n  summarize(\n    n = n(),\n    mean_ftotval = mean(ftotval), \n    mean_inctot = mean(inctot)\n  )\n\n# A tibble: 16 × 4\n   pernum     n mean_ftotval mean_inctot\n    &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 60460       94094.      57508.\n 2      2 45151      108700.   77497357.\n 3      3 25650      117966.  473030618.\n 4      4 15797      121815.  634999933.\n 5      5  6752      108609.  691504650.\n 6      6  2582       89448.  682810446.\n 7      7   922       78889.  682218196.\n 8      8   353       72284.  682725646.\n 9      9   158       54599.  632917559.\n10     10    73       58145.  657543632.\n11     11    37       61847.  702708584 \n12     12    18       50249.  777780725.\n13     13     3       25152   666666666 \n14     14     1       18000       18000 \n15     15     1       25000       25000 \n16     16     1       15000       15000 \n\n\nDataframes can be grouped by multiple variables.\nGrouped tibbles include metadata about groups. For example, Groups:   pernum, offpov [40]. One grouping is dropped each time summarize() is used. It is easy to forget if a dataframe is grouped, so it is safe to include ungroup() at the end of a section of functions.\n\nasec %&gt;%\n  group_by(pernum, offpov) %&gt;%\n  summarize(\n    n = n(),\n    mean_ftotval = mean(ftotval), \n    mean_inctot = mean(inctot)\n  ) %&gt;%\n  arrange(offpov) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'pernum'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 40 × 5\n   pernum offpov                 n mean_ftotval mean_inctot\n    &lt;dbl&gt; &lt;chr&gt;              &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 Above Poverty Line 53872      104451.      63642.\n 2      2 Above Poverty Line 40978      118691.   59082162.\n 3      3 Above Poverty Line 23052      129891.  463440562.\n 4      4 Above Poverty Line 14076      135039.  631720097.\n 5      5 Above Poverty Line  5805      123937.  688206447.\n 6      6 Above Poverty Line  2118      105867.  683199297.\n 7      7 Above Poverty Line   724       96817.  697520661.\n 8      8 Above Poverty Line   269       90328.  672870019.\n 9      9 Above Poverty Line   114       70438.  622815186.\n10     10 Above Poverty Line    57       71483.  666678408.\n# ℹ 30 more rows\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nfilter() to only include observations with \"In Poverty Universe\" in offpovuniv.\ngroup_by() offpov.\nUse summarize() and n() to count the number of observations in poverty.\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\nfilter() to only include observations with \"In Poverty Universe\".\ngroup_by() cpsid.\nUse mutate(family_size = n()) to calculate the family size for each observation in asec.\nungroup()\nCreate a new variable called in_poverty. If offtotval is less than offcutoff then use \"Below Poverty Line\". Otherwise, use \"Above Poverty Line\".\ngroup_by() family_size, offpov, and in_poverty\nUse summarize() and n() to see if you get the same result for offpov and in_poverty. You should only get two rows per family size if your poverty calculation is correct.\n\n\noffcutoff comes from Census Bureau poverty tables with 48 unique thresholds based on family composition. Do not confuse the tables with HHS poverty tables.\nThese data come from IPUMS CPS. IPUMS has cleaned and pre-processed the data to include variables like offcutoff.\n\nAre the estimates from the previous two exercises correct?\nLet’s look at a Census Report to see how many people were in poverty in 2019. We estimated about 16,500 people. The Census Bureau says 34.0 million people.\nNo! We did not account for sampling weights, so our estimates are incorrect. Assignment 3 will demonstrate how to incorporate sampling weights into an analysis.\n\n\n\n\n2.4.9 BONUS: count()\ncount() is a shortcut to df %&gt;% group_by(var) %&gt;% summarize(n()). count() counts the number of observations with a level of a variable or levels of several variables. It is too useful to skip:\n\ncount(asec, pernum)\n\n# A tibble: 16 × 2\n   pernum     n\n    &lt;dbl&gt; &lt;int&gt;\n 1      1 60460\n 2      2 45151\n 3      3 25650\n 4      4 15797\n 5      5  6752\n 6      6  2582\n 7      7   922\n 8      8   353\n 9      9   158\n10     10    73\n11     11    37\n12     12    18\n13     13     3\n14     14     1\n15     15     1\n16     16     1\n\n\n\ncount(x = asec, pernum, offpov)\n\n# A tibble: 40 × 3\n   pernum offpov                 n\n    &lt;dbl&gt; &lt;chr&gt;              &lt;int&gt;\n 1      1 Above Poverty Line 53872\n 2      1 Below Poverty Line  6588\n 3      2 Above Poverty Line 40978\n 4      2 Below Poverty Line  4156\n 5      2 NIU                   17\n 6      3 Above Poverty Line 23052\n 7      3 Below Poverty Line  2527\n 8      3 NIU                   71\n 9      4 Above Poverty Line 14076\n10      4 Below Poverty Line  1648\n# ℹ 30 more rows",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#functions",
    "href": "02_tidyverse.html#functions",
    "title": "2  Introduction to the Tidyverse",
    "section": "2.2 Functions",
    "text": "2.2 Functions\nFunctions are collections of code that take inputs, perform operations, and return outputs. R functions are similar to mathematical functions.\nR functions typically contain arguments. For example, mean() has x, trim, and na.rm. Many arguments have default values and don’t need to be included in function calls. Default values can be seen in the documentation. trim = 0 and na.rm = FALSE are the defaults for mean().\n\n2.2.1 == vs. =\n== is a binary comparison operator.\n\n1 == 1\n\n[1] TRUE\n\n1 == 2\n\n[1] FALSE\n\n\n= is an equals sign, it is most frequently used for passing arguments to functions.\n\nmean(x = c(1, 2, 3))",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Tidyverse</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#six-principles-for-data-analysis",
    "href": "01_intro-r.html#six-principles-for-data-analysis",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1.1 Accuracy\nDeliberate steps should be taken to minimize the chance of making an error and maximize the chance of catching errors when errors inevitably occur.\nEmbrace Your Fallibility: Thoughts on Code Integrity\n\n\n1.1.2 Computational reproducibility\nComputational reproducibility should be embraced to improve accuracy, promote transparency, and prove the quality of analytic work.\nReplication: the recreation of findings across repeated studies, is a cornerstone of science.\nReproducibility: the ability to access data, source code, tools, and documentation and recreate all calculations, visualizations, and artifacts of an analysis.\nComputational reproducibility should be the minimum standard for computational social sciences and statistical programming.\n\n\n1.1.3 Human interpretability\nCode should be written so humans can easily understand what’s happening—even if it occasionally sacrifices machine performance.\n\n\n1.1.4 Portability\nAnalyses should be designed so strangers can understand each and every step without additional instruction or inquiry from the original analyst.\n\n\n1.1.5 Accessibility\nResearch and data are non-rivalrous and can be non-excludable. They are public goods that should be widely and easily shared. Decisions about tools, methods, data, and language during the research process should be made in ways that promote the ability of anyone and everyone to access an analysis.\n\n\n1.1.6 Efficiency\nAnalysts should seek to make all parts of the research process more efficient with clear communication, by adopting best practices, and by managing computation.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#r",
    "href": "01_intro-r.html#r",
    "title": "1  Introduction to R",
    "section": "1.2 R",
    "text": "1.2 R\nR is a free, open-source software for statistical computing. It is a fully-functioning programming language and it is known for intuitive, crisp graphics and an extensive, growing library of statistical and analytic methods. Above all, R boasts an enthusiastic community of developers, instructors, and users. The copyright and documentation for R is held by a not-for-profit organization called the R Foundation.\nR comes from the S programming language and S-PLUS system. In addition to offering better graphics and more extensibility than proprietary languages, R has a pedagogical advantage:\n\nThe ambiguity [of the S language] is real and goes to a key objective: we wanted users to be able to begin in an interactive environment, where they did not consciously think of themselves as programming. Then as their needs became clearer and their sophistication increased, they should be able to slide gradually into programming, when the language and system aspects would become more important.\n\nSource: “Stages in the Evolution of S” via Roger Peng",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#rstudio",
    "href": "01_intro-r.html#rstudio",
    "title": "1  Introduction to R",
    "section": "1.3 RStudio",
    "text": "1.3 RStudio\nRStudio is a free, open-source integrated development environment (IDE) that runs on top of R. In practice, R users almost exclusively open RStudio and rarely directly open R. When we say IDE, we mean a piece of software where you can write, or develop, code in an efficient way.\nRStudio is developed by a for-profit company called RStudio. RStudio, the company, employs some of the R community’s most prolific, open-source developers and creates many open-source tools and resources.\nWhile R code can be written in any text editor, the RStudio IDE is a powerful tool with a console, syntax-highlighting, and debugging tools. The RStudio IDE cheatsheet outlines some of the power of RStudio.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#rstudio-console",
    "href": "01_intro-r.html#rstudio-console",
    "title": "1  Introduction to R",
    "section": "1.4 RStudio Console",
    "text": "1.4 RStudio Console\n\nThe RStudio Console contains the R command line and R output from previously submitted R code.\nCode can be submitted by typing to the right of the last blue &gt; and the hitting enter.\n\n\n\n\n\n\nExercise 1\n\n\n\nR has all of the functionality of a basic calculator. Let’s run some simple calculations with addition (+), subtraction (-), multiplication (*), division (/), exponentiation (^), and grouping (()).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#r-scripts",
    "href": "01_intro-r.html#r-scripts",
    "title": "1  Introduction to R",
    "section": "1.5 .R Scripts",
    "text": "1.5 .R Scripts\n\nBy default, there isn’t a record of code directly typed and submitted into the RStudio Console. So, most R programmers use .R scripts to develop R code before submitting code to the console.\n.R scripts are simple text files with R code. They are similar to .py files in Python, .sas files in SAS, and .do files in Stata.\n\n\n\n\n\n\nExercise 2\n\n\n\nClick the new script button in the top left corner of RStudio to create a new script.\n\nAdd some R code to your new script. Place your cursor on a line of R code and hit Command-Enter on Macs or Control-Enter on Windows. Alternatively, highlight the code (it can be multiple lines) and click the run button.\n\nIn both cases, the code from your .R script should move to the R Console and evaluate.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#comments",
    "href": "01_intro-r.html#comments",
    "title": "1  Introduction to R",
    "section": "1.6 Comments",
    "text": "1.6 Comments\nR will interpret all text in a .R script as R code unless the code follows #, the comment symbol. Comments are essential to writing clear code in any programming language.\n\n## Demonstrate the value of a comment and make a simple calculation\n2 + 2\n\nIt should be obvious what a line of clear R code accomplishes. It isn’t always obvious why a clear line of R code is included in a script. Comments should focus on the why of R code and not the what of R code.\nThe following comment isn’t useful because it just restates the R code, which is clear:\n\n# divide every value by 1.11\ncost / 1.11\n\nThe following comment is useful because it adds context to the R code:\n\n# convert costs from dollars to Euros using the 2020-01-13 exchange rate\ncost / 1.11\n\n\n\n\n\n\n\nExercise 3\n\n\n\nAdd comments to your .R that clarify the why. Since we only know a few operations, the comments may need to focus on your why you picked your favorite numbers.\n\n\n#Style\nGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread. ~ Hadley Wickham\nHuman interpretability is one of the six principles because clear code can save time and reduce the chance of making errors. After time, eyes can be trained to quickly spot incorrect code if a consistent R style is adopted.\nFirst, note that R is case-sensitive. Capitalization is rare and deviations from the capitalization will throw errors. For example, mean() is a function but Mean() is not a function.\nThe tidyverse style guide is a comprehensive style guide that, in general, reflects the style of the plurality of R programmers. For now, just focus on consistency.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html",
    "href": "07_advanced-quarto.html",
    "title": "7  Advanced Quarto",
    "section": "",
    "text": "7.1 Math Notation\nThis course uses probability and statistics. Occasionally, we want to easily communicate with mathematical notation. For example, it may be convenient to type that \\(X\\) is a random variable that follows a standard normal distribution (mean = 0 and standard deviation = 1).\n\\[X \\sim N(\\mu = 0, \\sigma = 1)\\]",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#sec-review",
    "href": "07_advanced-quarto.html#sec-review",
    "title": "7  Advanced Quarto",
    "section": "",
    "text": "7.1.1 Motivation\nThere are many problems worth avoiding in an analysis:\n\nCopying-and-pasting, transposing, and manual repetition\nRunning code out-of-order\nMaintaining parallel documents like a script for analysis and a doc for narrative\nCode written for computers that is tough to parse by humans\n\nNot convinced? Maybe we just want to make cool stuff like websites, blogs, books, and slide decks.\nQuarto, a literate statistical programming framework for R, Python, and Julia helps us solve many of these problems. Quarto uses\n\nplain text files ending in .qmd that are similar to .R and .Rmd files\nlibrary(knitr)\npandoc1\n\nQuarto uses library(knitr) and pandoc to convert plain text .qmd documents into rich output documents like these class notes. The “Render” button appears in RStudio with a .qmd file is open in the editor window.\nClicking the “Render” button begins the process of rendering .qmd files.\n\n\n\n\n\n\n\n\n\nWhen the button is clicked, Quarto calls library(knitr) and renders .qmd (Quarto files) into .md (Markdown files), which Pandoc then converts into any specified output type. Quarto and library(knitr) don’t need to be explicitly loaded as the entire process is handled by clicking the “Render” button in RStudio.\n\n\n\n\n\n\n\n\n\nSource: Quarto website\nQuarto, library(knitr), and Pandoc are all installed with RStudio. You will need to install a LaTeX distribution to render PDFs. We recommend library(tinytex) as a LaTeX distribution (installation instructions).\n\n\n\n\n\n\nExercise 1\n\n\n\n\nClick the new script button in RStudio and add a “Quarto Document”.\nGive the document a name, an author, and ensure that HTML is selected.\nSave the document as “hello-quarto.qmd”.\nClick “Render”.\n\n\n\nQuarto has three main ingredients:\n\nYAML header\nMarkdown text\nCode chunks\n\n\n\n7.1.2 (1) YAML Header\nYAML stands for “yet another markup language”. The YAML header contains meta information about the document including output type, document settings, and parameters that can be passed to the document. The YAML header starts with --- and ends with ---.\nHere is the simplest YAML header for a PDF document:\n---\nformat: pdf\n---\nYAML headers can contain many output specific settings. This YAML header creates an HTML document with code folding and a floating table of contents:\n---\nformat: \n  html:\n    embed-resources: true\n    code-fold: true\n    toc: true\n---  \nParameters can be specified as follows\n---\nformat: pdf\nparams:\n  state: \"Virginia\"\n---\nNow state can be referred to anywhere in R code as params$state. Parameters are useful for a couple of reasons:\n\nWe can clearly change key values for a Quarto document in the YAML header.\nWe can create a template and programmatically iterate the template over a set of values with the quarto_render() function and library(purrr). This blog outlines the idea. The Mobility Metrics Data Tables and SLFI State Fiscal Briefs are key examples of this workflow.\n\n\n\n\n\n\n\nWarning\n\n\n\nUnlike R Markdown, images and other content are not embedded in .html from Quarto by default. Be sure to include embed-resources: true in YAML headers to embed content and make documents easier to share.\nSuppose we embed an image called image.png in a Quarto document called example.qmd, which, when rendered, creates example.html. If we don’t include embed-resources: true, then we will need to share image.png and example.html to see the embedded image. This is also true for other files like .css.\n\n\n\n\n7.1.3 (2) Markdown text\nMarkdown is a shortcut for HyperText Markup Language (HTML). Essentially, simple meta characters corresponding to formatting are added to plain text.\nTitles and subtitltes\n------------------------------------------------------------\n\n# Title 1\n\n## Title 2\n\n### Title 3\n\n\nText formatting \n------------------------------------------------------------\n\n*italic*  \n\n**bold**   \n\n`code`\n\nLists\n------------------------------------------------------------\n\n* Bulleted list item 1\n* Item 2\n  * Item 2a\n  * Item 2b\n\n1. Item 1\n2. Item 2\n\nLinks and images\n------------------------------------------------------------\n\n[text](http://link.com)\n\n![Penguins](images/penguins.png)\n\n\n7.1.4 (3) Code chunks\n\n\n\n\n\n\n\n\n\nMore frequently, code is added in code chunks:\n\n```{r}\n2 + 2\n```\n\n[1] 4\n\n\nThe first argument inline or in a code chunk is the language engine. Most commonly, this will just be a lower case r. knitr allows for many different language engines:\n\nR\nJulia\nPython\nSQL\nBash\nRcpp\nStan\nJavascript\nCSS\n\nQuarto has a rich set of options that go inside of the chunks and control the behavior of Quarto.\n\n```{r}\n#| label: important-calculation\n#| eval: false\n\n2 + 2\n```\n\nIn this case, eval makes the code not run. Other chunk-specific settings can be added inside the brackets. Here2 are the most important options:\n\n\n\nOption\nEffect\n\n\n\n\necho: false\nHides code in output\n\n\neval: false\nTurns off evaluation\n\n\noutput: false\nHides code output\n\n\nwarning: false\nTurns off warnings\n\n\nmessage: false\nTurns off messages\n\n\nfig-height: 8\nChanges figure width in inches3\n\n\nfig-width: 8\nChanges figure height in inches4\n\n\n\nDefault settings for the entire document can be changed in the YAML header with the execute option:\nexecute:\n  warning: false\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAdd date: today to your YAML header after title. This will update every time the document is rendered.\nCopy the Markdown table from this table generator and add it to your .qmd document.\nCreate a scatter plot of the cars data with library(ggplot2). Adjust the figure width and height using options within the chunk.\nClick “Render”.\n\n\n\n\n\n7.1.5 Organizing a Quarto Document\nIt is important to clearly organize a Quarto document and the constellation of files that typically support an analysis.\n\nAlways use .Rproj files.\nUse sub-directories to sort images, .css, data.\n\nLater, we will learn how to use library(here) to effectively organize sub-directories.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#math-notation",
    "href": "07_advanced-quarto.html#math-notation",
    "title": "7  Advanced Quarto",
    "section": "",
    "text": "7.1.1 Math Mode\nUse $ to start and stop in-line math notation and $$ to start multi-line math notation. Math notation uses LaTeX’s syntax for mathematical notation.\nHere’s an example with in-line math:\nConsider a binomially distributed random variable, $X \\sim binom(n, p)$. \nConsider a binomially distributed random variable, \\(X \\sim binom(n, p)\\).\nHere’s an example with a chunk of math:\n$$\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n$${#eq-binomial}\n\\[\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n\\tag{7.1}\\]\n\n\n7.1.2 Important Syntax\nMath mode recognizes basic math symbols available on your keyboard including +, -, *, /, &gt;, &lt;, (, and ).\nMath mode contains all greek letters. For example, \\alpha (\\(\\alpha\\)) and \\beta (\\(\\beta\\)).\n\n\n\nTable 7.1: My Caption\n\n\n\n\n\nLaTeX\nSymbol\n\n\n\n\n\\alpha\n\\(\\alpha\\)\n\n\n\\beta\n\\(\\beta\\)\n\n\n\\gamma\n\\(\\gamma\\)\n\n\n\\Delta\n\\(\\Delta\\)\n\n\n\\epsilon\n\\(\\epsilon\\)\n\n\n\\theta\n\\(\\theta\\)\n\n\n\\pi\n\\(\\pi\\)\n\n\n\\sigma\n\\(\\sigma\\)\n\n\n\\chi\n\\(\\chi\\)\n\n\n\n\n\n\nMath mode also recognizes \\(\\log(x)\\) (\\log(x)) and \\(\\sqrt{x}\\) (\\sqrt{x}).\nSuperscripts (^) are important for exponentiation and subscripts (_) are important for adding indices. y = x ^ 2 renders as \\(y = x ^ 2\\) and x_1, x_2, x_3 renders as \\(x_1, x_2, x_3\\). Brackets are useful for multi-character superscripts and subscripts like \\(s_{11}\\) (s_{11}).\nIt is useful to add symbols to letters. For example, \\bar{x} is useful for sample means (\\(\\bar{x}\\)), \\hat{y} is useful for predicted values (\\(\\hat{y}\\)), and \\vec{\\beta} is useful for vectors of coefficients (\\(\\vec{\\beta}\\)).\nMath mode supports fractions with \\frac{x}{y} (\\(\\frac{x}{y}\\)), big parentheses with \\left(\\right) (\\(\\left(\\right)\\)), and brackets with \\left[\\right] (\\(\\left[\\right]\\)).\nMath mode has a symbol for summation. Let’s combine it with bars, fractions, subscripts, and superscipts to show sample mean \\bar{x} = \\frac{1}{n}\\sum_i^n x_i, which looks like \\(\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\).\n\\sim is how to add the tilde for distributed as. For example, X \\sim N(\\mu = 0, \\sigma = 1) shows the normal distribution \\(X \\sim N(\\mu = 0, \\sigma = 1)\\).\nMatrices are are a little bit more work in math mode. Consider the follow variance-covariance matrix:\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\[\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\]\nThis guide provides and exhaustive look at math options in Quarto.\n\n\n\n\n\n\nWarning\n\n\n\nMath mode is finicky! Small errors like mismatched parentheses or superscript and subscript errors will cause Quarto documents to fail to render. Write math carefully and render early and often.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nUse math mode to type out the equation for root mean square error (RMSE).\nDo you divide by n or n - 1?",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#cross-references",
    "href": "07_advanced-quarto.html#cross-references",
    "title": "7  Advanced Quarto",
    "section": "7.2 Cross References",
    "text": "7.2 Cross References\nCross references are useful for organizing documents that include sections, figures, tables, and equations. Cross references create hyperlinks within documents that jump to the locations of these elements. Linking sections, figures, tables, or equations helps readers navigate the document.\nCross references also automatically number the referenced elements. This means that if there are two tables (ie. Table 1 and Table 2) and a table is added between the two tables, all of the table numbers and references to the tables will automatically update.\nCross references require two bits of code within a Quarto document:\n\nA label associated with the section, figure, table, or equation.\nA reference to the labelled section, figure, table, or equation.\n\nLabels are written in brackets or as arguments in code chunks, and begin with the the type object being linked. References begin with @ followed by the label of object being linked.\n\n7.2.1 Sections\nLinking sections helps readers navigate between sections. Use brackets to label sections after headers and always begin labels with sec-. Then you can reference that section with @sec-.\n## Review {#sec-review}\n\nSee @sec-review if you are totally lost.\nThe cross references shows up like this: See ?sec-review if you are totally lost.\nIt can be helpful to turn on section numbering with number-sections: true in the YAML header. Additionally, Markdown has a native method for linking between sections.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAdd a few section headers to your Quarto document.\nAdd a cross reference to one of the section headers.\n\n\n\n\n\n7.2.2 Figures\n\n\n\n\n\n\nFigure 7.1: Penguins\n\n\n\nWe can reference figures like Figure 7.1 with @fig-penguins.\n\n\n7.2.3 Tables\nWe can link to tables in our documents. For example, we can link to the greek table with @tbl-greek Table 7.1.\n\n\n7.2.4 Equations\nWe can link to equations in our documents. For example, we can link to the binomial distribution earlier with @eq-binomial Equation 8.4.\n\n\n\n\n\n\nExercise 3\n\n\n\n\nAdd a cross reference to your RMSE equation from earlier.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#citations",
    "href": "07_advanced-quarto.html#citations",
    "title": "7  Advanced Quarto",
    "section": "7.3 Citations",
    "text": "7.3 Citations\n\n7.3.1 Zotero\nZotero is a free and open-source software for organizing research and managing citations.\n\n\n\n\n\n\nDigital Object Identifier (DOI)\n\n\n\nDOIs are persistent identifiers that uniquely identify objects including many academic papers. For example, 10.1198/jcgs.2009.07098 identifies “A Layered Grammar of Graphics” by Hadley Wickham.\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nInstall Zotero.\nFind the DOI for “Tidy Data” by Hadley Wickham.\nClick the magic wand in Zotero and paste the DOI.\n\n\n\n\n\n\n\n\n\n\n\nReview the new entry in Zotero.\n\n\n\n\n\n7.3.2 Zotero Integration\nZotero has a powerful integration with Quarto. In practice, it’s one click to add a DOI to Zotero and then one click to add a citation to Quarto.\nRStudio automatically adds My Library from Zotero. Simply switch to the Visual Editor (top left in RStudio), click “Insert”, and click “Citation”. This will open a prompt to insert a citation into the Quarto document.\nThe citation is automatically added with parentheses to go at the end of sentences. Delete the square brackets to convert the citation to an in-line citation.\nInserting the citation automatically adds the citation to the references section. Deleting the reference automatically deletes the citation from the references section.\nZotero Groups are useful for sharing citations and Zotero Group Libraries need to be added to RStudio. To set this up:\nTo set this up, in RStudio:\n\nGo to Tools and select “Global Options”\nSelect “RMarkdown” and then click “Citations”\nFor “Use Libraries” choose “Selected Libraries”\nSelect the group libraries to add\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCite “Tidy Data” by Hadley Wickham in your Quarto document.\nClick “Render”",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#more-resources",
    "href": "07_advanced-quarto.html#more-resources",
    "title": "7  Advanced Quarto",
    "section": "7.4 More Resources",
    "text": "7.4 More Resources\n\nQuarto Guide\nIterating fact sheets and web pages with Quarto",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#footnotes",
    "href": "07_advanced-quarto.html#footnotes",
    "title": "7  Advanced Quarto",
    "section": "",
    "text": "Pandoc is free software that converts documents between markup formats. For example, Pandoc can convert files to and from markdown, LaTeX, jupyter notebook (ipynb), and Microsoft Word (.docx) formats, among many others. You can see a comprehensive list of files Pandoc can convert on their About Page.↩︎\nThis table was typed as Markdown code. But sometimes it is easier to use a code chunk to create and print a table. Pipe any data frame into knitr::kable() to create a table that will be formatted in the output of a rendered Quarto document.↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more.↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more.↩︎",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html",
    "href": "14_simulation-and-sampling.html",
    "title": "8  Simulation and Sampling",
    "section": "",
    "text": "8.1 Review\nParameters are to populations what statistics are to samples. The process of learning about population parameters from statistics calculated from samples is called statistical inference.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#sec-review5",
    "href": "14_simulation-and-sampling.html#sec-review5",
    "title": "8  Simulation and Sampling",
    "section": "",
    "text": "Population\n\n\n\nA population is the entire set of observations of interest.\nFor example, a population could be everyone residing in France at a point in time. A different population could be every American ages 65 or older.\n\n\n\n\n\n\n\n\nParameter\n\n\n\nA parameter is a numerical quantity that summarizes a population.\nFor example, the population mean and population standard deviation describe important characteristics of many populations.\nMore generally, location parameters, scale parameters, and shape parameters describe many populations.\n\n\n\n\n\n\n\n\nRandom Sample\n\n\n\nA random sample is a random subset of a population.\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nA statistic is a numerical quantity that summarizes a sample.\nFor example, the sample mean and sample standard deviation describe important characteristics of many random samples.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#introduction",
    "href": "14_simulation-and-sampling.html#introduction",
    "title": "8  Simulation and Sampling",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nSimulation and sampling are important tools for statistics and data science. After reviewing/introducing basic concepts about probability theory and probability distributions, we will discuss two important applications of simulation and sampling.\n\nMonte Carlo simulation: A class of methods where values are repeatedly sampled/simulated from theoretical distributions that model a data generation process. Theoretical distributions, like the normal distribution, have closed-from representations and a finite number of parameters like mean and variance.\nResampling methods: A class of methods where values are repeatedly sampled from observed data to approximate repeatedly sampling from a population. Bootstrapping is a common resampling method.\n\nMonte Carlo methods and resampling methods have a wide range of applications. Monte Carlo simulation is used by election forecasters to predict electoral outcomes and econometricians to understand the properties of estimators. Resampling methods are used in machine learning and causal inference. Both are fundamental methods for agent-based models including microsimulation.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "href": "14_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "title": "8  Simulation and Sampling",
    "section": "8.3 Fundamentals of Probability Theory",
    "text": "8.3 Fundamentals of Probability Theory\n\n\n\n\n\n\nRandom Variable\n\n\n\n\\(X\\) is a random variable if its value is unknown and/or could change.\n\n\n\\(X\\) could be the outcome from the flip of a coin or the roll of a die. \\(X\\) could also be the amount of rain next July 4th.\n\n\n\n\n\n\nExperiment\n\n\n\nAn experiment is a process that results in a fixed set of possible outcomes.\n\n\n\n\n\n\n\n\nSet\n\n\n\nA set is a collection of objects.\n\n\n\n\n\n\n\n\nSample Space\n\n\n\nA sample space is the set of all possible outcomes for an experiment. We will denote a sample space with \\(\\Omega\\).\n\n\n\n\n\n\n\n\nDiscrete Random Variable\n\n\n\nA set is countable if there is a one-to-one correspondence from the elements of the set to some (finite) or all (countably infinite) positive integers (i.e. 1 = heads and 2 = tails).\nA random variable is discrete if its sample space is countable (finite or countably infinite).\n\n\n\n\n\n\n\n\nContinuous Random Variable\n\n\n\nA random variable is continuous if its sample space is any value in a \\(\\mathbb{R}\\) (real) interval.\nThere are infinite possible values in a real interval so the sample space is uncountable.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#discrete-random-variables",
    "href": "14_simulation-and-sampling.html#discrete-random-variables",
    "title": "8  Simulation and Sampling",
    "section": "8.4 Discrete Random Variables",
    "text": "8.4 Discrete Random Variables\n\n\n\n\n\n\nProbability Mass Function\n\n\n\nA probability mass function (PMF) computes the probability of an event in the sample space of a discrete random variable.\n\\[\np(x) = P(X = x)\n\\tag{8.1}\\]\nwhere \\(0 \\le p(x) \\le 1\\) and \\(\\sum_{x \\in \\Omega} p(x) = 1\\)\n\n\n\n\nCode\ntibble(\n  a = factor(1:6),\n  `P(X = a)` = rep(1 / 6, 6)\n) |&gt;\n  ggplot(aes(a, `P(X = a)`)) +\n  geom_col() +\n  labs(title = \"PMF for rolling a fair die\")\n\n\n\n\n\nFigure 8.1: PMF for rolling a fair die\n\n\n\n\n\n\n\n\nNow we can make statements like \\(P(X = a)\\). For example, \\(P(X = 3) = \\frac{1}{6}\\).\n\n8.4.1 Bernoulli Distribution\nA Bernoulli random variable takes on the value \\(1\\) with probability \\(p\\) and \\(0\\) with probability \\(1 - p\\). It is often used to represent coins. When \\(p = \\frac{1}{2}\\) we refer to the coin as “fair”.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Ber(p)\n\\tag{8.2}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) =\n\\begin{cases}\n1 - p &\\text{ if } x = 0 \\\\\np &\\text{ if } x = 1\n\\end{cases} = p^x(1 - p)^{1-x}\n\\tag{8.3}\\]\n\n\n8.4.2 Binomial Distribution\nA binomial random variable is the number of events observed in \\(n\\) repeated Bernoulli trials.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Bin(n, p)\n\\tag{8.4}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) = {n \\choose x} p^x(1 - p)^{n - x}\n\\tag{8.5}\\]\nWe can calculate the theoretical probability of a given draw from a binomial distribution using this PDF. For example, suppose we have a binomial distribution with \\(10\\) trials and \\(p = \\frac{1}{2}\\). The probability of drawing exactly six \\(1\\)s and four \\(0\\)s is\n\\[\np(X = 6) = \\frac{10!}{6!4!} 0.5^6(1 - 0.5)^{10 - 6} \\approx 0.2051\n\\tag{8.6}\\]\nWe can do similar calculations for each value between \\(0\\) and \\(10\\).\nWe can also take random draws from the distribution. Figure 8.2 shows 1,000 random draws from a binomial distribution with 10 trials and p = 0.5. The theoretical distribution is overlaid in red.\n\n\nCode\ntibble(\n  x = rbinom(n = 1000, size = 10, prob = 0.5)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(breaks = 0:10) +\n  geom_point(data = tibble(x = 0:10, y = map_dbl(0:10, dbinom, size = 10, prob = 0.5)),\n             aes(x, y),\n             color = \"red\") +\n  labs(\n    title = \"1,000 samples of a binomial RV\",\n    subtitle = \"Size = 10; prob = 0.5\",\n    y = NULL\n  ) \n\n\n\n\n\nFigure 8.2: 1,000 random draws from a binomial distribution with 10 trials and p = 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Error\n\n\n\nSampling error is the difference between sample statistics (estimates of population parameters) and population parameters.\n\n\nThe difference between the red dots and black bars in Figure 8.2 is caused by sampling error.\n\n\n8.4.3 Distributions Using R\nMost common distributions have R functions to\n\ncalculate the density of the pdf/pmf for a specific value\ncalculate the probability of observing a value less than \\(a\\)\ncalculate the value associated with specific quantiles of the pdf/pmf\nrandomly sample from the probability distribution\n\nLet’s consider a few examples:\nThe following answers the question: “What is the probability of observing 10 events in 10 trials when p = 0.5?”\n\ndbinom(x = 10, size = 10, prob = 0.5)\n\n[1] 0.0009765625\n\n\n\nThe following answers the question: “What’s the probability of observing 3 or fewer events in 10 trials when p = 0.5”\n\npbinom(q = 3, size = 10, prob = 0.5)\n\n[1] 0.171875\n\n\n\nThe following answers the question: “What is a 10th percentile number of events to see in 10 trials when p = 0.5?\n\nqbinom(p = 0.1, size = 10, prob = 0.5)\n\n[1] 3\n\n\n\nThe following randomly draw ten different binomially distributed random variables.\n\nrbinom(n = 10, size = 10, prob = 0.5)\n\n [1] 2 6 5 6 7 5 3 5 5 6\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nAdd dbinom() for 0, 1, and 2 when size = 10 and prob = 0.5.\nCalculate pbinom() with size = 10 and prob = 0.5. Why do you get the same answer?\nPlug the result from step 1/step 2 into qbinom() with size = 10 and prob = 0.5. Why do you get the answer 2?\nUse rbinom() with size = 10 and prob = 0.5 to sample 10,000 binomially distributed random variables and assign the output to x. Calculate mean(x &lt;= 2). How does the answer compare to step 1/step 2?\n\n\n\n\n\n\n\n\n\nPseudo-random numbers\n\n\n\nComputers use pseudo-random numbers to generate samples from probability distributions. Modern pseudo-random samplers are very random.\nUse set.seed() to make pseudo-random sampling reproducible.\n\n\n\n\n8.4.4 Poisson Random Variable\nA poisson random variable is the number of events that occur in a fixed period of time. For example, a poisson distribution can be used to model the number of visits in an emergency room between 1AM and 2AM.\nWe show that a random variable is poisson-distributed with\n\\[\nX \\sim Pois(\\lambda)\n\\tag{8.7}\\]\nThe parameter \\(\\lambda\\) is both the mean and variance of the poisson distribution. The PMF of a poisson random variable is\n\\[\np(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\tag{8.8}\\]\nFigure 8.3 shows 1,000 draws from a poisson distribution with \\(\\lambda = 10\\).\n\n\nCode\nset.seed(20200905)\n\ntibble(\n  x = rpois(1000, lambda = 10)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(limits = c(0, 29)) +\n  stat_function(fun = dpois, n = 30, color = \"red\", args = list(lambda = 10)) + \n  labs(\n    title = \"1,000 samples of a Poisson RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\nFigure 8.3: 1,000 samples of a Poisson RV\n\n\n\n\n\n\n\n\n\n\n8.4.5 Categorical Random Variable\nWe can create a custom discrete probability distribution by enumerating the probability of each event in the sample space. For example, the PMF for the roll of a fair die is\n\\[\np(x) =\n\\begin{cases}\n\\frac{1}{6} & \\text{if } x = 1\\\\\n\\frac{1}{6} & \\text{if } x = 2\\\\\n\\frac{1}{6} & \\text{if } x = 3\\\\\n\\frac{1}{6} & \\text{if } x = 4\\\\\n\\frac{1}{6} & \\text{if } x = 5\\\\\n\\frac{1}{6} & \\text{if } x = 6\n\\end{cases}\n\\tag{8.9}\\]\nThis PMF is visualized in Figure 8.1. We can sample from this PMF with\n\nsample(x = 1:6, size = 1)\n\n[1] 3\n\n\nWe can also sample with probabilities that differ for each event:\n\nsample(\n  x = c(\"rain\", \"sunshine\"), \n  size = 1, \n  prob = c(0.1, 0.9)\n)\n\n[1] \"sunshine\"\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nSample 1,000 observations from a Poisson distribution with \\(\\lambda = 20\\).\nSample 1,000 observations from a normal distribution with \\(\\mu = 20\\) and \\(\\sigma = \\sqrt{20}\\).\nVisualize and compare both distribution.\n\n\n\nWhen \\(\\lambda\\) is sufficiently large, the normal distribution is a reasonable approximation of the poisson distribution.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#continuous-random-variables",
    "href": "14_simulation-and-sampling.html#continuous-random-variables",
    "title": "8  Simulation and Sampling",
    "section": "8.5 Continuous Random Variables",
    "text": "8.5 Continuous Random Variables\n\n\n\n\n\n\nProbability Density Function (PDF)\n\n\n\nA probability density function is a non-negative, integrable function for each real value \\(x\\) that shows the relative probability of values of \\(x\\) for an absolutely continuous random variable \\(X\\).\nWe note PDF with \\(f_X(x)\\).\n\n\n\n\n\n\n\n\nCumulative Distribution Function (CDF)\n\n\n\nA cumulative distribution function (cdf) shows the probability of a random variable \\(X\\) taking on any value less than or equal to \\(x\\).\nWe note CDF with \\(F_X(x) = P(X \\le x)\\)\n\n\nHere is the PDF for a standard normal random variable:\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"PDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\nIf we integrate the entire function we get the CDF.\nCumulative Density Function (CDF): A function of a random variable \\(X\\) that returns the probability that the value \\(X &lt; x\\).\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = pnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"CDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n8.5.1 Uniform Distribution\nUniform random variables have equal probability for every value in the sample space. The distribution has two parameters: minimum and maximum. A standard uniform random has minimum = 0 and maximum = 1.\nWe show that a random variable is uniform distributed with\n\\[\nX \\sim U(a, b)\n\\tag{8.10}\\]\nThe PDF of a uniform random variable is\n\\[\nf(x) =\n\\begin{cases}\n\\frac{1}{b - a} & \\text{if } x \\in [a, b] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\tag{8.11}\\]\nStandard uniform random variables are useful for generating other random processes and imputation.\n\n\nCode\nset.seed(20200904)\n\ntibble(\n  x = runif(1000)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dunif, n = 101, color = \"red\") + \n  labs(\n    title = \"1,000 samples of a standard uniform RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\n\n\n8.5.2 Normal Distribution\nThe normal distribution is the backbone of statistical inference because of the central limit theorem.\nWe show that a random variable is normally distributed with\n\\[\nX \\sim N(\\mu, \\sigma)\n\\tag{8.12}\\]\nThe PDF of a normally distributed random variable is\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right) ^ 2\\right]\n\\tag{8.13}\\]\n\n\n\n\n\n\nFundamental Probability Formula for Intervals\n\n\n\nThe probability that an absolutely continuous random variable takes on any specific value is always zero because the sample space is uncountable. Accordingly, we express the probability of observing events within a region for absolutely continuous random variables.\nIf \\(X\\) has a PDF and \\(a &lt; b\\), then\n\\[\nP(a \\le X \\le b) = P(a \\le X &lt; b) = P(a &lt; X \\le b) = P(a &lt; X &lt; b) = \\int_a^bf(x)dx = F_X(b) - F_X(a)\n\\tag{8.14}\\]\n\n\nThe last portion of this inequality is fundamental to working with continuous probability distributions and is the backbone of much of any intro to statistics course. For example, the probability, \\(P(X &lt; 0)\\) is represented by the blue region below.\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  geom_area(stat = \"function\", fun = dnorm, fill = \"blue\", xlim = c(-4, 0)) +\n  labs(\n    title = \"PDF for a standard normal random variable\",\n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nStudent’s t-distribution and the normal distribution are closely related.\n\nUse pnorm() to calculate \\(P(X &lt; -1)\\) for a standard normal distribution.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 10.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 100.\n\n\n\nObserve how the normal distribution becomes a better approximation for Student’s t-distribution when the degrees of freedom increases.\n\n\n8.5.3 Exponential Distribution\nAn exponential random variable is the wait time between events for a poisson random variable. It is useful for modeling wait time. For example, an exponential distribution can be used to model the wait time between arrivals in an emergency room between 1AM and 2AM. It has one parameter: rate (\\(\\lambda\\)).\nWe show that a random variable is exponentially distributed with\n\\[\nX \\sim Exp(\\lambda)\n\\tag{8.15}\\]\nThe PDF of an exponential random variable is\n\\[\nf(x) = \\lambda\\exp(-\\lambda x)\n\\tag{8.16}\\]\n\n\nCode\ntibble(\n  x = rexp(n = 1000, rate = 1)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_density() +\n  stat_function(fun = dexp, n = 101, args = list(rate = 1), color = \"red\") + \n  labs(\n    title = \"1,000 samples of an exponential RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\n\n\n8.5.4 Other Distributions\n\nGeometric RV: Number of Bernoulli trials up to and including the \\(1^{st}\\) event\nNegative Binomial RV: Number of Bernoulli trials up to and including the \\(r^{th}\\) event\nGamma RV: Time until the \\(\\alpha\\) person arrives",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#parametric-density-estimation",
    "href": "14_simulation-and-sampling.html#parametric-density-estimation",
    "title": "8  Simulation and Sampling",
    "section": "8.6 Parametric Density Estimation",
    "text": "8.6 Parametric Density Estimation\nA key exercise in statistics is selecting a probability distribution to represent data and then learning the parameters of probability distributions from the data. The process is often called model fitting.\nWe are focused on parametric density estimation. Later, we will focus on nonparameteric density estimation. This section will focus on frequentist inference of population parameters from observed data. Later, we will adopt a Bayesian approach to inference.\n\n8.6.1 Maximum Likelihood Estimation\nAll of the probability distributions we have observed have a finite number of parameters. Maximum likelihood estimation is a common method for estimating these parameters.\nThe general process is\n\nPick the probability distribution that fits the observed data.\nIdentify the finite number of parameters associated with the probability distribution.\nCalculate the parameters that maximize the probability of the observed data.\n\n\n\n\n\n\n\nLikelihood\n\n\n\nLet \\(\\vec{x}\\) be observed data and \\(\\theta\\) be a parameter or parameters from a chosen probability distribution. The likelihood is the joint probability of the observed data conditional on values of the parameters.\nThe likelihood of discrete data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n p(x_i|\\theta)\n\\tag{8.17}\\]\nThe likelihood of continuous data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n f(x_i|\\theta)\n\\tag{8.18}\\]\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\n\nMaximum likelihood estimation is a process for estimating parameters for a given distribution that maximizes the log likelihood.\nIn other words, MLEs find the estimated parameters that maximize the probability of observing the observed set of data.\n\n\nWe won’t unpack how to derive the maximum likelihood estimators1 but it is easy to look up most MLEs.\n\nBinomial distribution MLEs\nSuppose we have a sample of data \\(x_1, ..., x_m\\). If the number of trials \\(n\\) is already known, then \\(p\\) is the only parameter for the binomial distribution that needs to be estimated. The MLE for \\(p\\) is \\(\\hat{p} = \\frac{\\sum_{i = 1}^n x_i}{mn}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat{p}\\).\n\nset.seed(20230909)\nx &lt;- rbinom(n = 8, size = 10, prob = 0.3)\n\nx\n\n[1] 4 3 6 3 4 3 3 2\n\n\n\nmle_binom &lt;- sum(x) / (10 * 8)\n\nmle_binom\n\n[1] 0.35\n\n\n\n\nNormal distribution MLEs\n\\(\\mu\\) and \\(\\sigma\\) are the parameters of a normal distribution. The MLEs for a normal distribution are \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i = \\bar{x}\\) and \\(s^2 = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^2\\).2\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i\\) and \\(\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\).\n\nset.seed(20230909)\nx &lt;- rnorm(n = 200, mean = 0, sd = 1)\n\n\nmean_hat &lt;- mean(x)\n\nmean_hat\n\n[1] 0.02825125\n\nsigma2_hat &lt;- mean((x - mean(x)) ^ 2)\n\nsigma2_hat\n\n[1] 0.8119682\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = mean_hat, sd = sqrt(sigma2_hat)))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nExponential distribution MLEs\n\\(\\lambda\\) is the only parameter of an exponential distribution. The MLE for an exponential distribution is \\(\\hat\\lambda = \\frac{1}{\\bar{x}}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\frac{1}{\\bar{x}}\\).\n\nset.seed(20230909)\nx &lt;- rexp(n = 200, rate = 10)\n\nmle_exp &lt;- 1 / mean(x)\n\nmle_exp\n\n[1] 10.58221\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dexp, color = \"red\", args = list(rate = mle_exp))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCreate the vector in the code chunk below:\n\n\n\nCode\nx &lt;- c(\n  30970.787, 10901.544, 15070.015, 10445.772, 8972.258, \n  15759.614, 13341.328, 18498.858, 134462.066, 17498.930, \n  7112.306, 27336.795, 75526.381, 110123.606, 32910.618, \n  16764.452, 21244.380, 18952.455, 954373.470, 4219.635,\n  7078.766, 27657.996, 18337.097, 14566.525, 14220.000, \n  21457.202, 9322.311, 26018.018, 96325.728, 26780.329, \n  25833.356, 10719.360, 8642.935, 29302.623, 10517.174,\n  33831.547, 339077.456, 5805.707, 141505.710, 28168.790, \n  10446.378, 4993.349, 27502.949, 35519.162, 45761.505, \n  26163.096, 72163.668, 15515.435, 69396.895, 84972.590, \n  67248.460, 26966.374, 24624.339, 4779.110, 23330.279,\n  196311.913, 20517.739, 80257.587, 32108.466, 9735.061, \n  20502.579, 2544.004, 165909.040, 20949.512, 16643.695, \n  30267.741, 8359.024, 13355.154, 8425.988, 4491.550,\n  32071.872, 61648.149, 75074.135, 62842.985, 26040.648, \n  68733.979, 63368.710, 11157.211, 5782.610, 3629.674, \n  44399.230, 2852.381, 8200.453, 41249.003, 15006.791,\n  808974.653, 30705.915, 6341.954, 28208.144, 5409.821,\n  54566.805, 10894.864, 4583.550, 31110.875, 43474.872, \n  69059.161, 33054.574, 8789.910, 218887.477, 11051.292, \n  3366.743, 63853.329, 68756.561, 48031.259, 11707.191,\n  26593.634, 8868.942, 19225.309, 27704.670, 10666.549, \n  47151.963, 20343.604, 123932.502, 33030.986, 5412.023, \n  23540.382, 9894.513, 52742.541, 21397.990, 25100.143,\n  23757.882, 48347.300, 4325.134, 23816.776, 11907.656, \n  24179.849, 25967.574, 7531.294, 15131.240, 21595.781, \n  40473.936, 35390.849, 4060.563, 55334.157, 37058.771, \n  34050.456, 17351.500, 7453.829, 48131.565, 10576.746,\n  26450.754, 33592.986, 21425.018, 34729.337, 77370.078, \n  5819.325, 9067.356, 19829.998, 20120.706, 3637.042, \n  44812.638, 22930.229, 29683.776, 76366.822, 15464.594, \n  1273.101, 53036.266, 2846.294, 114076.200, 14492.680, \n  55071.554, 31597.849, 199724.125, 52332.510, 98411.129, \n  43108.506, 6580.620, 12833.836, 8846.348, 7599.796, \n  6952.447, 30022.143, 24829.739, 40784.581, 8997.219,\n  3786.354, 11515.298, 116515.617, 137873.967, 3282.185,\n  107886.676, 13184.850, 51083.235, 2907.886, 51827.538, \n  37564.196, 23196.399, 20169.037, 9020.364, 11118.250, \n  56930.060, 11657.302, 84642.584, 44948.450, 16610.166, \n  5509.231, 4770.262, 15614.233, 5993.999, 22628.114\n)\n\n\n\nVisualize the data with a relative frequency histogram.\nCalculate the MLEs for a normal distribution and add a normal distribution to the visualization in red.\nCalculate the MLEs for a log-normal distribution and add a log-normal distribution to the visualization in blue.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#multivariate-random-variables",
    "href": "14_simulation-and-sampling.html#multivariate-random-variables",
    "title": "8  Simulation and Sampling",
    "section": "8.7 Multivariate Random Variables",
    "text": "8.7 Multivariate Random Variables\nWe’ve explored univariate or marginal distributions thus far. Next, we will focus on multivariate distributions.\n\n\n\n\n\n\nMultivariate Distribution\n\n\n\nA multivariate distribution is a probability distribution that shows the probability (discrete) or relative probability (continuous) of more than one random variable.\nMultivariate distributions require describing characteristics of random variables and the relationships between random variables.\n\n\n\n8.7.1 Multivariate Normal Distribution\nThe multivariate normal distribution is a higher-dimensional version of the normal distribution.\nInstead of a single mean and a single variance, the \\(k\\)-dimensional multivariate normal distribution has a vector of means of length \\(k\\) and a \\(k\\)-by-\\(k\\) variance-covariance matrix3. The vector describes the central tendencies of each dimension of the multivariate distribution and the matrix describe the variance of the distributions and relationships between the distributions.\nWe show that a random vector is multivariate normally distributed with\n\\[\n\\vec{X} \\sim \\mathcal{N}(\\vec\\mu, \\boldsymbol\\Sigma)\n\\tag{8.19}\\]\nThe PDF of a multivariate normally distributed random variable is\n\\[\nf(x) = (2\\pi)^{-k/2}det(\\boldsymbol\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\vec{x} - \\vec\\mu)^T\\boldsymbol\\Sigma^{-1}(\\vec{x} - \\vec\\mu)\\right)\n\\tag{8.20}\\]\nFunctions for working with multi-variate normal distributions from library(MASS). Figure 8.4 shows three different random samples from 2-dimensional multivariate normal distributions.\nSigma1 &lt;- matrix(c(1, 0.8, \n                   0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n  \nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma1) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nSigma2 &lt;- matrix(c(1, 0.2, \n                   0.2, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma2) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nSigma3 &lt;- matrix(c(1, -0.8, \n                   -0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma3) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\n\n\nFigure 8.4: Samples from Multivariate Normal Distributions\n\n\n\n\n\n\n\n(a) Strong Positive Covariance\n\n\n\n\n\n\n\n\n\n\n\n(b) Weak Covariance\n\n\n\n\n\n\n\n\n\n\n\n(c) Strong Negative Covariance",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#monte-carlo-methods",
    "href": "14_simulation-and-sampling.html#monte-carlo-methods",
    "title": "8  Simulation and Sampling",
    "section": "8.8 Monte Carlo Methods",
    "text": "8.8 Monte Carlo Methods\nSimulation methods, including Monte Carlo simulation, are used for policy analysis:\n\nFiveThirtyEight and the New York Times use Monte Carlo simulation to predict the outcomes of elections.\nThe Social Security Administration uses microsimulation to evaluate the distributional impact of Social Security reforms.\nThe Census Bureau uses simulation to understand the impact of statistical disclosure control on released data.\nEconometricians and statisticians use Monte Carlo simulation to demonstrate the properties of estimators.\n\nWe can make probabilistic statements about common continuous random variables because their PDFs are integrable or at least easy enough to approximate with lookup tables. We can make probabilistic statements about common discrete random variables with summation.\nBut we often want to make probabilistic statements about uncommon or complex probability distributions. Maybe the probability distribution of the random variable doesn’t have a tractable integral (i.e. the area under the curve can’t practically be computed). Or maybe there are too many potential outcomes (e.g. rays of light emitting from a light bulb in the Marble Science video linked at the top).\nMonte Carlo: A Monte Carlo method estimates a deterministic quantity using stochastic (random) sampling.\nMonte Carlo but easier this time: A Monte Carlo method takes hundreds or thousands of independent samples from a random variable or variables and then approximates fixed population quantities with summaries of those draws. The quantities could be population parameters like a population mean or probabilities.\nMonte Carlo methods have three major applications:\n\nSampling – Monte Carlo simulation allows for sampling from complex probability distributions. The samples can be used to model real-world events (queues), to model outcomes with uncertain model inputs (election modeling), to generate fake data with known parameters to evaluate statistical methods (model selection when assumptions fail), and to draw from the posteriors of Bayesian models.\nNumerical integration – Integration, as noted above, is important to calculating probabilities and ultimately calculating quantities like expected value or the intervals. Monte Carlo methods can approximate multidimensional integrals that will never be directly solved by computers or simplify estimating probabilities when there are uncountably many potential outcomes (Solitaire).\nOptimization – Monte Carlo methods can be used for complex optimization. We will not focus on optimization.\n\nLet’s explore some examples:\n\n8.8.1 Example 1: Coin Tossing\nWe can calculate the proportion of tosses of a fair coin that we expect to turn up heads by finding the expected value of the binomial distribution and dividing by the number of tosses. But suppose we can’t… Or maybe we wish to confirm our calculations with simulations…\nLet’s try repeated sampling from a binomial distribution to approximate this process:\n\n#' Count the proportion of n tosses that turn up heads\n#'\n#' @param n An integer for the number of tosses\n#'\n#' @return The proportion of n tosses that turn up heads\n#' \ncount_heads &lt;- function(n) {\n  \n  # toss the fair coin n times\n  coin_tosses &lt;- rbinom(n = n, size = 1, prob = 0.5)\n    \n  coin_tosses &lt;- if_else(coin_tosses == 1, \"heads\", \"tails\")\n  \n  # calculate the proportion of heads\n  prop_heads &lt;- mean(coin_tosses == \"heads\")\n  \n  return(prop_heads)\n  \n}\n\nLet’s toss the coin ten times.\n\nset.seed(11)\ncount_heads(n = 10)\n\n[1] 0.3\n\n\nOk, we got 0.3, which we know isn’t close to the expected proportion of 0.5. What if we toss the coin 1 million times.\n\nset.seed(20)\ncount_heads(n = 1000000)\n\n[1] 0.499872\n\n\nOk, that’s more like it.\n\\[\\cdot\\cdot\\cdot\\]\nMonte Carlo simulation works because of the law of large numbers. The law of large numbers states that the probability that the average of trials differs from the expected value converges to zero as the number of trials goes to infinity.\nMonte Carlo simulation basically repeats the ideas behind frequentist inferential statistics. If we can’t measure every unit in a population then we can sample a representative population and estimate parameters about that population.\nThe keys to Monte Carlo simulation are randomness and independent and identically distributed sampling (i.i.d.).\n\n\n8.8.2 Example 2: Bootstrap Sampling\nOn average, a bootstrap sample includes about 63% of the observations from the data that are sampled. This means that an individual bootstrap sample excludes 37% of the observations from the source data!\nSo if we bootstrap sample from a vector of length 100, then \\(\\frac{63}{100}\\) values will end up in the bootstrap sample on average and \\(\\frac{37}{100}\\) of the values will be repeats on average.\nWe can explore this fact empirically with Monte Carlo simulation using repeated samples from a categorical distribution. We will use sample().\n\n#' Calculate the proportion of unique values from a vector of integers included \n#' in a bootstrap sample\n#'\n#' @param integers A vector of integers\n#'\n#' @return The proportion of integers included in the bootstrap sample\n#' \ncount_uniques &lt;- function(integers) {\n  \n  # generate a bootstrap sample\n  samples &lt;- sample(integers, size = length(integers), replace = TRUE)\n  \n  # calculate the proportion of unique values from the original vector\n  prop_unique &lt;- length(unique(samples)) / length(integers)\n  \n  return(prop_unique)\n  \n}\n\nLet’s bootstrap sample 100,000 times.\n\n# pre-allocate the output vector for efficient computation\nprop_unique &lt;- vector(mode = \"numeric\", length = 100000)\nfor (i in seq_along(prop_unique)) {\n  \n  prop_unique[i] &lt;- count_uniques(integers = 1:100)\n  \n}\n\nFinally, calculate the mean proportion and estimate the expected value.\n\nmean(prop_unique)\n\n[1] 0.6337935\n\n\nWe can also calculate a 95% confidence interval using the bootstrap samples.\n\nquantile(prop_unique, probs = c(0.025, 0.975))\n\n 2.5% 97.5% \n 0.57  0.69 \n\n\n\n\n8.8.3 Example 3: \\(\\pi\\)\nConsider one of the examples from Marble Science: Monte Carlo Simulation. Imagine we don’t know \\(\\pi\\) but we know that the equation for the area of a square is \\(r ^ 2\\) and the equation for the area of a circle is \\(\\pi r ^ 2\\). If we know the ratio of the areas of the circle and the square, then we can solve for \\(\\pi\\).\n\\[\n\\frac{\\text{Area of Cirle}}{\\text{Area of Square}} = \\frac{\\pi r ^ 2}{r ^ 2} = \\pi\n\\tag{8.21}\\]\nThis is simply solved with Monte Carlo simulation. Randomly sample a bivariate uniform random variables and count how frequently the values are inside of the square or inside the circle.\n\nexpand_grid(\n  x = seq(0, 4, 0.1),\n  y = seq(0, 2, 0.1)\n) |&gt;\nggplot() +\n  ggforce::geom_circle(aes(x0 = 2, y0 = 1, r = 1), fill = \"blue\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), fill = \"red\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 3, ymin = 0, ymax = 2), fill = NA, color = \"black\") +\n  coord_fixed()\n\nWarning in ggforce::geom_circle(aes(x0 = 2, y0 = 1, r = 1), fill = \"blue\", : All aesthetics have length 1, but the data has 861 rows.\nℹ Did you mean to use `annotate()`?\n\n\nWarning in geom_rect(aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), fill = \"red\", : All aesthetics have length 1, but the data has 861 rows.\nℹ Did you mean to use `annotate()`?\n\n\nWarning in geom_rect(aes(xmin = 0, xmax = 3, ymin = 0, ymax = 2), fill = NA, : All aesthetics have length 1, but the data has 861 rows.\nℹ Did you mean to use `annotate()`?\n\n\n\n\n\n\n\n\n\n\nnumber_of_samples &lt;- 2000000\n\n# sample points in a rectangle with x in [0, 3] and y in [0, 2]\nset.seed(20210907)\nsamples &lt;- tibble(\n  x = runif(number_of_samples, min = 0, max = 3),\n  y = runif(number_of_samples, min = 0, max = 2)\n)\n\n# calculate if (x, y) is in the circle, the square, or neither\nsamples &lt;- samples |&gt;\n  mutate(\n    in_square = between(x, 0, 1) & between(y, 0, 1),\n    in_circle = (x - 2) ^ 2 + (y - 1) ^ 2 &lt; 1\n  ) \n\n# calculate the proportion of samples in each shape\nprop_in_shapes &lt;- samples |&gt;\n  summarize(\n    prop_in_square = mean(in_square), \n    prop_in_circle = mean(in_circle)\n  ) \n\n# calculate the ratio\nprop_in_shapes |&gt;\n  mutate(prop_in_circle / prop_in_square) |&gt;\n  print(digits = 3)\n\n# A tibble: 1 × 3\n  prop_in_square prop_in_circle `prop_in_circle/prop_in_square`\n           &lt;dbl&gt;          &lt;dbl&gt;                           &lt;dbl&gt;\n1          0.166          0.524                            3.15\n\n\nThe answer approximates \\(\\pi\\)!\n\n\n8.8.4 Example 4: Simple Linear Regression\nThe goal of statistical inference is to use data, statistics, and assumptions to infer parameters and probabilities about a population. Typically we engage in point estimation and interval estimation.\nSometimes it is useful to reverse this process to understand and confirm the properties of estimators. That means starting with known population parameters, simulating hundreds or thousands of samples from that population, and then observing point estimates and interval estimates over those samples.\n\nLinear Regression Assumptions\n\nThe population model is of the linear form \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\nThe estimation data come from a random sample or experiment\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) independently and identically distributed (i.i.d.)\n\\(x\\) has variance and there is no perfect collinearity in \\(x\\)\n\n\n\nStatistics\nIf we have one sample of data, we can estimate points and intervals with the following estimators:\nThe residual standard error is\n\\[\n\\hat\\sigma = \\frac{\\sum e_i^2}{(n - 2)}\n\\tag{8.22}\\]\nThe estimate of the slope is\n\\[\n\\hat\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\n\\tag{8.23}\\]\nThe standard error of the estimate of the slope, which can be used to calculate t-statistics and confidence intervals, is\n\\[\n\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2}\n\\tag{8.24}\\]\nThe estimate of the intercept term is\n\\[\n\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1\\bar{x}\n\\tag{8.25}\\]\nThe standard error of the intercept is\n\\[\n\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]}\n\\tag{8.26}\\]\n\n\nMonte Carlo Simulation\nConsider a simple linear regression model with the following population model:\n\\[y = 5 + 15x + \\epsilon\\]\nWe can calculate the above statistics over repeated sampling and confirm their asymptotic properties with Monte Carlo simulation.\nFirst, create 1,000 random samples from the population.\n\nset.seed(20210906)\n\ndata &lt;- map(\n  .x = 1:1000,\n  .f = ~ tibble(\n    x = rnorm(n = 10000, mean = 0, sd = 2),\n    epsilon = rnorm(n = 10000, mean = 0, sd = 10),\n    y = 5 + 15 * x + epsilon\n  )\n)\n\nNext, estimate a simple linear regression model for each draw of the population. This step includes calculating \\(\\hat\\sigma\\), \\(\\hat\\beta_1\\), \\(\\hat\\beta_0\\), \\(\\hat{SE}(\\hat\\beta_1)\\), and \\(\\hat{SE}(\\hat\\beta_0)\\).\n\nestimated_models &lt;- map(\n  .x = data,\n  .f = ~ lm(y ~ x, data = .x)\n)\n\nNext, we extract the coefficients and confidence intervals.\n\ncoefficients &lt;- map_df(\n  .x = estimated_models,\n  .f = tidy,\n  conf.int = TRUE\n)\n\nLet’s look at estimates of the residual standard error. The center of the distribution closely matches the population standard deviation of the error term.\n\nmodel_metrics &lt;- map_df(\n  .x = estimated_models,\n  .f = glance\n) \n\nmodel_metrics |&gt;\n  ggplot(aes(sigma)) +\n  geom_histogram() +\n  labs(title = \"Plot of the estimated residual standard errors\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLet’s plot the coefficients. The centers approximately match the population intercept of 5 and slope of 15.\n\ncoefficients |&gt;\n  ggplot(aes(estimate)) +\n  geom_histogram() +\n  facet_wrap(~term, scales = \"free_x\") +\n  labs(title = \"Coefficients estimates across 10,000 samples\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe standard deviation of the coefficients also matches the standard errors.\n\\[\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2} = \\sqrt\\frac{10^2}{40,000} = 0.05\\]\n\\[\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]} = \\sqrt{10^2\\left[\\frac{1}{10,000} + 0\\right]} = 0.1\\]\n\ncoefficients |&gt;\n  group_by(term) |&gt;\n  summarize(\n    mean(estimate), \n    sd(estimate)\n  )\n\n# A tibble: 2 × 3\n  term        `mean(estimate)` `sd(estimate)`\n  &lt;chr&gt;                  &lt;dbl&gt;          &lt;dbl&gt;\n1 (Intercept)             5.00         0.100 \n2 x                      15.0          0.0482\n\n\nLet’s look at how often the true parameter is inside the 95% confidence interval. It’s close although not exactly 95%.\n\ncoefficients |&gt;\n  filter(term == \"x\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 15 & conf.high &gt;= 15))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1           0.959\n\ncoefficients |&gt;\n  filter(term == \"(Intercept)\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 5 & conf.high &gt;= 5))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1            0.95\n\n\n\n\n\n8.8.5 Example 5: Queuing Example\nSuppose we have a queue at a Social Security field office. Let \\(t\\) be time. When the office opens, \\(t = 0\\) and the queue is empty.\nLet, \\(T_i\\) be the interarrival time and \\(T_i \\sim exp(\\lambda_1)\\)\nLet, \\(S_i\\) be the service time time and \\(S_I \\sim exp(\\lambda_2)\\)\nFrom these two random variables, we can calculate the arrival times, departure times, and wait times for each customer.\n\nThe arrival times are the cumulative sum of the interarrival times.\nThe wait times are zero if a person arrives after the person before them and the difference between the prior person’s departure and the current person’s arrival otherwise.\nThe departure time is arrival time plus the wait time plus the service time.\n\n\nset.seed(19920401)\nqueue &lt;- generate_queue(t = 100, lambda = 1, mu = 1)\n\nqueue\n\n# A tibble: 100 × 5\n   interarrival_time arrival_time service_time wait_time departure_time\n               &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1             0.467        0.467       5.80        0              6.27\n 2             1.97         2.43        0.0892      3.83           6.36\n 3             2.70         5.13        1.26        1.22           7.61\n 4             0.335        5.47        4.85        2.14          12.5 \n 5             0.372        5.84        1.89        6.62          14.3 \n 6             1.72         7.56        0.507       6.78          14.9 \n 7             2.28         9.84        0.932       5.01          15.8 \n 8             0.339       10.2         1.18        5.61          17.0 \n 9             2.54        12.7         1.42        4.25          18.4 \n10             0.572       13.3         0.157       5.10          18.5 \n# ℹ 90 more rows\n\n\n\nflow &lt;- tibble::tibble(\n  time = c(queue$arrival_time, queue$departure_time),\n  type = c(rep(\"arrival\", length(queue$arrival_time)), \n           rep(\"departure\", length(queue$departure_time))), \n  change = c(rep(1, length(queue$arrival_time)), rep(-1, length(queue$departure_time))),\n) |&gt;\n  arrange(time) |&gt; \n  filter(time &lt; 100) |&gt; \n  mutate(queue = cumsum(change) - 1)\n\nflow |&gt;\n  ggplot(aes(time, queue)) +\n  geom_step() +\n  labs(title = \"Simulated queue at the Social Security office\")\n\n\n\n\n\n\n\n\nThis is interesting, but it’s still only one draw from a Monte Carlo simulation. What if we are interested in the distribution of wait times for the fifth customer?\n\n#' Generate wait times at the queue\n#'\n#' @param person_number An integer for the person of interest\n#' @param iterations An integer for the number of Monte Carlo iterations\n#' @param t A t for the maximum time\n#'\n#' @return A vector of wait times\n#' \ngenerate_waits &lt;- function(person_number, iterations, t) {\n  \n  wait_time &lt;- vector(mode = \"numeric\", length = iterations)\n  for (i in seq_along(wait_time)) {\n    \n    wait_time[i] &lt;- generate_queue(t = t, lambda = 1, mu = 1)$wait_time[person_number]\n    \n  }\n  \n  return(wait_time)\n  \n}\n\nset.seed(20200908)\nwait_time &lt;- generate_waits(person_number = 5, iterations = 10000, t = 50)\n\nmean(wait_time)\n\n[1] 1.464371\n\nquantile(wait_time, probs = c(0.025, 0.5, 0.975))\n\n     2.5%       50%     97.5% \n0.0000000 0.9222193 5.8742015 \n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCreate a Monte Carlo simulation of an unfair coin toss where p = 0.6.\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nSuppose we have three independent normally-distributed random variables.\n\\[X_1 \\sim N(\\mu = 0, \\sigma = 1)\\]\n\\[X_2 \\sim N(\\mu = 1, \\sigma = 1)\\]\n\\[X_3 \\sim N(\\mu = 2, \\sigma = 1)\\]\n\nUse Monte Carlo simulation with 10,000 repetitions to estimate how often \\(X_{i1} &lt; X_{i2} &lt; X_{i3}\\).\n\n\n\n\n\n8.8.6 More examples of Monte Carlo simulation\n\nfivethirtyeight 2020 election forecast use\nU.S. Census Bureau simulation of data collection operations\n\n\nMarkov Chain Monte Carlo\nBayesian statisticians estimate posterior distributions of parameters that are combinations of prior distributions and sampling distributions. Outside of special cases, posterior distributions are difficult to identify. Accordingly, most Bayesian estimation uses an extension of Monte Carlo simulation called Markov Chain Monte Carlo or MCMC.\n\n\n\n8.8.7 One Final Note\nMonte Carlo simulations likely underestimate uncertainty. Monte Carlo simulations only capture aleatoric uncertainty and they don’t capture epistemic uncertainty.\nAleatoric uncertainty: Uncertainty due to probabilistic variety\nEpistemic uncertainty: Uncertainty due to a lack of knowledge\nIn other words, Monte Carlo simulations estimates assume the model is correct, which is almost certainly never fully true. Be transparent. Be humble.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#sampling-from-observed-data",
    "href": "14_simulation-and-sampling.html#sampling-from-observed-data",
    "title": "8  Simulation and Sampling",
    "section": "8.9 Sampling from Observed Data",
    "text": "8.9 Sampling from Observed Data\nUntil now, we’ve only discussed sampling from closed-form theoretical distributions. We also called this process simulation. There are many applications where we may want to sample from observed data.\nWe can break these methods into two general approaches:\n\nSampling\nResampling\n\n\n8.9.1 Sampling\n\n\n\n\n\n\nSampling\n\n\n\nSampling is the process of selecting a subset of data. Probability sampling is the process of selecting a sample when the selection uses randomization.\n\n\nSampling has many applications:\n\nReducing costs for the collection of data\nImplementing machine learning algorithms\nResampling\n\n\n\n8.9.2 Resampling\n\n\n\n\n\n\nResampling\n\n\n\nResampling is the process of repeatedly sampling from observed data to approximate the generation of new data.\n\n\nThere are at least three popular resampling methods:\n\nCross Validation: Partitioning the data and shuffling the partitions to understand the accuracy of predictive models.\nBootstrap sampling: Repeated sampling with replacement to estimate sampling distributions from observed data.\nJackknife: Leave-one-out sampling to estimate the bias and standard error of a statistic.\n\nWe focused on cross-validation for machine learning and predictive modeling in data science for public policy. We will use this approach again for predictive modeling.\nWe will also learn about bootstrap sampling when we discuss nonparametric statistics.\n\n\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical Inference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#footnotes",
    "href": "14_simulation-and-sampling.html#footnotes",
    "title": "8  Simulation and Sampling",
    "section": "",
    "text": "(Casella and Berger 2002) offers a robust introduction to deriving maximum likelihood estimators.↩︎\nNote that the MLE for variance is biased.↩︎\nCorrelation may be more familiar than covariance. Sample correlation is standardized sample covariance. \\(Corr(\\vec{x}, \\vec{y}) = \\frac{Cov(\\vec{x}, \\vec{y})}{S_{\\vec{x}}S_{\\vec{y}}}\\). Correlation is also between -1 and 1 inclusive. Covariance can take on any real value.↩︎",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#data-structures",
    "href": "01_intro-r.html#data-structures",
    "title": "1  Introduction to R",
    "section": "1.7 Data Structures",
    "text": "1.7 Data Structures\nData analysis is not possible without data structures for data. R has several important data structures that shape how information is stored and processed.\n\n1.7.1 Vectors\nVectors are one-dimensional arrays that contain one and only one type of data. Atomic vectors in R are homogeneous. There are six types of atomic vectors:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex (uncommon)\nraw (uncommon)\n\nFor now, the simplest way to create a vector is with c(), the combine function.\n\n# a logical vector\nc(TRUE, FALSE, FALSE)\n\n[1]  TRUE FALSE FALSE\n\n# an integer vector\nc(1, 2, 3)\n\n[1] 1 2 3\n\n# a double vector\nc(1.1, 2.2, 3.3)\n\n[1] 1.1 2.2 3.3\n\n# a character vector\nc(\"District of Columbia\", \"Virginia\", \"Maryland\")\n\n[1] \"District of Columbia\" \"Virginia\"             \"Maryland\"            \n\n\nThe class() function can be used to identify the type, or class, of an object. For example:\n\nclass(c(TRUE, FALSE, FALSE))\n\n[1] \"logical\"\n\nclass(c(1, 2, 3))\n\n[1] \"numeric\"\n\nclass(c(\"District of Columbia\", \"Virginia\", \"Maryland\"))\n\n[1] \"character\"\n\n\nIf you create a vector with mixed data types, R will coerce all of the values to a single type:\n\nc(TRUE, 1, \"District of Columbia\")\n\n[1] \"TRUE\"                 \"1\"                    \"District of Columbia\"\n\nclass(c(TRUE, 1, \"District of Columbia\"))\n\n[1] \"character\"\n\n\nLists are one- or multi-dimensional arrays that are made up of other lists. Lists are heterogeneous - they can contain many lists of different types and dimensions. A vector is a list but a list is not necessarily a vector.\nNULL is the null object in R. It means a value does not exist.\n![Source: R4DS\nNA is a missing value of lenth 1 in R. NAs are powerful representations in R with special properties. NA is a contagious value in R that will override all calculations.\n\n1 + 2 + 3 + NA\n\n[1] NA\n\n\nThis forces programmers to be deliberate about missing values. This is a feature, not a bug!\nR contains special functions and function arguments for handling NAs. For example, we can wrap a vector with missing values in is.na() to create a vector of Booleans where TRUE represents an element that is an NA and FALSE represents an element that is not an NA.\n\nis.na(c(1, 2, NA))\n\n[1] FALSE FALSE  TRUE\n\n\nNote: NA and NULL have different meanings! NULL means no value exists. NA means a value could exist but it is unknown.\n\n\n1.7.2 Matrices\nMatrices are multi-dimensional arrays where every element is of the same type. Most data in data science contains at least numeric information and character information. Accordingly, we will not use matrices much in this course.\n\n\n1.7.3 Data frames\nInstead, data frames, and their powerful cousins tibbles, are the backbone of data science and this course. Data frames are two-dimensional arrays where each column is a list (usually a vector). Most times, each column will be of one type while a given row will contain many different types. We usually refer to columns as variables and rows as observations.\nHere are the first six rows of a data frame with information about diamond prices and diamond characteristics:\n\nhead(ggplot2::diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\n\n\n1.7.4 tibbles\ntibbles are special data frames that have a few extra features:\n\nOnly the first ten rows of tibbles print by default\nExtra meta data are printed with tibbles\nThey have some convenient protections against partial subsetting\nThey are easier to create from scratch in a .R script\n\n\ntibble(\n  a = c(TRUE, FALSE, FALSE),\n  b = c(1, 2, 3),\n  c = c(1.1, 2.2, 3.3),\n  d = c(\"District of Columbia\", \"Virginia\", \"Maryland\")\n)\n\n# A tibble: 3 × 4\n  a         b     c d                   \n  &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;               \n1 TRUE      1   1.1 District of Columbia\n2 FALSE     2   2.2 Virginia            \n3 FALSE     3   3.3 Maryland            \n\n\nFrom this moment forward, I will use data frame to mean tibble.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#assignment",
    "href": "01_intro-r.html#assignment",
    "title": "1  Introduction to R",
    "section": "1.8 Assignment",
    "text": "1.8 Assignment\nR can operate on many different vectors and data frames in the same R session. This creates much flexibility. It also means most unique objects in an R session need unique names.\n&lt;- is the assignment operator. An object created on the right side of an assignment operator is assigned to a name on the left side of an assignment operator. Assignment operators are important for saving the consequences of operations and functions. Without assignment, the result of a calculation is not saved for use in a future calculation. Operations without assignment operators will typically be printed to the console but not saved for future use.\n\n# this important calculation is saved to the R environment\nimportant_calculation &lt;- 2 + 2\n\n# this important calculation is NOT saved to the R environment\n2 + 2\n\n[1] 4\n\n\nStyle note: Objects should be given names that are “concise and meaningful”. Generally the names should be nouns and only use lowercase letters, numbers, and underscores _ (this is referred to as snake case).\n\n\n\n\n\n\nExercise 4\n\n\n\nWrite three arithmetic operations in R and assign them to unique names. Then perform arithmetic operations using the named results. For example:\n\na &lt;- 5 + 5 + 5\nb &lt;- 6 - 6 - 6\n\na + b\n\n[1] 9",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#functions",
    "href": "01_intro-r.html#functions",
    "title": "1  Introduction to R",
    "section": "1.9 Functions",
    "text": "1.9 Functions\n+, -, *, and / are great, but data science requires a lot more than just basic arithmetic.\nR contains many more functions that can perform mathematical operations, control your computer operating system, process text data, and more. In fact, R is built around functions.\nBecause R was developed by statisticians, R’s functions have a lot in common with mathematical functions.\n\nFunctions have inputs and outputs\nEach input has one and only one output (unless is involves a random process)\n\nFunctions are recognizable because they end in (). For example, the following calculates the mean of a numeric vector two ways:\n\nmean(x = c(1, 2, 3))\n\n[1] 2\n\nnumeric_vector &lt;- c(1, 2, 3)\nmean(x = numeric_vector)\n\n[1] 2\n\n\nThis free book chapter contains more information about functional programming in R.\n\n1.9.1 ?\nDocumentation for functions can be easily accessed by prefacing the function name with ? and dropping the ().\n\n?mean\n\nThe documentation typically includes a description, a list of the arguments, references, a list of related functions, and examples. The examples are incredibly useful.\n\n\n1.9.2 Arguments\nR functions typically contain many arguments. For example, mean() has x, trim, and na.rm. Many arguments have default values and don’t need to be included in function calls. Default values can be seen in the documentation. trim = 0 and na.rm = FALSE are the defaults for mean().\nArguments can be passed to functions implicitly by position or explicitly by name.\n\nnumeric_vector &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# by position (correctly)\nmean(numeric_vector, 0.2)\n\n[1] 5.5\n\n\n\n# by position (incorrectly)\nmean(0.2, numeric_vector)\n\n\n# by name\nmean(x = numeric_vector, trim = 0.2)\n\n[1] 5.5\n\n\nFunction calls can include arguments by position and by name. The first argument in most functions is data or x. It is custom to usually include the first argument by position and then all subsequent arguments by name.\n\nmean(numeric_vector, trim = 0.2)\n\n[1] 5.5",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#r-packages",
    "href": "01_intro-r.html#r-packages",
    "title": "1  Introduction to R",
    "section": "1.10 R Packages",
    "text": "1.10 R Packages\n\n1.10.1 Base R\nOpening RStudio automatically loads “base R”, a fundamental collection of code and functions that handles simple operations like math and system management.\nFor years, R was only base R. New paradigms in R have developed over the last fifteen years that are more intuitive and more flexible than base R. Next week, we’ll discuss the “tidyverse”, the most popular paradigm for R programming.\nAll R programming will involve some base R, but much base R has been replaced with new tools that are more concise. Just know that at some point you may end up on a Stack Overflow page that looks like alphabet soup because it’s in a paradigm that you have not learned.\nOne other popular R paradigm is data.table. We will not discuss data.table in this class.\n\n\n1.10.2 Extensibility\nR is an extensible programming language. It was designed to allow for new capabilities and functionality.\nR is also open source. All of it’s source code is publicly available.\nThese two features have allowed R users to contribute millions of lines of code that can be used by other users without condition or compensation. The main mode of contribution are R packages. Packages are collections of functions and data that expand the power and usefulness of R.\nThe predecessor of R, the S programming language, was designed to call FORTRAN subroutines. Accordingly, many R packages used to call compiled FORTRAN code. Now, many R packaged call compiled C++ code. This gives users the intuition of R syntax with better performance. Here’s a brief history of R and S.\n\n\n1.10.3 CRAN\nMost R packages are stored on the Comprehensive R Archive Network (CRAN). Packages must pass a modest number of tests for stability and design to be added to CRAN.\n\n\n1.10.4 install.packages()\nPackages can be directly installed from CRAN using install.packages(). Simply include the name of the desired package in quotes as the only argument to the function.\nInstallation need only happen once per computer per package version. It is customary to never include install.packages() in a .R script.\n\n\n\n\n\n\nExercise 5\n\n\n\nFor practice, let’s install the RXKCD package so we can view some comics in R.\ninstall.packages(\"RXKCD\")\n\n\n\n\n1.10.5 library()\nAfter installation, packages need to be loaded once per R session using the library() function. While install.packages() expects a quoted package name, it is best practice to use unquoted names in library().\nIt is a good idea to include library() statements at the top of scripts for each package used in the script. This way it is obvious at the top of the script which packages are necessary.\n\nExercise 6\nFor practice let’s load the RXKCD package.\n\nlibrary(RXKCD)\n\nFinally, let’s look at some comics!\n\nmeta_data &lt;- getXKCD(which = 327)\n\n\n\n\n\n\n\n\n\nmeta_data &lt;- getXKCD(which = 303)\n\n\n\n\n\n\n\n\n\nmeta_data &lt;- getXKCD(which = 149)\n\n\n\n\n\n\n\n\n\\[\\cdot\\cdot\\cdot\\]\n\n\n\n1.10.6 ::\nSometimes two packages have functions with the same name. :: can be used to directly access an exported R object from a package’s namespace.\ndplyr::select()\nMASS::select()",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#organizing-an-analysis",
    "href": "01_intro-r.html#organizing-an-analysis",
    "title": "1  Introduction to R",
    "section": "1.11 Organizing an Analysis",
    "text": "1.11 Organizing an Analysis\n\n1.11.1 R Projects\nR Projects, proper noun, are the best way to organize an analysis. They have several advantages:\n\nThey make it possible to concurrently run multiple RStudio sessions.\nThey allow for project-specific RStudio settings.\nThey integrate well with Git version control.\nThey are the “node” of relative file paths. (more on this in a second) This makes code highly portable.\n\n\n\n\n\n\n\nExercise 7\n\n\n\nBefore setting up an R Project, go to Tools &gt; Global Options and uncheck “Restore most recently opened project at startup”.\n\n\nEvery new analysis in R should start with an R Project. First, create a directory that holds all data, scripts, and files for the analysis. You can do this right in RStudio by clicking the “New Folder” button at the top of the “Files” tab located in the top or bottom right of RStudio. Storing files and data in a sub-directories is encouraged. For example, data can be stored in a folder called data/.\nNext, click “New Project…” in the top right corner.\n\nWhen prompted, turn your recently created “Existing Directory” into a project.\n\nUpon completion, the name of the R Project should now be displayed in the top right corner of RStudio where it previously displayed “Project: (None)”. Once opened, .RProj files do not need to be saved. Double-clicking .Rproj files in the directory is now the best way to open RStudio. This will allow for the concurrent use of multiple R sessions and ensure the portability of file paths. Once an RStudio project is open, scripts can be opened by double-clicking individual files in the computer directory or clicking files in the “Files” tab.\n\n\n\n\n\n\nExercise 8\n\n\n\nLet’s walk through this process and create an R project for this class.\n\n\n\n\n1.11.2 Filepaths\nWindows file paths are usually delimited with \\. *nix file paths are usually delimited with /. Never use \\ in file paths in R. \\ is an escape character in R and will complicate an analysis. Fortunately, RStudio understands / in file paths regardless of operating system.\nNever use setwd() in R. It is unnecessary, it makes code unreproducible across machines, and it is rude to collaborators. R Projects create a better framework for file paths. Simply treat the directory where the R Project lives as the working directory and directories inside of that directory as sub-directories.\nFor example, say there’s a .Rproj called starwars-analysis.Rproj in a directory called starwars-analysis/. If there is a .csv in that folder called jedi.csv, the file can be loaded with read_csv(\"jedi.csv\") instead of read_csv(\"H:/alena/analyses/starwars-analysis/jedi.csv\"). If that file is in a sub-directory of starwars-analysis called data, it can be loaded with read_csv(\"data/jedi.csv\"). The same concepts hold for writing data and graphics.\nThis simplifies code and makes it portable because all relative file paths will be identical on all computers. To share an analysis, simply send the entire directory to a collaborator or share it with GitHub.\nHere’s an example directory:",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#getting-help",
    "href": "01_intro-r.html#getting-help",
    "title": "1  Introduction to R",
    "section": "1.12 Getting help",
    "text": "1.12 Getting help\n\n1.12.1 Googling\nWhen Googling for R or data science help, set the search range to the last year or less to avoid out-of-date solutions and to focus on up-to-date practices. The search window can be set by clicking Tools after a Google search.\n\n\n1.12.2 Stack Overflow\nStack Overflow contains numerous solutions. If a problem is particularly perplexing, it is simple to submit questions. Exercise caution when submitting questions because the Stack Overflow community has strict norms about questions and loose norms about respecting novices.\n\n\n1.12.3 RStudio community\nRStudio Community is a new forum for R Users. It has a smaller back catalog than Stack Overflow but users are friendlier than on Stack Overflow.\n\n\n1.12.4 CRAN task views\nCRAN task views contains thorough introductions to packages and techniques organized by subject matter. The Econometrics, Reproducible Research, and and Social Sciences task views are good starting places.\n\n\n1.12.5 Twitter\nTwitter is mostly bad. But the #rstats hashtag and #rstats community are mostly good. They are also generally inclusive and civil. In particular, open sources developers like Hadley Wickham (@hadleywickham), Jenny Bryan (@JennyBryan), and Joe Cheng (@jcheng) are active.\n\n\n1.12.6 Data Science for Public Policy Slack\nWe’ve created a Slack workspace for this class (which will be shared across both sections) and encourage you to ask questions in Slack. In general, we ask that you try to answer questions on your own using the sources above before posting in Slack. Practicing finding and applying the relevant information to answer your questions is an important data science skill! The teaching staff will be checking the Slack to help answer questions in a reasonable time frame and we also encourage you to answer each other’s questions - it’s a great way to improve your R skills!\nQuestions on Slack must be asked using reproducible examples. Simply copying-and-pasting questions or answers in the Slack channel is not allowed. If you’re unsure how to share a reproducible example without sharing your answers in a public channel, you can DM the teaching staff to be safe.\n\n\n1.12.7 ChatGPT:\nSince there is R code on the internet, ChatGPT has been trained on R code and has the capability to answer R coding questions. Exercise extreme caution when using ChatGPT! ChatGPT saves and uses the queries you provide it. This means that asking a question about sensitive data or code could expose that data. If you decide to use ChatGPT, only ask queries of it using a reproducible example with non-sensitive data. The diamonds dataset, loaded with ggplot::diamonds() is a great candidate.\nThis warning aside, ChatGPT can be a powerful tool. Some helpful tips for using ChatGPT for coding questions are: - Provide it detailed questions - Give it reproducible example code - Refine queries when the initial responses are unsatisfactory",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html",
    "href": "04_data-viz.html",
    "title": "4  Data Visualization with ggplot2",
    "section": "",
    "text": "4.1 Motivation",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#mutating-joins",
    "href": "02_tidyverse.html#mutating-joins",
    "title": "2  Introduction to the Tidyverse",
    "section": "2.5 Mutating joins",
    "text": "2.5 Mutating joins\nMutating joins join one dataframe to columns from another dataframe by matching values common in both dataframes. The syntax is derived from Structured Query Language (SQL).\nEach function requires an x (or left) dataframe, a y (or right) data frame, and by variables that exist in both dataframes. Note that below we’re creating dataframes using the tribble() function, which creates a tibble using a row-wise layout.\n\nmath_scores &lt;- tribble(\n  ~name, ~math_score,\n  \"Alec\", 95,\n  \"Bart\", 97,\n  \"Carrie\", 100\n)\n\nreading_scores &lt;- tribble(\n  ~name, ~reading_score,\n  \"Alec\", 88,\n  \"Bart\", 67,\n  \"Carrie\", 100,\n  \"Zeta\", 100\n)\n\n\n2.5.1 left_join()\nleft_join() matches observations from the y dataframe to the x dataframe. It only keeps observations from the y data frame that have a match in the x dataframe.\n\nleft_join(x = math_scores, y = reading_scores, by = \"name\")\n\n# A tibble: 3 × 3\n  name   math_score reading_score\n  &lt;chr&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1 Alec           95            88\n2 Bart           97            67\n3 Carrie        100           100\n\n\nObservations that exist in the x (left) dataframe but not in the y (right) dataframe result in NAs.\n\nleft_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 4 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n4 Zeta             100         NA\n\n\n\n\n2.5.2 inner_join()\ninner_join() matches observations from the y dataframe to the x dataframe. It only keeps observations from either data frame that have a match.\n\ninner_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 3 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n\n\n\n\n2.5.3 full_join()\nfull_join() matches observations from the y dataframe to the x dataframe. It keeps observations from both dataframes.\n\nfull_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 4 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n4 Zeta             100         NA\n\n\n\n\n2.5.4 anti_join()\nanti_join() returns all rows from x where there are not matching values in y. anti_join() complements inner_join(). Together, they should exhaust the x dataframe.\n\nanti_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 1 × 2\n  name  reading_score\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Zeta            100\n\n\nThe Combine Tables column in the Data Transformation Cheat Sheet is an invaluable resource for navigating joins. The “column matching for joins” section of that cheat sheet outlines how to join tables by matching on multiple columns or match on columns with different names in each table.\n\n\n2.5.5 readr\nreadr is a core tidyverse package for reading and parsing rectangular data from text files (.csv, .tsv, etc.). read_csv() reads .csv files and has a bevy of advantages versus read.csv(). We recommend never using read.csv().\nMany .csvs can be read without issue with simple syntax read_csv(file = \"relative/path/to/data\").\nreadr and read_csv() have powerful tools for resolving parsing issues. More can be learned in the data import section in R4DS.\n\n\n2.5.6 readxl\nreadxl is a tidyverse package for reading data from Microsoft Excel files. It is not a core tidyverse package so it needs to be explicitly loaded in each R session.\nThe tidyverse website has a good tutorial on readxl.\n\n\n2.5.7 Next Skills\n\nacross() can be used with library(dplyr) functions such as summarise() and mutate() to apply the same transformations to multiple columns. For example, it can be used to calculate the mean of many columns with summarize(). across() uses the same tidyselect select language and helpers discussed earlier to select the columns to transform.\npivot_wider() and pivot_longer() can be used to switch between wide and long formats of the data. This is important for tidying data and data visualization.\n\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” https://doi.org/10.18637/jss.v059.i10.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Tidyverse</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#motivation",
    "href": "04_data-viz.html#motivation",
    "title": "4  Data Visualization with ggplot2",
    "section": "",
    "text": "Data visualization is exploratory data analysis (EDA)\nData visualization is diagnosis and validation\nData visualization is communication",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#motivation-going-beyond-excel",
    "href": "04_data-viz.html#motivation-going-beyond-excel",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.2 Motivation (going beyond Excel)",
    "text": "4.2 Motivation (going beyond Excel)\n\nFlexibility\nReproducibility\nScalability\nRelational data vs. positional data",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#background",
    "href": "04_data-viz.html#background",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.3 Background",
    "text": "4.3 Background\n\nThe toughest part of data visualization is data munging.\nData frames are the only appropriate input for library(ggplot2).\n\nggplot2 is an R package for data visualization that was developed during Hadley Wickham’s graduate studies at Iowa State University. ggplot2 is formalized in A Layered Grammar of Graphics by Hadley Wickham Wickham (2010).\nThe grammar of graphics, originally by Leland Wilkinson, is a theoretical framework that breaks all data visualizations into their component pieces. With the layered grammar of graphics, Wickham extends Wilkinson’s grammar of graphics and implements it in R. The cohesion is impressive, and the theory flows to the code which informs the data visualization process in a way not reflected in any other data viz tool.\nThere are eight main ingredients to the grammar of graphics. We will work our way through the ingredients with many hands-on examples.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nOpen your .Rproj.\nCreate a new .R script in your directory called 03_data-visualization.R.\nType (don’t copy & paste) the following code below library(tidyverse) in 03_data-visualization.R.\n\nggplot(data = storms) + \n  geom_point(mapping = aes(x = pressure, y = wind))\n\nAdd a comment above the ggplot2 code that describes the plot we created.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#functions",
    "href": "04_data-viz.html#functions",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.4 Functions",
    "text": "4.4 Functions\nThis is a summary of the functions we discussed in this chapter. While by no means comprehensive, these are an excellent starting point to visualizing data using ggplot.\n\nggplot()\naes()\ngeom_*()\n\ngeom_point()\ngeom_line()\ngeom_col()\n\n\nscale_*()\n\nscale_y_continuous()\n\ncoord_*()\nfacet_*()\nlabs()",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#theory",
    "href": "04_data-viz.html#theory",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.5 Theory",
    "text": "4.5 Theory\n\nData\nAesthetic mappings\nGeometric objects\nScales\nCoordinate systems\nFacets\nStatistical transformations\nTheme",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#resources",
    "href": "04_data-viz.html#resources",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.6 Resources",
    "text": "4.6 Resources\n\nUrban Institute R Users Group website\nWhy the Urban Institute visualizes data with ggplot2\nR for Data Science: data visualization\nggplot2: ELegant Graphics for Data Analysis (3e)\nawunderground themes\nR Graph Gallery\n\n\n\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#summary",
    "href": "04_data-viz.html#summary",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.5 Summary",
    "text": "4.5 Summary\n\n4.5.1 Functions\nThis is a summary of the functions we discussed in this chapter. While by no means comprehensive, these are an excellent starting point to visualizing data using ggplot2.\n\nggplot()\naes()\ngeom_*()\n\ngeom_point()\ngeom_line()\ngeom_col()\n\n\nscale_*()\n\nscale_y_continuous()\n\ncoord_*()\nfacet_*()\nlabs()\n\n\n\n4.5.2 Theory\n\nData\nAesthetic mappings\nGeometric objects\nScales\nCoordinate systems\nFacets\nStatistical transformations\nTheme",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html",
    "href": "05_exploratory-data-analysis.html",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "5.1 Reading in Data\nWe’ve already covered reading in csv files using the read_csv() function from the readr package, but you may also need to read in data that is stored in a number of other common file formats:",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#excel-spreadsheets",
    "href": "05_exploratory-data-analysis.html#excel-spreadsheets",
    "title": "5  Exploratory Data Analysis",
    "section": "6.1 Excel Spreadsheets",
    "text": "6.1 Excel Spreadsheets\nreadxl is a tidyverse package for reading data from Microsoft Excel files. It is not a core tidyverse package so it needs to be explicitly loaded in each R session.\nThe tidyverse website has a good tutorial on readxl.\nMany excel files can be read with the simple syntax data &lt;- read_excel(path = \"relative/file/path/to/data\"). In cases where the Excel spreadsheet contains multiple sheets, you can use the sheet argument to specify the sheet to read as a string (the name of the sheet) or an integer (the position of the sheet).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#stata-sas-and-spss-files",
    "href": "05_exploratory-data-analysis.html#stata-sas-and-spss-files",
    "title": "5  Exploratory Data Analysis",
    "section": "6.2 STATA, SAS, and SPSS files",
    "text": "6.2 STATA, SAS, and SPSS files\nhaven is a tidyverse package for reading data from SAS (read_sas()), STATA (read_dta()), and SPSS (read_sav()) files. Like the readxl package, it is not a core tidyverse package and also needs to be explicitly loaded in each R session.\nNote that the haven package can only read and write STATA .dta files through version 15 of STATA. For files created in more recent versions of STATA, the readstat13 package’s read.dta13 file can be used.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#zip-files",
    "href": "05_exploratory-data-analysis.html#zip-files",
    "title": "5  Exploratory Data Analysis",
    "section": "6.3 Zip Files",
    "text": "6.3 Zip Files\nYou may also want to read in data that is saved in a zip file. In order to do this, you can use the unzip() function to unzip the files using the following syntax: unzip(zipfile = \"path/to/zip/file\", exdir = \"path/to/directory/for/unzipped/files\").\nOften times, you may want to read in a zip file from a website into R. In order to do this, you will need to first download the zip file to your computer using the download.file() function, unzip the file using unzip() and then read in the data using the appropriate function for the given file type.\nTo download the week 40 public use file data for the Census Household Pulse Survey, run the following code:\n\nbase_url &lt;- \"https://www2.census.gov/programs-surveys/demo/datasets/hhp/\"\nweek_url &lt;- \"2021/wk40/HPS_Week40_PUF_CSV.zip\"\n\npulse_url &lt;- paste0(base_url, week_url)\n\n# creates data directory in working directory\n# gives warning if directory already exists\ndir.create(\"data\")\n\n# For Mac, *.nix systems:\ndownload.file(\n  pulse_url, \n  destfile = \"data/pulse40.zip\"\n)\n\n# For Windows systems, you need to add the mode = \"wb\" \n# argument to prevent an invalid zip file \ndownload.file(\n  pulse_url, \n  destfile = \"data/pulse40.zip\", \n  mode = \"wb\"\n)\n\n\n6.3.1 Exercise 01\n\nCopy and paste the code chunk above and keep the appropriate download.file command for your computer.\nWrite code using the unzip() function to unzip the zip file downloaded. Set zipfile to be the same the destfile parameter you used in part 1. Set exdir to be the same directory where you just downloaded the zip file. Run both of these commands.\nExamine the unzipped files and select the appropriate function to read in the pulse2021_puf_40 file. Write code to read that file into R, assigning the output to the pulse object.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#string-manipulation-with-stringr",
    "href": "05_exploratory-data-analysis.html#string-manipulation-with-stringr",
    "title": "5  Exploratory Data Analysis",
    "section": "9.1 String manipulation with stringr",
    "text": "9.1 String manipulation with stringr\nBefore converting column types, it is critical to clean the column values to ensure that NA values aren’t accidentally introduced by coercion. The stringr package in the tidyverse offers a number of excellent functions for cleaning character data. This package is part of the core tidyverse and is automatically loaded with library(tidyverse). The stringr cheat sheet offers a great guide to the stringr functions.\nTo demonstrate some of the stringr functions, let’s create a state column with the two digit state FIPS code for DC and a geoid column in the dc_centroid dataframe which contains the 11-digit census tract FIPS code, which can be useful for joining this dataframe with other dataframes that commonly use the FIPS code as a unique identifier. We will need to first remove the period from the tract column and then concatenate the county and tract columns into a geoid column. We can do that using stringr as follows:\n\ndc_centroids &lt;- dc_centroids %&gt;%\n  mutate(\n    #replace first instance of pattern\n    tract = str_replace(tract, \"\\\\.\", \"\"), \n    #join multiple strings into single string\n    geoid = str_c(county, tract, sep = \"\")\n  )\n\n\n9.1.1 Exercise 03\n\nCopy the code above into an R script and edit it to add the creation of a variable called state that is equal to the first two characters of the county variable using the str_sub() function.\n\nNote that the str_replace() function uses regular expressions to match the pattern that gets replaced. Regular expressions is a concise and flexible tool for describing patterns in strings - but it’s syntax can be complex and not particularly intuitive. This vignette provides a useful introduction to regular expressions, and when in doubt - there are plentiful Stack Overflow posts to help when you search your specific case.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#date-manipulation-with-lubridate",
    "href": "05_exploratory-data-analysis.html#date-manipulation-with-lubridate",
    "title": "5  Exploratory Data Analysis",
    "section": "9.2 Date manipulation with lubridate",
    "text": "9.2 Date manipulation with lubridate\nThe lubridate package makes it much easier to work with dates and times in R. While also part of the tidyverse, it is not a core tidyverse package and must be explicitly loaded in each session with library(lubridate).\nWe’ll use a dataset on political violence and protest events across the continent of Africa in 2022 from the Armed Conflict Location & Event Data Project (ACLED) to illustrate the power of lubridate. The lubridate package allows users to easily and quickly parse date-time variables in a range of different formats. For example, the ymd() function takes a date variable in year-month-day format (e.g. 2022-01-31) and converts it to a date-time format. The different formats are outlined in the lubridate documentation.\n\nlibrary(lubridate)\nacled_2022 &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/acled/acled_africa_2022.csv\")\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(event_date = ymd(event_date)) %&gt;%\n  select(event_date, region, country, event_type, sub_event_type)\n\nRows: 22101 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (15): EVENT_ID_CNTY, EVENT_TYPE, SUB_EVENT_TYPE, ACTOR1, ASSOC_ACTOR_1,...\ndbl  (12): ISO, EVENT_ID_NO_CNTY, YEAR, TIME_PRECISION, INTER1, INTER2, INTE...\nlgl   (1): ADMIN3\ndttm  (1): EVENT_DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCreating datetime columns enables the use of a number of different operations, such as filtering based on date:\n\nacled_2022 %&gt;%\n  filter(event_date &gt; \"2022-06-01\")\n\n# A tibble: 6,859 × 5\n   event_date region          country event_type             sub_event_type     \n   &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;                  &lt;chr&gt;              \n 1 2022-06-04 Northern Africa Algeria Protests               Peaceful protest   \n 2 2022-06-05 Northern Africa Algeria Protests               Peaceful protest   \n 3 2022-06-05 Northern Africa Algeria Protests               Peaceful protest   \n 4 2022-06-08 Northern Africa Algeria Protests               Peaceful protest   \n 5 2022-06-09 Northern Africa Algeria Protests               Peaceful protest   \n 6 2022-06-09 Northern Africa Algeria Protests               Peaceful protest   \n 7 2022-06-11 Northern Africa Algeria Protests               Peaceful protest   \n 8 2022-06-15 Northern Africa Algeria Strategic developments Looting/property d…\n 9 2022-06-18 Northern Africa Algeria Protests               Protest with inter…\n10 2022-06-19 Northern Africa Algeria Protests               Peaceful protest   \n# ℹ 6,849 more rows\n\n\nOr calculating durations:\n\nacled_2022 %&gt;%\n  #number of days into 2022 event occurred\n  mutate(days_into_2022 = event_date - ymd(\"2022-01-01\")) \n\n# A tibble: 22,101 × 6\n   event_date region          country event_type sub_event_type   days_into_2022\n   &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;drtn&gt;        \n 1 2022-01-03 Northern Africa Algeria Protests   Peaceful protest 2 days        \n 2 2022-01-03 Northern Africa Algeria Protests   Peaceful protest 2 days        \n 3 2022-01-04 Northern Africa Algeria Protests   Peaceful protest 3 days        \n 4 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 5 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 6 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 7 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 8 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 9 2022-01-06 Northern Africa Algeria Protests   Peaceful protest 5 days        \n10 2022-01-06 Northern Africa Algeria Protests   Peaceful protest 5 days        \n# ℹ 22,091 more rows\n\n\nOr extracting components of dates:\n\nacled_2022 %&gt;%\n  mutate(event_month = month(event_date)) #month event occurred\n\n# A tibble: 22,101 × 6\n   event_date region          country event_type sub_event_type   event_month\n   &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;\n 1 2022-01-03 Northern Africa Algeria Protests   Peaceful protest           1\n 2 2022-01-03 Northern Africa Algeria Protests   Peaceful protest           1\n 3 2022-01-04 Northern Africa Algeria Protests   Peaceful protest           1\n 4 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 5 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 6 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 7 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 8 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 9 2022-01-06 Northern Africa Algeria Protests   Peaceful protest           1\n10 2022-01-06 Northern Africa Algeria Protests   Peaceful protest           1\n# ℹ 22,091 more rows\n\n\nDatetimes can also much more easily be plotted using ggplot2. For example, it is easy to visualize the distribution of events across the year:\n\nacled_2022 %&gt;% \n  ggplot(aes(x = event_date)) + \n  geom_freqpoly(binwidth = 1) # each bin is 1 day\n\n\n\n\n\n\n\n\nFor more information on lubridate, see the lubridate cheat sheet.\nSource: R for Data Science, Ch 16",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#categorical-and-factor-variables",
    "href": "05_exploratory-data-analysis.html#categorical-and-factor-variables",
    "title": "5  Exploratory Data Analysis",
    "section": "9.3 Categorical and Factor Variables",
    "text": "9.3 Categorical and Factor Variables\nCategorical variables can be stored as characters in R. The case_when() function makes it very easy to create categorical variables based on other columns. For example:\n\npulse &lt;- pulse %&gt;%\n  mutate(\n    hisp_rrace = case_when(\n      rrace == 1 ~ \"White alone, not Hispanic\",\n      rrace == 2 ~ \"Black alone, not Hispanic\",\n      rrace == 3 ~ \"Asian alone, not Hispanic\",\n      rrace == 4 ~ \"Two or more races + Other races, not Hispanic\",\n      TRUE ~ NA_character_)\n  )\n\nFactors are a data type specifically made to work with categorical variables. The forcats library in the core tidyverse is made to work with factors. Factors are particularly valuable if the values have a ordering that is not alphanumeric.\n\nx1 &lt;- c(\"Dec\", \"Apr\", \"Jan\", \"Mar\")\n\nmonth_levels &lt;- c(\n  \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n  \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n)\n\ny1 &lt;- factor(x1, levels = month_levels)\n\nsort(x1)\n\n[1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"\n\nsort(y1)\n\n[1] Jan Mar Apr Dec\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nFactors are also valuable if you want to show all possible values of the categorical variable, even when they have no observations.\n\ntable(x1)\n\nx1\nApr Dec Jan Mar \n  1   1   1   1 \n\ntable(y1)\n\ny1\nJan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec \n  1   0   1   1   0   0   0   0   0   0   0   1",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#identifying-outliersunexpected-values",
    "href": "05_exploratory-data-analysis.html#identifying-outliersunexpected-values",
    "title": "5  Exploratory Data Analysis",
    "section": "11.1 Identifying Outliers/Unexpected Values",
    "text": "11.1 Identifying Outliers/Unexpected Values\nUsing R to examine the distribution of your data is one way to identify outliers or unexpected values. For example, we can examine the distribution of the bodywt variable in the msleep dataset both by examining the mathematical distribution using the summary() function and visually using ggplot.\n\nsummary(msleep$bodywt)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   0.005    0.174    1.670  166.136   41.750 6654.000 \n\n\n\nmsleep %&gt;%\n  ggplot(aes(bodywt, 1)) +\n  geom_point(alpha = 0.2) +\n  scale_y_continuous(breaks = 0) +\n  labs(y = NULL) +\n  theme_bw() +\n  theme(panel.border = ggplot2::element_blank())",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#unit-tests",
    "href": "05_exploratory-data-analysis.html#unit-tests",
    "title": "5  Exploratory Data Analysis",
    "section": "11.2 Unit Tests",
    "text": "11.2 Unit Tests\nWriting tests in R is a great way to test that your data does not have unexpected/incorrect values. These tests can also be used to catch mistakes that can be introduced by errors in the data cleaning process. There are a number of R packages that have tools for writing tests, including:\n\ntestthat\nassertthat\nassertr\n\n\nlibrary(assertr)\n\ndf &lt;- tibble(age = c(20, 150, 47, 88),\n             height = c(60, 2, 72, 66))\n\ndf %&gt;%\n  assertr::verify(age &lt; 120) %&gt;%\n  summarise(mean(age, na.rm = TRUE))\n\nverification [age &lt; 120] failed! (1 failure)\n\n    verb redux_fn predicate column index value\n1 verify       NA age &lt; 120     NA     2    NA\n\n\nError: assertr stopped execution\n\n\nCritically, adding the test caused the code to return an error before calculating the mean of the age variable. This is a feature, not a bug! It can prevent you from introducing errors into your analyses. Moreover, by writing a set of tests in your analysis code, you can run the same checks every time you perform the analysis which can help you catch errors caused by changes in the input data.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#handling-outliersunexpected-values",
    "href": "05_exploratory-data-analysis.html#handling-outliersunexpected-values",
    "title": "5  Exploratory Data Analysis",
    "section": "11.3 Handling Outliers/Unexpected Values",
    "text": "11.3 Handling Outliers/Unexpected Values\nWhen you identify outliers or unexpected values, you will have to decide how you want to handle those values in your data. The proper way to handle those values will depend on the reason for the outlier value and the objectives of your analysis.\n\nIf the outlier is caused by data errors, such as an implausible age or population value, you can replace the outlier values with NA using mutate and if_else or case_when as described above.\nIf the outlier represents a different population than the one you are studying - e.g. the different consumption behaviors of individual consumers versus wholesale business orders - you can remove it from the data.\nYou can transform the data, such as taking the natural log to reduce the variation caused by outliers.\nYou can select a different analysis method that is less sensitive to outliers, such as using the median rather than the mean to measure central tendency.\n\n\n11.3.1 Exercise 05\n\nRead the column descriptions in the csv file for the DC centroids.\nUse one of the methods above to identify whether the pop10 column contains any outliers. According to the Census Bureau, tracts generally have a population between 1,200 and 8,000 people.\nCalculate the mean of the pop10 column in the dc_centroids dataframe, but first write one test using assertr::verify() to test for invalid values based on the column definition.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#data",
    "href": "04_data-viz.html#data",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.4 Data",
    "text": "4.4 Data\n\n\n\n\n\n\nNote\n\n\n\nData are the values represented in the visualization.\n\n\nggplot(data = ) or data %&gt;% ggplot()\n\nstorms %&gt;%\n  select(name, year, category, lat, long, wind, pressure) %&gt;%\n  sample_n(10) %&gt;%\n  kable()\n\n\n\n\nname\nyear\ncategory\nlat\nlong\nwind\npressure\n\n\n\n\nAna\n2003\nNA\n31.7\n-43.1\n30\n1003\n\n\nAlberto\n1988\nNA\n35.2\n-74.6\n25\n1012\n\n\nSam\n2021\n3\n35.8\n-57.9\n100\n954\n\n\nGladys\n1975\n3\n32.9\n-72.1\n110\n942\n\n\nArlene\n1987\nNA\n34.5\n-44.5\n40\n1002\n\n\nGonzalo\n2014\nNA\n16.5\n-59.7\n45\n1001\n\n\nGloria\n1976\nNA\n24.9\n-58.1\n30\n1001\n\n\nAmy\n1975\nNA\n34.0\n-77.0\n30\n1006\n\n\nGloria\n1979\n1\n28.0\n-40.3\n75\n985\n\n\nDennis\n2005\n3\n22.7\n-81.6\n100\n960",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#aesthetic-mappings",
    "href": "04_data-viz.html#aesthetic-mappings",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.5 Aesthetic Mappings:",
    "text": "4.5 Aesthetic Mappings:\n\n\n\n\n\n\nNote\n\n\n\nAesthetic mappings are directions for how data are mapped in a plot in a way that we can perceive. Aesthetic mappings include linking variables to the x-position, y-position, color, fill, shape, transparency, and size.\n\n\naes(x = , y = , color = )\nX or Y\n\n\n\n\n\n\n\n\n\nContinuous Color or Fill\n\n\n\n\n\n\n\n\n\nDiscrete Color or Fill\n\n\n\n\n\n\n\n\n\nSize\n\n\n\n\n\n\n\n\n\nShape\n\n\n\n\n\n\n\n\n\nOthers: transparency, line type",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#geometric-objects",
    "href": "04_data-viz.html#geometric-objects",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.6 Geometric Objects:",
    "text": "4.6 Geometric Objects:\n\n\n\n\n\n\nNote\n\n\n\nGeometric objects are representations of the data, including points, lines, and polygons.\n\n\ngeom_bar() or geom_col()\nPlots are often called their geometric object(s).\n\n\n\n\n\n\n\n\n\ngeom_line()\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\ngeom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nDuplicate the code from Exercise 1. Add comments below the data visualization code that describes the argument or function that corresponds to each of the first three components of the grammar of graphics.\nInside aes(), add color = category. Run the code.\nReplace color = category with color = \"green\". Run the code. What changed? Is this unexpected?\nRemove color = \"green\" from aes() and add it inside inside of geom_point() but outside of aes(). Run the code.\nThis is a little cluttered. Add alpha = 0.2 inside geom_point() but outside of aes().\n\n\n\nAesthetic mappings like x and y almost always vary with the data. Aesthetic mappings like color, fill, shape, transparency, and size can vary with the data. But those arguments can also be added as styles that don’t vary with the data. If you include those arguments in aes(), they will show up in the legend (which can be annoying! and is also a sign that something should be changed!).\n\n\n\n\n\n\nExercise 3\n\n\n\n\nCreate a new scatter plot using the msleep data set. Use bodywt on the x-axis and sleep_total on the y-axis.\nThe y-axis doesn’t contain zero. Below geom_point(), add scale_y_continuous(limits = c(0, NA)). Hint: add + after geom_point().\nThe x-axis is clustered near zero. Add scale_x_log10() above scale_y_continuous(limits = c(0, NA)).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#scales",
    "href": "04_data-viz.html#scales",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.7 Scales:",
    "text": "4.7 Scales:\n\n\n\n\n\n\nNote\n\n\n\nScales turn data values, which are continuous, discrete, or categorical into aesthetic values. scale_*_*() functions control the specific behaviors of aesthetic mappings. This includes not only the x-axis and y-axis, but the ranges of sizes, types of shapes, and specific colors of aesthetics.\n\n\n\nBefore\nscale_x_continuous()\n\n\n\n\n\n\n\n\n\n\n\nAfter\nscale_x_reverse()\n\n\n\n\n\n\n\n\n\n\n\nBefore\nscale_size_continuous(breaks = c(25, 75, 125))\n\n\n\n\n\n\n\n\n\n\n\nAfter\nscale_size_continuous(range = c(0.5, 20), breaks = c(25, 75, 125))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nType the following code in your script.\n\ndata &lt;- tibble(x = 1:10, y = 1:10)\nggplot(data = data) +\n  geom_blank(mapping = aes(x = x, y = y))\n\nAdd coord_polar() to your plot.\nAdd labs(title = \"Polar coordinate system\") to your plot.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#coordinate-systems",
    "href": "04_data-viz.html#coordinate-systems",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.8 Coordinate Systems:",
    "text": "4.8 Coordinate Systems:\n\n\n\n\n\n\nNote\n\n\n\nCoordinate systems map scaled geometric objects to the position of objects on the plane of a plot. The two most popular coordinate systems are the Cartesian coordinate system and the polar coordinate system.\n\n\n\n\n\n\n\n\n\n\n\ncoord_polar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCreate a scatter plot of the storms data set with pressure on the x-axis and wind on the y-axis.\nAdd facet_wrap(~ category)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#facets",
    "href": "04_data-viz.html#facets",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.9 Facets:",
    "text": "4.9 Facets:\n\n\n\n\n\n\nNote\n\n\n\nFacets (optional) break data into meaningful subsets. facet_wrap(), facet_grid(), and facet_geo().\n\n\nfacet_wrap and facet_grid are similar but have distinct functions. As the diagram below illustrates, facet_wrap organizes a 1 dimensional set of charts, but wraps the charts in two dimensions to preserve space. facet_grid organizes a set of charts in two dimensions. facet_geo organizes charts in a way that attempts to preserve some geographic component of the data.\n\n\n\nSource: ggplot2: Elegant Graphics for Data Analysis (3e)\n\n\n\n4.9.1 Facet wrap\nfacet_wrap(~ category)\n\n\n\n\n\n\n\n\n\n\n\n4.9.2 Facet grid\nfacet_grid(month ~ year)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\nAdd the following code to your script. Submit it!\n\nggplot(storms) +\n  geom_bar(mapping = aes(x = category))",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#statistical-transformations",
    "href": "04_data-viz.html#statistical-transformations",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.10 Statistical Transformations:",
    "text": "4.10 Statistical Transformations:\n\n\n\n\n\n\nNote\n\n\n\nStatistical transformations (optional) transform the data, typically through summary statistics and functions, before aesthetic mapping.\nBefore transformations, each observation in data is represented by one geometric object (i.e. a scatter plot). After a transformation, a geometric object can represent more than one observation (i.e. a bar in a histogram).\n\n\nNote: geom_bar() performs statistical transformation. Use geom_col() to create a column chart with bars that encode individual observations in the data set.\n\n\n\n\n\n\nExercise 7\n\n\n\n\nDuplicate Exercise 6.\nAdd theme_minimal() to the plot.\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\n\nDuplicate Exercise 6.\nRun install.packages(\"remotes\") and remotes::install_github(\"UrbanInstitute/urbnthemes\") in the console.\nIn the lines preceding the chart add and run the following code:\n\nlibrary(urbnthemes)\nset_urbn_defaults(style = \"print\")\n\nRun the code to make the chart.\nAdd scale_y_continuous(expand = expansion(mult = c(0, 0.1))) and rerun the code.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#themes",
    "href": "04_data-viz.html#themes",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.11 Themes:",
    "text": "4.11 Themes:\n\n\n\n\n\n\nNote\n\n\n\nThemes control the visual style of plots with font types, font sizes, background colors, margins, and positioning.\n\n\n\n4.11.1 Default theme\n\n\n\n\n\n\n\n\n\n\n\n4.11.2 fivethirtyeight theme\n\n\n\n\n\n\n\n\n\n\n\n4.11.3 urbnthemes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\n\nAdd the following exercise to you script. Run it!\n\nstorms %&gt;%  \n  filter(category &gt; 0) %&gt;%\n  distinct(name, year) %&gt;%\n  count(year) %&gt;%\n  ggplot() + \n  geom_line(mapping = aes(x = year, y = n))\n\nAdd geom_point() after geom_line() with the same aesthetic mappings.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLayers allow for multiple geometric objects to be plotted in the same data visualization.\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\n\nAdd the following exercise to you script. Run it!\n\nggplot(data = storms, mapping = aes(x = pressure, y = wind)) + \n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\nNote\n\n\n\nInheritances pass aesthetic mappings from ggplot() to later geom_*() functions.\n\n\nNotice how the aesthetic mappings are passed to ggplot() in example 10. This is useful when using layers!\n\n\n\n\n\n\nExercise 11\n\n\n\n\nPick your favorite plot from exercises 1 through 10 and duplicate the code.\nAdd ggsave(filename = \"favorite-plot.png\") on a new line without + and then save the file. Look at the saved file.\nAdd width = 6 and height = 4 to ggsave(). Run the code and then look at the saved file.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#eight-ingredients-in-the-grammar-of-graphics",
    "href": "04_data-viz.html#eight-ingredients-in-the-grammar-of-graphics",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.4 Eight Ingredients in the Grammar of Graphics:",
    "text": "4.4 Eight Ingredients in the Grammar of Graphics:\n\n4.4.1 Data\n\n\n\n\n\n\nNote\n\n\n\nData are the values represented in the visualization.\n\n\nggplot(data = ) or data %&gt;% ggplot()\n\nstorms %&gt;%\n  select(name, year, category, lat, long, wind, pressure) %&gt;%\n  sample_n(10) %&gt;%\n  kable()\n\n\n\n\nname\nyear\ncategory\nlat\nlong\nwind\npressure\n\n\n\n\nFranklin\n2017\n1\n20.3\n-95.5\n75\n981\n\n\nGaston\n2004\nNA\n31.6\n-78.0\n25\n1014\n\n\nDennis\n1999\n1\n23.8\n-73.1\n65\n995\n\n\nDorian\n2019\n3\n27.0\n-78.5\n105\n950\n\n\nArlene\n1987\nNA\n39.0\n-4.0\n10\n1008\n\n\nErin\n1989\n1\n29.7\n-45.2\n70\n983\n\n\nCindy\n2017\nNA\n36.7\n-88.1\n25\n1002\n\n\nHortense\n1984\nNA\n30.5\n-63.2\n45\n999\n\n\nAlex\n2010\nNA\n17.5\n-88.2\n55\n995\n\n\nBeryl\n1982\nNA\n17.0\n-35.0\n55\n993\n\n\n\n\n\n\n\n4.4.2 Aesthetic Mappings:\n\n\n\n\n\n\nNote\n\n\n\nAesthetic mappings are directions for how data are mapped in a plot in a way that we can perceive. Aesthetic mappings include linking variables to the x-position, y-position, color, fill, shape, transparency, and size.\n\n\naes(x = , y = , color = )\nX or Y\n\n\n\n\n\n\n\n\n\nContinuous Color or Fill\n\n\n\n\n\n\n\n\n\nDiscrete Color or Fill\n\n\n\n\n\n\n\n\n\nSize\n\n\n\n\n\n\n\n\n\nShape\n\n\n\n\n\n\n\n\n\nOthers: transparency, line type\n\n\n4.4.3 Geometric Objects:\n\n\n\n\n\n\nNote\n\n\n\nGeometric objects are representations of the data, including points, lines, and polygons.\n\n\ngeom_bar() or geom_col()\nPlots are often called their geometric object(s).\n\n\n\n\n\n\n\n\n\ngeom_line()\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\ngeom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nDuplicate the code from Exercise 1. Add comments below the data visualization code that describes the argument or function that corresponds to each of the first three components of the grammar of graphics.\nInside aes(), add color = category. Run the code.\nReplace color = category with color = \"green\". Run the code. What changed? Is this unexpected?\nRemove color = \"green\" from aes() and add it inside inside of geom_point() but outside of aes(). Run the code.\nThis is a little cluttered. Add alpha = 0.2 inside geom_point() but outside of aes().\n\n\n\nAesthetic mappings like x and y almost always vary with the data. Aesthetic mappings like color, fill, shape, transparency, and size can vary with the data. But those arguments can also be added as styles that don’t vary with the data. If you include those arguments in aes(), they will show up in the legend (which can be annoying! and is also a sign that something should be changed!).\n\n\n\n\n\n\nExercise 3\n\n\n\n\nCreate a new scatter plot using the msleep data set. Use bodywt on the x-axis and sleep_total on the y-axis.\nThe y-axis doesn’t contain zero. Below geom_point(), add scale_y_continuous(limits = c(0, NA)). Hint: add + after geom_point().\nThe x-axis is clustered near zero. Add scale_x_log10() above scale_y_continuous(limits = c(0, NA)).\n\n\n\n\n\n4.4.4 Scales:\n\n\n\n\n\n\nNote\n\n\n\nScales turn data values, which are continuous, discrete, or categorical into aesthetic values. scale_*_*() functions control the specific behaviors of aesthetic mappings. This includes not only the x-axis and y-axis, but the ranges of sizes, types of shapes, and specific colors of aesthetics.\n\n\n\nBefore\nscale_x_continuous()\n\n\n\n\n\n\n\n\n\n\n\nAfter\nscale_x_reverse()\n\n\n\n\n\n\n\n\n\n\n\nBefore\nscale_size_continuous(breaks = c(25, 75, 125))\n\n\n\n\n\n\n\n\n\n\n\nAfter\nscale_size_continuous(range = c(0.5, 20), breaks = c(25, 75, 125))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nType the following code in your script.\n\ndata &lt;- tibble(x = 1:10, y = 1:10)\nggplot(data = data) +\n  geom_blank(mapping = aes(x = x, y = y))\n\nAdd coord_polar() to your plot.\nAdd labs(title = \"Polar coordinate system\") to your plot.\n\n\n\n\n\n\n4.4.5 Coordinate Systems:\n\n\n\n\n\n\nNote\n\n\n\nCoordinate systems map scaled geometric objects to the position of objects on the plane of a plot. The two most popular coordinate systems are the Cartesian coordinate system and the polar coordinate system.\n\n\n\n\n\n\n\n\n\n\n\ncoord_polar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCreate a scatter plot of the storms data set with pressure on the x-axis and wind on the y-axis.\nAdd facet_wrap(~ category)\n\n\n\n\n\n4.4.6 Facets:\n\n\n\n\n\n\nNote\n\n\n\nFacets (optional) break data into meaningful subsets. facet_wrap(), facet_grid(), and facet_geo().\n\n\nfacet_wrap and facet_grid are similar but have distinct functions. As the diagram below illustrates, facet_wrap organizes a 1 dimensional set of charts, but wraps the charts in two dimensions to preserve space. facet_grid organizes a set of charts in two dimensions. facet_geo organizes charts in a way that attempts to preserve some geographic component of the data.\n\n\n\nSource: ggplot2: Elegant Graphics for Data Analysis (3e)\n\n\n\nFacet wrap\nfacet_wrap(~ category)\n\n\n\n\n\n\n\n\n\n\n\nFacet grid\nfacet_grid(month ~ year)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\nAdd the following code to your script. Submit it!\n\nggplot(storms) +\n  geom_bar(mapping = aes(x = category))\n\n\n\n\n\n4.4.7 Statistical Transformations:\n\n\n\n\n\n\nNote\n\n\n\nStatistical transformations (optional) transform the data, typically through summary statistics and functions, before aesthetic mapping.\nBefore transformations, each observation in data is represented by one geometric object (i.e. a scatter plot). After a transformation, a geometric object can represent more than one observation (i.e. a bar in a histogram).\n\n\nNote: geom_bar() performs statistical transformation. Use geom_col() to create a column chart with bars that encode individual observations in the data set.\n\n\n\n\n\n\nExercise 7\n\n\n\n\nDuplicate Exercise 6.\nAdd theme_minimal() to the plot.\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\n\nDuplicate Exercise 6.\nRun install.packages(\"remotes\") and remotes::install_github(\"UrbanInstitute/urbnthemes\") in the console.\nIn the lines preceding the chart add and run the following code:\n\nlibrary(urbnthemes)\nset_urbn_defaults(style = \"print\")\n\nRun the code to make the chart.\nAdd scale_y_continuous(expand = expansion(mult = c(0, 0.1))) and rerun the code.\n\n\n\n\n\n4.4.8 Themes:\n\n\n\n\n\n\nNote\n\n\n\nThemes control the visual style of plots with font types, font sizes, background colors, margins, and positioning.\n\n\n\nDefault theme\n\n\n\n\n\n\n\n\n\n\n\nfivethirtyeight theme\n\n\n\n\n\n\n\n\n\n\n\nurbnthemes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\n\nAdd the following exercise to you script. Run it!\n\nstorms %&gt;%  \n  filter(category &gt; 0) %&gt;%\n  distinct(name, year) %&gt;%\n  count(year) %&gt;%\n  ggplot() + \n  geom_line(mapping = aes(x = year, y = n))\n\nAdd geom_point() after geom_line() with the same aesthetic mappings.\n\n\n\n\n\n\n4.4.9 Layers (bonus!):\n\n\n\n\n\n\nNote\n\n\n\nLayers allow for multiple geometric objects to be plotted in the same data visualization.\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\n\nAdd the following exercise to you script. Run it!\n\nggplot(data = storms, mapping = aes(x = pressure, y = wind)) + \n  geom_point() +\n  geom_smooth()\n\n\n\n\n4.4.10 Inheritances (bonus!):\n\n\n\n\n\n\nNote\n\n\n\nInheritances pass aesthetic mappings from ggplot() to later geom_*() functions.\n\n\nNotice how the aesthetic mappings are passed to ggplot() in example 10. This is useful when using layers!\n\n\n\n\n\n\nExercise 11\n\n\n\n\nPick your favorite plot from exercises 1 through 10 and duplicate the code.\nAdd ggsave(filename = \"favorite-plot.png\") on a new line without + and then save the file. Look at the saved file.\nAdd width = 6 and height = 4 to ggsave(). Run the code and then look at the saved file.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html",
    "href": "06_reproducible-research-with-quarto.html",
    "title": "6  Reproducible Research with Quarto",
    "section": "",
    "text": "6.1 Motivation\nThere are many problems worth avoiding in an analysis:\nNot convinced? Maybe we just want to make cool stuff.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#motivation",
    "href": "06_reproducible-research-with-quarto.html#motivation",
    "title": "6  Reproducible Research with Quarto",
    "section": "",
    "text": "Copying-and-pasting, transposing, and manual repetition\nOut-of-sequence documents\nParallel documents (a script and a narrative Word doc)\nCode written for computers that is tough to parse by humans",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#literate-statistical-programming",
    "href": "06_reproducible-research-with-quarto.html#literate-statistical-programming",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.2 Literate (Statistical) Programming",
    "text": "6.2 Literate (Statistical) Programming\n\n\n\nSource: Jacob Applebaum\n\n\nLiterature Programming and LaTeX\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do. ~Literate Programming (1984) by Donald Knuth",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#example",
    "href": "06_reproducible-research-with-quarto.html#example",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.3 Example",
    "text": "6.3 Example\nWe used a linear model because there is reason to believe that the population model is linear. The observations are independent and the errors are independently and identically distributed with an approximately normal distribution.\n\nmodel1 &lt;- lm(formula = dist ~ speed, data = cars)\nmodel1\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nAn increase in travel speed of one mile per hour is associated with a 3.93 foot increase in stopping distance on average.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#quarto",
    "href": "06_reproducible-research-with-quarto.html#quarto",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.4 Quarto",
    "text": "6.4 Quarto\nQuarto is a new literate statistical programming tool for R, Julia, Python, JavaScript, and more. It is an important tool for reproducible research. It combines narrative text with styles, code, and the output of code and can be used to create many types of documents including PDFs, html websites, slides, and more.\nQuarto builds on the success of R Markdown. In fact, Quarto will Render R Markdown (.Rmd) documents without any edits or changes.\nSweave is a competing framework that it is out-of-date and Jupyter (Julia, Python, and R) is a competing framework that is popular for Python but has not caught on for R.\nAccording to Wickham and Grolemund, there are three main reasons to use R Markdown (they hold for Quarto):\n\n\n“For communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis.”\n“For collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them (i.e. the code).”\n“As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking.”\n\n\nQuarto uses\n\nplain text files ending in .qmd that are similar to .R files.\nlibrary(knitr).\npandoc.\n\nQuarto calls library(knitr) and “knits” .qmd (Quarto files) into .md (Markdown files), which Pandoc then converts into any specified output type. Quarto and library(knitr) don’t need to be explicitly loaded and the entire process is handled by clicking the “Render” button in RStudio.\n\n\n\n\n\n\n\n\n\nSource: Quarto website\nClicking the “Render” button starts this process.\n\n\n\n\n\n\n\n\n\nQuarto, library(knitr), and Pandoc are all installed with RStudio. The only additional software you will need is a LaTeX distribution. Follow these instructions to install library(tinytex) if you want to make PDF documents.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nIf you already have a LaTeX distribution like tinytext or MiKTeX, then skip this exercise.\nFollow these instructions to install library(tinytex).\n\n\n\nThe “Render” workflow has a few advantages:\n\nAll code is rerun in a clean environment when “Rendering”. This ensures that the code runs in order and is reproducible.\nIt is easier to document code than with inline comments.\nThe output types are really appealing. By creating publishable documents with code, there is no need to copy-and-paste or transpose results.\nThe process is iterable and scalable.\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nClick the new script button and add a “Quarto Document”.\nGive the document a name, an author, and ensure that HTML is selected.\nSave the document as “hello-quarto.qmd”.\nClick “Render”.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#three-ingredients-in-a-.qmd",
    "href": "06_reproducible-research-with-quarto.html#three-ingredients-in-a-.qmd",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.5 Three Ingredients in a .qmd",
    "text": "6.5 Three Ingredients in a .qmd\n\nYAML header\nMarkdown text\nCode chunks\n\n\n6.5.1 YAML header\nYAML stands for “yet another markup language”. The YAML header contains meta information about the document including output type, document settings, and parameters that can be passed to the document. The YAML header starts with --- and ends with ---.\nHere is the simplest YAML header for a PDF document:\n---\nformat: pdf\n---\nYAML headers can contain many output specific settings. This YAML header creates an HTML document with code folding and a floating table of contents:\n---\nformat: \n  html:\n    code-fold: true\n    toc: true\n---  \nParameters can be specified as follows\n---\nformat: pdf\nparams:\n  state: \"Virginia\"\n---\nNow state can be referred to anywhere in R code as params$state.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nSwitch the output type to PDF and render the document.\nSwitch the output type back to HTML.\n\n\n\n\n\n6.5.2 Markdown text\nMarkdown is a shortcut for HyperText Markup Language (HTML). Essentially, simple meta characters corresponding to formatting are added to plain text.\nTitles and subtitltes\n------------------------------------------------------------\n\n# Title 1\n\n## Title 2\n\n### Title 3\n\n\nText formatting \n------------------------------------------------------------\n\n*italic*  \n\n**bold**   \n\n`code`\n\nLists\n------------------------------------------------------------\n\n* Bulleted list item 1\n* Item 2\n  * Item 2a\n  * Item 2b\n\n1. Item 1\n2. Item 2\n\nLinks and images\n------------------------------------------------------------\n\n[text](http://link.com)\n\n\n\n\n\n\nExercise 1\n\n\n\n\nAdd text with formatting like headers and bold to your Quarto document.\nRender!\n\n\n\n\n\n6.5.3 Code chunks\n\n\n\n\n\n\n\n\n\nMore frequently, code is added in code chunks:\n\n```{r}\n2 + 2\n```\n\n[1] 4\n\n\nThe first argument inline or in a code chunk is the language engine. Most commonly, this will just be a lower case r. knitr allows for many different language engines:\n\nR\nJulia\nPython\nSQL\nBash\nRcpp\nStan\nJavascript\nCSS\n\nQuarto has a rich set of options that go inside of the chunks and control the behavior of Quarto.\n\n```{r}\n#| eval: false\n\n2 + 2\n```\n\nIn this case, eval makes the code not run. Other chunk-specific settings can be added inside the brackets. Here are the most important options:\n\n\n\nOption\nEffect\n\n\n\n\necho: false\nHides code in output\n\n\nwarning: false\nTurns off warnings\n\n\nfig-height: 8\nChanges figure width\n\n\nfig-width: 8\nChanges figure height\n\n\n\nDefault settings for the entire document can be changed in the YAML header with the execute option:\nexecute:\n  warning: false\nThe table added above was typed as Markdown code. But sometimes it is easier to use a code chunk to create and print a table. Pipe any data frame into knitr::kable() to create a table that will be formatted in the output of a rendered Quarto document.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nAdd a code chunk.\nLoad the storms data set.\nFilter the data to only include hurricanes.\nMake a data visualization with ggplot2 using the data from\nInclude an option to hide the R code.\nRender!",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#applications",
    "href": "06_reproducible-research-with-quarto.html#applications",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.6 Applications",
    "text": "6.6 Applications\n\n6.6.1 PDF documents\n---\nformat: pdf\n---\n\nThese notes!\n\n\n\n6.6.2 html documents\n---\nformat: html\n---\n\nRegression in R notes\nR at the Urban Institute website\n\n\n\n6.6.3 GitHub README\n---\nformat: gfm\n---\n\nurbnthemes\n\n\n\n6.6.4 Bookdown\nBookdown is an R package by Yihui Xie for authoring books in R Markdown. Many books, including R for Data Science (GitHub) by Hadley Wickham and Garrett Grolemund, have been written in R Markdown.\nQuarto book replaces bookdown. It is oriented around Quarto projects.\n\n\n6.6.5 Blogdown\nBlogdown is an R package by Yihui Xie for creating and managing a blog in R Markdown. Up & Running with blogdown in 2021 by Alison Hill is a great tutorial for getting started with Blogdown.\nThere is no good Quarto replacement right now.\n\n\n6.6.6 Microsoft Word and Microsoft PowerPoint\nIt is possible to write to Word and PowerPoint. In general, I’ve found the functionality to be limited and it is difficult to match institutional branding standards.\n\n\n6.6.7 Slides\n---\nformat:\n  revealjs:\n    css: styles.css\n    incremental: true\n    reveal_options:\n      slideNumber: true\n      previewLinks: true\n---\n\n\n6.6.8 Fact sheets\nAn alternative to rendering a Quarto document with the Render button is to use the quarto::render() function. This allows for iterating the rendering of documents. By passing different parameters to each rendering, it’s possible to create documents for different geographies, organizations, people, or periods of time.\nAt the Urban Institute, we regularly iterate fact sheets at the state and county level.\n\nExpanding the EITC for Workers without Resident Children\nData@Urban includes an outline.\n\n\n\n6.6.9 Fact pages\nIt’s also possible to iterate websites with quarto::render().\n\nThe Urban Institute State and Local Finance Initiative creates State Fiscal Briefs by iterating R Markdown documents.\nData@Urban",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#suggestions",
    "href": "06_reproducible-research-with-quarto.html#suggestions",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.7 Suggestions",
    "text": "6.7 Suggestions\n\nRender early, and render often.\nSelect the gear to the right of “Render” and select “Chunk Output in Console”\nLearn math mode. Also, library(equatiomatic) (CRAN, GitHub) is amazing.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#resources",
    "href": "06_reproducible-research-with-quarto.html#resources",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.8 Resources",
    "text": "6.8 Resources\n\nQuarto intro\nR4DS R Markdown chapter\nHappy Git R Markdown tutorial",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#reading-in-data",
    "href": "05_exploratory-data-analysis.html#reading-in-data",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "5.1.1 Excel Spreadsheets\nreadxl is a tidyverse package for reading data from Microsoft Excel files. It is not a core tidyverse package, so it needs to be explicitly loaded in each R session.\nThe tidyverse website has a good tutorial on readxl.\nMany excel files can be read with the simple syntax data &lt;- read_excel(path = \"relative/file/path/to/data\"). In cases where the Excel spreadsheet contains multiple sheets, you can use the sheet argument to specify the sheet to read as a string (the name of the sheet) or an integer (the position of the sheet). You can also read only components of a sheet using Excel-style cell-ranges (ex: A3:L44).\n\n\n5.1.2 STATA, SAS, and SPSS files\nhaven is a tidyverse package for reading data from SAS (read_sas()), STATA (read_dta()), and SPSS (read_sav()) files. Like the readxl package, it is not a core tidyverse package and also needs to be explicitly loaded in each R session.\nNote that the haven package can only read and write STATA .dta files through version 15 of STATA. For files created in more recent versions of STATA, the readstat13 package’s read.dta13 file can be used.\n\n\n5.1.3 Zip Files\nYou may also want to read in data that is saved in a zip file. In order to do this, you can use the unzip() function to unzip the files using the following syntax: unzip(zipfile = \"path/to/zip/file\", exdir = \"path/to/directory/for/unzipped/files\").\nOften times, you may want to read in a zip file from a website into R. In order to do this, you will need to first download the zip file to your computer using the download.file() function, unzip the file using unzip() and then read in the data using the appropriate function for the given file type.\nTo download the week 40 public use file data for the Census Household Pulse Survey, run the following code:\n\nbase_url &lt;- \"https://www2.census.gov/programs-surveys/demo/datasets/hhp/\"\nweek_url &lt;- \"2021/wk40/HPS_Week40_PUF_CSV.zip\"\n\npulse_url &lt;- paste0(base_url, week_url)\n\n# creates data directory in working directory\n# gives warning if directory already exists\ndir.create(\"data\")\n\n# For Mac, *.nix systems:\ndownload.file(\n  pulse_url, \n  destfile = \"data/pulse40.zip\"\n)\n\n# For Windows systems, you need to add the mode = \"wb\" \n# argument to prevent an invalid zip file \ndownload.file(\n  pulse_url, \n  destfile = \"data/pulse40.zip\", \n  mode = \"wb\"\n)\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCopy and paste the code chunk above and keep the appropriate download.file command for your computer.\nWrite code using the unzip() function to unzip the zip file downloaded. Set zipfile to be the same the destfile parameter you used in part 1. Set exdir to be the same directory where you just downloaded the zip file. Run both of these commands.\nExamine the unzipped files and select the appropriate function to read in the pulse2021_puf_40 file. Write code to read that file into R, assigning the output to the pulse object.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#column-names",
    "href": "05_exploratory-data-analysis.html#column-names",
    "title": "5  Exploratory Data Analysis",
    "section": "5.2 Column Names",
    "text": "5.2 Column Names\nAs we discussed in week 1, dataframe columns - like other objects - should be given names that are “concise and meaningful”. Generally column names should be nouns and only use lowercase letters, numbers, and underscores _ (this is referred to as snake case). Columns should not begin with numbers. You should not include white space in column names (e.g “Birth Month” = bad, “birth_month” = good). It is also best practice for column names to be singular (use “birth_month” instead of “birth_months”).\nThe janitor package is a package that contains a number of useful functions to clean data in accordance with the tidyverse principles. One such function is the clean_names() function, which converts column names to snake case according to the tidyverse type guide (along with some other useful cleaning functions outlined in the link above). The clean_names() function works well with the |&gt; operator.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nTake a look at the column names in the Pulse data file you read in for the exercise earlier.\nThen edit the command in the R script that you wrote to read in the CSV file to pipe the results of that command to the janitor::clean_names() function. Note that you may have to install and import the janitor package first.\nNow look at the column names again after running the modified command. How have they changed?\n\n\n\nAs discussed in the introduction to the tidyverse, you can also directly rename columns using the rename() function from the dplyr package as follows: rename(data, new_col = old_col).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#data-overview",
    "href": "05_exploratory-data-analysis.html#data-overview",
    "title": "5  Exploratory Data Analysis",
    "section": "5.3 Data Overview",
    "text": "5.3 Data Overview\nOnce you’ve imported your data, a common first step is to get a very high-level summary of your data.\nAs introduced in the introduction to the tidyverse, the glimpse() function provides a quick view of your data, printing the type and first several values of each column in the dataset to the console.\n\nglimpse(x = storms)\n\nRows: 19,537\nColumns: 13\n$ name                         &lt;chr&gt; \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\",…\n$ year                         &lt;dbl&gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975,…\n$ month                        &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ day                          &lt;int&gt; 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 2…\n$ hour                         &lt;dbl&gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18,…\n$ lat                          &lt;dbl&gt; 27.5, 28.5, 29.5, 30.5, 31.5, 32.4, 33.3,…\n$ long                         &lt;dbl&gt; -79.0, -79.0, -79.0, -79.0, -78.8, -78.7,…\n$ status                       &lt;fct&gt; tropical depression, tropical depression,…\n$ category                     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wind                         &lt;int&gt; 25, 25, 25, 25, 25, 25, 25, 30, 35, 40, 4…\n$ pressure                     &lt;int&gt; 1013, 1013, 1013, 1013, 1012, 1012, 1011,…\n$ tropicalstorm_force_diameter &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hurricane_force_diameter     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThe summary() function enables you to quickly understand the distribution of a numeric or categorical variable. For a numeric variable, summary() will return the minimum, first quartile, median, mean, third quartile, and max values. For a categorical (or factor) variable, summary() will return the number of observations in each category. If you pass a dataframe to summary() it will summarize every column in the dataframe. You can also call summary on a single variable as shown below:\n\nsummary(storms$wind)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.00   30.00   45.00   50.05   65.00  165.00 \n\n\nThe str() function compactly displays the internal structure of any R object. If you pass a dataframe to str it will print the column type and first several values for each column in the dataframe, similar to the glimpse() function.\nGetting a high-level overview of the data can help you identify what questions you need to ask of your data during the exploratory data analysis process. The rest of this lecture will outline several questions that you should always ask when exploring your data - though this list is not exhaustive and will be informed by your specific data and analysis!",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#are-my-columns-the-right-types",
    "href": "05_exploratory-data-analysis.html#are-my-columns-the-right-types",
    "title": "5  Exploratory Data Analysis",
    "section": "5.4 Are my columns the right types?",
    "text": "5.4 Are my columns the right types?\nWe’ll read in the population-weighted centroids for the District of Columbia exported from the Missouri Census Data Center’s geocorr2014 tool.\n\ndc_centroids &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/geocorr/geocorr2014_dc.csv\"\n  )\n)\n\nRows: 180 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): county, tract, cntyname, intptlon, intptlat, pop10, afact\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(dc_centroids)\n\nRows: 180\nColumns: 7\n$ county   &lt;chr&gt; \"County code\", \"11001\", \"11001\", \"11001\", \"11001\", \"11001\", \"…\n$ tract    &lt;chr&gt; \"Tract\", \"0001.00\", \"0002.01\", \"0002.02\", \"0003.00\", \"0004.00…\n$ cntyname &lt;chr&gt; \"County name\", \"District of Columbia DC\", \"District of Columb…\n$ intptlon &lt;chr&gt; \"Wtd centroid W longitude, degrees\", \"-77.058857\", \"-77.07521…\n$ intptlat &lt;chr&gt; \"Wtd centroid latitude, degrees\", \"38.909434\", \"38.909223\", \"…\n$ pop10    &lt;chr&gt; \"Total population (2010)\", \"4890\", \"3916\", \"5425\", \"6233\", \"1…\n$ afact    &lt;chr&gt; \"tract to tract allocation factor\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nWe see that all of the columns have been read in as character vectors because the second line of the csv file has a character description of each column. By default, read_csv uses the first 1,000 rows of data to infer the column types of a file. We can avoid this by skipping the first two lines of the csv file and manually setting the column names.\n\n#save the column names from the dataframe\ncol_names &lt;- dc_centroids %&gt;% names()\n\ndc_centroids &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/geocorr/geocorr2014_dc.csv\"\n  ),\n  col_names = col_names,\n  skip = 2\n)\n\nRows: 179 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): tract, cntyname\ndbl (5): county, intptlon, intptlat, pop10, afact\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(dc_centroids)\n\nRows: 179\nColumns: 7\n$ county   &lt;dbl&gt; 11001, 11001, 11001, 11001, 11001, 11001, 11001, 11001, 11001…\n$ tract    &lt;chr&gt; \"0001.00\", \"0002.01\", \"0002.02\", \"0003.00\", \"0004.00\", \"0005.…\n$ cntyname &lt;chr&gt; \"District of Columbia DC\", \"District of Columbia DC\", \"Distri…\n$ intptlon &lt;dbl&gt; -77.05886, -77.07522, -77.06813, -77.07583, -77.06670, -77.05…\n$ intptlat &lt;dbl&gt; 38.90943, 38.90922, 38.90803, 38.91848, 38.92316, 38.92551, 3…\n$ pop10    &lt;dbl&gt; 4890, 3916, 5425, 6233, 1455, 3376, 3189, 4539, 4620, 3364, 6…\n$ afact    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nYou can convert column types by using the as.* set of functions. For example, we could convert the county column to a character vector as follows: mutate(dc_centroids, county = as.character(county)). We can also set the column types when reading in the data with read_csv() using the col_types argument. For example:\n\ndc_centroids &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/geocorr/geocorr2014_dc.csv\"\n  ),\n  col_names = col_names,\n  skip = 2,\n  col_types = c(\"county\" = \"character\")\n)\n\nglimpse(dc_centroids)\n\nRows: 179\nColumns: 7\n$ county   &lt;chr&gt; \"11001\", \"11001\", \"11001\", \"11001\", \"11001\", \"11001\", \"11001\"…\n$ tract    &lt;chr&gt; \"0001.00\", \"0002.01\", \"0002.02\", \"0003.00\", \"0004.00\", \"0005.…\n$ cntyname &lt;chr&gt; \"District of Columbia DC\", \"District of Columbia DC\", \"Distri…\n$ intptlon &lt;dbl&gt; -77.05886, -77.07522, -77.06813, -77.07583, -77.06670, -77.05…\n$ intptlat &lt;dbl&gt; 38.90943, 38.90922, 38.90803, 38.91848, 38.92316, 38.92551, 3…\n$ pop10    &lt;dbl&gt; 4890, 3916, 5425, 6233, 1455, 3376, 3189, 4539, 4620, 3364, 6…\n$ afact    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nAs you remember from week 1, a vector in R can only contain one data type. If R does not know how to convert a value in the vector to the given type, it may introduce NA values by coercion. For example:\n\nas.numeric(c(\"20\", \"10\", \"10+\", \"25\", \"~8\"))\n\nWarning: NAs introduced by coercion\n\n\n[1] 20 10 NA 25 NA\n\n\n\n5.4.1 String manipulation with stringr\nBefore converting column types, it is critical to clean the column values to ensure that NA values aren’t accidentally introduced by coercion. The stringr package in the tidyverse offers a number of excellent functions for cleaning character data. This package is part of the core tidyverse and is automatically loaded with library(tidyverse). The stringr cheat sheet offers a great guide to the stringr functions.\nTo demonstrate some of the stringr functions, let’s create a state column with the two digit state FIPS code for DC and a geoid column in the dc_centroid dataframe which contains the 11-digit census tract FIPS code, which can be useful for joining this dataframe with other dataframes that commonly use the FIPS code as a unique identifier. We will need to first remove the period from the tract column and then concatenate the county and tract columns into a geoid column. We can do that using stringr as follows:\n\ndc_centroids &lt;- dc_centroids %&gt;%\n  mutate(\n    #replace first instance of pattern\n    tract = str_replace(tract, \"\\\\.\", \"\"), \n    #join multiple strings into single string\n    geoid = str_c(county, tract, sep = \"\")\n  )\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nCopy the code above into an R script and edit it to add the creation of a variable called state that is equal to the first two characters of the county variable using the str_sub() function.\n\n\n\nNote that the str_replace() function uses regular expressions to match the pattern that gets replaced. Regular expressions is a concise and flexible tool for describing patterns in strings - but it’s syntax can be complex and not particularly intuitive. This vignette provides a useful introduction to regular expressions, and when in doubt - there are plentiful Stack Overflow posts to help when you search your specific case. ChatGPT is often effective at writing regular expressions, but recall our warnings about using ChatGPT in Chapter 1.\n\n\n5.4.2 Date manipulation with lubridate\nThe lubridate package makes it much easier to work with dates and times in R. While also part of the tidyverse, it is not a core tidyverse package and must be explicitly loaded in each session with library(lubridate).\nWe’ll use a dataset on political violence and protest events across the continent of Africa in 2022 from the Armed Conflict Location & Event Data Project (ACLED) to illustrate the power of lubridate. The lubridate package allows users to easily and quickly parse date-time variables in a range of different formats. For example, the ymd() function takes a date variable in year-month-day format (e.g. 2022-01-31) and converts it to a date-time format. The different formats are outlined in the lubridate documentation.\n\nlibrary(lubridate)\nacled_2022 &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/acled/acled_africa_2022.csv\")\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(event_date = ymd(event_date)) %&gt;%\n  select(event_date, region, country, event_type, sub_event_type)\n\nRows: 22101 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (15): EVENT_ID_CNTY, EVENT_TYPE, SUB_EVENT_TYPE, ACTOR1, ASSOC_ACTOR_1,...\ndbl  (12): ISO, EVENT_ID_NO_CNTY, YEAR, TIME_PRECISION, INTER1, INTER2, INTE...\nlgl   (1): ADMIN3\ndttm  (1): EVENT_DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCreating datetime columns enables the use of a number of different operations, such as filtering based on date:\n\nacled_2022 %&gt;%\n  filter(event_date &gt; \"2022-06-01\")\n\n# A tibble: 6,859 × 5\n   event_date region          country event_type             sub_event_type     \n   &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;                  &lt;chr&gt;              \n 1 2022-06-04 Northern Africa Algeria Protests               Peaceful protest   \n 2 2022-06-05 Northern Africa Algeria Protests               Peaceful protest   \n 3 2022-06-05 Northern Africa Algeria Protests               Peaceful protest   \n 4 2022-06-08 Northern Africa Algeria Protests               Peaceful protest   \n 5 2022-06-09 Northern Africa Algeria Protests               Peaceful protest   \n 6 2022-06-09 Northern Africa Algeria Protests               Peaceful protest   \n 7 2022-06-11 Northern Africa Algeria Protests               Peaceful protest   \n 8 2022-06-15 Northern Africa Algeria Strategic developments Looting/property d…\n 9 2022-06-18 Northern Africa Algeria Protests               Protest with inter…\n10 2022-06-19 Northern Africa Algeria Protests               Peaceful protest   \n# ℹ 6,849 more rows\n\n\nOr calculating durations:\n\nacled_2022 %&gt;%\n  #number of days into 2022 event occurred\n  mutate(days_into_2022 = event_date - ymd(\"2022-01-01\")) \n\n# A tibble: 22,101 × 6\n   event_date region          country event_type sub_event_type   days_into_2022\n   &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;drtn&gt;        \n 1 2022-01-03 Northern Africa Algeria Protests   Peaceful protest 2 days        \n 2 2022-01-03 Northern Africa Algeria Protests   Peaceful protest 2 days        \n 3 2022-01-04 Northern Africa Algeria Protests   Peaceful protest 3 days        \n 4 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 5 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 6 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 7 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 8 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 9 2022-01-06 Northern Africa Algeria Protests   Peaceful protest 5 days        \n10 2022-01-06 Northern Africa Algeria Protests   Peaceful protest 5 days        \n# ℹ 22,091 more rows\n\n\nOr extracting components of dates:\n\nacled_2022 %&gt;%\n  mutate(event_month = month(event_date)) #month event occurred\n\n# A tibble: 22,101 × 6\n   event_date region          country event_type sub_event_type   event_month\n   &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;\n 1 2022-01-03 Northern Africa Algeria Protests   Peaceful protest           1\n 2 2022-01-03 Northern Africa Algeria Protests   Peaceful protest           1\n 3 2022-01-04 Northern Africa Algeria Protests   Peaceful protest           1\n 4 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 5 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 6 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 7 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 8 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 9 2022-01-06 Northern Africa Algeria Protests   Peaceful protest           1\n10 2022-01-06 Northern Africa Algeria Protests   Peaceful protest           1\n# ℹ 22,091 more rows\n\n\nDatetimes can also much more easily be plotted using ggplot2. For example, it is easy to visualize the distribution of events across the year:\n\nacled_2022 %&gt;% \n  ggplot(aes(x = event_date)) + \n  geom_freqpoly(binwidth = 1) # each bin is 1 day\n\n\n\n\n\n\n\n\nFor more information on lubridate, see the lubridate cheat sheet.\nSource: R for Data Science, Ch 16\n\n\n5.4.3 Categorical and Factor Variables\nCategorical variables can be stored as characters in R. The case_when() function makes it very easy to create categorical variables based on other columns. For example:\n\npulse &lt;- pulse %&gt;%\n  mutate(\n    hisp_rrace = case_when(\n      rrace == 1 ~ \"White alone, not Hispanic\",\n      rrace == 2 ~ \"Black alone, not Hispanic\",\n      rrace == 3 ~ \"Asian alone, not Hispanic\",\n      rrace == 4 ~ \"Two or more races + Other races, not Hispanic\",\n      TRUE ~ NA_character_)\n  )\n\nFactors are a data type specifically made to work with categorical variables. The forcats library in the core tidyverse is made to work with factors. Factors are particularly valuable if the values have a ordering that is not alphanumeric.\n\nx1 &lt;- c(\"Dec\", \"Apr\", \"Jan\", \"Mar\")\n\nmonth_levels &lt;- c(\n  \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n  \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n)\n\ny1 &lt;- factor(x1, levels = month_levels)\n\nsort(x1)\n\n[1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"\n\nsort(y1)\n\n[1] Jan Mar Apr Dec\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nFactors are also valuable if you want to show all possible values of the categorical variable, even when they have no observations.\n\ntable(x1)\n\nx1\nApr Dec Jan Mar \n  1   1   1   1 \n\ntable(y1)\n\ny1\nJan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec \n  1   0   1   1   0   0   0   0   0   0   0   1",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#is-there-missing-data",
    "href": "05_exploratory-data-analysis.html#is-there-missing-data",
    "title": "5  Exploratory Data Analysis",
    "section": "5.5 Is there missing data?",
    "text": "5.5 Is there missing data?\nBefore you work with any dataset, you should understand how missing values are encoded. The best place to find this information is the data dictionary - which you should always read before working with any new dataset!\nThis is particularly important because while R automatically recognizes standard missing values as NA, it doesn’t recognize non-standard encodings like numbers representing missing values, “missing”, “na”, “N/A”, etc.\nNon-standard missing values should be converted to NA before conducting analysis. One way of doing this is with mutate and the if_else or case_when functions.\n\n\n\n\n\n\nExercise 3\n\n\n\n\nGo to the folder where you unzipped the Pulse data from earlier and open the data dictionary file. How does this dataset represent missing values for the RECVDVACC variable?\nUsing mutate and if_else or case_when, replace the missing values in the recvdvacc column with NA.\n\n\n\nOnce you have converted all missing value encodings to NA, the next question you need to ask is how you want to handle missing values in your analysis. The right approach will depend on what the missing value represents and the goals of your analysis.\n\nLeave as NA: This can be the best choice when the missing value truly represents a case when the true value is unknown. You will need to handle NAs by setting na.rm = TRUE in functions or filtering using is.na(). One drawback of this approach is that if the values aren’t missing at random (e.g. smokers may be less likely to answer survey questions about smoking habits), your results may be biased. Additionally, this can cause you to lose observations and reduce power of analyses.\nReplace with 0: This can be the best choice if a missing value represents a count of zero for a given entity. For example, a dataset on the number of Housing Choice Voucher tenants by zip code and quarter may have a value of NA if there were no HCV tenants in the given zip code and quarter.\nImpute missing data: Another approach is imputing the missing data with a reasonable value. There are a number of different imputation approaches:\n\nMean/median/mode imputation: Fills the missing values with the column mean or median. This approach is very easy to implement, but can artifically reduce variance in your data and be sensitive to outliers in the case of mean imputation.\nPredictive imputation: Fills the missing values with a predicted value based on a model that has been fit to the data or calculated probabilities based on other columns in the data. This is a more complex approach but is likely more accurate (for example, it can take into account variable correlation).\n\n\nThe replace_na() function in dplyr is very useful for replacing NA values in one or more columns.\n\ndf &lt;- tibble(x = c(1, 2, NA), y = c(\"a\", NA, \"b\"))\n\n# Using replace_na to replace one column\ndf %&gt;% \n  mutate(x = replace_na(x, 0))\n\n# A tibble: 3 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 &lt;NA&gt; \n3     0 b    \n\n# Using replace_na to replace multiple columns with different values\ndf %&gt;% \n  replace_na(list(x = 0, y = \"unknown\"))\n\n# A tibble: 3 × 2\n      x y      \n  &lt;dbl&gt; &lt;chr&gt;  \n1     1 a      \n2     2 unknown\n3     0 b      \n\n# Using if_else to perform mean imputation\ndf %&gt;% \n  mutate(x = if_else(is.na(x), mean(x, na.rm = TRUE), x))\n\n# A tibble: 3 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1   1   a    \n2   2   &lt;NA&gt; \n3   1.5 b",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#do-i-have-outliers-or-unexpected-values",
    "href": "05_exploratory-data-analysis.html#do-i-have-outliers-or-unexpected-values",
    "title": "5  Exploratory Data Analysis",
    "section": "5.6 Do I have outliers or unexpected values?",
    "text": "5.6 Do I have outliers or unexpected values?\n\n5.6.1 Identifying Outliers/Unexpected Values\nUsing R to examine the distribution of your data is one way to identify outliers or unexpected values. For example, we can examine the distribution of the bodywt variable in the msleep dataset both by examining the mathematical distribution using the summary() function and visually using ggplot.\n\nsummary(msleep$bodywt)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   0.005    0.174    1.670  166.136   41.750 6654.000 \n\n\n\nmsleep %&gt;%\n  ggplot(aes(bodywt, 1)) +\n  geom_point(alpha = 0.2) +\n  scale_y_continuous(breaks = 0) +\n  labs(y = NULL) +\n  theme_bw() +\n  theme(panel.border = ggplot2::element_blank())\n\n\n\n\n\n\n\n\n\n\n5.6.2 Unit Tests\nWriting tests in R is a great way to test that your data does not have unexpected/incorrect values. These tests can also be used to catch mistakes that can be introduced by errors in the data cleaning process. There are a number of R packages that have tools for writing tests, including:\n\ntestthat\nassertthat\nassertr\n\n\nlibrary(assertr)\n\ndf &lt;- tibble(age = c(20, 150, 47, 88),\n             height = c(60, 2, 72, 66))\n\ndf %&gt;%\n  assertr::verify(age &lt; 120) %&gt;%\n  summarise(mean(age, na.rm = TRUE))\n\nverification [age &lt; 120] failed! (1 failure)\n\n    verb redux_fn predicate column index value\n1 verify       NA age &lt; 120     NA     2    NA\n\n\nError: assertr stopped execution\n\n\nCritically, adding the test caused the code to return an error before calculating the mean of the age variable. This is a feature, not a bug! It can prevent you from introducing errors into your analyses. Moreover, by writing a set of tests in your analysis code, you can run the same checks every time you perform the analysis which can help you catch errors caused by changes in the input data.\n\n\n5.6.3 Handling Outliers/Unexpected Values\nWhen you identify outliers or unexpected values, you will have to decide how you want to handle those values in your data. The proper way to handle those values will depend on the reason for the outlier value and the objectives of your analysis.\n\nIf the outlier is caused by data errors, such as an implausible age or population value, you can replace the outlier values with NA using mutate and if_else or case_when as described above.\nIf the outlier represents a different population than the one you are studying - e.g. the different consumption behaviors of individual consumers versus wholesale business orders - you can remove it from the data.\nYou can transform the data, such as taking the natural log to reduce the variation caused by outliers.\nYou can select a different analysis method that is less sensitive to outliers, such as using the median rather than the mean to measure central tendency.\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nRead the column descriptions in the csv file for the DC centroids.\nUse one of the methods above to identify whether the pop10 column contains any outliers. According to the Census Bureau, tracts generally have a population between 1,200 and 8,000 people.\nCalculate the mean of the pop10 column in the dc_centroids dataframe, but first write one test using assertr::verify() to test for invalid values based on the column definition.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#data-quality-assurance",
    "href": "05_exploratory-data-analysis.html#data-quality-assurance",
    "title": "5  Exploratory Data Analysis",
    "section": "5.7 Data Quality Assurance",
    "text": "5.7 Data Quality Assurance\nData quality assurance is the foundation of high quality analysis. Four key questions that you should always ask when considering using a dataset for analysis are:\n\nDoes the data suit the research question? Examine the data quality (missingness, accuracy) of key columns, the number of observations in subgroups of interest, etc.\nDoes the data accurately represent the population of interest? Think about the data generation process (e.g. using 311 calls or Uber ride data to measure potholes) - are any populations likely to be over or underrepresented? Use tools like Urban’s Spatial Equity Data Tool to test data for unrepresentativeness.\nIs the data gathering reproducible? Wherever possible, eliminate manual steps to gather or process the data. This includes using reproducible processes for data ingest such as APIs or reading data directly from the website rather than manually downloading files. All edits to the data should be made programmatically (e.g. skipping rows when reading data rather than deleting extraneous rows manually). Document the source of the data including the URL, the date of the access, and specific metadata about the vintage.\nHow can you verify that you are accessing and using the data correctly? This may include writing tests to ensure that raw and calculated data values are plausible, comparing summary statistics against those provided in the data documentation (if applicable), published tables/statistics from the data provider, or published tables/statistics from a trusted third party.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  }
]