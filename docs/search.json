[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Public Policy Part II",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "01_advanced-quarto.html#sec-review",
    "href": "01_advanced-quarto.html#sec-review",
    "title": "1  Advanced Quarto",
    "section": "1.1 Review",
    "text": "1.1 Review\n\n1.1.1 Motivation\nThere are many problems worth avoiding in an analysis:\n\nCopying-and-pasting, transposing, and manual repetition\nRunning code out-of-order\nMaintaining parallel documents like a script for analysis and a doc for narrative\nCode written for computers that is tough to parse by humans\n\nNot convinced? Maybe we just want to make cool stuff like websites, blogs, books, and slide decks.\nQuarto, a literate statistical programming framework for R, Python, and Julia helps us solve many of these problems. Quarto uses\n\nplain text files ending in .qmd that are similar to .R and .Rmd files\nlibrary(knitr)\npandoc1\n\nQuarto uses library(knitr) and pandoc to convert plain text .qmd documents into rich output documents like these class notes. The “Render” button appears in RStudio with a .qmd file is open in the editor window.\nClicking the “Render” button begins the process of rendering .qmd files.\n\n\n\n\n\n\n\n\n\nWhen the button is clicked, Quarto calls library(knitr) and renders .qmd (Quarto files) into .md (Markdown files), which Pandoc then converts into any specified output type. Quarto and library(knitr) don’t need to be explicitly loaded as the entire process is handled by clicking the “Render” button in RStudio.\n\n\n\n\n\n\n\n\n\nSource: Quarto website\nQuarto, library(knitr), and Pandoc are all installed with RStudio. You will need to install a LaTeX distribution to render PDFs. We recommend library(tinytex) as a LaTeX distribution (installation instructions).\n\n\n\n\n\n\nExercise 1\n\n\n\n\nClick the new script button in RStudio and add a “Quarto Document”.\nGive the document a name, an author, and ensure that HTML is selected.\nSave the document as “hello-quarto.qmd”.\nClick “Render”.\n\n\n\nQuarto has three main ingredients:\n\nYAML header\nMarkdown text\nCode chunks\n\n\n\n1.1.2 (1) YAML Header\nYAML stands for “yet another markup language”. The YAML header contains meta information about the document including output type, document settings, and parameters that can be passed to the document. The YAML header starts with --- and ends with ---.\nHere is the simplest YAML header for a PDF document:\n---\nformat: pdf\n---\nYAML headers can contain many output specific settings. This YAML header creates an HTML document with code folding and a floating table of contents:\n---\nformat: \n  html:\n    embed-resources: true\n    code-fold: true\n    toc: true\n---  \nParameters can be specified as follows\n---\nformat: pdf\nparams:\n  state: \"Virginia\"\n---\nNow state can be referred to anywhere in R code as params$state. Parameters are useful for a couple of reasons:\n\nWe can clearly change key values for a Quarto document in the YAML header.\nWe can create a template and programmatically iterate the template over a set of values with the quarto_render() function and library(purrr). This blog outlines the idea. The Mobility Metrics Data Tables and SLFI State Fiscal Briefs are key examples of this workflow.\n\n\n\n\n\n\n\nWarning\n\n\n\nUnlike R Markdown, images and other content are not embedded in .html from Quarto by default. Be sure to include embed-resources: true in YAML headers to embed content and make documents easier to share.\nSuppose we embed an image called image.png in a Quarto document called example.qmd, which, when rendered, creates example.html. If we don’t include embed-resources: true, then we will need to share image.png and example.html to see the embedded image. This is also true for other files like .css.\n\n\n\n\n1.1.3 (2) Markdown text\nMarkdown is a shortcut for HyperText Markup Language (HTML). Essentially, simple meta characters corresponding to formatting are added to plain text.\nTitles and subtitltes\n------------------------------------------------------------\n\n# Title 1\n\n## Title 2\n\n### Title 3\n\n\nText formatting \n------------------------------------------------------------\n\n*italic*  \n\n**bold**   \n\n`code`\n\nLists\n------------------------------------------------------------\n\n* Bulleted list item 1\n* Item 2\n  * Item 2a\n  * Item 2b\n\n1. Item 1\n2. Item 2\n\nLinks and images\n------------------------------------------------------------\n\n[text](http://link.com)\n\n![Penguins](images/penguins.png)\n\n\n1.1.4 (3) Code chunks\n\n\n\n\n\nMore frequently, code is added in code chunks:\n\n```{r}\n2 + 2\n```\n\n[1] 4\n\n\nThe first argument inline or in a code chunk is the language engine. Most commonly, this will just be a lower case r. knitr allows for many different language engines:\n\nR\nJulia\nPython\nSQL\nBash\nRcpp\nStan\nJavascript\nCSS\n\nQuarto has a rich set of options that go inside of the chunks and control the behavior of Quarto.\n\n```{r}\n#| label: important-calculation\n#| eval: false\n\n2 + 2\n```\n\nIn this case, eval makes the code not run. Other chunk-specific settings can be added inside the brackets. Here2 are the most important options:\n\n\n\nOption\nEffect\n\n\n\n\necho: false\nHides code in output\n\n\neval: false\nTurns off evaluation\n\n\noutput: false\nHides code output\n\n\nwarning: false\nTurns off warnings\n\n\nmessage: false\nTurns off messages\n\n\nfig-height: 8\nChanges figure width in inches3\n\n\nfig-width: 8\nChanges figure height in inches4\n\n\n\nYou can see the quarto defaults for figure dimensions by format here.\nDefault settings for the entire document can be changed in the YAML header with the execute option:\nexecute:\n  warning: false\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAdd date: today to your YAML header after title. This will update every time the document is rendered.\nCopy the Markdown table from this table generator and add it to your .qmd document.\nCreate a scatter plot of the cars data with library(ggplot2). Adjust the figure width and height using options within the chunk.\nClick “Render”.\n\n\n\n\n\n1.1.5 Organizing a Quarto Document\nIt is important to clearly organize a Quarto document and the constellation of files that typically support an analysis.\n\nAlways use .Rproj files.\nUse sub-directories to sort images, .css, data.\n\nLater, we will learn how to use library(here) to effectively organize sub-directories."
  },
  {
    "objectID": "01_advanced-quarto.html#math-notation",
    "href": "01_advanced-quarto.html#math-notation",
    "title": "1  Advanced Quarto",
    "section": "1.2 Math Notation",
    "text": "1.2 Math Notation\nThis course uses probability and statistics. Occasionally, we want to easily communicate with mathematical notation. For example, it may be convenient to type that \\(X\\) is a random variable that follows a standard normal distribution (mean = 0 and standard deviation = 1).\n\\[X \\sim N(\\mu = 0, \\sigma = 1)\\]\n\n1.2.1 Math Mode\nUse $ to start and stop in-line math notation and $$ to start multi-line math notation. Math notation uses LaTeX’s syntax for mathematical notation.\nHere’s an example with in-line math:\nConsider a binomially distributed random variable, $X \\sim binom(n, p)$. \nConsider a binomially distributed random variable, \\(X \\sim binom(n, p)\\).\nHere’s an example with a chunk of math:\n$$\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n$${#eq-binomial}\n\\[\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n\\tag{1.1}\\]\n\n\n1.2.2 Important Syntax\nMath mode recognizes basic math symbols available on your keyboard including +, -, *, /, &gt;, &lt;, (, and ).\nMath mode contains all greek letters. For example, \\alpha (\\(\\alpha\\)) and \\beta (\\(\\beta\\)).\n\n\nTable 1.1: My Caption\n\n\nLaTeX\nSymbol\n\n\n\n\n\\alpha\n\\(\\alpha\\)\n\n\n\\beta\n\\(\\beta\\)\n\n\n\\gamma\n\\(\\gamma\\)\n\n\n\\Delta\n\\(\\Delta\\)\n\n\n\\epsilon\n\\(\\epsilon\\)\n\n\n\\theta\n\\(\\theta\\)\n\n\n\\pi\n\\(\\pi\\)\n\n\n\\sigma\n\\(\\sigma\\)\n\n\n\\chi\n\\(\\chi\\)\n\n\n\n\nMath mode also recognizes \\(\\log(x)\\) (\\log(x)) and \\(\\sqrt{x}\\) (\\sqrt{x}).\nSuperscripts (^) are important for exponentiation and subscripts (_) are important for adding indices. y = x ^ 2 renders as \\(y = x ^ 2\\) and x_1, x_2, x_3 renders as \\(x_1, x_2, x_3\\). Brackets are useful for multi-character superscripts and subscripts like \\(s_{11}\\) (s_{11}).\nIt is useful to add symbols to letters. For example, \\bar{x} is useful for sample means (\\(\\bar{x}\\)), \\hat{y} is useful for predicted values (\\(\\hat{y}\\)), and \\vec{\\beta} is useful for vectors of coefficients (\\(\\vec{\\beta}\\)).\nMath mode supports fractions with \\frac{x}{y} (\\(\\frac{x}{y}\\)), big parentheses with \\left(\\right) (\\(\\left(\\right)\\)), and brackets with \\left[\\right] (\\(\\left[\\right]\\)).\nMath mode has a symbol for summation. Let’s combine it with bars, fractions, subscripts, and superscipts to show sample mean \\bar{x} = \\frac{1}{n}\\sum_i^n x_i, which looks like \\(\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\).\n\\sim is how to add the tilde for distributed as. For example, X \\sim N(\\mu = 0, \\sigma = 1) shows the normal distribution \\(X \\sim N(\\mu = 0, \\sigma = 1)\\).\nMatrices are are a little bit more work in math mode. Consider the follow variance-covariance matrix:\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\[\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\]\nThis guide provides and exhaustive look at math options in Quarto.\n\n\n\n\n\n\nWarning\n\n\n\nMath mode is finicky! Small errors like mismatched parentheses or superscript and subscript errors will cause Quarto documents to fail to render. Write math carefully and render early and often.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse math mode to type out the equation for root mean square error (RMSE).\nDo you divide by n or n - 1?"
  },
  {
    "objectID": "01_advanced-quarto.html#cross-references",
    "href": "01_advanced-quarto.html#cross-references",
    "title": "1  Advanced Quarto",
    "section": "1.3 Cross References",
    "text": "1.3 Cross References\nCross references are useful for organizing documents that include sections, figures, tables, and equations. Cross references create hyperlinks within documents that jump to the locations of these elements. Linking sections, figures, tables, or equations helps readers navigate the document.\nCross references also automatically number the referenced elements. This means that if there are two tables (ie. Table 1 and Table 2) and a table is added between the two tables, all of the table numbers and references to the tables will automatically update.\nCross references require two bits of code within a Quarto document:\n\nA label associated with the section, figure, table, or equation.\nA reference to the labelled section, figure, table, or equation.\n\nLabels are written in brackets or as arguments in code chunks, and begin with the the type object being linked. References begin with @ followed by the label of object being linked.\n\n1.3.1 Sections\nLinking sections helps readers navigate between sections. Use brackets to label sections after headers and always begin labels with sec-. Then you can reference that section with @sec-.\n## Review {sec-review}\n\nSee @sec-review if you are totally lost.\nThe cross references shows up like this: See Section 1.1 if you are totally lost.\nIt can be helpful to turn on section numbering with number-sections: true in the YAML header. Additionally, Markdown has a native method for linking between sections.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nAdd a few section headers to your Quarto document.\nAdd a cross reference to one of the section headers.\n\n\n\n\n\n1.3.2 Figures\n\n\n\nFigure 1.1: Penguins\n\n\nWe can reference figures like Figure 1.1 with @fig-penguins.\n\n\n1.3.3 Tables\nWe can link to tables in our documents. For example, we can link to the greek table with @tbl-greek Table 1.1.\n\n\n1.3.4 Equations\nWe can link to equations in our documents. For example, we can link to the binomial distribution earlier with @eq-binomial Equation 1.1.\n\n\n\n\n\n\nExercise 5\n\n\n\n\nAdd a cross reference to your RMSE equation from earlier."
  },
  {
    "objectID": "01_advanced-quarto.html#citations",
    "href": "01_advanced-quarto.html#citations",
    "title": "1  Advanced Quarto",
    "section": "1.4 Citations",
    "text": "1.4 Citations\n\n1.4.1 Zotero\nZotero is a free and open-source software for organizing research and managing citations.\n\n\n\n\n\n\nDigital Object Identifier (DOI)\n\n\n\nDOIs are persistent identifiers that uniquely identify objects including many academic papers. For example, 10.1198/jcgs.2009.07098 identifies “A Layered Grammar of Graphics” by Hadley Wickham.\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\nInstall Zotero.\nFind the DOI for “Tidy Data” by Hadley Wickham.\nClick the magic wand in Zotero and paste the DOI.\n\n\n\n\n\n\n\n\n\n\n\nReview the new entry in Zotero.\n\n\n\n\n\n1.4.2 Zotero Integration\nZotero has a powerful integration with Quarto. In practice, it’s one click to add a DOI to Zotero and then one click to add a citation to Quarto.\nRStudio automatically adds My Library from Zotero. Simply switch to the Visual Editor (top left in RStudio), click “Insert”, and click “Citation”. This will open a prompt to insert a citation into the Quarto document.\nThe citation is automatically added with parentheses to go at the end of sentences. Delete the square brackets to convert the citation to an in-line citation.\nInserting the citation automatically adds the citation to the references section. Deleting the reference automatically deletes the citation from the references section.\nZotero Groups are useful for sharing citations and Zotero Group Libraries need to be added to RStudio. To set this up:\nTo set this up, in RStudio:\n\nGo to Tools and select “Global Options”\nSelect “RMarkdown” and then click “Citations”\nFor “Use Libraries” choose “Selected Libraries”\nSelect the group libraries to add\n\n\n\n\n\n\n\nExercise 7\n\n\n\n\nCite “Tidy Data” by Hadley Wickham in your Quarto document.\nClick “Render”"
  },
  {
    "objectID": "01_advanced-quarto.html#more-resources",
    "href": "01_advanced-quarto.html#more-resources",
    "title": "1  Advanced Quarto",
    "section": "1.5 More Resources",
    "text": "1.5 More Resources\n\nQuarto Guide\nIterating fact sheets and web pages with Quarto"
  },
  {
    "objectID": "01_advanced-quarto.html#footnotes",
    "href": "01_advanced-quarto.html#footnotes",
    "title": "1  Advanced Quarto",
    "section": "",
    "text": "Pandoc is free software that converts documents between markup formats. For example, Pandoc can convert files to and from markdown, LaTeX, jupyter notebook (ipynb), and Microsoft Word (.docx) formats, among many others. You can see a comprehensive list of files Pandoc can convert on their About Page.↩︎\nThis table was typed as Markdown code. But sometimes it is easier to use a code chunk to create and print a table. Pipe any data frame into knitr::kable() to create a table that will be formatted in the output of a rendered Quarto document.↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more.↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more.↩︎"
  },
  {
    "objectID": "04_web-scraping.html#sec-review4",
    "href": "04_web-scraping.html#sec-review4",
    "title": "2  Web Scraping",
    "section": "2.1 Review",
    "text": "2.1 Review\nWe explored pulling data from web APIs in DSPP1. With web APIs, stewards are often carefully thinking about how to share information. This will not be the case with web scraping.\nWe also explored extracting data from Excel workbooks in Section 02. We will build on some of the ideas in that section.\nRecall that if we have a list of elements, we can extract the \\(i^{th}\\) element with [[]]. For example, we can extract the third data frame from a list of data frames called data with data[[3]].\nRecall that we can use map() to iterate a function across each element of a vector. Consider the following example:\n\ntimes2 &lt;- function(x) x * 2\n\nx &lt;- 1:3\n\nmap(.x = x, .f = times2)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6"
  },
  {
    "objectID": "04_web-scraping.html#introduction-and-motivation",
    "href": "04_web-scraping.html#introduction-and-motivation",
    "title": "2  Web Scraping",
    "section": "2.2 Introduction and Motivation",
    "text": "2.2 Introduction and Motivation\nThe Internet is an immense source of information for research. Sometimes we can easily download data of interest in an ideal format with the click of a download button or a single API call.\nBut it probably won’t be long until we need data that require many download button clicks. Or worse, we may want data from web pages that don’t have a download button at all.\nConsider a few examples.\n\nThe Urban Institute’s Boosting Upward Mobility from Poverty project programmatically downloaded 51 .xslx workbooks when building the Upward Mobility Data Tables.\nWe worked with the text of executive orders going back to the Clinton Administration when learning text analysis in DSPP1. Unfortunately, the Federal Register doesn’t publish a massive file of executive orders. So we iterated through websites for each executive order, scraped the text, and cleaned the data.\nThe Urban Institute scraped course descriptions from Florida community colleges to understand opportunities for work-based learning.\nThe Billion Prices Project web scraped millions of prices each day from online retailers. The project used the data to construct real-time price indices that limited political interference and to research concepts like price stickiness.\n\nWe will explore two approaches for gathering information from the web.\n\nIteratively downloading files: Sometimes websites contain useful information across many files that need to be separately downloaded. We will use code to download these files. Ultimately, these files can be combined into one larger data set for research.\nScraping content from the body of websites: Sometimes useful information is stored as tables or lists in the body of websites. We will use code to scrape this information and then parse and clean the result.\n\nSometimes we download many PDF files using the first approach. A related method that we will not cover that is useful for gathering information from the web is extracting text data from PDFs."
  },
  {
    "objectID": "04_web-scraping.html#legal-and-ethical-considerations",
    "href": "04_web-scraping.html#legal-and-ethical-considerations",
    "title": "2  Web Scraping",
    "section": "2.3 Legal and Ethical Considerations",
    "text": "2.3 Legal and Ethical Considerations\nIt is important to consider the legal and ethical implications of any data collection. Collecting data from the web through methods like web scraping raises serious ethical and legal considerations.\n\n2.3.1 Legal1\nDifferent countries have different laws that affect web scraping. The United States has different laws and legal interpretations than countries in Europe, which are largely regulated by the European Union. In general, the United States has more relaxed policies than the European when it comes to gathering data from the web.\nR for Data Science (2e) contains a clear and approachable rundown of legal consideration for gathering information for the web. We adopt their three-part standard of “public, non-personal, and factual”, which relate to terms of service, personally identifiable information, and copyright.\nWe will focus solely on laws in the United States.\n\nTerms of Service\nThe legal environment for web scraping is in flux, but US Courts have created an environment that is legally supportive of gathering public information from the web.\nFirst, we need to understand how many websites bar web scraping. Second, we need to understand when we can ignore these rules.\n\n\n\n\n\n\nTerms of Service\n\n\n\nA terms of service is a list of rules posted by the provider of a website, web service, or software.\n\n\nTerms of Service for many websites bar web scraping.\nFor example, LinkedIn’s Terms of Service says users agree to not “Develop, support or use software, devices, scripts, robots or any other means or processes (including crawlers, browser plugins and add-ons or any other technology) to scrape the Services or otherwise copy profiles and other data from the Services;”\nThis sounds like the end of web scraping, but as Wickham, Çetinkaya-Rundel, and Grolemund (2023) note, Terms of Service end up being a “legal land grab” for companies. It isn’t clear how LinkedIn would legally enforce this. HiQ Labs v. LinkedIn from the United States Court of Appeals for the Ninth Circuit bars Computer Fraud and Abuse Act (CFAA) claims against web scraping public information.2\nWe follow a simple guideline: it is acceptable to scrape information when we don’t need to create an account.\n\n\n\n2.3.2 PII\n\n\n\n\n\n\nPersonal Identifiable Information\n\n\n\nPersonal Identifiable Information (PII) is any information that can be used to directly identify an individual.\n\n\nPublic information on the Internet often contains PII, which raises legal and ethical challenges. We will discuss the ethics of PII later.\nThe legal considerations are trans-Atlantic. The General Data Protection Regulation (GDPR) is a European Union regulation about information privacy. It contains strict rules about the collection and storage of PII. It applies to almost everyone collecting data inside the EU. The GDPR is also extraterritorial, which means its rules can apply outside of the EU under certain circumstances like when an American company gathers information about EU individuals.\nWe will avoid gathering PII, so we don’t need to consider PII.\n\nCopyright\n\n\n\n\n\n\nCopyright Law\n\n\n\n\nCopyright protection subsists, in accordance with this title, in original works of authorship fixed in any tangible medium of expression, now known or later developed, from which they can be perceived, reproduced, or otherwise communicated, either directly or with the aid of a machine or device. Works of authorship include the following categories:\n\n\nliterary works;\n\n\nmusical works, including any accompanying words;\n\n\ndramatic works, including any accompanying music;\n\n\npantomimes and choreographic works;\n\n\npictorial, graphic, and sculptural works;\n\n\nmotion pictures and other audiovisual works;\n\n\nsound recordings; and\n\n\narchitectural works.\n\n\nIn no case does copyright protection for an original work of authorship extend to any idea, procedure, process, system, method of operation, concept, principle, or discovery, regardless of the form in which it is described, explained, illustrated, or embodied in such work.\n\n17 U.S.C.\n\n\nOur final legal concern for gathering information from the Internet is copyright law. We have two main options for avoiding copyright limitations.\n\nWe can avoid copyright protections by not scraping authored content in the protected categories (i.e. literary works and sound recordings). Fortunately, factual data are not typically protected by copyright.\nWe can scrape information that is fair use. This is important if we want to use images, films, music, or extended text as data.\n\nWe will focus on data that are not copyrighted.\n\n\n\n2.3.3 Ethical\nWe now turn to ethical considerations and some of the best-practices for gathering information from the web. In general, we will aim to be polite, give credit, and respect individual information.\n\nBe polite\nIt is expensive and time-consuming to host data on the web. Hosts experience a small burden every time we access a website. This burden is small but can quickly grow with repeated queries. Just like with web APIs, we want to pace the burden of our access to be polite.\n\n\n\n\n\n\nRate Limiting\n\n\n\nRate limiting is the intentional slowing of web traffic for a user or users.\n\n\nWe will use Sys.sleep() in custom functions to slow our web scraping and ease the burden of our web scraping on web hosts.\n\n\n\n\n\n\nrobots.txt\n\n\n\nrobots.txt tells web crawlers and scrapers which URLs the crawler is allowed to access on a website.\n\n\nMany websites contain a robots.txt file. Consider examples from the Urban Institute and White House.\nWe can manually look at the robots.txt. For example, just visit https://www.urban.org/robots.txt or https://www.whitehouse.gov/robots.txt. We can also use library(polite), which will automatically look at the robots.txt.\n\n\nGive Credit\nAcademia and the research profession undervalue the collection and curation of data. Generally speaking, no one gets tenure for constructing even the most important data sets. It is important to give credit for data accessed from the web. Ideally, add the citation to Zotero and then easily add it to your manuscript in Quarto.\nBe sure to make it easy for others to cite data sets that you create. Include an example citation like IPUMS or create a DOI for your data.\nThe rise of generative AI models like GPT-3, Stable Diffusion, DALL-E 2 makes urgent considerations of giving credit. These models consume massive amounts of training data, and it isn’t clear where the training data come from or the legal and ethical implications of the training data.3\nConsider a few current events:\n\nSarah Silverman is suing OpenAI because she “never gave permission for OpenAI to ingest the digital version of her 2010 book to train its AI models, and it was likely stolen from a ‘shadow library’ of pirated works.”\nSomepalli et al. (2023) use state-of-the-art image retrieval models to find that generative AI models like the popular the popular Stable Diffusion model “blatantly copy from their training data.” This is a major problem if the training data are copyrighted. The first page of their paper (here) contains some dramatic examples.\nFinally, this Harvard Business Review article discusses the intellectual property problem facing generative AI.\n\n\n\nRespect Individual Information\nData science methods should adhere to the same ethical standards as any research method. The social sciences have ethical norms about protecting privacy (discussed later) and informed consent.\n\n\n\n\n\n\nDiscussion\n\n\n\nIs it appropriate to collect and share public PII?\nDo these norms apply to data that is already public on the Internet?\n\n\nLet’s consider an example. In 2016, researchers posted data about 70,000 OkCupid accounts. The data didn’t contain names but did contain usernames. The data also contained many sensitive variables including topics like sexual habits and politics.\nThe release drew strong reactions from some research ethicists including Michael Zimmer and Os Keyes.4\nFellegi (1972) defines data privacy as the ability “to determine what information about ourselves we will share with others”. Maybe OkCupid users made the decision to forego confidentiality when they published their accounts. Many institutional ethics committees do not require informed consent for public data.\nRavn, Barnwell, and Barbosa Neves (2020) do a good job developing a conceptual framework that bridges the gap between the view that all public data require informed consent and the view that no public data require informed consent.\nIt’s possible to conceive of a web scraping research project that is purely observational that adheres to the ethical standards of research and contains potentially disclosive information about individuals. Fortunately, researchers can typically use Institutional Review Boards and research ethicists to navigate these questions.\nAs a basic standard, we will avoid collecting PII and use anonymization techniques to limit the risk of re-identification.\nWe will also focus on applications where the host of information crudely shares the information. There are ample opportunities to create value by gathering information from government sources and converting it into more useful formats. For example, the government too often shares information in .xls and .xlsx files, clunky web interfaces, and PDFs."
  },
  {
    "objectID": "04_web-scraping.html#programatically-downloading-data",
    "href": "04_web-scraping.html#programatically-downloading-data",
    "title": "2  Web Scraping",
    "section": "2.4 Programatically Downloading Data",
    "text": "2.4 Programatically Downloading Data\nThe County Health Rankings & Roadmaps is a source of state and local information.\nSuppose we are interested in Injury Deaths at the state level. We can click through the interface and download a .xlsx file for each state.\n\n2.4.1 Downloading a Single File\n\nStart here.\nUsing the interface at the bottom of the page, we can navigate to the page for “Virginia.”\nNext, we can click “View State Data.”\nNext, we can click “Download Virginia data sets.”\n\nThat’s a lot of clicks to get here.\nIf we want to download “2023 Virginia Data”, we can typically right click on the link and select “Copy Link Address”. This should return one of the following two URLS:\nhttps://www.countyhealthrankings.org/sites/default/files/media/document/2023%20County%20Health%20Rankings%20Virginia%20Data%20-%20v2.xlsx\nhttps://www.countyhealthrankings.org/sites/default/files/media/document/2023 County Health Rankings Virginia Data - v2.xlsx\nSpaces are special characters in URLs and they are sometimes encoded as %20. Both URLs above work in the web browser, but only the URL with %20 will work with code.\nAs we’ve seen several times before, we could use read_csv() to directly download the data from the Internet if the file was a .csv.5 We need to download this file because it is an Excel file, which we can do with download.file() provided we include a destfile.\n\ndownload.file(\n  url = \"https://www.countyhealthrankings.org/sites/default/files/media/document/2023%20County%20Health%20Rankings%20Virginia%20Data%20-%20v2.xlsx\", \n  destfile = \"data/virginia-injury-deaths.xlsx\"\n)\n\n\n\n2.4.2 Downloading Multiple Files\nIf we click through and find the links for several states, we see that all of the download links follow a common pattern. For example, the URL for Vermont is\nhttps://www.countyhealthrankings.org/sites/default/files/media/document/2023 County Health Rankings Vermont Data - v2.xlsx\nThe URLs only differ by \"Virginia\" and \"Vermont\". If we can create a vector of URLs by changing state name, then it is simple to iterate downloading the data. We will only download data for two states, but we can imagine downloading data for many states or many counties. Here are three R tips:\n\npaste0() and str_glue() from library(stringr) are useful for creating URLs and destination files.\nwalk() from library(purrr) can iterate functions. It’s like map(), but we use it when we are interested in the side-effect of a function.6\nSometimes data are messy and we want to be polite. Custom functions can help with rate limiting and cleaning data.\n\n\ndownload_chr &lt;- function(url, destfile) {\n\n  download.file(url = url, destfile = destfile)\n\n  Sys.sleep(0.5)\n\n}\n\nstates &lt;- c(\"Virginia\", \"Vermont\")\n\nurls &lt;- paste0(\n  \"https://www.countyhealthrankings.org/sites/default/files/\",\n  \"media/document/2023%20County%20Health%20Rankings%20\",\n  states,\n  \"%20Data%20-%20v2.xlsx\"\n)\n\noutput_files &lt;- paste0(\"data/\", states, \".xlsx\")\n\nwalk2(.x = urls, .y = output_files, .f = download_chr)\n\n\n\n\n\n\n\nExercise 1\n\n\n\nSOI Tax Stats - Historic Table 2 provides individual income and tax data, by state and size of adjusted gross income. The website contains a bulleted list of URLs and each URL downloads a .xlsx file.\n\nUse download.file() to download the file for Alabama.\nExplore the URLs using “Copy Link Address”.\nIterate pulling the data for Alabama, Alaska, and Arizona."
  },
  {
    "objectID": "04_web-scraping.html#web-scraping-with-rvest",
    "href": "04_web-scraping.html#web-scraping-with-rvest",
    "title": "2  Web Scraping",
    "section": "2.5 Web Scraping with rvest",
    "text": "2.5 Web Scraping with rvest\nWe now pivot to situations where useful information is stored in the body of web pages.\n\n2.5.1 Web Design\nIt’s simple to build a website with Quarto because it abstracts away most of web development. For example, Markdown is just a shortcut to write HTML. Web scraping requires us to learn more about web development than when we use Quarto.\nThe user interface of websites can be built with just HTML, but most websites contain HTML, CSS, and JavaScript. The development the interface of websites with HTML, CSS, and JavaScript is called front-end web development.\n\n\n\n\n\n\nHyper Text Markup Language\n\n\n\nHyper Text Markup Language (HTML) is the standard language for creating web content. HTML is a markup language, which means it has code for creating structure and and formatting.\n\n\nThe following HTML generates a bulleted list of names.\n&lt;ul&gt;\n  &lt;li&gt;Alex&lt;/li&gt;\n  &lt;li&gt;Aaron&lt;/li&gt;\n  &lt;li&gt;Alena&lt;/li&gt;\n&lt;/ul&gt;\n\n\n\n\n\n\nCascading Style Sheets\n\n\n\nCascading Style Sheets (CSS) describes hot HTML elements should be styled when they are displayed.\n\n\nFor example, the following CSS adds extra space after sections with ## in our class notes.\n.level2 {\n  margin-bottom: 80px;\n}\n\n\n\n\n\n\nJavaScript\n\n\n\nJavaScript is a programming language that runs in web browsers and is used to build interactivity in web interfaces.\n\n\nQuarto comes with default CSS and JavaScript. library(leaflet) and Shiny are popular tools for building JavaScript applications with R. We will focus on web scraping using HTML and CSS.\nFirst, we will cover a few important HTML concepts. W3Schools offers a thorough introduction. Consider the following simple website built from HTML:\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Hello World!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1 class='important'&gt;Bigger Title!&lt;/h1&gt;\n&lt;h2 class='important'&gt;Big Title!&lt;/h1&gt;\n&lt;p&gt;My first paragraph.&lt;/p&gt;\n&lt;p id='special-paragraph'&gt;My first paragraph.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nAn HTML element is a start tag, some content, and an end tag. Every start tag has a matching end tag. For example, &lt;body and &lt;/body&gt;. &lt;html&gt;, &lt;head&gt;, and &lt;body&gt; are required elements for all web pages. Other HTML elements include &lt;h1&gt;, &lt;h2&gt;, and &lt;p&gt;.\nHTML attributes are name/value pairs that provide additional information about elements. HTML attributes are optional and are like function arguments for HTML elements.\nTwo HTML attributes, classes and ids, are particularly important for web scraping.\n\nHTML classes are HTML attributes that label multiple HTML elements. These classes are useful for styling HTML elements using CSS. Multiple elements can have the same class.\nHTML ids are HTML attributes that label one HTML element. Ids are useful for styling singular HTML elements using CSS. Each ID can be used only one time in an HTML document.\n\nWe can view HTML for any website by right clicking in our web browser and selecting “View Page Source.”7\n\n\n\n\n\n\nExercise 2\n\n\n\n\nInspect the HTML behind this list of “Hello World examples”.\nInspect the HTML behind the Wikipedia page for Jerzy Neyman.\n\n\n\nSecond, we will explore CSS. CSS relies on HTML elements, HTML classes, and HTML ids to style HTML content. CSS selectors can directly reference HTML elements. For example, the following selectors change the style of paragraphs and titles.\np {\n  color: red;\n}\n\nh1 {\n  font-family: wingdings;\n}\nCSS selectors can reference HTML classes. For example, the following selector changes the style of HTML elements with class='important'.\n.important {\n  font-family: wingdings;\n}\nCSS selectors can reference also reference HTML IDs. For example, the following selector changes the style of the one element with id='special-paragraph'\n#special-paragraph {\n  color: pink;\n}\nWe can explore CSS by right clicking and selecting Inspect. Most modern websites have a lot of HTML and a lot of CSS. We can find the CSS for specific elements in a website with the button at the top left of the new window that just appeared.\n\n\n\nInspecting CSS\n\n\n\n\n2.5.2 Tables\nlibrary(rvest) is the main tool for scraping static websites with R. We’ll start with examples that contain information in HTML tables.8\nHTML tables store information in tables in websites using the &lt;table&gt;, &lt;tr&gt;, &lt;th&gt;, and &lt;td&gt;. If the data of interest are stored in tables, then it can be trivial to scrape the information.\nConsider the Wikipedia page for the 2012 Presidential Election. We can scrape all 46 tables from the page with two lines of code. We use the WayBack Machine to ensure the content is stable.\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\ntables &lt;- read_html(\"https://web.archive.org/web/20230814004444/https://en.wikipedia.org/wiki/2012_United_States_presidential_election\") |&gt;\n  html_table()\n\nSuppose we are interested in the table about presidential debates. We can extract that element from the list of tables.\n\ntables[[18]]\n\n# A tibble: 12 × 9\n   `Presidential candidate`     Party `Home state` `Popular vote` `Popular vote`\n   &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;         \n 1 \"Presidential candidate\"     Party Home state   Count          Percentage    \n 2 \"Barack Hussein Obama II\"    Demo… Illinois     65,915,795     51.06%        \n 3 \"Willard Mitt Romney\"        Repu… Massachuset… 60,933,504     47.20%        \n 4 \"Gary Earl Johnson\"          Libe… New Mexico   1,275,971      0.99%         \n 5 \"Jill Ellen Stein\"           Green Massachuset… 469,627        0.36%         \n 6 \"Virgil Hamlin Goode Jr.\"    Cons… Virginia     122,389        0.11%         \n 7 \"Roseanne Cherrie Barr\"      Peac… Utah         67,326         0.05%         \n 8 \"Ross Carl \\\"Rocky\\\" Anders… Just… Utah         43,018         0.03%         \n 9 \"Thomas Conrad Hoefling\"     Amer… Nebraska     40,628         0.03%         \n10 \"Other\"                      Other Other        217,152        0.17%         \n11 \"Total\"                      Total Total        129,085,410    100%          \n12 \"Needed to win\"              Need… Needed to w… Needed to win  Needed to win \n# ℹ 4 more variables: Electoralvote &lt;chr&gt;, `Running mate` &lt;chr&gt;,\n#   `Running mate` &lt;chr&gt;, `Running mate` &lt;chr&gt;\n\n\nOf course, we want to be polite. library(polite) makes this very simple. “The three pillars of a polite session are seeking permission, taking slowly and never asking twice.”\nWe’ll use bow() to start a session and declare our user agent, and scrape() instead of read_html().9\n\nlibrary(polite)\n\nsession &lt;- bow(\n  url = \"https://web.archive.org/web/20230814004444/https://en.wikipedia.org/wiki/2012_United_States_presidential_election\",\n  user_agent = \"Georgetown students learning scraping -- arw109@georgetown.edu\"\n)\n\nsession\n\n&lt;polite session&gt; https://web.archive.org/web/20230814004444/https://en.wikipedia.org/wiki/2012_United_States_presidential_election\n    User-agent: Georgetown students learning scraping -- arw109@georgetown.edu\n    robots.txt: 1 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n\nelection_page &lt;- session |&gt;\n  scrape() \n  \ntables &lt;- election_page |&gt;\n  html_table()\n\ntables[[18]]\n\n# A tibble: 12 × 9\n   `Presidential candidate`     Party `Home state` `Popular vote` `Popular vote`\n   &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;         \n 1 \"Presidential candidate\"     Party Home state   Count          Percentage    \n 2 \"Barack Hussein Obama II\"    Demo… Illinois     65,915,795     51.06%        \n 3 \"Willard Mitt Romney\"        Repu… Massachuset… 60,933,504     47.20%        \n 4 \"Gary Earl Johnson\"          Libe… New Mexico   1,275,971      0.99%         \n 5 \"Jill Ellen Stein\"           Green Massachuset… 469,627        0.36%         \n 6 \"Virgil Hamlin Goode Jr.\"    Cons… Virginia     122,389        0.11%         \n 7 \"Roseanne Cherrie Barr\"      Peac… Utah         67,326         0.05%         \n 8 \"Ross Carl \\\"Rocky\\\" Anders… Just… Utah         43,018         0.03%         \n 9 \"Thomas Conrad Hoefling\"     Amer… Nebraska     40,628         0.03%         \n10 \"Other\"                      Other Other        217,152        0.17%         \n11 \"Total\"                      Total Total        129,085,410    100%          \n12 \"Needed to win\"              Need… Needed to w… Needed to win  Needed to win \n# ℹ 4 more variables: Electoralvote &lt;chr&gt;, `Running mate` &lt;chr&gt;,\n#   `Running mate` &lt;chr&gt;, `Running mate` &lt;chr&gt;\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nInstall and load library(rvest).\nInstall and load library(polite).\nScrape the Presidential debates table from the Wikipedia article for the 2008 presidential election.\n\n\n\n\n\n2.5.3 Other HTML Content\nSuppose we want to scrape every URL in the body of the 2012 Presidential Election webpage. html_table() no longer works.\nWe could manually poke through the source code to find the appropriate CSS selectors. Fortunately, SelectorGadget often eliminates this tedious work by telling you the name of the html elements that you click on.\n\nClick the SelectorGadget gadget browser extension. You may need to click the puzzle piece to the right of the address bar and then click the SelectorGadget browser extension.\nSelect an element you want to scrape. The elements associated with the CSS selector provided at the bottom will be in green and yellow.\n\nIf SelectorGadget selects too few elements, select additional elements. If SelectorGadget selects too many elements, click those elements. They should turn red.\n\nEach click should refine the CSS selector.\nAfter a few clicks, it’s clear we want p a. This should select any element a in p. a is the element for URLs.\nWe’ll need a few more functions to finish this example.\n\nhtml_elements() filters the output of read_html()/scrape() based on the provided CSS selector. html_elements() can return multiple elements while html_element() always returns one element.\nhtml_text2() retrieves text from HTML elements.\nhtml_attrs() retrieves HTML attributes from HTML elements. html_attrs() can return multiple attributes while html_attr() always returns one attribute.\n\n\ntibble(\n  text = election_page |&gt;\n    html_elements(css = \"p a\") |&gt;\n    html_text2(),\n  url = election_page |&gt;\n    html_elements(css = \"p a\") |&gt;\n    html_attr(name = \"href\")\n)\n\n# A tibble: 355 × 2\n   text                  url                                                    \n   &lt;chr&gt;                 &lt;chr&gt;                                                  \n 1 Barack Obama          /web/20230814004444/https://en.wikipedia.org/wiki/Bara…\n 2 Democratic            /web/20230814004444/https://en.wikipedia.org/wiki/Demo…\n 3 Barack Obama          /web/20230814004444/https://en.wikipedia.org/wiki/Bara…\n 4 Democratic            /web/20230814004444/https://en.wikipedia.org/wiki/Demo…\n 5 presidential election /web/20230814004444/https://en.wikipedia.org/wiki/Unit…\n 6 Democratic            /web/20230814004444/https://en.wikipedia.org/wiki/Demo…\n 7 President             /web/20230814004444/https://en.wikipedia.org/wiki/Pres…\n 8 Barack Obama          /web/20230814004444/https://en.wikipedia.org/wiki/Bara…\n 9 running mate          /web/20230814004444/https://en.wikipedia.org/wiki/Runn…\n10 Vice President        /web/20230814004444/https://en.wikipedia.org/wiki/Vice…\n# ℹ 345 more rows\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nSuppose we are interested in examples of early websites. Wikipedia has a list of URLs from before 1995.\n\nAdd the SelectorGadget web extension to your browser.\nUse library(polite) and library(rvest) to scrape() the following URL.\n\nhttps://web.archive.org/web/20230702163608/https://en.wikipedia.org/wiki/List_of_websites_founded_before_1995\n\nWe are interested in scraping the names of early websites and their URLs. Use SelectorGadget to determine the CSS selectors associated with these HTML elements.\nCreate a tibble with a variable called name and a variable called url.\nRemove duplicate rows with distinct() or filter().\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nFind your own HTML table of interest to scrape.\nUse library(rvest) and library(polite) to scrape the table.\n\n\n\n\n\n\n\nFellegi, I. P. 1972. “On the Question of Statistical Confidentiality.” Journal of the American Statistical Association 67 (337): 7–18. https://www.jstor.org/stable/2284695?seq=1#metadata_info_tab_contents.\n\n\nRavn, Signe, Ashley Barnwell, and Barbara Barbosa Neves. 2020. “What Is “Publicly Available Data”? Exploring Blurred PublicPrivate Boundaries and Ethical Practices Through a Case Study on Instagram.” Journal of Empirical Research on Human Research Ethics 15 (1-2): 40–45. https://doi.org/10.1177/1556264619850736.\n\n\nSomepalli, Gowthami, Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023. “Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 6048–58. https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualie, and Model Data. 2nd edition. Sebastopol, CA: O’Reilly."
  },
  {
    "objectID": "04_web-scraping.html#footnotes",
    "href": "04_web-scraping.html#footnotes",
    "title": "2  Web Scraping",
    "section": "",
    "text": "We are not lawyers. This is not official legal advise. If in-doubt, please contact a legal professional.↩︎\nThis blog and this blog support this statement. Again, we are not lawyers and the HiQ Labs v. LinkedIn decision is complicated because of its long history and conclusion in settlement.↩︎\nThe scale of crawling is so great that there is concern about models converging once all models use the same massive training data. Common Crawl is one example. This isn’t a major issue for generating images but model homogeneity is a big concern in finance.↩︎\nWho deserves privacy is underdiscussed and inconsistent. Every year, newspapers across the country FOIA information about government employees and publish their full names, job titles, and salaries.↩︎\nConsequently, code that may once have worked can break, but using read_csv(&lt;file_path&gt;) to access data once it has been downloaded will work consistently.↩︎\nThe only difference between map() and walk() is their outputs. map() returns the results of a function in a list. walk() returns nothing when used without assignment, and we never use walk() with assignment. walk() is useful when we don’t care about the output of functions and are only interested in their “side-effects”. Common functions to use with walk() are ggsave() and write_csv(). For more information on walk(), see Advanced R.↩︎\nWe recommend using Google Chrome, which has excellent web development tools.↩︎\nIf a website is static, that means that the website is not interactive and will remain the same unless the administrator actively makes changes. Hello World examples is an example of a static website.↩︎\nThe polite documentation describes the bow() function as being used to “introduce the client to the host and ask for permission to scrape (by inquiring against the host’s robots.txt file).”↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Fellegi, I. P. 1972. “On the Question of Statistical\nConfidentiality.” Journal of the American Statistical\nAssociation 67 (337): 7–18. https://www.jstor.org/stable/2284695?seq=1#metadata_info_tab_contents.\n\n\nRavn, Signe, Ashley Barnwell, and Barbara Barbosa Neves. 2020.\n“What Is “Publicly Available Data”?\nExploring Blurred PublicPrivate Boundaries and Ethical\nPractices Through a Case Study on Instagram.” Journal of\nEmpirical Research on Human Research Ethics 15 (1-2): 40–45. https://doi.org/10.1177/1556264619850736.\n\n\nSomepalli, Gowthami, Singla, Micah Goldblum, Jonas Geiping, and Tom\nGoldstein. 2023. “Diffusion Art or Digital Forgery? Investigating\nData Replication in Diffusion Models.” Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 6048–58. https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualie, and Model\nData. 2nd edition. Sebastopol, CA: O’Reilly."
  }
]