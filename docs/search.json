[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Public Policy",
    "section": "",
    "text": "Welcome\nThis book contains the notes for Intro to Data Science and Advanced Data Science in the McCourt School of Public Policy at Georgetown University. Together, the notes are the text Data Science for Public Policy.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements-more-to-come",
    "href": "index.html#acknowledgements-more-to-come",
    "title": "Data Science for Public Policy",
    "section": "Acknowledgements (More to come!)",
    "text": "Acknowledgements (More to come!)\nThis book has benefited from many great teachers, collaborators, and students. First, I want to thank Gabe Morrison for excellent reviews and proofreading. Second, I want to thank Alex Engler and Alena Stern for collaborating on Intro to Data Science for Public Policy during six semesters and eight classes.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01_intro-r.html",
    "href": "01_intro-r.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 Six Principles for Data Analysis",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#six-principles-for-data-analysis",
    "href": "01_intro-r.html#six-principles-for-data-analysis",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1.1 Accuracy\nDeliberate steps should be taken to minimize the chance of making an error and maximize the chance of catching errors when errors inevitably occur. 1\n\n\n1.1.2 Computational reproducibility\nComputational reproducibility should be embraced to improve accuracy, promote transparency, and prove the quality of analytic work.\nReplication: the recreation of findings across repeated studies, is a cornerstone of science.\nReproducibility: the ability to access data, source code, tools, and documentation and recreate all calculations, visualizations, and artifacts of an analysis.\nComputational reproducibility should be the minimum standard for computational social sciences and statistical programming.\n\n\n1.1.3 Human interpretability\nCode should be written so humans can easily understand what’s happening—even if it occasionally sacrifices machine performance.\n\n\n1.1.4 Portability\nAnalyses should be designed so strangers can understand each and every step without additional instruction or inquiry from the original analyst.\n\n\n1.1.5 Accessibility\nResearch and data are non-rivalrous and can be non-excludable. They are public goods that should be widely and easily shared. Decisions about tools, methods, data, and language during the research process should be made in ways that promote the ability of anyone and everyone to access an analysis.\n\n\n1.1.6 Efficiency\nAnalysts should seek to make all parts of the research process more efficient with clear communication, by adopting best practices, and by managing computation.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#r",
    "href": "01_intro-r.html#r",
    "title": "1  Introduction to R",
    "section": "1.2 R",
    "text": "1.2 R\nR is a free, open-source software for statistical computing. It is a fully-functioning programming language and it is known for intuitive, crisp graphics and an extensive, growing library of statistical and analytic methods. Above all, R boasts an enthusiastic community of developers, instructors, and users. The copyright and documentation for R is held by a not-for-profit organization called the R Foundation.\nR comes from the S programming language and S-PLUS system. In addition to offering better graphics and more extensibility than proprietary languages, R has a pedagogical advantage:\n\nThe ambiguity [of the S language] is real and goes to a key objective: we wanted users to be able to begin in an interactive environment, where they did not consciously think of themselves as programming. Then as their needs became clearer and their sophistication increased, they should be able to slide gradually into programming, when the language and system aspects would become more important.\n\nSource: Peng (2018)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#rstudio",
    "href": "01_intro-r.html#rstudio",
    "title": "1  Introduction to R",
    "section": "1.3 RStudio",
    "text": "1.3 RStudio\nRStudio is a free, open-source integrated development environment (IDE) that runs on top of R. In practice, R users almost exclusively open RStudio and rarely directly open R. When we say IDE, we mean a piece of software where you can write, or develop, code in an efficient way.\nRStudio is developed by a for-profit company called Posit. Posit used to be called RStudio. Posit employs some of the R community’s most prolific, open-source developers and creates many open-source tools and resources. 2\nWhile R code can be written in any text editor, the RStudio IDE is a powerful tool with a console, syntax-highlighting, and debugging tools. The RStudio IDE cheatsheet outlines some of the power of RStudio.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#rstudio-console",
    "href": "01_intro-r.html#rstudio-console",
    "title": "1  Introduction to R",
    "section": "1.4 RStudio Console",
    "text": "1.4 RStudio Console\n\nThe RStudio Console contains the R command line and R output from previously submitted R code.\nCode can be submitted by typing to the right of the last blue &gt; and the hitting enter.\n\n\n\n\n\n\nExercise 1\n\n\n\nR has all of the functionality of a basic calculator. Let’s run some simple calculations with addition (+), subtraction (-), multiplication (*), division (/), exponentiation (^), and grouping (()).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#r-scripts",
    "href": "01_intro-r.html#r-scripts",
    "title": "1  Introduction to R",
    "section": "1.5 .R Scripts",
    "text": "1.5 .R Scripts\n\nBy default, there isn’t a record of code directly typed and submitted into the RStudio Console. So, most R programmers use .R scripts to develop R code before submitting code to the console.\n.R scripts are simple text files with R code. They are similar to .py files in Python, .sas files in SAS, and .do files in Stata.\n\n\n\n\n\n\nExercise 2\n\n\n\nClick the new script button in the top left corner of RStudio to create a new script.\n\nAdd some R code to your new script. Place your cursor on a line of R code and hit Command-Enter on Macs or Control-Enter on Windows. Alternatively, highlight the code (it can be multiple lines) and click the run button.\n\nIn both cases, the code from your .R script should move to the R Console and evaluate.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#comments",
    "href": "01_intro-r.html#comments",
    "title": "1  Introduction to R",
    "section": "1.6 Comments",
    "text": "1.6 Comments\nR will interpret all text in a .R script as R code unless the code follows #, the comment symbol. Comments are essential to writing clear code in any programming language.\n\n# Demonstrate the value of a comment and make a simple calculation\n2 + 2\n\nIt should be obvious what a line of clear R code accomplishes. It isn’t always obvious why a clear line of R code is included in a script. Comments should focus on the why of R code and not the what of R code.\nThe following comment isn’t useful because it just restates the R code, which is clear:\n\n# divide every value by 1.11\ncost / 1.11\n\nThe following comment is useful because it adds context to the R code:\n\n# convert costs from dollars to Euros using the 2020-01-13 exchange rate\ncost / 1.11\n\nThe following is useful because it avoids magic numbers.\n\n# dollars to Euros 2020-01-13 exchange rate\nexchange_rate &lt;- 1.11\n\n# convert costs from dollars to Euros\ncost / exchange_rate\n\n\n\n\n\n\n\nExercise 3\n\n\n\nAdd comments to your .R that clarify the why. Since we only know a few operations, the comments may need to focus on your why you picked your favorite numbers.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#style",
    "href": "01_intro-r.html#style",
    "title": "1  Introduction to R",
    "section": "1.7 Style",
    "text": "1.7 Style\nGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread. ~ Wickham (n.d.)\nHuman interpretability is one of the six principles because clear code can save time and reduce the chance of making errors. After time, eyes can be trained to quickly spot incorrect code if a consistent R style is adopted.\nFirst, note that R is case-sensitive. Capitalization is rare and deviations from the capitalization will throw errors. For example, mean() is a function but Mean() is not a function.\nThe tidyverse style guide is a comprehensive style guide that, in general, reflects the style of the plurality of R programmers. For now, just focus on consistency.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#data-structures",
    "href": "01_intro-r.html#data-structures",
    "title": "1  Introduction to R",
    "section": "1.8 Data Structures",
    "text": "1.8 Data Structures\nData analysis is not possible without data structures for data. R has several important data structures that shape how information is stored and processed.\n\n1.8.1 Vectors\nVectors are one-dimensional arrays that contain one and only one type of data. Atomic vectors in R are homogeneous. There are six types of atomic vectors:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex (uncommon)\nraw (uncommon)\n\nFor now, the simplest way to create a vector is with c(), the combine function.\n\n# a logical vector\nc(TRUE, FALSE, FALSE)\n\n[1]  TRUE FALSE FALSE\n\n# an integer vector\nc(1, 2, 3)\n\n[1] 1 2 3\n\n# a double vector\nc(1.1, 2.2, 3.3)\n\n[1] 1.1 2.2 3.3\n\n# a character vector\nc(\"District of Columbia\", \"Virginia\", \"Maryland\")\n\n[1] \"District of Columbia\" \"Virginia\"             \"Maryland\"            \n\n\nThe class() function can be used to identify the type, or class, of an object. For example:\n\nclass(c(TRUE, FALSE, FALSE))\n\n[1] \"logical\"\n\nclass(c(1, 2, 3))\n\n[1] \"numeric\"\n\nclass(c(\"District of Columbia\", \"Virginia\", \"Maryland\"))\n\n[1] \"character\"\n\n\nIf you create a vector with mixed data types, R will coerce all of the values to a single type:\n\nc(TRUE, 1, \"District of Columbia\")\n\n[1] \"TRUE\"                 \"1\"                    \"District of Columbia\"\n\nclass(c(TRUE, 1, \"District of Columbia\"))\n\n[1] \"character\"\n\n\nLists are one- or multi-dimensional arrays that are made up of other lists. Lists are heterogeneous - they can contain many lists of different types and dimensions. A vector is a list but a list is not necessarily a vector.\nNULL is the null object in R. It means a value does not exist.\nNA is a missing value of length 1 in R. NAs are powerful representations in R with special properties. NA is a contagious value in R that will override all calculations (Wickham and Grolemund 2017, sec. 20.2).\n\n1 + 2 + 3 + NA\n\n[1] NA\n\n\nThis forces programmers to be deliberate about missing values. This is a feature, not a bug!\nR contains special functions and function arguments for handling NAs. For example, we can wrap a vector with missing values in is.na() to create a vector of Booleans where TRUE represents an element that is an NA and FALSE represents an element that is not an NA.\n\nis.na(c(1, 2, NA))\n\n[1] FALSE FALSE  TRUE\n\n\nNote: NA and NULL have different meanings! NULL means no value exists. NA means a value could exist but it is unknown.\n\n\n1.8.2 Matrices\nMatrices are multi-dimensional arrays where every element is of the same type. Most data in data science contains at least numeric information and character information. Accordingly, we will not use matrices much in this course.\n\n\n1.8.3 Data frames\nInstead, data frames, and their powerful cousins tibbles, are the backbone of data science and this course. Data frames are two-dimensional arrays where each column is a list (usually a vector). Most times, each column will be of one type while a given row will contain many different types. We usually refer to columns as variables and rows as observations.\nHere are the first six rows of a data frame with information about mammal sleep habits:\n\nhead(ggplot2::msleep)\n\n# A tibble: 6 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\n\n\n1.8.4 tibbles\ntibbles are special data frames that have a few extra features:\n\nOnly the first ten rows of tibbles print by default\nExtra meta data are printed with tibbles\nThey have some convenient protections against partial subsetting (Wickham, Çetinkaya-Rundel, and Grolemund 2023, sec. 3.1.2)\nThey are easier to create from scratch in a .R script\n\n\n\n# A tibble: 3 × 4\n  a         b     c d                   \n  &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;               \n1 TRUE      1   1.1 District of Columbia\n2 FALSE     2   2.2 Virginia            \n3 FALSE     3   3.3 Maryland            \n\n\nFrom this moment forward, I will use data frame to mean tibble.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#assignment",
    "href": "01_intro-r.html#assignment",
    "title": "1  Introduction to R",
    "section": "1.9 Assignment",
    "text": "1.9 Assignment\nR can operate on many different vectors and data frames in the same R session. This creates much flexibility. It also means most unique objects in an R session need unique names.\n&lt;- is the assignment operator. An object created on the right side of an assignment operator is assigned to a name on the left side of an assignment operator. Assignment operators are important for saving the consequences of operations and functions. Without assignment, the result of a calculation is not saved for use in a future calculation. Operations without assignment operators will typically be printed to the console but not saved for future use.\n\n# this important calculation is saved to the R environment\nimportant_calculation &lt;- 2 + 2\n\n# this important calculation is NOT saved to the R environment\n2 + 2\n\n[1] 4\n\n\nStyle note: Objects should be given names that are “concise and meaningful” (Wickham, n.d., sec. 2). Generally the names should be nouns and only use lowercase letters, numbers, and underscores _ (this is referred to as snake case).\n\n\n\n\n\n\nExercise 4\n\n\n\nWrite three arithmetic operations in R and assign them to unique names. Then perform arithmetic operations using the named results. For example:\n\na &lt;- 5 + 5 + 5\nb &lt;- 6 - 6 - 6\n\na + b\n\n[1] 9",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#functions",
    "href": "01_intro-r.html#functions",
    "title": "1  Introduction to R",
    "section": "1.10 Functions",
    "text": "1.10 Functions\n+, -, *, and / are great, but data science requires a lot more than just basic arithmetic.\nR contains many more functions that can perform mathematical operations, control your computer operating system, process text data, and more. In fact, R is built around functions.\nBecause R was developed by statisticians, R’s functions have a lot in common with mathematical functions.\n\nFunctions have inputs and outputs\nEach input has one and only one output (unless is involves a random process)\n\nFunctions are recognizable because they end in (). For example, the following calculates the mean of a numeric vector two ways:\n\nmean(x = c(1, 2, 3))\n\n[1] 2\n\nnumeric_vector &lt;- c(1, 2, 3)\nmean(x = numeric_vector)\n\n[1] 2\n\n\nModern R with the tidyverse (Rodrigues 2022) has a functional programming chapter that contains more information about functional programming in R.\n\n1.10.1 ?\nDocumentation for functions can be easily accessed by prefacing the function name with ? and dropping the ().\n\n?mean\n\nThe documentation typically includes a description, a list of the arguments, references, a list of related functions, and examples. The examples are incredibly useful.\n\n\n1.10.2 Arguments\nR functions typically contain many arguments. For example, mean() has x, trim, and na.rm. Many arguments have default values and don’t need to be included in function calls. Default values can be seen in the documentation. trim = 0 and na.rm = FALSE are the defaults for mean().\nArguments can be passed to functions implicitly by position or explicitly by name.\n\nnumeric_vector &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# by position (correctly)\nmean(numeric_vector, 0.2)\n\n[1] 5.5\n\n\n\n# by position (incorrectly)\nmean(0.2, numeric_vector)\n\n\n# by name\nmean(x = numeric_vector, trim = 0.2)\n\n[1] 5.5\n\n\nFunction calls can include arguments by position and by name. The first argument in most functions is data or x. It is custom to usually include the first argument by position and then all subsequent arguments by name.\n\nmean(numeric_vector, trim = 0.2)\n\n[1] 5.5",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#r-packages",
    "href": "01_intro-r.html#r-packages",
    "title": "1  Introduction to R",
    "section": "1.11 R Packages",
    "text": "1.11 R Packages\n\n1.11.1 Base R\nOpening RStudio automatically loads “base R”, a fundamental collection of code and functions that handles simple operations like math and system management.\nFor years, R was only base R. New paradigms in R have developed over the last fifteen years that are more intuitive and more flexible than base R. Next week, we’ll discuss the “tidyverse”, the most popular paradigm for R programming.\nAll R programming will involve some base R, but much base R has been replaced with new tools that are more concise. Just know that at some point you may end up on a Stack Overflow page that looks like alphabet soup because it’s in a paradigm that you have not learned. Chapter 27 of R for Data Science (2e) (sometimes abbreviated R4DS) provides a great introduction to base R (Wickham, Çetinkaya-Rundel, and Grolemund 2023).\nOne other popular R paradigm is data.table. We will not discuss data.table in this class.\n\n\n1.11.2 Extensibility\nR is an extensible programming language. It was designed to allow for new capabilities and functionality.\nR is also open source. All of it’s source code is publicly available.\nThese two features have allowed R users to contribute millions of lines of code that can be used by other users without condition or compensation. The main mode of contribution are R packages. Packages are collections of functions and data that expand the power and usefulness of R.\nThe predecessor of R, the S programming language, was designed to call FORTRAN subroutines. Accordingly, many R packages used to call compiled FORTRAN code. Now, many R packaged call compiled C++ code. This gives users the intuition of R syntax with better performance. Here’s a brief history of R and S.\n\n\n1.11.3 CRAN\nMost R packages are stored on the Comprehensive R Archive Network (CRAN). Packages must pass a modest number of tests for stability and design to be added to CRAN.\n\n\n1.11.4 install.packages()\nPackages can be directly installed from CRAN using install.packages(). Simply include the name of the desired package in quotes as the only argument to the function.\nInstallation need only happen once per computer per package version. It is customary to never include install.packages() in a .R script.\n\n\n\n\n\n\nExercise 5\n\n\n\nFor practice, let’s install the palmerpenguins package. This package allows us to see some penguins data (our favorite, non-policy dataset!).\ninstall.packages(\"palmerpenguins\")\n\n\n\n\n1.11.5 library()\nAfter installation, packages need to be loaded once per R session using the library() function. While install.packages() expects a quoted package name, it is best practice to use unquoted names in library().\nIt is a good idea to include library() statements at the top of scripts for each package used in the script. This way it is obvious at the top of the script which packages are necessary.\n\n\n\n\n\n\nExercise 6\n\n\n\nFor practice let’s load the palmerpenguins package.\n\nlibrary(palmerpenguins)\n\nThe package contains a dataset called penguins. We can view that data by simply typing penguins in the console or in a .R script.\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nBecause this is a bit messy, we can get a summary of the data with the summary() function.\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\n\n\n\n1.11.6 ::\nSometimes two packages have functions with the same name. :: can be used to directly access an exported R object from a package’s namespace.\ndplyr::select()\nMASS::select()",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#organizing-an-analysis",
    "href": "01_intro-r.html#organizing-an-analysis",
    "title": "1  Introduction to R",
    "section": "1.12 Organizing an Analysis",
    "text": "1.12 Organizing an Analysis\n\n1.12.1 R Projects\nR Projects, proper noun, are the best way to organize an analysis. They have several advantages:\n\nThey make it possible to concurrently run multiple RStudio sessions.\nThey allow for project-specific RStudio settings.\nThey integrate well with Git version control.\nThey are the “node” of relative file paths. (more on this in a second) This makes code highly portable.\n\n\n\n\n\n\n\nExercise 7\n\n\n\nBefore setting up an R Project, go to Tools &gt; Global Options and uncheck “Restore most recently opened project at startup”.\n\n\nEvery new analysis in R should start with an R Project. First, create a directory that holds all data, scripts, and files for the analysis. You can do this right in RStudio by clicking the “New Folder” button at the top of the “Files” tab located in the top or bottom right of RStudio. Storing files and data in a sub-directories is encouraged. For example, data can be stored in a folder called data/.\nNext, click “New Project…” in the top right corner.\n\nWhen prompted, turn your recently created “Existing Directory” into a project.\n\nUpon completion, the name of the R Project should now be displayed in the top right corner of RStudio where it previously displayed “Project: (None)”. Once opened, .RProj files do not need to be saved. Double-clicking .Rproj files in the directory is now the best way to open RStudio. This will allow for the concurrent use of multiple R sessions and ensure the portability of file paths. Once an RStudio project is open, scripts can be opened by double-clicking individual files in the computer directory or clicking files in the “Files” tab.\n\n\n\n\n\n\nExercise 8\n\n\n\nLet’s walk through this process and create an R project for this class.\n\n\n\n\n1.12.2 Filepaths\nWindows file paths are usually delimited with \\. *nix file paths are usually delimited with /. Never use \\ in file paths in R. \\ is an escape character in R and will complicate an analysis. Fortunately, RStudio understands / in file paths regardless of operating system.\nNever use setwd() in R. It is unnecessary, it makes code unreproducible across machines, and it is rude to collaborators. R Projects create a better framework for file paths. Simply treat the directory where the R Project lives as the working directory and directories inside of that directory as sub-directories.\nFor example, say there’s a .Rproj called starwars-analysis.Rproj in a directory called starwars-analysis/. If there is a .csv in that folder called jedi.csv, the file can be loaded with read_csv(\"jedi.csv\") instead of read_csv(\"H:/alena/analyses/starwars-analysis/jedi.csv\"). If that file is in a sub-directory of starwars-analysis called data, it can be loaded with read_csv(\"data/jedi.csv\"). The same concepts hold for writing data and graphics.\nThis simplifies code and makes it portable because all relative file paths will be identical on all computers. To share an analysis, simply send the entire directory to a collaborator or share it with GitHub.\nHere’s an example directory:",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#getting-help",
    "href": "01_intro-r.html#getting-help",
    "title": "1  Introduction to R",
    "section": "1.13 Getting help",
    "text": "1.13 Getting help\n\n1.13.1 Googling\nWhen Googling for R or data science help, set the search range to the last year or less to avoid out-of-date solutions and to focus on up-to-date practices. The search window can be set by clicking Tools after a Google search.\n\n\n1.13.2 Stack Overflow\nStack Overflow contains numerous solutions. If a problem is particularly perplexing, it is simple to submit questions. Exercise caution when submitting questions because the Stack Overflow community has strict norms about questions and loose norms about respecting novices.\n\n\n1.13.3 RStudio community\nRStudio Community is a new forum for R Users. It has a smaller back catalog than Stack Overflow but users are friendlier than on Stack Overflow.\n\n\n1.13.4 CRAN task views\nCRAN task views contains thorough introductions to packages and techniques organized by subject matter. The Econometrics, Reproducible Research, and and Social Sciences task views are good starting places.\n\n\n1.13.5 Data Science for Public Policy Slack\nWe’ve created a Slack workspace for this class (which will be shared across both sections) and encourage you to ask questions in Slack. In general, we ask that you try to answer questions on your own using the sources above before posting in Slack. Practicing finding and applying the relevant information to answer your questions is an important data science skill! The teaching staff will be checking the Slack to help answer questions in a reasonable time frame and we also encourage you to answer each other’s questions - it’s a great way to improve your R skills!\nQuestions on Slack must be asked using reproducible examples. Simply copying-and-pasting questions or answers in the Slack channel is not allowed. If you’re unsure how to share a reproducible example without sharing your answers in a public channel, you can DM the teaching staff to be safe.\n\n\n1.13.6 ChatGPT:\n\n\n\n\n\n\nWarning\n\n\n\nSince there is R code on the internet, ChatGPT has been trained on R code and has the capability to answer R coding questions. Exercise extreme caution when using ChatGPT! ChatGPT saves and uses the queries you provide it. This means that asking a question about sensitive data or code could expose that data. If you decide to use ChatGPT, only ask queries of it using a reproducible example with non-sensitive data. The diamonds dataset, loaded with ggplot::diamonds(), is a great candidate.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nJust like machine translators are useful for communication but can stymie the ability to learn a second language, large language models are useful for creating code but can stymie learning R.\nWe encourage everyone to pursue mastery of R. This means feeling the terror of the blank page and internalizing the syntax of R without the crutch of LLMs before regularly using LLMs.\n\n\nChatGPT can be a powerful tool. Some helpful tips for using ChatGPT for coding questions are:\n\nProvide it detailed questions\nGive it reproducible example code\nRefine queries when the initial responses are unsatisfactory",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#conclusion",
    "href": "01_intro-r.html#conclusion",
    "title": "1  Introduction to R",
    "section": "1.14 Conclusion",
    "text": "1.14 Conclusion\nR is a powerful programming language that was developed initially for statistics but is now used in a variety of contexts. Key advantages to R include that it has a fantastic, custom-built IDE called RStudio and a large open source community of users that provides help and develops new packages. Like other open source programming languages, conducting data science in R supports the six principles of data analysis introduced at the start of this chapter.\n\n\n\n\nEubank, Nick. 2016. “Embrace Your Fallibility: Thoughts on Code Integrity.” https://www.nickeubank.com/wp-content/uploads/2016/06/Eubank_EmbraceYourFallibility.pdf.\n\n\nPeng, Roger. 2018. “Teaching r to New Users - from Tapply to the Tidyverse.” https://simplystatistics.org/posts/2018-07-12-use-r-keynote-2018/.\n\n\nRodrigues, Bruno. 2022. Modern R with the tidyverse. https://modern-rstats.eu.\n\n\nWickham, Hadley. n.d. The tidyverse style guide. https://style.tidyverse.org/index.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualie, and Model Data. 2nd edition. Sebastopol, CA: O’Reilly.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O’Reilly Media. http://r4ds.had.co.nz/.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01_intro-r.html#footnotes",
    "href": "01_intro-r.html#footnotes",
    "title": "1  Introduction to R",
    "section": "",
    "text": "(Eubank 2016) influenced our thinking on this issue and provides proactive steps for writing high-quality code using defensive programming.↩︎\nThough we do not focus on the Python programming language in this course, Posit appears to be expanding its focus to Python in addition to R. This is an important development to track in the data science landscape. One key update is that, in 2023, Posit hired Wes McKinney. McKinney developed the Python pandas package which offers similar functionality to the tidyverse, a collection of R packages which we will discuss next week.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html",
    "href": "02_tidyverse.html",
    "title": "2  Introduction to the tidyverse",
    "section": "",
    "text": "2.1 Review",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#review",
    "href": "02_tidyverse.html#review",
    "title": "2  Introduction to the tidyverse",
    "section": "",
    "text": "2.1.1 Assignment operator\n&lt;- is the assignment operator. An object created on the right side of an assignment operator is assigned to a name on the left side of an assignment operator. Assignment operators are important for saving the consequences of operations and functions. Without assignment, the result of a calculation is not saved for use in future calculations. Operations without assignment operators will typically be printed to the console but not saved for future use.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#functions",
    "href": "02_tidyverse.html#functions",
    "title": "2  Introduction to the tidyverse",
    "section": "2.2 Functions",
    "text": "2.2 Functions\nFunctions are collections of code that take inputs, perform operations, and return outputs. R functions are similar to mathematical functions.\nR functions typically contain arguments. For example, mean() has x, trim, and na.rm. Many arguments have default values and don’t need to be included in function calls. Default values can be seen in the documentation. trim = 0 and na.rm = FALSE are the defaults for mean().\n\n2.2.1 == vs. =\n== is a binary comparison operator.\n\n1 == 1\n\n[1] TRUE\n\n1 == 2\n\n[1] FALSE\n\n\n= is an equals sign, it is most frequently used for passing arguments to functions.\n\nmean(x = c(1, 2, 3))",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#tidy-data",
    "href": "02_tidyverse.html#tidy-data",
    "title": "2  Introduction to the tidyverse",
    "section": "2.3 Tidy data",
    "text": "2.3 Tidy data\n\n2.3.1 tidyverse\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. ~ tidyverse.org\n\nlibrary(tidyverse) contains:\n\nggplot2, for data visualization.\ndplyr, for data manipulation.\ntidyr, for data tidying.\nreadr, for data import.\npurrr, for functional programming.\ntibble, for tibbles, a modern re-imagining of data frames.\nstringr, for strings.\nforcats, for factors.\n\n\n\n2.3.2 Opinionated software\n\nOpinionated software is a software product that believes a certain way of approaching a business process is inherently better and provides software crafted around that approach. ~ Stuart Eccles\n\n\n\n2.3.3 Tidy data\nThe defining opinion of the tidyverse is its wholehearted adoption of tidy data. Tidy data has three features:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a dataframe.1\n\n\n\n\nSource: R4DS\n\n\nTidy data was formalized by Hadley Wickham (2014) in the Journal of Statistical Software It is equivalent to Codd’s 3rd normal form (Codd, 1990) for relational databases.\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. ~ Hadley Wickham\n\nThe tidy approach to data science is powerful because it breaks data work into two distinct parts.\n\nGet the data into a tidy format.\nUse tools optimized for tidy data.\n\nBy standardizing the data structure for most community-created tools, the framework orients diffuse development and reduces the friction of data work.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#dplyr",
    "href": "02_tidyverse.html#dplyr",
    "title": "2  Introduction to the tidyverse",
    "section": "2.4 dplyr",
    "text": "2.4 dplyr\nlibrary(dplyr) contains workhorse functions for manipulating and summarizing data once it is in a tidy format. library(tidyr) contains functions for getting data into a tidy format.\ndplyr can be explicitly loaded with library(dplyr) or loaded with library(tidyverse):\n\nlibrary(tidyverse)\n\nWe’ll focus on the key dplyr syntax using the March 2020 Annual Social and Economic Supplement (ASEC) to the Current Population Survey (CPS). Run the following code to load the data.\n\nasec &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/cps/cps-asec.csv\"\n  )\n)\n\nRows: 157959 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): month, ftype, offpov, offpovuniv\ndbl (13): year, serial, cpsid, asecflag, asecwth, pernum, cpsidp, asecwt, ft...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can use glimpse(asec) to quickly view the data. We can also use View(asec) to open up asec in RStudio.\n\nglimpse(x = asec)\n\nRows: 157,959\nColumns: 17\n$ year       &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020,…\n$ serial     &lt;dbl&gt; 1, 1, 2, 2, 3, 4, 4, 5, 5, 5, 5, 7, 8, 9, 10, 10, 10, 12, 1…\n$ month      &lt;chr&gt; \"March\", \"March\", \"March\", \"March\", \"March\", \"March\", \"Marc…\n$ cpsid      &lt;dbl&gt; 2.01903e+13, 2.01903e+13, 2.01812e+13, 2.01812e+13, 2.01902…\n$ asecflag   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ asecwth    &lt;dbl&gt; 1552.90, 1552.90, 990.49, 990.49, 1505.27, 1430.70, 1430.70…\n$ pernum     &lt;dbl&gt; 1, 2, 1, 2, 1, 1, 2, 1, 2, 3, 4, 1, 1, 1, 1, 2, 3, 1, 2, 3,…\n$ cpsidp     &lt;dbl&gt; 2.01903e+13, 2.01903e+13, 2.01812e+13, 2.01812e+13, 2.01902…\n$ asecwt     &lt;dbl&gt; 1552.90, 1552.90, 990.49, 990.49, 1505.27, 1430.70, 1196.57…\n$ ftype      &lt;chr&gt; \"Primary family\", \"Primary family\", \"Primary family\", \"Prim…\n$ ftotval    &lt;dbl&gt; 127449, 127449, 64680, 64680, 40002, 8424, 8424, 59114, 591…\n$ inctot     &lt;dbl&gt; 52500, 74949, 44000, 20680, 40002, 0, 8424, 610, 58001, 503…\n$ incwage    &lt;dbl&gt; 52500, 56000, 34000, 0, 40000, 0, 8424, 0, 58000, 0, 0, 0, …\n$ offpov     &lt;chr&gt; \"Above Poverty Line\", \"Above Poverty Line\", \"Above Poverty …\n$ offpovuniv &lt;chr&gt; \"In Poverty Universe\", \"In Poverty Universe\", \"In Poverty U…\n$ offtotval  &lt;dbl&gt; 127449, 127449, 64680, 64680, 40002, 8424, 8424, 59114, 591…\n$ offcutoff  &lt;dbl&gt; 17120, 17120, 17120, 17120, 13300, 15453, 15453, 26370, 263…\n\n\nWe’re going to learn seven functions and one new piece of syntax from library(dplyr) that will be our main tools for manipulating tidy frames. These functions and a few extensions outlined in the Data Transformation Cheat Sheet are the core of data analysis in the Tidyverse.\n\n2.4.1 select()\nselect() drops columns from a dataframe and/or reorders the columns in a dataframe. The arguments after the name of the dataframe should be the names of columns you wish to keep, without quotes. All other columns not listed are dropped.\n\nselect(.data = asec, year, month, serial)\n\n# A tibble: 157,959 × 3\n    year month serial\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  2020 March      1\n 2  2020 March      1\n 3  2020 March      2\n 4  2020 March      2\n 5  2020 March      3\n 6  2020 March      4\n 7  2020 March      4\n 8  2020 March      5\n 9  2020 March      5\n10  2020 March      5\n# ℹ 157,949 more rows\n\n\nThis works great until the goal is to select 99 of 100 variables. Fortunately, - can be used to remove variables. You can also select all but multiple variables by listing them with the - symbol separated by commas.\n\nselect(.data = asec, -asecflag)\n\n# A tibble: 157,959 × 16\n    year serial month   cpsid asecwth pernum  cpsidp asecwt ftype ftotval inctot\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  2020      1 March 2.02e13   1553.      1 2.02e13  1553. Prim…  127449  52500\n 2  2020      1 March 2.02e13   1553.      2 2.02e13  1553. Prim…  127449  74949\n 3  2020      2 March 2.02e13    990.      1 2.02e13   990. Prim…   64680  44000\n 4  2020      2 March 2.02e13    990.      2 2.02e13   990. Prim…   64680  20680\n 5  2020      3 March 2.02e13   1505.      1 2.02e13  1505. Nonf…   40002  40002\n 6  2020      4 March 2.02e13   1431.      1 2.02e13  1431. Prim…    8424      0\n 7  2020      4 March 2.02e13   1431.      2 2.02e13  1197. Prim…    8424   8424\n 8  2020      5 March 2.02e13   1133.      1 2.02e13  1133. Prim…   59114    610\n 9  2020      5 March 2.02e13   1133.      2 2.02e13  1133. Prim…   59114  58001\n10  2020      5 March 2.02e13   1133.      3 2.02e13  1322. Prim…   59114    503\n# ℹ 157,949 more rows\n# ℹ 5 more variables: incwage &lt;dbl&gt;, offpov &lt;chr&gt;, offpovuniv &lt;chr&gt;,\n#   offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\ndplyr contains powerful helper functions that can select variables based on patterns in column names:\n\ncontains(): Contains a given string\nstarts_with(): Starts with a prefix\nends_with(): Ends with a suffix\nmatches(): Matches a regular expression\nnum_range(): Matches a numerical range\n\nThese are a subset of the tidyselect selection language and helpers which enable users to apply library(dplyr) functions to select variables.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nSelect pernum and inctot from asec.\npull() is related to select() but can only select one variable. What is the other difference with pull()?\n\n\n\n\n\n2.4.2 rename()\nrename() renames columns in a data frame. The pattern is new_name = old_name.\n\nrename(.data = asec, serial_number = serial)\n\n# A tibble: 157,959 × 17\n    year serial_number month   cpsid asecflag asecwth pernum  cpsidp asecwt\n   &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  2020             1 March 2.02e13        1   1553.      1 2.02e13  1553.\n 2  2020             1 March 2.02e13        1   1553.      2 2.02e13  1553.\n 3  2020             2 March 2.02e13        1    990.      1 2.02e13   990.\n 4  2020             2 March 2.02e13        1    990.      2 2.02e13   990.\n 5  2020             3 March 2.02e13        1   1505.      1 2.02e13  1505.\n 6  2020             4 March 2.02e13        1   1431.      1 2.02e13  1431.\n 7  2020             4 March 2.02e13        1   1431.      2 2.02e13  1197.\n 8  2020             5 March 2.02e13        1   1133.      1 2.02e13  1133.\n 9  2020             5 March 2.02e13        1   1133.      2 2.02e13  1133.\n10  2020             5 March 2.02e13        1   1133.      3 2.02e13  1322.\n# ℹ 157,949 more rows\n# ℹ 8 more variables: ftype &lt;chr&gt;, ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;,\n#   offpov &lt;chr&gt;, offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nYou can also rename a selection of variables using rename_with(). The .cols argument is used to select the columns to rename and takes a tidyselect statement like those we introduced above. Here, we’re using the where() selection helper which selects all columns where a given condition is TRUE. The default value for the .cols argument is everything() which selects all columns in the dataset.\n\nrename_with(.data = asec, .fn = toupper, .cols = where(is.numeric))\n\n# A tibble: 157,959 × 17\n    YEAR SERIAL month   CPSID ASECFLAG ASECWTH PERNUM  CPSIDP ASECWT ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: FTOTVAL &lt;dbl&gt;, INCTOT &lt;dbl&gt;, INCWAGE &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, OFFTOTVAL &lt;dbl&gt;, OFFCUTOFF &lt;dbl&gt;\n\n\nMost dplyr functions can rename columns simply by prefacing the operation with new_name =. For example, this can be done with select():\n\nselect(.data = asec, year, month, serial_number = serial)\n\n# A tibble: 157,959 × 3\n    year month serial_number\n   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1  2020 March             1\n 2  2020 March             1\n 3  2020 March             2\n 4  2020 March             2\n 5  2020 March             3\n 6  2020 March             4\n 7  2020 March             4\n 8  2020 March             5\n 9  2020 March             5\n10  2020 March             5\n# ℹ 157,949 more rows\n\n\n\n\n2.4.3 filter()\nfilter() reduces the number of observations in a dataframe. Every column in a dataframe has a name. Rows do not necessarily have names in a dataframe, so rows need to be filtered based on logical conditions.\n==, &lt;, &gt;, &lt;=, &gt;=, !=, %in%, and is.na() are all operators that can be used for logical conditions. ! can be used to negate a condition and & and | can be used to combine conditions. | means or.\n\n# return rows with pernum of 1 and incwage &gt; $100,000\nfilter(.data = asec, pernum == 1 & incwage &gt; 100000)\n\n# A tibble: 5,551 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020     28 March 2.02e13        1    678.      1 2.02e13   678. Primary fa…\n 2  2020    134 March 0              1    923.      1 0         923. Primary fa…\n 3  2020    136 March 2.02e13        1    906.      1 2.02e13   906. Primary fa…\n 4  2020    137 March 2.02e13        1   1493.      1 2.02e13  1493. Nonfamily …\n 5  2020    359 March 2.02e13        1    863.      1 2.02e13   863. Primary fa…\n 6  2020    372 March 2.02e13        1   1338.      1 2.02e13  1338. Primary fa…\n 7  2020    404 March 0              1    677.      1 0         677. Primary fa…\n 8  2020    420 March 2.02e13        1    747.      1 2.02e13   747. Primary fa…\n 9  2020    450 March 2.02e13        1   1309.      1 2.02e13  1309. Primary fa…\n10  2020    491 March 0              1   1130.      1 0        1130. Primary fa…\n# ℹ 5,541 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nIPUMS CPS contains full documentation with information about pernum and incwage.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nFilter asec to rows with month equal to \"March\".\nFilter asec to rows with inctot less than 999999999.\nFilter asec to rows with pernum equal to 3 and inctot less than 999999999.\n\n\n\n\n\n2.4.4 arrange()\narrange() sorts the rows of a data frame in alpha-numeric order based on the values of a variable or variables. The dataframe is sorted by the first variable first and each subsequent variable is used to break ties. desc() is used to reverse the sort order for a given variable.\n\n# sort pernum is descending order because high pernums are interesting\narrange(.data = asec, desc(pernum))\n\n# A tibble: 157,959 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020  91430 March 0              1    505.     16 0         604. Secondary …\n 2  2020  91430 March 0              1    505.     15 0         465. Secondary …\n 3  2020  91430 March 0              1    505.     14 0         416. Secondary …\n 4  2020  15037 March 2.02e13        1   2272.     13 2.02e13  2633. Primary fa…\n 5  2020  78495 March 0              1   1279.     13 0        1424. Related su…\n 6  2020  91430 March 0              1    505.     13 0         465. Secondary …\n 7  2020  15037 March 2.02e13        1   2272.     12 2.02e13  1689. Primary fa…\n 8  2020  18102 March 0              1   2468.     12 0        2871. Primary fa…\n 9  2020  22282 March 0              1   2801.     12 0        3879. Related su…\n10  2020  30274 March 2.02e13        1    653.     12 2.02e13   858. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nSort asec in descending order by pernum and ascending order by inctot.\n\n\n\n\n\n2.4.5 mutate()\nmutate() creates new variables or edits existing variables. We can use arithmetic arguments, such as +, -, *, /, and ^. We can also custom functions and functions from packages. For example, we can use library(stringr) for string manipulation and library(lubridate) for date manipulation.\nVariables are created by adding a new column name, like inctot_adjusted, to the left of = in mutate().\n\n# adjust inctot for underreporting\nmutate(.data = asec, inctot_adjusted = inctot * 1.1)\n\n# A tibble: 157,959 × 18\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 8 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;, inctot_adjusted &lt;dbl&gt;\n\n\nVariables are edited by including an existing column name, like inctot, to the left of = in mutate().\n\n# adjust income because of underreporting\nmutate(.data = asec, inctot = inctot * 1.1)\n\n# A tibble: 157,959 × 17\n    year serial month   cpsid asecflag asecwth pernum  cpsidp asecwt ftype      \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1  2020      1 March 2.02e13        1   1553.      1 2.02e13  1553. Primary fa…\n 2  2020      1 March 2.02e13        1   1553.      2 2.02e13  1553. Primary fa…\n 3  2020      2 March 2.02e13        1    990.      1 2.02e13   990. Primary fa…\n 4  2020      2 March 2.02e13        1    990.      2 2.02e13   990. Primary fa…\n 5  2020      3 March 2.02e13        1   1505.      1 2.02e13  1505. Nonfamily …\n 6  2020      4 March 2.02e13        1   1431.      1 2.02e13  1431. Primary fa…\n 7  2020      4 March 2.02e13        1   1431.      2 2.02e13  1197. Primary fa…\n 8  2020      5 March 2.02e13        1   1133.      1 2.02e13  1133. Primary fa…\n 9  2020      5 March 2.02e13        1   1133.      2 2.02e13  1133. Primary fa…\n10  2020      5 March 2.02e13        1   1133.      3 2.02e13  1322. Primary fa…\n# ℹ 157,949 more rows\n# ℹ 7 more variables: ftotval &lt;dbl&gt;, inctot &lt;dbl&gt;, incwage &lt;dbl&gt;, offpov &lt;chr&gt;,\n#   offpovuniv &lt;chr&gt;, offtotval &lt;dbl&gt;, offcutoff &lt;dbl&gt;\n\n\nConditional logic inside of mutate() with functions like if_else() and case_when() is key to mastering data munging in R.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCreate a new variable called in_poverty. If offtotval is less than offcutoff then use \"Below Poverty Line\". Otherwise, use \"Above Poverty Line\". Hint: if_else() is useful and works like the IF command in Microsoft Excel.\n\n\n\n\n\n2.4.6 |&gt;\nData munging is tiring when each operation needs to be assigned to a name with &lt;-. The pipe, |&gt;, allows lines of code to be chained together so the assignment operator only needs to be used once.\n|&gt; passes the output from function as the first argument in a subsequent function. For example, this line can be rewritten:\nLegacy R code may use %&gt;%, the pipe from the magrittr package. It was (and remains) popular, particularly in the tidyverse framework. Due to this popularity, base R incorporated a similar concept in the base pipe. In many cases, these pipes work the same way, but there are some differences. Because |&gt; is new and continues to be developed, developers have increased |&gt;’s abilities over time. To see a list of key differences between %&gt;% and |&gt;, see this blog.\n\n# old way\nmutate(.data = asec, inctot_adjusted = inctot * 1.1)\n\n# new way\nasec |&gt;\n  mutate(inctot_adjusted = inctot * 1.1)\n\nSee the power:\n\nnew_asec &lt;- asec |&gt;\n  filter(pernum == 1) |&gt;\n  select(year, month, pernum, inctot) |&gt;\n  mutate(inctot_adjusted = inctot * 1.1) |&gt;\n  select(-inctot)\n\nnew_asec\n\n# A tibble: 60,460 × 4\n    year month pernum inctot_adjusted\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;           &lt;dbl&gt;\n 1  2020 March      1          57750 \n 2  2020 March      1          48400 \n 3  2020 March      1          44002.\n 4  2020 March      1              0 \n 5  2020 March      1            671 \n 6  2020 March      1          19279.\n 7  2020 March      1          12349.\n 8  2020 March      1          21589.\n 9  2020 March      1          47306.\n10  2020 March      1          10949.\n# ℹ 60,450 more rows\n\n\n\n\n2.4.7 summarize()\nsummarize() collapses many rows in a dataframe into fewer rows with summary statistics of the many rows. n(), mean(), and sum() are common summary statistics. Renaming is useful with summarize()!\n\n# summarize without renaming the statistics\nasec |&gt;\n  summarize(mean(ftotval), mean(inctot))\n\n# A tibble: 1 × 2\n  `mean(ftotval)` `mean(inctot)`\n            &lt;dbl&gt;          &lt;dbl&gt;\n1         105254.     209921275.\n\n# summarize and rename the statistics\nasec |&gt;\n  summarize(mean_ftotval = mean(ftotval), mean_inctot = mean(inctot))\n\n# A tibble: 1 × 2\n  mean_ftotval mean_inctot\n         &lt;dbl&gt;       &lt;dbl&gt;\n1      105254.  209921275.\n\n\nsummarize() returns a data frame. This means all dplyr functions can be used on the output of summarize(). This is powerful! Manipulating summary statistics in Stata and SAS can be a chore. Here, it’s just another dataframe that can be manipulated with a tool set optimized for dataframes: dplyr.\n\n\n2.4.8 group_by()\ngroup_by() groups a dataframe based on specified variables. summarize() with grouped dataframes creates subgroup summary statistics. mutate() with group_by() calculates grouped summaries for each row.\n\nasec |&gt;\n  group_by(pernum) |&gt;\n  summarize(\n    n = n(),\n    mean_ftotval = mean(ftotval), \n    mean_inctot = mean(inctot)\n  )\n\n# A tibble: 16 × 4\n   pernum     n mean_ftotval mean_inctot\n    &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 60460       94094.      57508.\n 2      2 45151      108700.   77497357.\n 3      3 25650      117966.  473030618.\n 4      4 15797      121815.  634999933.\n 5      5  6752      108609.  691504650.\n 6      6  2582       89448.  682810446.\n 7      7   922       78889.  682218196.\n 8      8   353       72284.  682725646.\n 9      9   158       54599.  632917559.\n10     10    73       58145.  657543632.\n11     11    37       61847.  702708584 \n12     12    18       50249.  777780725.\n13     13     3       25152   666666666 \n14     14     1       18000       18000 \n15     15     1       25000       25000 \n16     16     1       15000       15000 \n\n\nDataframes can be grouped by multiple variables.\nGrouped tibbles include metadata about groups. For example, Groups:   pernum, offpov [40]. One grouping is dropped each time summarize() is used. It is easy to forget if a dataframe is grouped, so it is safe to include ungroup() at the end of a section of functions.\n\nasec |&gt;\n  group_by(pernum, offpov) |&gt;\n  summarize(\n    n = n(),\n    mean_ftotval = mean(ftotval), \n    mean_inctot = mean(inctot)\n  ) |&gt;\n  arrange(offpov) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'pernum'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 40 × 5\n   pernum offpov                 n mean_ftotval mean_inctot\n    &lt;dbl&gt; &lt;chr&gt;              &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 Above Poverty Line 53872      104451.      63642.\n 2      2 Above Poverty Line 40978      118691.   59082162.\n 3      3 Above Poverty Line 23052      129891.  463440562.\n 4      4 Above Poverty Line 14076      135039.  631720097.\n 5      5 Above Poverty Line  5805      123937.  688206447.\n 6      6 Above Poverty Line  2118      105867.  683199297.\n 7      7 Above Poverty Line   724       96817.  697520661.\n 8      8 Above Poverty Line   269       90328.  672870019.\n 9      9 Above Poverty Line   114       70438.  622815186.\n10     10 Above Poverty Line    57       71483.  666678408.\n# ℹ 30 more rows\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nfilter() to only include observations with \"In Poverty Universe\" in offpovuniv.\ngroup_by() offpov.\nUse summarize() and n() to count the number of observations in poverty.\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\nfilter() to only include observations with \"In Poverty Universe\".\ngroup_by() cpsid.\nUse mutate(family_size = n()) to calculate the family size for each observation in asec.\nungroup()\nCreate a new variable called in_poverty. If offtotval is less than offcutoff then use \"Below Poverty Line\". Otherwise, use \"Above Poverty Line\".\ngroup_by() family_size, offpov, and in_poverty\nUse summarize() and n() to see if you get the same result for offpov and in_poverty. You should only get two rows per family size if your poverty calculation is correct.\n\n\noffcutoff comes from Census Bureau poverty tables with 48 unique thresholds based on family composition. Do not confuse the tables with HHS poverty tables.\nThese data come from IPUMS CPS. IPUMS has cleaned and pre-processed the data to include variables like offcutoff.\n\nAre the estimates from the previous two exercises correct?\nLet’s look at a Census Report to see how many people were in poverty in 2019. We estimated about 16,500 people. The Census Bureau says 34.0 million people.\nNo! We did not account for sampling weights, so our estimates are incorrect. Assignment 3 will demonstrate how to incorporate sampling weights into an analysis.\n\n\n\n\n2.4.9 BONUS: count()\ncount() is a shortcut to df |&gt; group_by(var) |&gt; summarize(n()). count() counts the number of observations with a level of a variable or levels of several variables. It is too useful to skip:\n\ncount(asec, pernum)\n\n# A tibble: 16 × 2\n   pernum     n\n    &lt;dbl&gt; &lt;int&gt;\n 1      1 60460\n 2      2 45151\n 3      3 25650\n 4      4 15797\n 5      5  6752\n 6      6  2582\n 7      7   922\n 8      8   353\n 9      9   158\n10     10    73\n11     11    37\n12     12    18\n13     13     3\n14     14     1\n15     15     1\n16     16     1\n\n\n\ncount(x = asec, pernum, offpov)\n\n# A tibble: 40 × 3\n   pernum offpov                 n\n    &lt;dbl&gt; &lt;chr&gt;              &lt;int&gt;\n 1      1 Above Poverty Line 53872\n 2      1 Below Poverty Line  6588\n 3      2 Above Poverty Line 40978\n 4      2 Below Poverty Line  4156\n 5      2 NIU                   17\n 6      3 Above Poverty Line 23052\n 7      3 Below Poverty Line  2527\n 8      3 NIU                   71\n 9      4 Above Poverty Line 14076\n10      4 Below Poverty Line  1648\n# ℹ 30 more rows",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#mutating-joins",
    "href": "02_tidyverse.html#mutating-joins",
    "title": "2  Introduction to the tidyverse",
    "section": "2.5 Mutating Joins",
    "text": "2.5 Mutating Joins\nMutating joins join one dataframe to columns from another dataframe by matching values common in both dataframes. The syntax is derived from Structured Query Language (SQL).\nEach function requires an x (or left) dataframe, a y (or right) data frame, and by variables that exist in both dataframes. Note that below we’re creating dataframes using the tribble() function, which creates a tibble using a row-wise layout.\nlibrary(tidylog) is a useful function for monitoring the behavior of joins. It prints out summaries of the number of rows in each dataframe that successfully join.\n\nmath_scores &lt;- tribble(\n  ~name, ~math_score,\n  \"Alec\", 95,\n  \"Bart\", 97,\n  \"Carrie\", 100\n)\n\nreading_scores &lt;- tribble(\n  ~name, ~reading_score,\n  \"Alec\", 88,\n  \"Bart\", 67,\n  \"Carrie\", 100,\n  \"Zeta\", 100\n)\n\n\n2.5.1 left_join()\nleft_join() matches observations from the y dataframe to the x dataframe. It only keeps observations from the y data frame that have a match in the x dataframe.\n\nleft_join(x = math_scores, y = reading_scores, by = \"name\")\n\n# A tibble: 3 × 3\n  name   math_score reading_score\n  &lt;chr&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1 Alec           95            88\n2 Bart           97            67\n3 Carrie        100           100\n\n\nObservations that exist in the x (left) dataframe but not in the y (right) dataframe result in NAs.\n\nleft_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 4 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n4 Zeta             100         NA\n\n\n\n\n2.5.2 inner_join()\ninner_join() matches observations from the y dataframe to the x dataframe. It only keeps observations from either data frame that have a match.\n\ninner_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 3 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n\n\n\n\n2.5.3 full_join()\nfull_join() matches observations from the y dataframe to the x dataframe. It keeps observations from both dataframes.\n\nfull_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 4 × 3\n  name   reading_score math_score\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Alec              88         95\n2 Bart              67         97\n3 Carrie           100        100\n4 Zeta             100         NA",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#filtering-joins",
    "href": "02_tidyverse.html#filtering-joins",
    "title": "2  Introduction to the tidyverse",
    "section": "2.6 Filtering Joins",
    "text": "2.6 Filtering Joins\nFiltering joins drop observations based on the presence of their key (identifier) in another data frame. They use the same syntax as mutating joins with an x (or left) dataframe, a y (or right) data frame, and by variables that exist in both dataframes.\n\n2.6.1 anti_join()\nanti_join() returns all rows from x where there are not matching values in y. anti_join() complements inner_join(). Together, they should exhaust the x dataframe.\n\nanti_join(x = reading_scores, y = math_scores, by = \"name\")\n\n# A tibble: 1 × 2\n  name  reading_score\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Zeta            100\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Combine Tables column in the Data Transformation Cheat Sheet is an invaluable resource for navigating joins. The “column matching for joins” section of that cheat sheet outlines how to join tables by matching on multiple columns or match on columns with different names in each table.\nR for Data Science (2e) also has an excellent chapter covering joins.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#readr",
    "href": "02_tidyverse.html#readr",
    "title": "2  Introduction to the tidyverse",
    "section": "2.7 readr",
    "text": "2.7 readr\nreadr is a core tidyverse package for reading and parsing rectangular data from text files (.csv, .tsv, etc.). read_csv() reads .csv files and has a bevy of advantages versus read.csv(). We recommend never using read.csv().\nMany .csvs can be read without issue with simple syntax read_csv(file = \"relative/path/to/data\").\nreadr and read_csv() have powerful tools for resolving parsing issues. More can be learned in the data import section in R for Data Science (2e).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#readxl",
    "href": "02_tidyverse.html#readxl",
    "title": "2  Introduction to the tidyverse",
    "section": "2.8 readxl",
    "text": "2.8 readxl\nreadxl is a tidyverse package for reading data from Microsoft Excel files. It is not a core tidyverse package so it needs to be explicitly loaded in each R session.\nWe introduce the package more thoroughly in Section 3.2.2. The tidyverse website has a good tutorial on readxl.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#conclusion",
    "href": "02_tidyverse.html#conclusion",
    "title": "2  Introduction to the tidyverse",
    "section": "2.9 Conclusion",
    "text": "2.9 Conclusion\nThis chapter introduced tidy data in which columns reflect a variable, observations reflect a row, and cells reflect an individual observation. It also introduced key functions from dplyr, a tidyverse package built to support data cleaning operations:\n\nselect() for removing columns\nfilter() for removing rows\nrename() for renaming columns\narrange() for reordering columns\nmutate() for creating new columns\n|&gt; (the pipe operator) for chaining together multiple functions (part of base, but extremely useful!)\nsummarize() for collapsing many rows of a data frame into fewer rows\ngroup_by() to group a data frame by certain specified variables\ncount() a shortcut for group_by() |&gt; summarize(n())\n\nThe chapter also introduced mutating joins (left_join(), inner_join(), and full_join()) which create new columns and filtering joins (anti_join()) which drop observations depending on the presence of their key in another data frame.\nThe chapter concluded by introducing the readr and readxl packages which are useful for reading data into R.\n\n2.9.1 Next Skills\n\nacross() can be used with library(dplyr) functions such as summarise() and mutate() to apply the same transformations to multiple columns. For example, it can be used to calculate the mean of many columns with summarize(). across() uses the same tidyselect select language and helpers discussed earlier to select the columns to transform.\npivot_wider() and pivot_longer() can be used to switch between wide and long formats of the data. This is important for tidying data and data visualization.\n\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” https://doi.org/10.18637/jss.v059.i10.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "02_tidyverse.html#footnotes",
    "href": "02_tidyverse.html#footnotes",
    "title": "2  Introduction to the tidyverse",
    "section": "",
    "text": "This definition is from the tidy data paper, not R for Data Science, which uses a slightly different definition.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the tidyverse</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html",
    "href": "03_advanced-data-cleaning.html",
    "title": "3  Advanced Data Cleaning",
    "section": "",
    "text": "3.1 Review\nR for Data Science (2e) displays the first steps of the data science process as “Import”, “Tidy”, and “Transform”. Recall from the previous lecture techniques for importing data like read_csv() and for transforming data like mutate().\nIn the last lecture, we introduced mutating joins and filtering joins.\nLet their be two data frames x and y and let both data frames have a key variable that uniquely identifies rows. In practice, in R, we often use the following functions:\nTo learn more, read the joins chapter of R for Data Science (2e).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#sec-review2",
    "href": "03_advanced-data-cleaning.html#sec-review2",
    "title": "3  Advanced Data Cleaning",
    "section": "",
    "text": "Exercise 1\n\n\n\n\nUse mutate() and case_when() to add a new variable called speed_cat to cars where the values are \"slow\" when speed &lt; 10, \"moderate\" when speed &lt; 20, and \"fast\" otherwise.\n\n\n\n\n\n\n\n\n\n\nMutating Joins\n\n\n\nMutating joins add new variables to a data frame by matching observations from one data frame to observations in another data frame.\n\n\n\n\n\n\n\n\nFiltering Joins\n\n\n\nFiltering joins drop observations based on the presence of their key (identifier) in another data frame.\nFor example, we may have a list of students in detention and a list of all students. We can use a filtering join to create a list of student not in detention.\n\n\n\n\nleft_join(x, y) appends variables from y on to x but only keeps observations from x.\ninner_join(x, y) appends variables from y onto x but only keeps observations for which there is a match between the x and ydata frames.\nfull_join(x, y) appends variables from y on to x and keeps all observations from x and y.\nanti_join(x, y) returns all observations from x without a match in y. anti_join() is traditionally only used for filtering joins, but it is useful for writing tests for mutating joins.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#sec-import",
    "href": "03_advanced-data-cleaning.html#sec-import",
    "title": "3  Advanced Data Cleaning",
    "section": "3.2 Import",
    "text": "3.2 Import\n\n3.2.1 library(here)\nDeveloping Quarto documents in subdirectories is a pain. When interactively running code in the console, file paths are read as if the .qmd file is in the same folder as the .Rproj. When clicking render, paths are treated as if they are in the subdirectory where the .qmd file is.\nlibrary(here) resolves headaches around file referencing in project-oriented workflows.\nLoading library(here) will print your working directory.\n\nlibrary(here)\n\nhere() starts at /Users/aaronwilliams/presentations/data-science-for-public-policy2\n\n\nAfter this, here() will use reasonable heuristics to find project files using relative file paths. When placing Quarto documents in a directory below the top-level directory, use here() and treat each folder and file as a different string.\nBefore\n\nread_csv(\"data/raw/important-data.csv\")\n\nAfter\n\nread_csv(here(\"data\", \"raw\", \"important-data.csv\"))\n\n\n\n3.2.2 library(readxl)\nWe will focus on reading data from Excel workbooks. Excel is a bad tool with bad design that has led to many analytical errors. Unfortunately, it’s a dominant tool for storing data and often enters the data science workflow.\nlibrary(readxl) is the premier package for reading data from .xls and .xlsx files. read_excel(), which works like read_csv(), loads data from .xls and .xlsx files. Consider data from the Urban Institute’s Debt in America feature accessed through the Urban Institute Data Catalog.\n\nlibrary(readxl)\n\nread_excel(here(\"data\", \"state_dia_delinquency_ 7 Jun 2022.xlsx\"))\n\n# A tibble: 51 × 28\n   fips  state_name          state Share with Any Debt …¹ Share with Any Debt …²\n   &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;                  &lt;chr&gt;                 \n 1 01    Alabama             AL    .3372881               .5016544              \n 2 02    Alaska              AK    .1672429               .221573               \n 3 04    Arizona             AZ    .2666938               .3900013              \n 4 05    Arkansas            AR    .3465793               .5426918              \n 5 06    California          CA    .2087713               .2462195              \n 6 08    Colorado            CO    .213803                .3554938              \n 7 09    Connecticut         CT    .2194708               .3829038              \n 8 10    Delaware            DE    .2866829               .469117               \n 9 11    District of Columb… DC    .2232908               .3485817              \n10 12    Florida             FL    .2893825               .3439322              \n# ℹ 41 more rows\n# ℹ abbreviated names: ¹​`Share with Any Debt in Collections, All`,\n#   ²​`Share with Any Debt in Collections, Communities of Color`\n# ℹ 23 more variables:\n#   `Share with Any Debt in Collections, Majority White Communities` &lt;chr&gt;,\n#   `Median Debt in Collections, All` &lt;chr&gt;,\n#   `Median Debt in Collections, Communities of Color` &lt;chr&gt;, …\n\n\nread_excel() has several useful arguments:\n\nsheet selects the sheet to read.\nrange selects the cells to read and can use Excel-style ranges like “C34:D50”.\nskip skips the selected number of rows.\nn_max selects the maximum number of rows to read.\n\nExcel encourages bad habits and untidy data, so these arguments are useful for extracting data from messy Excel workbooks.\nreadxl_example() contains a perfect example. The workbook contains two sheets, which we can see with excel_sheets().\n\nreadxl_example(\"clippy.xlsx\") |&gt;\n  excel_sheets()\n\n[1] \"list-column\"    \"two-row-header\"\n\n\nAs is common with many Excel workbooks, the second sheet contains a second row of column names with parenthetical comments about each column.1\n\nreadxl_example(\"clippy.xlsx\") |&gt;  \n  read_excel(sheet = \"two-row-header\")\n\n# A tibble: 2 × 4\n  name       species              death                 weight    \n  &lt;chr&gt;      &lt;chr&gt;                &lt;chr&gt;                 &lt;chr&gt;     \n1 (at birth) (office supply type) (date is approximate) (in grams)\n2 Clippy     paperclip            39083                 0.9       \n\n\nThis vignette suggests a simple solution to this problem.\n\n# extract the column names\ncol_names &lt;- readxl_example(\"clippy.xlsx\") |&gt;  \n  read_excel(sheet = \"two-row-header\", n_max = 0) |&gt;\n  names()\n\n# load the data and add the column names\nreadxl_example(\"clippy.xlsx\") |&gt;  \n    read_excel(\n      sheet = \"two-row-header\", \n      skip = 2,\n      col_names = col_names\n    )\n\n# A tibble: 1 × 4\n  name   species   death               weight\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dttm&gt;               &lt;dbl&gt;\n1 Clippy paperclip 2007-01-01 00:00:00    0.9\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nThe IRS Statistics of Income Division is one of the US’s 13 principal statistical agencies. They publish rich information derived from tax returns. We will focus on Table 1, Adjusted Gross Income (AGI) percentiles by state.\n\nRead in the 52 cells in the first column that contain “United States”, all 50 states, and the “District of Columbia”.\nIdentify the cells containing data for “Adjusted gross income floor on percentiles”. Read in the data with read_excel(). Either programmatically read in the column names (i.e. “Top 1 Percent”, …) or assign them with col_names().\nUse bind_cols() to combine the data from step 1 and step 2.\n\n\n\nlibrary(tidyxl) contains tools for working with messy Excel workbooks, library(openxlsx) contains tools for creating Excel workbooks with R, and library(googlesheets4) contains tools for working with Google Sheets.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#sec-tidy",
    "href": "03_advanced-data-cleaning.html#sec-tidy",
    "title": "3  Advanced Data Cleaning",
    "section": "3.3 Tidy",
    "text": "3.3 Tidy\nThe defining opinion of the tidyverse is its wholehearted adoption of tidy data. Tidy data has three features:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a dataframe.\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. ~ Hadley Wickham\n\nlibrary(tidyr) is the main package for tidying untidy data. We’ll practice some skills using examples from three workbooks from the IRS SOI.\npivot_longer() is commonly used for tidying data and for making data longer for library(ggplot2). pivot_longer() reorients data so that key-value pairs expressed as column name-column value are column value-column value in adjacent columns. pivot_longer() has three essential arguments:\n\ncols is a vector of columns to pivot (or not pivot).\nnames_to is a string for the name of the column where the old column names will go (i.e. “series” in the figure).\nvalues_to is a string for the name of the column where the values will go (i.e. “rate” in the figure).\n\n\n\n\n\n\n\n\n\n\npivot_wider() is the inverse of pivot_longer().\n\nTidying Example 1\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable1 &lt;- tribble(\n  ~state, ~agi2006, ~agi2016, ~agi2020,\n  \"Alabama\", 95067, 114510, 138244,\n  \"Alaska\", 17458, 23645, 26445,\n  \"Arizona\", 146307, 181691, 245258\n)\n\ntable1\n\n# A tibble: 3 × 4\n  state   agi2006 agi2016 agi2020\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Alabama   95067  114510  138244\n2 Alaska    17458   23645   26445\n3 Arizona  146307  181691  245258\n\n\n\n\nYear is a variable. This data is untidy because year is included in the column names.\n\ntable1 &lt;- tribble(\n  ~state, ~agi2006, ~agi2016, ~agi2020,\n  \"Alabama\", 95067, 114510, 138244,\n  \"Alaska\", 17458, 23645, 26445,\n  \"Arizona\", 146307, 181691, 245258\n)\n\ntable1\n\n# A tibble: 3 × 4\n  state   agi2006 agi2016 agi2020\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Alabama   95067  114510  138244\n2 Alaska    17458   23645   26445\n3 Arizona  146307  181691  245258\n\npivot_longer(\n  data = table1, \n  cols = -state, \n  names_to = \"year\", \n  values_to = \"agi\"\n)\n\n# A tibble: 9 × 3\n  state   year       agi\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama agi2006  95067\n2 Alabama agi2016 114510\n3 Alabama agi2020 138244\n4 Alaska  agi2006  17458\n5 Alaska  agi2016  23645\n6 Alaska  agi2020  26445\n7 Arizona agi2006 146307\n8 Arizona agi2016 181691\n9 Arizona agi2020 245258\n\n\nThe year column isn’t useful yet. We’ll fix that later.\n\n\n\n\nlibrary(tidyr) contains several functions to split values into multiple cells.\n\nseparate_wider_delim() separates a value based on a delimeter and creates wider data.\nseparate_wider_position() separates a value based on position and creates wider data.\nseparate_longer_delim() separates a value based on a delimeter and creates longer data.\nseparate_longer_position() separates a value based on position and creates longer data.\n\n\n\nTidying Example 2\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable2 &lt;- tribble(\n  ~state, ~`agi2006|2016|2020`,\n  \"Alabama\", \"95067|114510|138244\",\n  \"Alaska\", \"17458|23645|26445\",\n  \"Arizona\", \"146307|181691|245258\"\n)\n\ntable2\n\n# A tibble: 3 × 2\n  state   `agi2006|2016|2020` \n  &lt;chr&gt;   &lt;chr&gt;               \n1 Alabama 95067|114510|138244 \n2 Alaska  17458|23645|26445   \n3 Arizona 146307|181691|245258\n\n\n\n\nThe values for 2006, 2016, and 2020 are all squished into one cell.\n\ntable2 &lt;- tribble(\n  ~state, ~`agi2006|2016|2020`,\n  \"Alabama\", \"95067|114510|138244\",\n  \"Alaska\", \"17458|23645|26445\",\n  \"Arizona\", \"146307|181691|245258\"\n)\n\ntable2\n\n# A tibble: 3 × 2\n  state   `agi2006|2016|2020` \n  &lt;chr&gt;   &lt;chr&gt;               \n1 Alabama 95067|114510|138244 \n2 Alaska  17458|23645|26445   \n3 Arizona 146307|181691|245258\n\nseparate_wider_delim(\n  data = table2, \n  cols = `agi2006|2016|2020`, \n  delim = \"|\",\n  names = c(\"2006\", \"2016\", \"2020\")\n) |&gt;\n  pivot_longer(\n    cols = -state,\n    names_to = \"year\", \n    values_to = \"agi\"\n  )\n\n# A tibble: 9 × 3\n  state   year  agi   \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; \n1 Alabama 2006  95067 \n2 Alabama 2016  114510\n3 Alabama 2020  138244\n4 Alaska  2006  17458 \n5 Alaska  2016  23645 \n6 Alaska  2020  26445 \n7 Arizona 2006  146307\n8 Arizona 2016  181691\n9 Arizona 2020  245258\n\n\n\n\n\n\nbind_rows() combines data frames by stacking the rows.\n\none &lt;- tribble(\n  ~id, ~var,\n  \"1\", 3.14,\n  \"2\", 3.15,\n)\n\ntwo &lt;- tribble(\n  ~id, ~var,\n  \"3\", 3.16,\n  \"4\", 3.17,\n)\n\nbind_rows(one, two)\n\n# A tibble: 4 × 2\n  id      var\n  &lt;chr&gt; &lt;dbl&gt;\n1 1      3.14\n2 2      3.15\n3 3      3.16\n4 4      3.17\n\n\nbind_cols() combines data frames by appending columns.\n\nthree &lt;- tribble(\n  ~id, ~var1,\n  \"1\", 3.14,\n  \"2\", 3.15,\n)\n\nfour &lt;- tribble(\n  ~id, ~var2,\n  \"1\", 3.16,\n  \"2\", 3.17,\n)\n\nbind_cols(three, four)\n\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...3`\n\n\n# A tibble: 2 × 4\n  id...1  var1 id...3  var2\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 1       3.14 1       3.16\n2 2       3.15 2       3.17\n\n\nWhen possible, we recommend using relational joins like left_join() to combine by columns because it is easy to miss-align rows with bind_cols().\n\nleft_join(\n  x = three,\n  y = four,\n  by = \"id\"\n)\n\n# A tibble: 2 × 3\n  id     var1  var2\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1      3.14  3.16\n2 2      3.15  3.17\n\n\n\n\nTidying Example 3\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable3_2006 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", \"95067\",\n  \"Alaska\", \"17458\",\n  \"Arizona\", \"146307\"\n)\n\ntable3_2006\n\n# A tibble: 3 × 2\n  state   agi   \n  &lt;chr&gt;   &lt;chr&gt; \n1 Alabama 95067 \n2 Alaska  17458 \n3 Arizona 146307\n\ntable3_2016 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", \"114510\",\n  \"Alaska\", \"23645\",\n  \"Arizona\", \"181691\"\n)\n\ntable3_2016\n\n# A tibble: 3 × 2\n  state   agi   \n  &lt;chr&gt;   &lt;chr&gt; \n1 Alabama 114510\n2 Alaska  23645 \n3 Arizona 181691\n\ntable3_2020 &lt;- tribble(\n  ~state, ~`agi`,\n  \"Alabama\", \"138244\",\n  \"Alaska\", \"26445\",\n  \"Arizona\", \"245258\"\n)\n\ntable3_2020\n\n# A tibble: 3 × 2\n  state   agi   \n  &lt;chr&gt;   &lt;chr&gt; \n1 Alabama 138244\n2 Alaska  26445 \n3 Arizona 245258\n\n\n\n\nThe variable year is contained in the data set names. The .id argument in bind_rows() allows us to create the year variable.\n\ntable3_2006 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 95067,\n  \"Alaska\", 17458,\n  \"Arizona\", 146307\n)\n\ntable3_2006\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama  95067\n2 Alaska   17458\n3 Arizona 146307\n\ntable3_2016 &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 114510,\n  \"Alaska\", 23645,\n  \"Arizona\", 181691\n)\n\ntable3_2016\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama 114510\n2 Alaska   23645\n3 Arizona 181691\n\ntable3_2020 &lt;- tribble(\n  ~state, ~`agi`,\n  \"Alabama\", 138244,\n  \"Alaska\", 26445,\n  \"Arizona\", 245258\n)\n\ntable3_2020\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama 138244\n2 Alaska   26445\n3 Arizona 245258\n\nbind_rows(\n  `2006` = table3_2006,\n  `2016` = table3_2016,\n  `2020` = table3_2020,\n  .id = \"year\"\n)\n\n# A tibble: 9 × 3\n  year  state      agi\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 2006  Alabama  95067\n2 2006  Alaska   17458\n3 2006  Arizona 146307\n4 2016  Alabama 114510\n5 2016  Alaska   23645\n6 2016  Arizona 181691\n7 2020  Alabama 138244\n8 2020  Alaska   26445\n9 2020  Arizona 245258\n\n\n\n\n\n\nRelational joins are fundamental to working with tidy data. Tidy data can only contain one unit of observation (e.g. county or state not county and state). When data exist on multiple levels, they must be stored in separate tables that can later be combined.\n\n\nTidying Example 4\n\nUntidyCleaned\n\n\nWhy aren’t the data tidy?\n\ntable4a &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 95067,\n  \"Alaska\", 17458,\n  \"Arizona\", 146307\n)\n\ntable4a\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama  95067\n2 Alaska   17458\n3 Arizona 146307\n\ntable4b &lt;- tribble(\n  ~state, ~returns,\n  \"Alabama\", 1929941,\n  \"Alaska\", 322369,\n  \"Arizona\", 2454951\n)\n\ntable4b\n\n# A tibble: 3 × 2\n  state   returns\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Alabama 1929941\n2 Alaska   322369\n3 Arizona 2454951\n\n\n\n\nThese data are tidy! But keeping the data in two separate data frames may not make sense. Let’s use full_join() to combine the data and anti_join() to see if there are mismatches.\n\ntable4a &lt;- tribble(\n  ~state, ~agi,\n  \"Alabama\", 95067,\n  \"Alaska\", 17458,\n  \"Arizona\", 146307\n)\n\ntable4a\n\n# A tibble: 3 × 2\n  state      agi\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Alabama  95067\n2 Alaska   17458\n3 Arizona 146307\n\ntable4b &lt;- tribble(\n  ~state, ~returns,\n  \"Alabama\", 1929941,\n  \"Alaska\", 322369,\n  \"Arizona\", 2454951\n)\n\ntable4b\n\n# A tibble: 3 × 2\n  state   returns\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Alabama 1929941\n2 Alaska   322369\n3 Arizona 2454951\n\nfull_join(table4a, table4b, by = \"state\")\n\n# A tibble: 3 × 3\n  state      agi returns\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 Alabama  95067 1929941\n2 Alaska   17458  322369\n3 Arizona 146307 2454951\n\nanti_join(table4a, table4b, by = \"state\")\n\n# A tibble: 0 × 2\n# ℹ 2 variables: state &lt;chr&gt;, agi &lt;dbl&gt;\n\nanti_join(table4b, table4a, by = \"state\")\n\n# A tibble: 0 × 2\n# ℹ 2 variables: state &lt;chr&gt;, returns &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse pivot_longer() to make the SOI percentile data from the earlier exercise longer. After the transformation, there should be one row per percentile per state.\n\n\n\nTo see more examples, read the tidy data section in R for Data Science (2e)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#sec-transform",
    "href": "03_advanced-data-cleaning.html#sec-transform",
    "title": "3  Advanced Data Cleaning",
    "section": "3.4 Transform",
    "text": "3.4 Transform\n\n3.4.1 Strings\nCheck out the stringr cheat sheet.\nlibrary(stringr) contains powerful functions for working with strings in R. In data analysis, we may need to detect matches, subset strings, work with the lengths of strings, modify strings, and join and split strings.\n\nDetecting Matches\nstr_detect() is useful for detecting matches in strings, which can be useful with filter(). Consider the executive orders data set and suppose we want to return executive orders that contain the word \"Virginia\".\n\neos &lt;- read_csv(here(\"data\", \"executive-orders.csv\")) |&gt;\n  filter(!is.na(text)) |&gt;\n  group_by(executive_order_number) |&gt;\n  summarize(text = list(text)) |&gt;\n  mutate(text = map_chr(text, ~paste(.x, collapse = \" \")))\n\nRows: 196537 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): text, president\ndbl  (1): executive_order_number\ndate (1): signing_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\neos\n\n# A tibble: 1,126 × 2\n   executive_order_number text                                                  \n                    &lt;dbl&gt; &lt;chr&gt;                                                 \n 1                  12890 \"Executive Order 12890 of December 30, 1993 Amendment…\n 2                  12944 \"Executive Order 12944 of December 28, 1994 Adjustmen…\n 3                  12945 \"Executive Order 12945 of January 20, 1995 Amendment …\n 4                  12946 \"Executive Order 12946 of January 20, 1995 President'…\n 5                  12947 \"Executive Order 12947 of January 23, 1995 Prohibitin…\n 6                  12948 \"Executive Order 12948 of January 30, 1995 Amendment …\n 7                  12949 \"Executive Order 12949 of February 9, 1995 Foreign In…\n 8                  12950 \"Executive Order 12950 of February 22, 1995 Establish…\n 9                  12951 \"Executive Order 12951 of February 22, 1995 Release o…\n10                  12952 \"Executive Order 12952 of February 24, 1995 Amendment…\n# ℹ 1,116 more rows\n\neos |&gt;\n  filter(str_detect(string = text, pattern = \"Virginia\"))\n\n# A tibble: 6 × 2\n  executive_order_number text                                                   \n                   &lt;dbl&gt; &lt;chr&gt;                                                  \n1                  13150 Executive Order 13150 of April 21, 2000 Federal Workfo…\n2                  13508 Executive Order 13508 of May 12, 2009 Chesapeake Bay P…\n3                  13557 Executive Order 13557 of November 4, 2010 Providing an…\n4                  13775 Executive Order 13775 of February 9, 2017 Providing an…\n5                  13787 Executive Order 13787 of March 31, 2017 Providing an O…\n6                  13934 Executive Order 13934 of July 3, 2020 Building and Reb…\n\n\n\n\nSubsetting Strings\nstr_sub() can subset strings based on positions within the string. Consider an example where we want to extract state FIPS codes from county FIPS codes.\n\ntibble(fips = c(\"01001\", \"02013\", \"04001\")) |&gt;\n  mutate(state_fips = str_sub(fips, start = 1, end = 2))\n\n# A tibble: 3 × 2\n  fips  state_fips\n  &lt;chr&gt; &lt;chr&gt;     \n1 01001 01        \n2 02013 02        \n3 04001 04        \n\n\n\n\nManaging Lengths\nstr_pad() is useful for managing lengths. Consider the common situation when zeros are dropped from the beginning of FIPS codes.\n\ntibble(fips = c(1, 2, 4)) |&gt;\n  mutate(fips = str_pad(fips, side = \"left\", pad = \"0\", width = 2))\n\n# A tibble: 3 × 1\n  fips \n  &lt;chr&gt;\n1 01   \n2 02   \n3 04   \n\n\n\n\nModifying Strings\nstr_replace(), str_replace_all(), str_remove(), and str_remove_all() can delete or modify parts of strings. Consider an example where we have course names and we want to delete everything except numeric digits.2\n\ntibble(course = c(\"PPOL 670\", \"GOVT 8009\", \"PPOL 6819\")) |&gt;\n  mutate(course = str_remove(course, pattern = \"[:alpha:]*\\\\s\"))\n\n# A tibble: 3 × 1\n  course\n  &lt;chr&gt; \n1 670   \n2 8009  \n3 6819  \n\n\nstr_c() and str_glue() are useful for joining strings. Consider an example where we want to “fill in the blank” with a variable in a data frame.\n\ntibble(fruit = c(\"apple\", \"banana\", \"cantelope\")) |&gt;\n  mutate(sentence = str_glue(\"my favorite fruit is {fruit}\"))\n\n# A tibble: 3 × 2\n  fruit     sentence                      \n  &lt;chr&gt;     &lt;glue&gt;                        \n1 apple     my favorite fruit is apple    \n2 banana    my favorite fruit is banana   \n3 cantelope my favorite fruit is cantelope\n\n\n\ntibble(fruit = c(\"apple\", \"banana\", \"cantelope\")) |&gt;\n  mutate(\n    another_sentence = \n      str_c(\"Who doesn't like a good \", fruit, \".\")\n    )\n\n# A tibble: 3 × 2\n  fruit     another_sentence                  \n  &lt;chr&gt;     &lt;chr&gt;                             \n1 apple     Who doesn't like a good apple.    \n2 banana    Who doesn't like a good banana.   \n3 cantelope Who doesn't like a good cantelope.\n\n\nThis workflow is useful for building up URLs when accessing APIs, scraping information from the Internet, and downloading many files.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nUse mutate() and library(stringr) to create a variable for year from the earlier SOI exercise. For instance, \"agi2006\" should be \"2006\".\nUse as.numeric() to convert the string from step 1 into a numeric value.\nCreate a data visualization with year on the x-axis.\n\n\n\n\n\n\n3.4.2 Factors {sec-factors}\nCheck out the forcats cheat sheet.\nMuch of our work focuses on four of the six types of atomic vectors: logical, integer, double, and character. R also contains augmented vectors like factors.\nFactors are categorical data stored as integers with a levels attribute. Character vectors often work well for categorical data and many of R’s functions convert character vectors to factors. This happens with lm().\nFactors have many applications:\n\nGiving the levels of a categorical variable non-alpha numeric order in a ggplot2 data visualization.\nRunning calculations on data with empty groups.\nRepresenting categorical outcome variables in classification models.\n\n\nFactor Basics\n\nx1 &lt;- factor(c(\"a\", \"a\", \"b\", \"c\"), levels = c(\"d\", \"c\", \"b\", \"a\"))\n\nx1\n\n[1] a a b c\nLevels: d c b a\n\nattributes(x1)\n\n$levels\n[1] \"d\" \"c\" \"b\" \"a\"\n\n$class\n[1] \"factor\"\n\nlevels(x1)\n\n[1] \"d\" \"c\" \"b\" \"a\"\n\n\nx1 has order but it isn’t ordinal. Sometimes we’ll come across ordinal factor variables, like with the diamonds data set. Unintentional ordinal variables can cause unexpected errors. For example, including ordinal data as predictors in regression models will lead to different estimated coefficients than other variable types.\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\nx2 &lt;- factor(\n  c(\"a\", \"a\", \"b\", \"c\"), \n  levels = c(\"d\", \"c\", \"b\", \"a\"),\n  ordered = TRUE\n)\n\nx2\n\n[1] a a b c\nLevels: d &lt; c &lt; b &lt; a\n\nattributes(x2)\n\n$levels\n[1] \"d\" \"c\" \"b\" \"a\"\n\n$class\n[1] \"ordered\" \"factor\" \n\nlevels(x2)\n\n[1] \"d\" \"c\" \"b\" \"a\"\n\n\nFigure 3.1 shows how we can use a factor to give a variable a non-alpha numeric order and preserve empty levels. In this case, February and March have zero tropical depressions, tropical storms, and hurricanes and we want to demonstrate that emptiness.\n# use case_match to convert integers into month names\nstorms &lt;- storms |&gt;\n  mutate(\n    month = case_match(\n      month,\n      1 ~ \"Jan\",\n      4 ~ \"Apr\",\n      5 ~ \"May\",\n      6 ~ \"Jun\",\n      7 ~ \"Jul\",\n      8 ~ \"Aug\",\n      9 ~ \"Sep\",\n      10 ~ \"Oct\",\n      11 ~ \"Nov\",\n      12 ~ \"Dec\"\n    )\n  )\n\n# create data viz without factors\nstorms |&gt;\n  count(month) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n# add factor variable\nmonths &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n            \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nstorms &lt;- storms |&gt;\n  mutate(month = factor(month, levels = months)) \n\n# create data viz with factors\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n(a) Figure without a factor\n\n\n\n\n\n\n\n\n\n\n\n(b) Figure with a factor\n\n\n\n\n\n\n\nFigure 3.1: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\nFactors also change the behavior of summary functions like count().\n\nstorms |&gt;\n  count(month)\n\n# A tibble: 10 × 2\n   month     n\n   &lt;fct&gt; &lt;int&gt;\n 1 Jan      70\n 2 Apr      66\n 3 May     201\n 4 Jun     809\n 5 Jul    1651\n 6 Aug    4442\n 7 Sep    7778\n 8 Oct    3138\n 9 Nov    1170\n10 Dec     212\n\nstorms |&gt;\n  count(month, .drop = FALSE)\n\n# A tibble: 12 × 2\n   month     n\n   &lt;fct&gt; &lt;int&gt;\n 1 Jan      70\n 2 Feb       0\n 3 Mar       0\n 4 Apr      66\n 5 May     201\n 6 Jun     809\n 7 Jul    1651\n 8 Aug    4442\n 9 Sep    7778\n10 Oct    3138\n11 Nov    1170\n12 Dec     212\n\n\nlibrary(forcats) simplifies many common operations on factor vectors.\n\n\nChanging Order\nfct_relevel(), fct_rev(), and fct_reorder() are useful functions for modifying the order of factor variables. Figure 3.2 demonstrates using fct_rev() to flip the order of a categorical axis in ggplot2.\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\nstorms |&gt;\n  mutate(month = fct_rev(month)) |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n(a) Descending\n\n\n\n\n\n\n\n\n\n\n\n(b) Ascending\n\n\n\n\n\n\n\nFigure 3.2: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\nFigure 3.3 orders the factor variable based on the number of observations in each category using fct_reorder(). fct_reorder() can order variables based on more sophisticated summaries than just magnitude. For example, it can order box-and-whisker plots based on the median or even something as arbitrary at the 60th percentile.\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  mutate(month = fct_reorder(.f = month, .x = n, .fun = median)) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n(a) Alpha-numeric\n\n\n\n\n\n\n\n\n\n\n\n(b) Magnitude\n\n\n\n\n\n\n\nFigure 3.3: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\n\n\nChanging Values\nFunctions like fct_recode() and fct_lump_min() are useful for changing factor variables. Figure 3.4 combines categories with fewer than 1,000 observations into an \"Other\" group.\nstorms |&gt;\n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\nstorms |&gt;\n  mutate(month = fct_lump_min(month, min = 1000)) |&gt;  \n  count(month, .drop = FALSE) |&gt;\n  ggplot(aes(x = n, y = month)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n(a) All\n\n\n\n\n\n\n\n\n\n\n\n(b) Lumped\n\n\n\n\n\n\n\nFigure 3.4: Hurricane Season Peaks in Late Summer and Early Fall\n\n\n\n\n\n\n3.4.3 Dates and Date-Times\nCheck out the lubridate cheat sheet.\nThere are many ways to store dates.\n\nMarch 14, 1992\n03/14/1992\n14/03/1992\n14th of March ’92\n\nOne way of storing dates is the best. The ISO 8601 date format is an international standard with appealing properties like fixed lengths and self ordering. The format is YYYY-MM-DD.\nlibrary(lubridate) has useful functions that will take dates of any format and convert them to the ISO 8601 standard.\n\nlibrary(lubridate)\n\nmdy(\"March 14, 1992\")\n\n[1] \"1992-03-14\"\n\nmdy(\"03/14/1992\")\n\n[1] \"1992-03-14\"\n\ndmy(\"14/03/1992\")\n\n[1] \"1992-03-14\"\n\ndmy(\"14th of March '92\")\n\n[1] \"1992-03-14\"\n\n\nThese functions return variables of class \"Date\".\n\nclass(mdy(\"March 14, 1992\"))\n\n[1] \"Date\"\n\n\nlibrary(lubridate) also contains functions for parsing date times into ISO 8601 standard. Times are slightly trickier because of time zones.\n\nmdy_hms(\"12/02/2021 1:00:00\")\n\n[1] \"2021-12-02 01:00:00 UTC\"\n\nmdy_hms(\"12/02/2021 1:00:00\", tz = \"EST\")\n\n[1] \"2021-12-02 01:00:00 EST\"\n\nmdy_hms(\"12/02/2021 1:00:00\", tz = \"America/Chicago\")\n\n[1] \"2021-12-02 01:00:00 CST\"\n\n\nBy default, library(lubridate) will put the date times in Coordinated Universal Time (UTC), which is the successor to Greenwich Mean Time (GMT). I recommend carefully reading the data dictionary if time zones are important for your analysis or if your data cross time zones. This is especially important during time changes (e.g. “spring forward” and “fall back”).\nFortunately, if you encode your dates or date-times correctly, then library(lubridate) will automatically account for time changes, time zones, leap years, leap seconds, and all of the quirks of dates and times.\n\n\n\n\n\n\nExercise 5\n\n\n\n\ndates &lt;- tribble(\n  ~date,\n  \"12/01/1987\",\n  \"12/02/1987\",\n  \"12/03/1987\"\n)\n\n\nCreate the dates data from above with tribble().\nUse mutate() to convert the date column to the ISO 8601 standard (YYYY-MM-DD).\n\n\n\n\nExtracting Components\nlibrary(lubridate) contains functions for extracting components from dates like the year, month, day, and weekday. Conisder the follow data set about full moons in Washington, DC in 2023.\n\nfull_moons &lt;- tribble(\n  ~full_moon,\n  \"2023-01-06\",\n  \"2023-02-05\",\n  \"2023-03-07\",\n  \"2023-04-06\",\n  \"2023-05-05\",\n  \"2023-06-03\",\n  \"2023-07-03\",\n  \"2023-08-01\",\n  \"2023-08-30\",\n  \"2023-09-29\",\n  \"2023-10-28\",\n  \"2023-11-27\",\n  \"2023-12-26\"\n) |&gt;\n  mutate(full_moon = as_date(full_moon))\n\nSuppose we want to know the weekday of each full moon.\n\nfull_moons |&gt;\n  mutate(week_day = wday(full_moon, label = TRUE))\n\n# A tibble: 13 × 2\n   full_moon  week_day\n   &lt;date&gt;     &lt;ord&gt;   \n 1 2023-01-06 Fri     \n 2 2023-02-05 Sun     \n 3 2023-03-07 Tue     \n 4 2023-04-06 Thu     \n 5 2023-05-05 Fri     \n 6 2023-06-03 Sat     \n 7 2023-07-03 Mon     \n 8 2023-08-01 Tue     \n 9 2023-08-30 Wed     \n10 2023-09-29 Fri     \n11 2023-10-28 Sat     \n12 2023-11-27 Mon     \n13 2023-12-26 Tue     \n\n\n\n\nMath\nlibrary(lubridate) easily handles math with dates and date-times. Suppose we want to calculate the number of days since American Independence Day:\n\ntoday() - as_date(\"1776-07-04\")\n\nTime difference of 90666 days\n\n\nIn this case, subtraction creates an object of class difftime represented in days. We can use the difftimes() function to calculate differences in other units.\n\ndifftime(today(), as_date(\"1776-07-04\"), units = \"mins\")\n\nTime difference of 130559040 mins\n\n\n\n\nPeriods\nPeriods track clock time or a calendar time. We use periods when we set a recurring meetings on a calendar and when we set an alarm to wake up in the morning.\nThis can lead to some interesting results. Do we always add 365 days when we add 1 year to a date? With periods, this isn’t true. Sometimes we add 366 days during leap years. For example,\n\nstart &lt;- as_date(\"1999-03-14\")\nend &lt;- start + years(1)\n\nend\n\n[1] \"2000-03-14\"\n\nend - start\n\nTime difference of 366 days\n\n\n\n\nDurations\nDurations track the passage of physical time in exact seconds. Durations are like sand falling into an hourglass. Duration functions start with d like dyears() and dminutes().\n\nstart &lt;- as_date(\"1999-03-14\")\nend &lt;- start + dyears(1)\n\nend\n\n[1] \"2000-03-13 06:00:00 UTC\"\n\n\nNow we always add 365 days, but we see that March 13th is one year after March 14th.\n\n\nIntervals\nUntil now, we’ve focused on points in time. Intervals have length and have a starting point and an ending point.\nSuppose classes start on August 23rd and proceed every week for a while. Do any of these dates conflict with Georgetown’s fall break?\n\nclasses &lt;- as_date(\"2023-08-23\") + weeks(0:15)\n\nfall_break &lt;- interval(as_date(\"2023-11-22\"), as_date(\"2023-11-26\"))\n\nclasses %within% fall_break\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE  TRUE FALSE FALSE\n\n\nWe focused on dates, but many of the same principles hold for date-times.\n\n\n\n\n\n\nExercise 6\n\n\n\n\nCreate a date object for your birth date.\nCalculate the number of days since your birth date.\nCreate a vector of your birthdays from your birth date for the next 120 years. Do you use periods or durations?\n\n\n\n\n\n\n3.4.4 Missing Data\nMissing data are ever present in data analysis. R stores missing values as NA, which are contagious and are fortunately difficult to ignore.\nreplace_na() is the quickest function to replace missing values. It is a shortcut for a specific instance of if_else().\n\nx &lt;- c(1, NA, 3)\n\nif_else(condition = is.na(x), true = 2, false = x)\n\n[1] 1 2 3\n\nreplace_na(x, replace = 2)\n\n[1] 1 2 3\n\n\nWe recommend avoiding arguments like na.rm and using filter() for structurally missing values and replace_na() or imputation for nonresponse. We introduce more sophisticated methods to handle missing data in a later chapter.\n\n\n\n\n\n\nExercise 7\n\n\n\nLet’s focus on different data shared by SOI. Now we’ll focus on individual income and tax data by state.\nThis Excel workbook is a beast. For instance, it isn’t clear how the hierarchy works. I expected all of the rows nested under “Number of returns” to sum up to the number of returns. Unfortunately, the rows are not disjoint. Also, the merged cells for column headers are very difficult to use with programming languages.\n\nStart with 20in01al.xlsx.\nCreate a tidy data frame with rows 10 through 12 (“Number of single returns”, “Number of joint returns”, and “Number of head of household returns”) disaggregated by “size of adjusted gross income”.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#conclusion",
    "href": "03_advanced-data-cleaning.html#conclusion",
    "title": "3  Advanced Data Cleaning",
    "section": "3.5 Conclusion",
    "text": "3.5 Conclusion\nKey takeaways from this chapter are:\n\nhere is a helpful package for reading data, and it is especially helpful when using .Rprojs.\nreadxl is great for reading excel files into R.\npivot_wider() and pivot_longer() are functions from the tidyr package that are useful for reshaping data into a “tidy” format.\nThe tidyverse also contains useful package for handing strings (stringr) factors (forecats), and dates (lubridate).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "03_advanced-data-cleaning.html#footnotes",
    "href": "03_advanced-data-cleaning.html#footnotes",
    "title": "3  Advanced Data Cleaning",
    "section": "",
    "text": "The instinct to include these comments is good. The execution is poor because it creates big headaches for people using programming languages. We suggest using a data dictionary instead.↩︎\nThis example uses regular expressions (regex). Visit R4DS (2e) for a review of regex.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced Data Cleaning</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html",
    "href": "04_data-viz.html",
    "title": "4  Data Visualization with ggplot2",
    "section": "",
    "text": "4.1 Motivation",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#motivation",
    "href": "04_data-viz.html#motivation",
    "title": "4  Data Visualization with ggplot2",
    "section": "",
    "text": "Data visualization is exploratory data analysis (EDA)\nData visualization is diagnosis and validation\nData visualization is communication",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#motivation-going-beyond-excel",
    "href": "04_data-viz.html#motivation-going-beyond-excel",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.2 Motivation (going beyond Excel)",
    "text": "4.2 Motivation (going beyond Excel)\n\nFlexibility\nReproducibility\nScalability\nRelational data vs. positional data",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#background",
    "href": "04_data-viz.html#background",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.3 Background",
    "text": "4.3 Background\n\nThe toughest part of data visualization is data munging.\nData frames are the only appropriate input for library(ggplot2).\n\nggplot2 is an R package for data visualization that was developed during Hadley Wickham’s graduate studies at Iowa State University. ggplot2 is formalized in A Layered Grammar of Graphics (Wickham 2010).\nThe grammar of graphics, originally by Leland Wilkinson, is a theoretical framework that breaks all data visualizations into their component pieces. With the layered grammar of graphics, Wickham extends Wilkinson’s grammar of graphics and implements it in R. The cohesion is impressive, and the theory flows to the code which informs the data visualization process in a way not reflected in any other data viz tool.\nThere are eight main ingredients to the grammar of graphics. We will work our way through the ingredients with many hands-on examples.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nOpen your .Rproj.\nCreate a new .R script in your directory called 03_data-visualization.R.\nType (don’t copy & paste) the following code below library(tidyverse) in 03_data-visualization.R.\n\n\nggplot(data = storms) + \n  geom_point(mapping = aes(x = pressure, y = wind))\n\n\nAdd a comment above the ggplot2 code that describes the plot we created.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#eight-ingredients-in-the-grammar-of-graphics",
    "href": "04_data-viz.html#eight-ingredients-in-the-grammar-of-graphics",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.4 Eight Ingredients in the Grammar of Graphics:",
    "text": "4.4 Eight Ingredients in the Grammar of Graphics:\n\n4.4.1 Data\n\n\n\n\n\n\nTip\n\n\n\nData are the values represented in the visualization.\n\n\nggplot(data = ) or data |&gt; ggplot()\n\nstorms |&gt;\n  select(name, year, category, lat, long, wind, pressure) |&gt;\n  sample_n(10) |&gt;\n  kable()\n\n\n\n\nname\nyear\ncategory\nlat\nlong\nwind\npressure\n\n\n\n\nKate\n2021\nNA\n22.6\n-50.9\n35\n1006\n\n\nCandice\n1976\n1\n44.4\n-51.2\n80\n971\n\n\nOphelia\n2011\nNA\n18.4\n-59.8\n40\n1001\n\n\nIda\n2015\nNA\n13.2\n-36.4\n25\n1007\n\n\nKaren\n2001\n1\n37.9\n-64.0\n65\n985\n\n\nLili\n2002\nNA\n16.1\n-74.6\n35\n1003\n\n\nFrederic\n1979\nNA\n11.7\n-39.7\n45\n1000\n\n\nClaudette\n1991\nNA\n35.2\n-30.7\n25\n1015\n\n\nDennis\n2005\nNA\n38.1\n-86.4\n10\n1012\n\n\nMichael\n2012\nNA\n27.2\n-39.2\n25\n1015\n\n\n\n\n\n\n\n4.4.2 Aesthetic Mappings:\n\n\n\n\n\n\nTip\n\n\n\nAesthetic mappings are directions for how data are mapped in a plot in a way that we can perceive. Aesthetic mappings include linking variables to the x-position, y-position, color, fill, shape, transparency, and size.\n\n\naes(x = , y = , color = )\nX or Y\n\n\n\n\n\n\n\n\n\nContinuous Color or Fill\n\n\n\n\n\n\n\n\n\nDiscrete Color or Fill\n\n\n\n\n\n\n\n\n\nSize\n\n\n\n\n\n\n\n\n\nShape\n\n\n\n\n\n\n\n\n\nOthers: transparency, line type\n\n\n4.4.3 Geometric Objects:\n\n\n\n\n\n\nTip\n\n\n\nGeometric objects are representations of the data, including points, lines, and polygons.\n\n\ngeom_bar() or geom_col()\nPlots are often called their geometric object(s).\n\n\n\n\n\n\n\n\n\ngeom_line()\n\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\ngeom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nDuplicate the code from Exercise 1. Add comments below the data visualization code that describes the argument or function that corresponds to each of the first three components of the grammar of graphics.\nInside aes(), add color = category. Run the code.\nReplace color = category with color = \"green\". Run the code. What changed? Is this unexpected?\nRemove color = \"green\" from aes() and add it inside inside of geom_point() but outside of aes(). Run the code.\nThis is a little cluttered. Add alpha = 0.2 inside geom_point() but outside of aes().\n\n\n\nAesthetic mappings like x and y almost always vary with the data. Aesthetic mappings like color, fill, shape, transparency, and size can vary with the data. But those arguments can also be added as styles that don’t vary with the data. If you include those arguments in aes(), they will show up in the legend (which can be annoying! and is also a sign that something should be changed!).\n\n\n\n\n\n\nExercise 3\n\n\n\n\nCreate a new scatter plot using the msleep data set. Use bodywt on the x-axis and sleep_total on the y-axis.\nThe y-axis doesn’t contain zero. Below geom_point(), add scale_y_continuous(limits = c(0, NA)). Hint: add + after geom_point().\nThe x-axis is clustered near zero. Add scale_x_log10() above scale_y_continuous(limits = c(0, NA)).\n\n\n\n\n\n4.4.4 Scales:\n\n\n\n\n\n\nTip\n\n\n\nScales turn data values, which are continuous, discrete, or categorical into aesthetic values. scale_*_*() functions control the specific behaviors of aesthetic mappings. This includes not only the x-axis and y-axis, but the ranges of sizes, types of shapes, and specific colors of aesthetics.\n\n\n\nBefore\nscale_x_continuous()\n\n\n\n\n\n\n\n\n\n\n\nAfter\nscale_x_reverse()\n\n\n\n\n\n\n\n\n\n\n\nBefore\nscale_size_continuous(breaks = c(25, 75, 125))\n\n\n\n\n\n\n\n\n\n\n\nAfter\nscale_size_continuous(range = c(0.5, 20), breaks = c(25, 75, 125))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nType the following code in your script.\n\n\ndata &lt;- tibble(x = 1:10, y = 1:10)\nggplot(data = data) +\n  geom_blank(mapping = aes(x = x, y = y))\n\n\nAdd coord_polar() to your plot.\nAdd labs(title = \"Polar coordinate system\") to your plot.\n\n\n\n\n\n\n4.4.5 Coordinate Systems:\n\n\n\n\n\n\nTip\n\n\n\nCoordinate systems map scaled geometric objects to the position of objects on the plane of a plot. The two most popular coordinate systems are the Cartesian coordinate system and the polar coordinate system.\n\n\n\n\n\n\n\n\n\n\n\ncoord_polar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCreate a scatter plot of the storms data set with pressure on the x-axis and wind on the y-axis.\nAdd facet_wrap(~ category)\n\n\n\n\n\n4.4.6 Facets:\n\n\n\n\n\n\nTip\n\n\n\nFacets (optional) break data into meaningful subsets.\n\n\nFaceting breaks data visualizations into meaningful subsets using a variable or variables in the data set. This type of visualization is sometimes called small multiples.\nfacet_wrap small multiples, in order, until the variable is exhausted. It does not provide much macro structure to the visualization. facet_grid creates a macro structure where each panel represents the combination of one level on the maco x-axis with one level on the macro y-axis.1\nYou can see a helpful chart illustrating these differences here.\n\nFacet wrap\nfacet_wrap(~ category)\n\n\n\n\n\n\n\n\n\n\n\nFacet grid\nfacet_grid(month ~ year)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\n\nAdd the following code to your script. Submit it!\n\n\nggplot(storms) +\n  geom_bar(mapping = aes(x = category))\n\n\n\n\n\n\n4.4.7 Statistical Transformations:\n\n\n\n\n\n\nTip\n\n\n\nStatistical transformations (optional) transform the data, typically through summary statistics and functions, before aesthetic mapping.\nBefore transformations, each observation in data is represented by one geometric object (i.e. a scatter plot). After a transformation, a geometric object can represent more than one observation (i.e. a bar in a histogram).\n\n\nNote: geom_bar() performs statistical transformation. Use geom_col() to create a column chart with bars that encode individual observations in the data set.\n\n\n\n\n\n\nExercise 7\n\n\n\n\nDuplicate Exercise 6.\nAdd theme_minimal() to the plot.\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\n\nDuplicate Exercise 6.\nRun install.packages(\"remotes\") and remotes::install_github(\"UrbanInstitute/urbnthemes\") in the console.\nIn the lines preceding the chart add and run the following code:\n\n\nlibrary(urbnthemes)\nset_urbn_defaults(style = \"print\")\n\n\nRun the code to make the chart.\nAdd scale_y_continuous(expand = expansion(mult = c(0, 0.1))) and rerun the code.\n\n\n\n\n\n4.4.8 Themes:\n\n\n\n\n\n\nNote\n\n\n\nThemes control the visual style of plots with font types, font sizes, background colors, margins, and positioning.\n\n\n\nDefault theme\n\n\n\n\n\n\n\n\n\n\n\nfivethirtyeight theme\n\n\n\n\n\n\n\n\n\n\n\nurbnthemes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\n\nAdd the following exercise to you script. Run it!\n\n\nstorms |&gt;  \n  filter(category &gt; 0) |&gt;\n  distinct(name, year) |&gt;\n  count(year) |&gt;\n  ggplot() + \n  geom_line(mapping = aes(x = year, y = n))\n\n\nAdd geom_point() after geom_line() with the same aesthetic mappings.\n\n\n\n\n\n\n4.4.9 Layers (bonus!):\n\n\n\n\n\n\nNote\n\n\n\nLayers allow for multiple geometric objects to be plotted in the same data visualization.\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\n\nAdd the following exercise to you script. Run it!\n\n\nggplot(data = storms, mapping = aes(x = pressure, y = wind)) + \n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n4.4.10 Inheritances (bonus!):\n\n\n\n\n\n\nNote\n\n\n\nInheritances pass aesthetic mappings from ggplot() to later geom_*() functions.\n\n\nNotice how the aesthetic mappings are passed to ggplot() in example 10. This is useful when using layers!\n\n\n\n\n\n\nExercise 11\n\n\n\n\nPick your favorite plot from exercises 1 through 10 and duplicate the code.\nAdd ggsave(filename = \"favorite-plot.png\") on a new line without + and then save the file. Look at the saved file.\nAdd width = 6 and height = 4 to ggsave(). Run the code and then look at the saved file.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#summary",
    "href": "04_data-viz.html#summary",
    "title": "4  Data Visualization with ggplot2",
    "section": "4.5 Summary",
    "text": "4.5 Summary\n\n4.5.1 Functions\nThis is a summary of the functions we discussed in this chapter. While by no means comprehensive, these are an excellent starting point to visualizing data using ggplot2.\n\nggplot()\naes()\ngeom_*()\n\ngeom_point()\ngeom_line()\ngeom_col()\n\n\nscale_*()\n\nscale_y_continuous()\n\ncoord_*()\nfacet_*()\nlabs()\n\n\n\n4.5.2 Theory\n\nData\nAesthetic mappings\nGeometric objects\nScales\nCoordinate systems\nFacets\nStatistical transformations\nTheme\n\n\n\n4.5.3 Resources\n\nUrban Institute R Users Group website\nWhy the Urban Institute visualizes data with ggplot2\nR for Data Science: data visualization\nggplot2: ELegant Graphics for Data Analysis (3e)\nawunderground themes\nR Graph Gallery\n\n\n\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "04_data-viz.html#footnotes",
    "href": "04_data-viz.html#footnotes",
    "title": "4  Data Visualization with ggplot2",
    "section": "",
    "text": "facet_geo organizes charts in a way that attempts to preserve some geographic component of the data. It is beyond the scope of this course. You can learn more at the geofacet package vignette website.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html",
    "href": "05_exploratory-data-analysis.html",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "5.1 Reading in Data\nWe’ve already covered reading in csv files using the read_csv() function from the readr package, but you may also need to read in data that is stored in a number of other common file formats:",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#reading-in-data",
    "href": "05_exploratory-data-analysis.html#reading-in-data",
    "title": "5  Exploratory Data Analysis",
    "section": "",
    "text": "5.1.1 Excel Spreadsheets\nWe introduced readxl in Section 3.2.2. As a reminder, it is not a core tidyverse package, so it needs to be explicitly loaded in each R session.\nMany excel files can be read with the simple syntax data &lt;- read_excel(path = \"relative/file/path/to/data\"). In cases where the Excel spreadsheet contains multiple sheets, you can use the sheet argument to specify the sheet to read as a string (the name of the sheet) or an integer (the position of the sheet). You can also read only components of a sheet using Excel-style cell-ranges (ex: A3:L44).\n\n\n5.1.2 STATA, SAS, and SPSS files\nhaven is a tidyverse package for reading data from SAS (read_sas()), STATA (read_dta()), and SPSS (read_sav()) files. Like the readxl package, it is not a core tidyverse package and also needs to be explicitly loaded in each R session.\nNote that the haven package can only read and write STATA .dta files through version 15 of STATA. For files created in more recent versions of STATA, the readstat13 package’s read.dta13 file can be used.\n\n\n5.1.3 Zip Files\nYou may also want to read in data that is saved in a zip file. In order to do this, you can use the unzip() function to unzip the files using the following syntax: unzip(zipfile = \"path/to/zip/file\", exdir = \"path/to/directory/for/unzipped/files\").\nOften times, you may want to read in a zip file from a website into R. In order to do this, you will need to first download the zip file to your computer using the download.file() function, unzip the file using unzip() and then read in the data using the appropriate function for the given file type.\nTo download the week 40 public use file data for the Census Household Pulse Survey, run the following code:\n\nbase_url &lt;- \"https://www2.census.gov/programs-surveys/demo/datasets/hhp/\"\nweek_url &lt;- \"2021/wk40/HPS_Week40_PUF_CSV.zip\"\n\npulse_url &lt;- paste0(base_url, week_url)\n\n# creates data directory in working directory\n# gives warning if directory already exists\ndir.create(\"data\")\n\n# For Mac, *.nix systems:\ndownload.file(\n  pulse_url, \n  destfile = \"data/pulse40.zip\"\n)\n\n# For Windows systems, you need to add the mode = \"wb\" \n# argument to prevent an invalid zip file \ndownload.file(\n  pulse_url, \n  destfile = \"data/pulse40.zip\", \n  mode = \"wb\"\n)\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCopy and paste the code chunk above and keep the appropriate download.file command for your computer.\nWrite code using the unzip() function to unzip the zip file downloaded. Set zipfile to be the same the destfile parameter you used in part 1. Set exdir to be the same directory where you just downloaded the zip file. Run both of these commands.\nExamine the unzipped files and select the appropriate function to read in the pulse2021_puf_40 file. Write code to read that file into R, assigning the output to the pulse object.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#column-names",
    "href": "05_exploratory-data-analysis.html#column-names",
    "title": "5  Exploratory Data Analysis",
    "section": "5.2 Column Names",
    "text": "5.2 Column Names\nAs discussed earlier in the course, dataframe columns - like other objects - should be given names that are “concise and meaningful”. Generally column names should be nouns and only use lowercase letters, numbers, and underscores _ (this is referred to as snake case). Columns should not begin with numbers. You should not include white space in column names (e.g “Birth Month” = bad, “birth_month” = good). It is also best practice for column names to be singular (use “birth_month” instead of “birth_months”).\nThe janitor package is a package that contains a number of useful functions to clean data in accordance with the tidyverse principles. One such function is the clean_names() function, which converts column names to snake case according to the tidyverse type guide (along with some other useful cleaning functions outlined in the link above). The clean_names() function works well with the |&gt; operator.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nTake a look at the column names in the Pulse data file you read in for the exercise earlier.\nThen edit the command in the R script that you wrote to read in the CSV file to pipe the results of that command to the janitor::clean_names() function. Note that you may have to install and import the janitor package first.\nNow look at the column names again after running the modified command. How have they changed?\n\n\n\nAs discussed in the introduction to the tidyverse, you can also directly rename columns using the rename() function from the dplyr package as follows: rename(data, new_col = old_col).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#data-overview",
    "href": "05_exploratory-data-analysis.html#data-overview",
    "title": "5  Exploratory Data Analysis",
    "section": "5.3 Data Overview",
    "text": "5.3 Data Overview\nOnce you’ve imported your data, a common first step is to get a very high-level summary of your data.\nAs introduced in the introduction to the tidyverse, the glimpse() function provides a quick view of your data, printing the type and first several values of each column in the dataset to the console.\n\nglimpse(x = storms)\n\nRows: 19,537\nColumns: 13\n$ name                         &lt;chr&gt; \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\",…\n$ year                         &lt;dbl&gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975,…\n$ month                        &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ day                          &lt;int&gt; 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 2…\n$ hour                         &lt;dbl&gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18,…\n$ lat                          &lt;dbl&gt; 27.5, 28.5, 29.5, 30.5, 31.5, 32.4, 33.3,…\n$ long                         &lt;dbl&gt; -79.0, -79.0, -79.0, -79.0, -78.8, -78.7,…\n$ status                       &lt;fct&gt; tropical depression, tropical depression,…\n$ category                     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wind                         &lt;int&gt; 25, 25, 25, 25, 25, 25, 25, 30, 35, 40, 4…\n$ pressure                     &lt;int&gt; 1013, 1013, 1013, 1013, 1012, 1012, 1011,…\n$ tropicalstorm_force_diameter &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hurricane_force_diameter     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThe summary() function enables you to quickly understand the distribution of a numeric or categorical variable. For a numeric variable, summary() will return the minimum, first quartile, median, mean, third quartile, and max values. For a categorical (or factor) variable, summary() will return the number of observations in each category. If you pass a dataframe to summary() it will summarize every column in the dataframe. You can also call summary on a single variable as shown below:\n\nsummary(storms$wind)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.00   30.00   45.00   50.05   65.00  165.00 \n\n\nThe str() function compactly displays the internal structure of any R object. If you pass a dataframe to str it will print the column type and first several values for each column in the dataframe, similar to the glimpse() function.\nGetting a high-level overview of the data can help you identify what questions you need to ask of your data during the exploratory data analysis process. The rest of this lecture will outline several questions that you should always ask when exploring your data - though this list is not exhaustive and will be informed by your specific data and analysis!",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#are-my-columns-the-right-types",
    "href": "05_exploratory-data-analysis.html#are-my-columns-the-right-types",
    "title": "5  Exploratory Data Analysis",
    "section": "5.4 Are my columns the right types?",
    "text": "5.4 Are my columns the right types?\nWe’ll read in the population-weighted centroids for the District of Columbia exported from the Missouri Census Data Center’s geocorr2014 tool.\n\ndc_centroids &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/geocorr/geocorr2014_dc.csv\"\n  )\n)\n\nRows: 180 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): county, tract, cntyname, intptlon, intptlat, pop10, afact\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(dc_centroids)\n\nRows: 180\nColumns: 7\n$ county   &lt;chr&gt; \"County code\", \"11001\", \"11001\", \"11001\", \"11001\", \"11001\", \"…\n$ tract    &lt;chr&gt; \"Tract\", \"0001.00\", \"0002.01\", \"0002.02\", \"0003.00\", \"0004.00…\n$ cntyname &lt;chr&gt; \"County name\", \"District of Columbia DC\", \"District of Columb…\n$ intptlon &lt;chr&gt; \"Wtd centroid W longitude, degrees\", \"-77.058857\", \"-77.07521…\n$ intptlat &lt;chr&gt; \"Wtd centroid latitude, degrees\", \"38.909434\", \"38.909223\", \"…\n$ pop10    &lt;chr&gt; \"Total population (2010)\", \"4890\", \"3916\", \"5425\", \"6233\", \"1…\n$ afact    &lt;chr&gt; \"tract to tract allocation factor\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\nWe see that all of the columns have been read in as character vectors because the second line of the csv file has a character description of each column. By default, read_csv uses the first 1,000 rows of data to infer the column types of a file. We can avoid this by skipping the first two lines of the csv file and manually setting the column names.\n\n#save the column names from the dataframe\ncol_names &lt;- dc_centroids |&gt; names()\n\ndc_centroids &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/geocorr/geocorr2014_dc.csv\"\n  ),\n  col_names = col_names,\n  skip = 2\n)\n\nRows: 179 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): tract, cntyname\ndbl (5): county, intptlon, intptlat, pop10, afact\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(dc_centroids)\n\nRows: 179\nColumns: 7\n$ county   &lt;dbl&gt; 11001, 11001, 11001, 11001, 11001, 11001, 11001, 11001, 11001…\n$ tract    &lt;chr&gt; \"0001.00\", \"0002.01\", \"0002.02\", \"0003.00\", \"0004.00\", \"0005.…\n$ cntyname &lt;chr&gt; \"District of Columbia DC\", \"District of Columbia DC\", \"Distri…\n$ intptlon &lt;dbl&gt; -77.05886, -77.07522, -77.06813, -77.07583, -77.06670, -77.05…\n$ intptlat &lt;dbl&gt; 38.90943, 38.90922, 38.90803, 38.91848, 38.92316, 38.92551, 3…\n$ pop10    &lt;dbl&gt; 4890, 3916, 5425, 6233, 1455, 3376, 3189, 4539, 4620, 3364, 6…\n$ afact    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nYou can convert column types by using the as.* set of functions. For example, we could convert the county column to a character vector as follows: mutate(dc_centroids, county = as.character(county)). We can also set the column types when reading in the data with read_csv() using the col_types argument. For example:\n\ndc_centroids &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/geocorr/geocorr2014_dc.csv\"\n  ),\n  col_names = col_names,\n  skip = 2,\n  col_types = c(\"county\" = \"character\")\n)\n\nglimpse(dc_centroids)\n\nRows: 179\nColumns: 7\n$ county   &lt;chr&gt; \"11001\", \"11001\", \"11001\", \"11001\", \"11001\", \"11001\", \"11001\"…\n$ tract    &lt;chr&gt; \"0001.00\", \"0002.01\", \"0002.02\", \"0003.00\", \"0004.00\", \"0005.…\n$ cntyname &lt;chr&gt; \"District of Columbia DC\", \"District of Columbia DC\", \"Distri…\n$ intptlon &lt;dbl&gt; -77.05886, -77.07522, -77.06813, -77.07583, -77.06670, -77.05…\n$ intptlat &lt;dbl&gt; 38.90943, 38.90922, 38.90803, 38.91848, 38.92316, 38.92551, 3…\n$ pop10    &lt;dbl&gt; 4890, 3916, 5425, 6233, 1455, 3376, 3189, 4539, 4620, 3364, 6…\n$ afact    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nAs you remember from week 1, a vector in R can only contain one data type. If R does not know how to convert a value in the vector to the given type, it may introduce NA values by coercion. For example:\n\nas.numeric(c(\"20\", \"10\", \"10+\", \"25\", \"~8\"))\n\nWarning: NAs introduced by coercion\n\n\n[1] 20 10 NA 25 NA\n\n\n\n5.4.1 String manipulation with stringr\nBefore converting column types, it is critical to clean the column values to ensure that NA values aren’t accidentally introduced by coercion. We can use the stringr package introduced in Section 3.4.1 to clean character data. A few reminders:\n\nThis package is part of the core tidyverse and is automatically loaded with library(tidyverse).\nThe stringr cheat sheet offers a great guide to the stringr functions.\n\nTo demonstrate some of the stringr functions, let’s create a state column with the two digit state FIPS code for DC and a geoid column in the dc_centroid dataframe which contains the 11-digit census tract FIPS code, which can be useful for joining this dataframe with other dataframes that commonly use the FIPS code as a unique identifier. We will need to first remove the period from the tract column and then concatenate the county and tract columns into a geoid column. We can do that using stringr as follows:\n\ndc_centroids &lt;- dc_centroids |&gt;\n  mutate(\n    #replace first instance of pattern\n    tract = str_replace(tract, \"\\\\.\", \"\"), \n    #join multiple strings into single string\n    geoid = str_c(county, tract, sep = \"\")\n  )\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nCopy the code above into an R script and edit it to add the creation of a variable called state that is equal to the first two characters of the county variable using the str_sub() function.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the str_replace() function uses regular expressions to match the pattern that gets replaced. Regular expressions is a concise and flexible tool for describing patterns in strings - but it’s syntax can be complex and not particularly intuitive. This vignette provides a useful introduction to regular expressions, and when in doubt - there are plentiful Stack Overflow posts to help when you search your specific case. ChatGPT is often effective at writing regular expressions, but recall our warnings about using ChatGPT in Chapter 1.\n\n\n\n\n5.4.2 Date manipulation with lubridate\nThe lubridate package, which we introduced in Section 3.4.3, makes it much easier to work with dates and times in R. As a reminder, lubridate is part of the tidyverse, but it is not a core tidyverse package. It must be explicitly loaded in each session with library(lubridate).\nWe’ll use a dataset on political violence and protest events across the continent of Africa in 2022 from the Armed Conflict Location & Event Data Project (ACLED) to illustrate the power of lubridate. The lubridate package allows users to easily and quickly parse date-time variables in a range of different formats. For example, the ymd() function takes a date variable in year-month-day format (e.g. 2022-01-31) and converts it to a date-time format. The different formats are outlined in the lubridate documentation.\n\nlibrary(lubridate)\nacled_2022 &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/acled/acled_africa_2022.csv\")\n  ) |&gt;\n  janitor::clean_names() |&gt;\n  mutate(event_date = ymd(event_date)) |&gt;\n  select(event_date, region, country, event_type, sub_event_type)\n\nRows: 22101 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (15): EVENT_ID_CNTY, EVENT_TYPE, SUB_EVENT_TYPE, ACTOR1, ASSOC_ACTOR_1,...\ndbl  (12): ISO, EVENT_ID_NO_CNTY, YEAR, TIME_PRECISION, INTER1, INTER2, INTE...\nlgl   (1): ADMIN3\ndttm  (1): EVENT_DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCreating datetime columns enables the use of a number of different operations, such as filtering based on date:\n\nacled_2022 |&gt;\n  filter(event_date &gt; \"2022-06-01\")\n\n# A tibble: 6,859 × 5\n   event_date region          country event_type             sub_event_type     \n   &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;                  &lt;chr&gt;              \n 1 2022-06-04 Northern Africa Algeria Protests               Peaceful protest   \n 2 2022-06-05 Northern Africa Algeria Protests               Peaceful protest   \n 3 2022-06-05 Northern Africa Algeria Protests               Peaceful protest   \n 4 2022-06-08 Northern Africa Algeria Protests               Peaceful protest   \n 5 2022-06-09 Northern Africa Algeria Protests               Peaceful protest   \n 6 2022-06-09 Northern Africa Algeria Protests               Peaceful protest   \n 7 2022-06-11 Northern Africa Algeria Protests               Peaceful protest   \n 8 2022-06-15 Northern Africa Algeria Strategic developments Looting/property d…\n 9 2022-06-18 Northern Africa Algeria Protests               Protest with inter…\n10 2022-06-19 Northern Africa Algeria Protests               Peaceful protest   \n# ℹ 6,849 more rows\n\n\nOr calculating durations:\n\nacled_2022 |&gt;\n  #number of days into 2022 event occurred\n  mutate(days_into_2022 = event_date - ymd(\"2022-01-01\")) \n\n# A tibble: 22,101 × 6\n   event_date region          country event_type sub_event_type   days_into_2022\n   &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;drtn&gt;        \n 1 2022-01-03 Northern Africa Algeria Protests   Peaceful protest 2 days        \n 2 2022-01-03 Northern Africa Algeria Protests   Peaceful protest 2 days        \n 3 2022-01-04 Northern Africa Algeria Protests   Peaceful protest 3 days        \n 4 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 5 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 6 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 7 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 8 2022-01-05 Northern Africa Algeria Protests   Peaceful protest 4 days        \n 9 2022-01-06 Northern Africa Algeria Protests   Peaceful protest 5 days        \n10 2022-01-06 Northern Africa Algeria Protests   Peaceful protest 5 days        \n# ℹ 22,091 more rows\n\n\nOr extracting components of dates:\n\nacled_2022 |&gt;\n  mutate(event_month = month(event_date)) #month event occurred\n\n# A tibble: 22,101 × 6\n   event_date region          country event_type sub_event_type   event_month\n   &lt;date&gt;     &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;\n 1 2022-01-03 Northern Africa Algeria Protests   Peaceful protest           1\n 2 2022-01-03 Northern Africa Algeria Protests   Peaceful protest           1\n 3 2022-01-04 Northern Africa Algeria Protests   Peaceful protest           1\n 4 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 5 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 6 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 7 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 8 2022-01-05 Northern Africa Algeria Protests   Peaceful protest           1\n 9 2022-01-06 Northern Africa Algeria Protests   Peaceful protest           1\n10 2022-01-06 Northern Africa Algeria Protests   Peaceful protest           1\n# ℹ 22,091 more rows\n\n\nDatetimes can also much more easily be plotted using ggplot2. For example, it is easy to visualize the distribution of events across the year:\n\nacled_2022 |&gt; \n  ggplot(aes(x = event_date)) + \n  geom_freqpoly(binwidth = 1) # each bin is 1 day\n\n\n\n\n\n\n\n\nFor more information on lubridate, see the lubridate cheat sheet.\nSource: R for Data Science, Ch 16\n\n\n5.4.3 Categorical and Factor Variables\nCategorical variables can be stored as characters in R. The case_when() function makes it very easy to create categorical variables based on other columns. For example:\n\npulse &lt;- pulse |&gt;\n  mutate(\n    hisp_rrace = case_when(\n      rrace == 1 ~ \"White alone, not Hispanic\",\n      rrace == 2 ~ \"Black alone, not Hispanic\",\n      rrace == 3 ~ \"Asian alone, not Hispanic\",\n      rrace == 4 ~ \"Two or more races + Other races, not Hispanic\",\n      TRUE ~ NA_character_)\n  )\n\nFactors are a data type specifically made to work with categorical variables. The forcats library in the core tidyverse is made to work with factors. Factors are particularly valuable if the values have a ordering that is not alphanumeric.\n\nx1 &lt;- c(\"Dec\", \"Apr\", \"Jan\", \"Mar\")\n\nmonth_levels &lt;- c(\n  \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n  \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n)\n\ny1 &lt;- factor(x1, levels = month_levels)\n\nsort(x1)\n\n[1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"\n\nsort(y1)\n\n[1] Jan Mar Apr Dec\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nFactors are also valuable if you want to show all possible values of the categorical variable, even when they have no observations.\n\ntable(x1)\n\nx1\nApr Dec Jan Mar \n  1   1   1   1 \n\ntable(y1)\n\ny1\nJan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec \n  1   0   1   1   0   0   0   0   0   0   0   1",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#is-there-missing-data",
    "href": "05_exploratory-data-analysis.html#is-there-missing-data",
    "title": "5  Exploratory Data Analysis",
    "section": "5.5 Is there missing data?",
    "text": "5.5 Is there missing data?\nBefore you work with any dataset, you should understand how missing values are encoded. The best place to find this information is the data dictionary - which you should always read before working with any new dataset!\nThis is particularly important because while R automatically recognizes standard missing values as NA, it doesn’t recognize non-standard encodings like numbers representing missing values, “missing”, “na”, “N/A”, etc.\nNon-standard missing values should be converted to NA before conducting analysis. One way of doing this is with mutate and the if_else or case_when functions.\n\n\n\n\n\n\nExercise 3\n\n\n\n\nGo to the folder where you unzipped the Pulse data from earlier and open the data dictionary file. How does this dataset represent missing values for the RECVDVACC variable?\nUsing mutate and if_else or case_when, replace the missing values in the recvdvacc column with NA.\n\n\n\nOnce you have converted all missing value encodings to NA, the next question you need to ask is how you want to handle missing values in your analysis. The right approach will depend on what the missing value represents and the goals of your analysis.\n\nLeave as NA: This can be the best choice when the missing value truly represents a case when the true value is unknown. You will need to handle NAs by setting na.rm = TRUE in functions or filtering using is.na(). One drawback of this approach is that if the values aren’t missing at random (e.g. smokers may be less likely to answer survey questions about smoking habits), your results may be biased. Additionally, this can cause you to lose observations and reduce power of analyses.\nReplace with 0: This can be the best choice if a missing value represents a count of zero for a given entity. For example, a dataset on the number of Housing Choice Voucher tenants by zip code and quarter may have a value of NA if there were no HCV tenants in the given zip code and quarter.\nImpute missing data: Another approach is imputing the missing data with a reasonable value. There are a number of different imputation approaches:\n\nMean/median/mode imputation: Fills the missing values with the column mean or median. This approach is very easy to implement, but can artifically reduce variance in your data and be sensitive to outliers in the case of mean imputation.\nPredictive imputation: Fills the missing values with a predicted value based on a model that has been fit to the data or calculated probabilities based on other columns in the data. This is a more complex approach but is likely more accurate (for example, it can take into account variable correlation).\n\n\nThese alternatives are discussed in more detail in a later chapter covering data imputation.\nThe replace_na() function in dplyr is very useful for replacing NA values in one or more columns.\n\ndf &lt;- tibble(x = c(1, 2, NA), y = c(\"a\", NA, \"b\"))\n\n# Using replace_na to replace one column\ndf |&gt; \n  mutate(x = replace_na(x, 0))\n\n# A tibble: 3 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 &lt;NA&gt; \n3     0 b    \n\n# Using replace_na to replace multiple columns with different values\ndf |&gt; \n  replace_na(list(x = 0, y = \"unknown\"))\n\n# A tibble: 3 × 2\n      x y      \n  &lt;dbl&gt; &lt;chr&gt;  \n1     1 a      \n2     2 unknown\n3     0 b      \n\n# Using if_else to perform mean imputation\ndf |&gt; \n  mutate(x = if_else(is.na(x), mean(x, na.rm = TRUE), x))\n\n# A tibble: 3 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1   1   a    \n2   2   &lt;NA&gt; \n3   1.5 b",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#do-i-have-outliers-or-unexpected-values",
    "href": "05_exploratory-data-analysis.html#do-i-have-outliers-or-unexpected-values",
    "title": "5  Exploratory Data Analysis",
    "section": "5.6 Do I have outliers or unexpected values?",
    "text": "5.6 Do I have outliers or unexpected values?\n\n5.6.1 Identifying Outliers/Unexpected Values\nUsing R to examine the distribution of your data is one way to identify outliers or unexpected values. For example, we can examine the distribution of the bodywt variable in the msleep dataset both by examining the mathematical distribution using the summary() function and visually using ggplot.\n\nsummary(msleep$bodywt)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   0.005    0.174    1.670  166.136   41.750 6654.000 \n\n\n\nmsleep |&gt;\n  ggplot(aes(bodywt, 1)) +\n  geom_point(alpha = 0.2) +\n  scale_y_continuous(breaks = 0) +\n  labs(y = NULL) +\n  theme_bw() +\n  theme(panel.border = ggplot2::element_blank())\n\n\n\n\n\n\n\n\n\n\n5.6.2 Unit Tests\nWriting tests in R is a great way to test that your data does not have unexpected/incorrect values. These tests can also be used to catch mistakes that can be introduced by errors in the data cleaning process. There are a number of R packages that have tools for writing tests, including:\n\ntestthat\nassertthat\nassertr\n\n\nlibrary(assertr)\n\ndf &lt;- tibble(age = c(20, 150, 47, 88),\n             height = c(60, 2, 72, 66))\n\ndf |&gt;\n  assertr::verify(age &lt; 120) |&gt;\n  summarise(mean(age, na.rm = TRUE))\n\nverification [age &lt; 120] failed! (1 failure)\n\n    verb redux_fn predicate column index value\n1 verify       NA age &lt; 120     NA     2    NA\n\n\nError: assertr stopped execution\n\n\nCritically, adding the test caused the code to return an error before calculating the mean of the age variable. This is a feature, not a bug! It can prevent you from introducing errors into your analyses. Moreover, by writing a set of tests in your analysis code, you can run the same checks every time you perform the analysis which can help you catch errors caused by changes in the input data.\n\n\n5.6.3 Handling Outliers/Unexpected Values\nWhen you identify outliers or unexpected values, you will have to decide how you want to handle those values in your data. The proper way to handle those values will depend on the reason for the outlier value and the objectives of your analysis.\n\nIf the outlier is caused by data errors, such as an implausible age or population value, you can replace the outlier values with NA using mutate and if_else or case_when as described above.\nIf the outlier represents a different population than the one you are studying - e.g. the different consumption behaviors of individual consumers versus wholesale business orders - you can remove it from the data.\nYou can transform the data, such as taking the natural log to reduce the variation caused by outliers.\nYou can select a different analysis method that is less sensitive to outliers, such as using the median rather than the mean to measure central tendency.\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nRead the column descriptions in the csv file for the DC centroids.\nUse one of the methods above to identify whether the pop10 column contains any outliers. According to the Census Bureau, tracts generally have a population between 1,200 and 8,000 people.\nCalculate the mean of the pop10 column in the dc_centroids dataframe, but first write one test using assertr::verify() to test for invalid values based on the column definition.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#data-quality-assurance",
    "href": "05_exploratory-data-analysis.html#data-quality-assurance",
    "title": "5  Exploratory Data Analysis",
    "section": "5.7 Data Quality Assurance",
    "text": "5.7 Data Quality Assurance\nData quality assurance is the foundation of high quality analysis. Four key questions that you should always ask when considering using a dataset for analysis are:\n\nDoes the data suit the research question? Examine the data quality (missingness, accuracy) of key columns, the number of observations in subgroups of interest, etc.\nDoes the data accurately represent the population of interest? Think about the data generation process (e.g. using 311 calls or Uber ride data to measure potholes) - are any populations likely to be over or underrepresented? Use tools like Urban’s Spatial Equity Data Tool to test data for unrepresentativeness.\nIs the data gathering reproducible? Wherever possible, eliminate manual steps to gather or process the data. This includes using reproducible processes for data ingest such as APIs or reading data directly from the website rather than manually downloading files. All edits to the data should be made programmatically (e.g. skipping rows when reading data rather than deleting extraneous rows manually). Document the source of the data including the URL, the date of the access, and specific metadata about the vintage.\nHow can you verify that you are accessing and using the data correctly? This may include writing tests to ensure that raw and calculated data values are plausible, comparing summary statistics against those provided in the data documentation (if applicable), published tables/statistics from the data provider, or published tables/statistics from a trusted third party.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05_exploratory-data-analysis.html#conclusion",
    "href": "05_exploratory-data-analysis.html#conclusion",
    "title": "5  Exploratory Data Analysis",
    "section": "5.8 Conclusion",
    "text": "5.8 Conclusion\nThis chapter outlined EDA in great detail. Some key reminders are: - R and the tidyverse have great packages for reading in data (readr for csv files, readxl for Excel files, haven for STATA, SAS, and SPSS files).\n\njanitor::clean_names() is a great way to get your column name structure to “snake case”.\nglimpse() is an easy way to get summary statistics on your columns.\nstringr and lubridate can help you handle string and date manipulation, respectively.\nThere a many good options for handling missing data. Be thoughtful!\nUnit tests are a good way to ensure your data is in an expected format.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html",
    "href": "06_reproducible-research-with-quarto.html",
    "title": "6  Reproducible Research with Quarto",
    "section": "",
    "text": "6.1 Motivation\nThere are many problems worth avoiding in an analysis:\nNot convinced? Maybe we just want to make cool stuff.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#motivation",
    "href": "06_reproducible-research-with-quarto.html#motivation",
    "title": "6  Reproducible Research with Quarto",
    "section": "",
    "text": "Copying-and-pasting, transposing, and manual repetition\nOut-of-sequence documents\nParallel documents (a script and a narrative Word doc)\nCode written for computers that is tough to parse by humans",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#literate-statistical-programming",
    "href": "06_reproducible-research-with-quarto.html#literate-statistical-programming",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.2 Literate (Statistical) Programming",
    "text": "6.2 Literate (Statistical) Programming\n\n\n\nSource: Jacob Applebaum\n\n\nAccording to Donald Knuth:\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do. ~ Knuth (1984).",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#example",
    "href": "06_reproducible-research-with-quarto.html#example",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.3 Example",
    "text": "6.3 Example\nWe used a linear model because there is reason to believe that the population model is linear. The observations are independent and the errors are independently and identically distributed with an approximately normal distribution.\n\nmodel1 &lt;- lm(formula = dist ~ speed, data = cars)\nmodel1\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nAn increase in travel speed of one mile per hour is associated with a 3.93 foot increase in stopping distance on average.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#quarto",
    "href": "06_reproducible-research-with-quarto.html#quarto",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.4 Quarto",
    "text": "6.4 Quarto\nQuarto is a literate statistical programming tool for R, Julia, Python, JavaScript, and more. It was released by Posit in 2022. Quarto is an important tool for reproducible research. It combines narrative text with styles, code, and the output of code and can be used to create many types of documents including PDFs, html websites, slides, and more.\nQuarto builds on the success of R Markdown. In fact, Quarto will Render R Markdown (.Rmd) documents without any edits or changes.\nJupyter (Julia, Python, and R) is a competing framework that is popular for Python but has not caught on for R.\nAccording to Wickham and Grolemund (2017) Chapter 27, there are three main reasons to use R Markdown (they hold for Quarto) :\n\n\n“For communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis.”\n“For collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them (i.e. the code).”\n“As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking.”\n\n\nQuarto uses\n\nplain text files ending in .qmd that are similar to .R files.\nlibrary(knitr).\npandoc.1\n\nQuarto calls library(knitr) and “knits” .qmd (Quarto files) into .md (Markdown files), which Pandoc then converts into any specified output type. Quarto and library(knitr) don’t need to be explicitly loaded, and the entire process is handled by clicking the “Render” button in RStudio.\n\nSource: Quarto website\nClicking the “Render” button starts this process.\n\nQuarto, library(knitr), and Pandoc are all installed with RStudio. The only additional software you will need is a LaTeX distribution. Follow these instructions to install library(tinytex) if you want to make PDF documents.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nIf you already have a LaTeX distribution like tinytext or MiKTeX, then skip this exercise.\nFollow these instructions to install library(tinytex).\n\n\n\nThe “Render” workflow has a few advantages:\n\nAll code is rerun in a clean environment when “Rendering”. This ensures that the code runs in order and is reproducible.\nIt is easier to document code than with inline comments.\nThe output types are really appealing. By creating publishable documents with code, there is no need to copy-and-paste or transpose results.\nThe process is iterable and scalable.\n\n\n\n\n\n\n\nWarning\n\n\n\nIn the RStudio IDE (or in VSCode), clicking the Render button is analogous to running quarto preview in the terminal. This is different from another quarto command line function quarto render2. quarto preview will cause the page to re-render and update. However, if you use Quarto to build a HTML-based book (like this one) or a website, by default, running quarto preview will not incorporate updated global options (often specified in _quarto.yml) or external files. To redeploy a website and/or incorporate those changes, run quarto render.\nYou can read more about this distinction on the Quarto documentation.\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nClick the new script button and add a “Quarto Document”.\nGive the document a name, an author, and ensure that HTML is selected.\nSave the document as “hello-quarto.qmd”.\nClick “Render”.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#three-ingredients-in-a-.qmd",
    "href": "06_reproducible-research-with-quarto.html#three-ingredients-in-a-.qmd",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.5 Three Ingredients in a .qmd",
    "text": "6.5 Three Ingredients in a .qmd\n\nYAML header\nMarkdown text\nCode chunks\n\n\n6.5.1 1. YAML header\nYAML stands for “yet another markup language.” The YAML header contains meta information about the document including output type, document settings, and parameters that can be passed to the document. The YAML header starts with --- and ends with ---.\nHere is the simplest YAML header for a PDF document:\n---\nformat: html\n---\nYAML headers can contain many output specific settings. This YAML header creates an HTML document with code folding and a floating table of contents:\n---\nformat: \n  html:\n    code-fold: true\n    toc: true\n---  \n\n\n6.5.2 2. Markdown text\nMarkdown is a shortcut for HyperText Markup Language (HTML). Essentially, simple meta characters corresponding to formatting are added to plain text.\nTitles and subtitltes\n------------------------------------------------------------\n\n# Title 1\n\n## Title 2\n\n### Title 3\n\n\nText formatting \n------------------------------------------------------------\n\n*italic*  \n\n**bold**   \n\n`code`\n\nLists\n------------------------------------------------------------\n\n* Bulleted list item 1\n* Item 2\n  * Item 2a\n  * Item 2b\n\n1. Item 1\n2. Item 2\n\nLinks and images\n------------------------------------------------------------\n\n[text](http://link.com)\n\n\n\n\n\n\nExercise 3\n\n\n\n\nAdd text with formatting like headers and bold to your Quarto document.\nRender!\n\n\n\n\n\n6.5.3 3. Code chunks\n\nMore frequently, code is added in code chunks:\n\n```{r}\n2 + 2\n```\n\n[1] 4\n\n\nThe first argument inline or in a code chunk is the language engine. Most commonly, this will just be a lower case r. knitr allows for many different language engines:\n\nR\nJulia\nPython\nSQL\nBash\nRcpp\nStan\nJavascript\nCSS\n\nQuarto has a rich set of options that go inside of the chunks and control the behavior of Quarto.\n\n```{r}\n#| eval: false\n\n2 + 2\n```\n\nIn this case, eval makes the code not run. Other chunk-specific settings can be added inside the brackets. Here3 are the most important options:\n\n\n\nOption\nEffect\n\n\n\n\necho: false\nHides code in output\n\n\neval: false\nTurns off evaluation\n\n\noutput: false\nHides code output\n\n\nwarning: false\nTurns off warnings\n\n\nmessage: false\nTurns off messages\n\n\nfig-height: 8\nChanges figure width in inches4\n\n\nfig-width: 8\nChanges figure height in inches5\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nAdd a code chunk.\nLoad the storms data set.\nFilter the data to only include hurricanes.\nMake a data visualization with ggplot2 using the data from\nInclude an option to hide the R code.\nRender!",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#applications",
    "href": "06_reproducible-research-with-quarto.html#applications",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.6 Applications",
    "text": "6.6 Applications\n\n6.6.1 PDF documents\n---\nformat: pdf\n---\n\nAny documents intended to be published or printed out. For example, this blog details how Cameron Patrick wrote their PhD thesis in Quarto.\n\n\n\n6.6.2 html documents\n---\nformat: html\n---\n\nRegression in R notes\nR at the Urban Institute website\n\n\n\n6.6.3 GitHub README\n---\nformat: gfm\n---\n\nurbnthemes\n\n\n\n6.6.4 Bookdown\nBookdown is an R package by Yihui Xie for authoring books in R Markdown. Many books, including the first edition of R for Data Science (Wickham and Grolemund (2017)), have been written in Quarto.\nQuarto book replaces bookdown. It is oriented around Quarto projects. The second edition of R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund (2023)) was written in Quarto.\n\n\n6.6.5 Blogdown\nBlogdown is an R package by Yihui Xie for creating and managing a blog in R Markdown. Up & Running with blogdown in 2021 by Alison Hill is a great tutorial for getting started with Blogdown.\nThere is no good Quarto replacement right now.\n\n\n6.6.6 Microsoft Word and Microsoft PowerPoint\nIt is possible to write to Word and PowerPoint. In general, We’ve found the functionality to be limited, and it is difficult to match institutional branding standards.\n\n\n6.6.7 Slides\n---\nformat:\n  revealjs:\n    css: styles.css\n    incremental: true\n    reveal_options:\n      slideNumber: true\n      previewLinks: true\n---\n\nDespite the challenge of rendering to Powerpoint, Quarto can be used effectively to create presentations. One of the authors of this book used Quarto to create a presentation about iterated fact sheet development which, coincidentally, also used Quarto!\n\n\n\n6.6.8 Fact sheets and fact pages\nAn alternative to rendering a Quarto document with the Render button is to use the quarto::render() function. This allows for iterating the rendering of documents which is particularly useful for the development of fact sheets and fact pages. The next chapter of the book expands on this use case.\n\nThe Urban Institute State and Local Finance Initiative creates State Fiscal Briefs by iterating R Markdown documents.\nData@Urban",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#conclusion",
    "href": "06_reproducible-research-with-quarto.html#conclusion",
    "title": "6  Reproducible Research with Quarto",
    "section": "6.7 Conclusion",
    "text": "6.7 Conclusion\nQuarto is an updated version of R Markdown that can handle not only R but also Python and Julia code (among other languages). Quarto combines a yaml header, markdown text, and code chunks. It can be used in a variety of settings to create technical documents and presentations. We love Quarto and hope you will learn to love it too!\n\n6.7.1 Suggestions\n\nRender early, and render often.\nSelect the gear to the right of “Render” and select “Chunk Output in Console”\nLearn math mode. Also, library(equatiomatic) (CRAN, GitHub) is amazing.\n\n\n\n6.7.2 Resources\n\nQuarto intro\nR4DS R Quarto chapter\nHappy Git R Markdown tutorial\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualie, and Model Data. 2nd edition. Sebastopol, CA: O’Reilly.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O’Reilly Media. http://r4ds.had.co.nz/.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "06_reproducible-research-with-quarto.html#footnotes",
    "href": "06_reproducible-research-with-quarto.html#footnotes",
    "title": "6  Reproducible Research with Quarto",
    "section": "",
    "text": "Pandoc is free software that converts documents between markup formats. For example, Pandoc can convert files to and from markdown, LaTeX, jupyter notebook (ipynb), and Microsoft Word (.docx) formats, among many others. You can see a comprehensive list of files Pandoc can convert on their About Page.↩︎\nThe quarto::quarto_render() function, which we describe below, wraps the bash quarto render function.↩︎\nThis table was typed as Markdown code. But sometimes it is easier to use a code chunk to create and print a table. Pipe any data frame into knitr::kable() to create a table that will be formatted in the output of a rendered Quarto document.↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more. Default settings for the entire document can be changed in the YAML header with the execute option:\nexecute: warning: false↩︎\nThe default dimensions for figures change based on the output format. Visit here to learn more. Default settings for the entire document can be changed in the YAML header with the execute option:\nexecute: warning: false↩︎",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible Research with Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html",
    "href": "07_advanced-quarto.html",
    "title": "7  Advanced Quarto",
    "section": "",
    "text": "7.1 Review Exercise\nLet’s start with an exercise to review content from the previous section of notes:",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#sec-review",
    "href": "07_advanced-quarto.html#sec-review",
    "title": "7  Advanced Quarto",
    "section": "",
    "text": "Exercise 1\n\n\n\n\nIn the hello-quarto.qmd file you previously created, add date: today to your YAML header after title. This will update every time the document is rendered.\nCopy the Markdown table from this table generator and add it to your .qmd document.\nCreate a scatter plot of the cars data with library(ggplot2). Adjust the figure width and height using options within the chunk.\nClick “Render”.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#yaml-header-specifications",
    "href": "07_advanced-quarto.html#yaml-header-specifications",
    "title": "7  Advanced Quarto",
    "section": "7.2 YAMl Header Specifications",
    "text": "7.2 YAMl Header Specifications\n\n7.2.1 params\nAs a review, Quarto documents have YAML headers, and YAML headers can contain many output specific settings. This YAML header creates an HTML document with code folding and a floating table of contents:\n---\nformat: \n  html:\n    embed-resources: true\n    code-fold: true\n    toc: true\n---  \nOne key specification in the YAML header are parameters or params. Here is an example:\n---\nformat: html\nparams:\n  state: \"Virginia\"\n---\nNow state can be referred to anywhere in R code as params$state.\nParameters are useful for a couple of reasons:\n\nWe can clearly change key values for a Quarto document in the YAML header. This could be a useful place to define constants and avoid magic numbers.\nWe can create a template and programmatically iterate the template over a set of values with the quarto_render() function and library(purrr). This blog describes the idea and provides example code! The Mobility Metrics Data Tables and SLFI State Fiscal Briefs are key examples of this workflow. The code to create the Mobility Metrics Data Tables is publicly available on Github.\n\n\n\n7.2.2 embed-resources\n\n\n\n\n\n\nWarning\n\n\n\nUnlike R Markdown, images and other content are not embedded in .html from Quarto by default. Be sure to include embed-resources: true in YAML headers to embed content and make documents easier to share.\nSuppose we embed an image called image.png in a Quarto document called example.qmd, which, when rendered, creates example.html. If we don’t include embed-resources: true, then we will need to share image.png and example.html to see the embedded image. This is also true for other files like .css which is useful to provide custom styling for websites.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#organizing-a-quarto-document",
    "href": "07_advanced-quarto.html#organizing-a-quarto-document",
    "title": "7  Advanced Quarto",
    "section": "7.3 Organizing a Quarto Document",
    "text": "7.3 Organizing a Quarto Document\nIt is important to clearly organize a Quarto document and the constellation of files that typically support an analysis.\n\nAlways use .Rproj files.\nUse sub-directories to sort images, .css, data.\n\nLater, we will learn how to use library(here) to effectively organize sub-directories.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#math-notation",
    "href": "07_advanced-quarto.html#math-notation",
    "title": "7  Advanced Quarto",
    "section": "7.4 Math Notation",
    "text": "7.4 Math Notation\nThis course uses probability and statistics. Occasionally, we want to easily communicate with mathematical notation. For example, it may be convenient to type that \\(X\\) is a random variable that follows a standard normal distribution (mean = 0 and standard deviation = 1).\n\\[X \\sim N(\\mu = 0, \\sigma = 1)\\]\n\n7.4.1 Math Mode\nUse $ to start and stop in-line math notation and $$ to start multi-line math notation. Math notation uses LaTeX’s syntax for mathematical notation.\nHere’s an example with in-line math:\nConsider a binomially distributed random variable, $X \\sim binom(n, p)$. \nConsider a binomially distributed random variable, \\(X \\sim binom(n, p)\\).\nHere’s an example with a chunk of math:\n$$\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n$${#eq-binomial}\n\\[\nP(X = x) = {n \\choose x} p ^ x (1 - p) ^ {n - x}\n\\tag{7.1}\\]\n\n\n7.4.2 Important Syntax\nMath mode recognizes basic math symbols available on your keyboard including +, -, *, /, &gt;, &lt;, (, and ).\nMath mode contains all greek letters. For example, \\alpha (\\(\\alpha\\)) and \\beta (\\(\\beta\\)).\n\n\n\nTable 7.1: My Caption\n\n\n\n\n\nLaTeX\nSymbol\n\n\n\n\n\\alpha\n\\(\\alpha\\)\n\n\n\\beta\n\\(\\beta\\)\n\n\n\\gamma\n\\(\\gamma\\)\n\n\n\\Delta\n\\(\\Delta\\)\n\n\n\\epsilon\n\\(\\epsilon\\)\n\n\n\\theta\n\\(\\theta\\)\n\n\n\\pi\n\\(\\pi\\)\n\n\n\\sigma\n\\(\\sigma\\)\n\n\n\\chi\n\\(\\chi\\)\n\n\n\n\n\n\nMath mode also recognizes \\(\\log(x)\\) (\\log(x)) and \\(\\sqrt{x}\\) (\\sqrt{x}).\nSuperscripts (^) are important for exponentiation and subscripts (_) are important for adding indices. y = x ^ 2 renders as \\(y = x ^ 2\\) and x_1, x_2, x_3 renders as \\(x_1, x_2, x_3\\). Brackets are useful for multi-character superscripts and subscripts like \\(s_{11}\\) (s_{11}).\nIt is useful to add symbols to letters. For example, \\bar{x} is useful for sample means (\\(\\bar{x}\\)), \\hat{y} is useful for predicted values (\\(\\hat{y}\\)), and \\vec{\\beta} is useful for vectors of coefficients (\\(\\vec{\\beta}\\)).\nMath mode supports fractions with \\frac{x}{y} (\\(\\frac{x}{y}\\)), big parentheses with \\left(\\right) (\\(\\left(\\right)\\)), and brackets with \\left[\\right] (\\(\\left[\\right]\\)).\nMath mode has a symbol for summation. Let’s combine it with bars, fractions, subscripts, and superscipts to show sample mean \\bar{x} = \\frac{1}{n}\\sum_i^n x_i, which looks like \\(\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\).\n\\sim is how to add the tilde for distributed as. For example, X \\sim N(\\mu = 0, \\sigma = 1) shows the normal distribution \\(X \\sim N(\\mu = 0, \\sigma = 1)\\).\nMatrices are are a little bit more work in math mode. Consider the follow variance-covariance matrix:\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\[\n\\begin{bmatrix}\ns_{11}^2 & s_{12}\\\\\ns_{21} & s_{22}^2\n\\end{bmatrix}\n\\]\nThis guide provides and exhaustive look at math options in Quarto.\n\n\n\n\n\n\nWarning\n\n\n\nMath mode is finicky! Small errors like mismatched parentheses or superscript and subscript errors will cause Quarto documents to fail to render. Write math carefully and render early and often.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nUse math mode to type out the equation for root mean square error (RMSE).\nDo you divide by n or n - 1?",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#cross-references",
    "href": "07_advanced-quarto.html#cross-references",
    "title": "7  Advanced Quarto",
    "section": "7.5 Cross References",
    "text": "7.5 Cross References\nCross references are useful for organizing documents that include sections, figures, tables, and equations. Cross references create hyperlinks within documents that jump to the locations of these elements. Linking sections, figures, tables, or equations helps readers navigate the document.\nCross references also automatically number the referenced elements. This means that if there are two tables (ie. Table 1 and Table 2) and a table is added between the two tables, all of the table numbers and references to the tables will automatically update.\nCross references require two bits of code within a Quarto document:\n\nA label associated with the section, figure, table, or equation.\nA reference to the labelled section, figure, table, or equation.\n\nLabels are written in brackets or as arguments in code chunks, and begin with the the type object being linked. References begin with @ followed by the label of object being linked.\n\n7.5.1 Sections\nLinking sections helps readers navigate between sections. Use brackets to label sections after headers and always begin labels with sec-. Then you can reference that section with @sec-.\n## Review {#sec-review}\n\nSee @sec-review if you are totally lost.\nThe cross references shows up like this: See Section 7.1 if you are totally lost.\nIt can be helpful to turn on section numbering with number-sections: true in the YAML header. Additionally, Markdown has a native method for linking between sections.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAdd a few section headers to your Quarto document.\nAdd a cross reference to one of the section headers.\n\n\n\n\n\n7.5.2 Figures\n\n\n\n\n\n\nFigure 7.1: Penguins\n\n\n\nWe can reference figures like Figure 7.1 with @fig-penguins.\n\n\n7.5.3 Tables\nWe can link to tables in our documents. For example, we can link to the greek table with @tbl-greek Table 7.1.\n\n\n7.5.4 Equations\nWe can link to equations in our documents. For example, we can link to the binomial distribution earlier with @eq-binomial Equation 13.4.\n\n\n\n\n\n\nExercise 3\n\n\n\n\nAdd a cross reference to your RMSE equation from earlier.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#citations",
    "href": "07_advanced-quarto.html#citations",
    "title": "7  Advanced Quarto",
    "section": "7.6 Citations",
    "text": "7.6 Citations\n\n7.6.1 Zotero\nZotero is a free and open-source software for organizing research and managing citations.\n\n\n\n\n\n\nDigital Object Identifier (DOI)\n\n\n\nDOIs are persistent identifiers that uniquely identify objects including many academic papers. For example, 10.1198/jcgs.2009.07098 identifies “A Layered Grammar of Graphics” by Hadley Wickham.\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nInstall Zotero.\nFind the DOI for “Tidy Data” by Hadley Wickham.\nClick the magic wand in Zotero and paste the DOI.\n\n\n\n\n\n\n\n\n\n\n\nReview the new entry in Zotero.\n\n\n\n\n\n7.6.2 Zotero Integration\nZotero has a powerful integration with Quarto. In practice, it’s one click to add a DOI to Zotero and then one click to add a citation to Quarto.\nRStudio automatically adds My Library from Zotero. Simply switch to the Visual Editor (top left in RStudio), click “Insert”, and click “Citation”. This will open a prompt to insert a citation into the Quarto document.\nThe citation is automatically added with parentheses to go at the end of sentences. Delete the square brackets to convert the citation to an in-line citation.\nInserting the citation automatically adds the citation to the references section. Deleting the reference automatically deletes the citation from the references section.\nZotero Groups are useful for sharing citations and Zotero Group Libraries need to be added to RStudio. To set this up:\nTo set this up, in RStudio:\n\nGo to Tools and select “Global Options”\nSelect “RMarkdown” and then click “Citations”\nFor “Use Libraries” choose “Selected Libraries”\nSelect the group libraries to add\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCite “Tidy Data” by Hadley Wickham in your Quarto document.\nClick “Render”",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "07_advanced-quarto.html#conclusion",
    "href": "07_advanced-quarto.html#conclusion",
    "title": "7  Advanced Quarto",
    "section": "7.7 Conclusion",
    "text": "7.7 Conclusion\nQuarto can handle mathematical equations using $. Quarto also has syntax to allow cross references to sections, figures, tables and equations. Lastly, Quarto integrates well with Zotero to handle citations.\n\n7.7.1 More Resources\n\nQuarto Guide\nIterating fact sheets and web pages with Quarto",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Quarto</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html",
    "href": "08_reproducible-research-with-git.html",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "",
    "text": "8.1 Command Line\nThe command line (also known as shell or console) is a way of controlling computers without using a graphical user interface (i.e. pointing-and-clicking). The command line is useful because pointing-and-clicking is tough to reproduce or scale and because lots of useful software is only available through the command line. Furthermore, cloud computing often requires use of the command line.\nWe will run Bash, a command line program, using Terminal on Mac and Git Bash on Windows. Open Terminal like any other program on Mac. Right-click in a desired directory and select “Git Bash Here” to access Git Bash on Windows.\nFortunately, we only need to know a little Bash for version control with Git and cloud computing.\npwd - print working directory - prints the file path to the current location in the\nls - list - lists files and folders in the current working directory.\ncd - change directory - move the current working directory.\nmkdir - make directory - creates a directory (folder) in the current working directory.\ntouch - creates a text file with the provided name.\nmv - move - moves a file from one location to the other.\ncat - concatenate - concatenate and print a file.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#command-line",
    "href": "08_reproducible-research-with-git.html#command-line",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "",
    "text": "Exercise 1\n\n\n\n\nCreate a new directory called cli-exercise.\nNavigate to this directory using cd in the Terminal or Git Bash.\nSubmit pwd to confirm you are in the correct directory.\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nUse the following code to create a new text document called haiku.txt in the working directory.\npwd\nls\ntouch haiku.txt\nls\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nUse following to add the haiku “The Old Pond” by Matsuo Basho to haiku.txt.\ncat haiku.txt\necho \"An old silent pond\" &gt;&gt; haiku.txt\ncat haiku.txt\necho \"A frog jumps into the pond-\" &gt;&gt; haiku.txt\necho \"Splash! Silence again.\" &gt;&gt; haiku.txt\necho \"~Matsuo Basho\" &gt;&gt; haiku.txt\ncat haiku.txt\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nUse the following code to move haiku.txt to a subdirectory called poems/.\nls\nmkdir poems\nls\nmv haiku.txt poems/haiku.txt\nls\ncat poems/haiku.txt\n\n\n\n8.1.1 Useful tips\n\nTab completion can save a ton of typing. Hitting tab twice shows all of the available options that can complete from the currently typed text.\nHit the up arrow to cycle through previously submitted commands.\nUse man &lt;command name&gt; to pull up help documentation. Hit q to exit.\nTyping .. refers to the directory above the working directory. Writing cd .. changes to the directory above the working directory.\nTyping just cd changes to the home directory.\n\n\n\n8.1.2 Programs\nWe can run programs from the command line. Commands from programs always start with the name of the program. Running git commands intuitively start with git. For example:\ngit init\ngit status",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#why-version-control",
    "href": "08_reproducible-research-with-git.html#why-version-control",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.2 Why version control?",
    "text": "8.2 Why version control?\nVersion control is a system for managing and recording changes to files over time. Version control is essential to managing code and analyses. Good version control can:\n\nLimit the chance of making a mistake\nMaximize the chance of catching a mistake when it happens\nCreate a permanent record of changes to code\nEasily undo mistakes by switching between iterations of code\nAllow multiple paths of development while protecting working versions of code\nEncourage communication between collaborators\nBe used for external communication",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#why-distributed-version-control",
    "href": "08_reproducible-research-with-git.html#why-distributed-version-control",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.3 Why distributed version control?",
    "text": "8.3 Why distributed version control?\nCentralized version control stores all files and the log of those files in one centralized location. Distributed version control stores files and logs in one or many locations and has tools for combining the different versions of files and logs.\nCentralized version control systems like Google Drive or Box are good for sharing a Microsoft Word document, but they are terrible for collaborating on code.\nDistributed version control allows for the simultaneous editing and running of code. It also allows for code development without sacrificing a working version of the code.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#git-vs.-github",
    "href": "08_reproducible-research-with-git.html#git-vs.-github",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.4 Git vs. GitHub",
    "text": "8.4 Git vs. GitHub\nGit is a distributed version-control system for tracking changes in code. Git is free, open-source software and can be used locally without an internet connection. It’s like a turbo-charged version of Microsoft Word’s track changes for code.\nGitHub, which is owned by Microsoft, is an online hosting service for version control using Git. It also contains useful tools for collaboration and project management. It’s like a turbo-charged version of Google Drive or Box for sharing repositories created using Git.\nAt first, it’s easy to mix up Git and GitHub. Just try to remember that they are separate tools that complement each other well.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#ssh-keys-for-authentication",
    "href": "08_reproducible-research-with-git.html#ssh-keys-for-authentication",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.5 SSH Keys for Authentication",
    "text": "8.5 SSH Keys for Authentication\nGitHub started requiring token-based or SSH-based authentication in 2021. We will focus on creating SSH keys for authentication. For instructions on creating a personal access token for authentication, see Section 8.10 below.\nWe will follow the instructions for setting up SSH keys using the console, or terminal window, from Jenny Bryan’s fantastic Happy Git with R.\n\n\n\n\n\n\nExercise 5\n\n\n\n\nFollow the instructions above for setting up SSH keys using the console. We recommend using the default key location and key name. You can choose whether or not to add a password for the key. Note that if you choose to add a password, you will need to enter that password every time you perform operations with GitHub - so make sure you’ll be able to remember it!\nWhen you get to the section of the instructions to provide the public key to GitHub, we recommend obtaining the public key as follows:\n\n\nIn a terminal window, run cat ~/.ssh/id_ed25519.pub\nHighlight the public key that is printed to the console and copy the text.\nFollow the instructions from Jenny Bryan at 10.5.3 to add the copied key to GitHub.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#git-github-workflow",
    "href": "08_reproducible-research-with-git.html#git-github-workflow",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.6 Git + GitHub Workflow",
    "text": "8.6 Git + GitHub Workflow\nRepository: A collection of files, often a directory, where files are organized and logged by Git.\nGit and GitHub organize projects into repositories. Typically, a “repo” will correspond with the place where you started a .Rproj. When working with Git and GitHub, your files will exist in two places: locally on your computer and remotely on GitHub.\nWhen creating a new repository, you can use either of the following alternatives:\n\nInitialize the repo locally on your computer and later add the repo to GitHub\nInitialize the repo remotely on GitHub and then copy (clone) the repo to your computer.\n\nTo create a repository (only needs to be done once per project):\ngit init initializes a local Git repository.\nOR\ngit clone &lt;link&gt; copies a remote repository from GitHub to the location of the working directory on your computer.\n\n\n\n\n\n\nExercise 5\n\n\n\nWe’re going to create a repo remotely on GitHub and clone it to our computer.\n\nGo to GitHub.\nClick the big green “New” button.\nCall the repo first-repo. Select a public repository and check the box that says Add a README file.\nClick the “Create Repository” button.\nClick the green “Code” button and copy the SSH link.\nNavigate to your cli-exercise folder on the command line using cd. Run git clone &lt; link &gt;, where you replace &lt; link &gt; with the SSH link you copied. This will create a folder called first-repo with your repo files, including the README.md file you created when you initialized the repo.\n\nSee Section 8.11 for the instructions on initializing a repo locally and then adding to GitHub.\n\n\n\n8.6.1 Basic Approach\n\nInitialize a repository for a project (we’ve already done this!).\nTell Git which files to track. Track scripts. Avoid tracking data or binary files like .xlsx. 1\nTake a snapshot of tracked files and add a commit message.\nSave the tracked files to the remote GitHub repository.\nRepeat, repeat, repeat\n\n\n\n\n\n\n\n\n8.6.2 Commands\ngit status prints out all of the important information about your repo. Use it before and after most commands to understand how code changes your repo.\ngit add &lt;file-name&gt; adds a file to staging. It says, “hey look at this!”.\ngit commit -m \"&lt;message&gt;\" commits changes made to added files to the repository. It says, “hey, take a snapshot of the things you looked at in the previous command.” Don’t forget the -m. 2\ngit push origin main pushes your local commits to the remote repository on GitHub. It says, “hey, take a look at these snapshots I just made”. It is possible to push to branches other than main. Simply replace main with the desired branch name.\ngit log --oneline shows the commit history in the repository.\ngit diff shows changes since the last commit.\n\n\n\n\n\n\nExercise 6\n\n\n\n\nNavigate to your first-repo directory on the command line and run git status to confirm that there is a git repo.\nIn RStudio, create a new R project. Select the option to associate the R project with an existing directory and select the first-repo directory. This will open the first-repo project.\nCreate a quarto document. Select the HTML option and give the document whatever title you’d like. Once the document is created, save the untitled document as “index.qmd”. Edit the YML header of the document to set self-contained to true as follows:\n\nformat: \n      html:\n        self-contained: true\n\nIn a code chunk, add library(tidyverse) and create any graph with library(ggplot2) and the storms dataset.\nRender the document to html.\nIn the terminal, run git add index.qmd index.html. Then run git status.\nThen run git commit -m \"add first graph\" to commit the two files with a commit message. Then run git status again and observe the change.\nFinally, run git push origin main to push the commit to GitHub. Navigate to the repo on GitHub to confirm that the files have been pushed to your remote repo.\nMake some changes to your the graph in your index.qmd file (e.g. change the color of a geometry, add labels, etc.). Then run steps 5-8 again (with a different commit message in step 7).\nIn the GitHub repo, click on index.qmd then click on the History icon in the top right corner. Note that you’ll see your two commits. Click on the most recent commit and notice that you can see the changes that you’ve made to the file.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#sec-ghp",
    "href": "08_reproducible-research-with-git.html#sec-ghp",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.7 GitHub Pages",
    "text": "8.7 GitHub Pages\nGitHub Pages are free websites hosted directly from a GitHub repository. With a free GitHub account, a GitHub repo must be public to create a GitHub page with that repo. When you create a GitHub page, you associate it with a specific branch of your repo. GitHub Pages will look for an index.html, index.md, or README.md file as the entry file for your site.\n\n\n\n\n\n\nExercise 7\n\n\n\n\nGo to your GitHub repository and select “Settings” on the top right.\nUnder the “Code and Automations” menu on the left side, select “Pages”.\nUnder “Build and Deployment” and “Branch”, use the drop down menu to change the branch to “main”. This will trigger the deployment of your GitHub page from the “main” branch of your repository. It will take a bit of time for your GitHub page to be ready.\nRefresh the page, and when you see a box that says “your site is live” at the top of your page, click the link to go to your website!",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#collaboration-using-git-github-with-branching",
    "href": "08_reproducible-research-with-git.html#collaboration-using-git-github-with-branching",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.8 Collaboration Using Git + GitHub (with branching)",
    "text": "8.8 Collaboration Using Git + GitHub (with branching)\nOur workflow so far has involved only one person, but the true power of GitHub comes through collaboration! There are two main models for collaborating with GitHub.\n\nShared repository with branching\nFork and pull\n\nWe have almost exclusively seen approach one used by collaborative teams. Approach two is more common when a repository has been publicized, and someone external to the work wants to propose changes while lacking the ability to “write” (or push) to that repository. Approach one is covered in more detail in the next chapter.\n\n\n\nA Common Branching Workflow\n\n\nTo add a collaborator to your repository, under Settings select “Collaborators” under the Access menu on the left. Select the “Add people” button and add your collaborators by entering their GitHub user name.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#conclusion",
    "href": "08_reproducible-research-with-git.html#conclusion",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.9 Conclusion",
    "text": "8.9 Conclusion\nGit is a distributed version-control system. It is used for tracking changes in the code. GitHub is an online hosting service for version control using git. Key workhorse commands are git status, git add, git commit -m &lt;message&gt; git push and git diff. GitHub is also great because it will host websites using GitHub Pages.\n\n8.9.1 Git is Confusing\n\n\n\n\n\n\n\n\n\nWe promise that it’s worth it.\n\n\n8.9.2 Resources\n\nGit Cheat Sheet\nHappy Git and GitHub for the UserR\nGit Pocket Guide\nGetting Git Right\nGit Learning Lab\nGit Handbook\nMastering Markdown\nUnderstanding the GitHub Flow\nDocumenting Your Projects on GitHub\nGit Tutorial",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#sec-ap-a",
    "href": "08_reproducible-research-with-git.html#sec-ap-a",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.10 Personal Access Tokens for Authentication",
    "text": "8.10 Personal Access Tokens for Authentication\n\nStarting on your GitHub account navigate through the following:\n\nClick your icon in the far right\nSelect Settings at the bottom of the drop down menu\nSelect Developer Settings on the bottom left\nSelect Personal access tokens on the bottom left\nSelect Generate new token\n\nSet up your Personal Access Token (PAT)\n\nAdd a note describing the use of your token. This is useful if you intend to generate multiple tokens for different uses.\nSelect “No expiration”. You may want tokens to expire if that access sensitive resources.\nSelect scopes. You must select at least the “repo” scope. You may want to add other scopes but they are not required for this course.\n\nClick Generate token\nThis is your only chance to view the token. Copy and paste the token and store it somewhere safe. If you lose the token, you can always generate a new token.\nGit will prompt you for your GitHub username and password sometimes while cloning repositories or pushing to private repositories. Use your GitHub username when prompted for username. Use your generated PAT when prompted for password.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#sec-app-b",
    "href": "08_reproducible-research-with-git.html#sec-app-b",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "8.11 Initialize a Repo Locally and Add to GitHub",
    "text": "8.11 Initialize a Repo Locally and Add to GitHub\nThis only needs to happen once per repository\n\nInitialize a local repository with git init as outlined above.\nOn GitHub, click the plus sign in the top, right corner and select New Repository.\nCreate a repository with the same name as your directory.\nCopy the code under …or push an existing repository from the command line and submit it in the command line that is already open.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "08_reproducible-research-with-git.html#footnotes",
    "href": "08_reproducible-research-with-git.html#footnotes",
    "title": "8  Reproducible Research with Git and GitHub",
    "section": "",
    "text": "Github refuses to store files larger than 100 MiB. This poses a challenge to writing reproducible code. However, many data sources can be downloaded directly from the web or via APIs, allowing code to be reproducible without relying on storing large data sets on Github. Materials later in this book discuss scaping data from the web and using APIs.↩︎\nThe -m stands for message. Writing a brief commit message like “fixes bug in data cleaning script” can help collaborators (including your future self) understand the purpose of your commits.↩︎",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reproducible Research with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "09_advanced-git.html",
    "href": "09_advanced-git.html",
    "title": "9  Advanced Git and Github",
    "section": "",
    "text": "9.1 The Issue &gt; Branch &gt; PR Workflow\nThere are several popular workflows for collaborating with Git and GitHub. This section outlines an issue-branch-pr workflow which is extremely common and which we use regularly.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Git and Github</span>"
    ]
  },
  {
    "objectID": "09_advanced-git.html#the-issue-branch-pr-workflow",
    "href": "09_advanced-git.html#the-issue-branch-pr-workflow",
    "title": "9  Advanced Git and Github",
    "section": "",
    "text": "9.1.1 GitHub issues\nGitHub issues are a GitHub project management tool for managing to-dos and conversations about a code base. This feature of Github can be seen as a built-in alternative to project management softwares like Jira, Trello, and monday, among many others.\n\nFor each substantial change to the code base, open a GitHub issue.\n\n\n9.1.2 Working with Branches\n\nBranching Motivation\nThe track changes feature in programs like with Microsoft Word follows each and every keystroke or click. In the previous chapter, we compared git to a super-charged version of track changes, and we think this comparison is fair. However, code is different than prose. If a program is not working, it isn’t desirable to document and share a change until that change creates a different, but still working, program. Working with other developers can complicate this desire to always commit sensible changes.\nMeet branching. Branching allows multiple versions of the same program to be developed simultaneously and then merged together when those versions are ready and operational.\n\n\nBranching Diagrammed\nRecall the following diagram from last chapter. We need a way to:\n\nCreate branches\nSwitch between branches\nCombine branches\n\n\n\n\nA standard git workflow\n\n\nA second stylized (and cute!) example of this workflow can be seen in this tweet from Jennifer Gilbert. The octopus on the left side of the image represents an existing, operational piece of code. Two programmers create separate copies (branches) of that code, work to create independent features (represented by the heart glasses and a cowboy hat), and then merge those features back to the master branch when those features are operational.\n\n\nHow to branch\ngit branch prints out important information about the available branches in a repo. It is similar to git status in that it provides useful information while not making any changes.\ngit switch -c &lt;new branch name&gt; creates a new branch and then navigates you to the new branch.\ngit switch &lt;new branch name&gt; navigates you from your current branch to the specified branch. It is only possible to switch between branches if there are no uncommitted changes. 1\n\nUse git switch main to navigate to the main branch. Use git pull origin main to update your local version of the main branch with any remote changes.\nUse git switch -c &lt;new branch name&gt; to create and switch to a new branch with the name iss&lt;number&gt;, where  is the issue number from GitHub.\nWork as if you are on the main branch but push to branch iss&lt;number&gt;.\n\nJenny Bryan provides a more thorough background.\n\n\n\n9.1.3 Pull requests\nThe easiest way to merge branches is on GitHub with pull requests. When you are ready to merge the code, push all of your code to your remote branch.\n\nOn GitHub, click the new pull request button.\n\n\n\n\nAn New Pull Request\n\n\n\nThen set a pull request from the branch on the right to the branch on the left.\n\n\n\n\n\n\n\nNavigate to the pull requests page and review the pull request.  \nMerge the pull request:\n\n\n\n\n\n\n\n\n9.1.4 Putting it all together\n\nOpen a GitHub issue for a substantial change to the project\nCreate a new branch named after the GitHub issue number\nAfter one or more commits, create a pull request and merge the code\n\n\n\n9.1.5 Merge conflicts\nIf you run into merge conflicts, either follow the GitHub instructions or follow Jenny Bryan’s instructions for resolving the conflict at the command line. Do not panic!",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Git and Github</span>"
    ]
  },
  {
    "objectID": "09_advanced-git.html#github-pages",
    "href": "09_advanced-git.html#github-pages",
    "title": "9  Advanced Git and Github",
    "section": "9.2 GitHub Pages",
    "text": "9.2 GitHub Pages\nAs we introduced in Section 8.7, GitHub offers a free and simple tool for a hosting website in a GitHub repository called GitHub pages. A basic setup is to use index.html or README.md as an index to connect individual pages together whose URLs are based on their file names.\nLet’s walk through an example:\n\n9.2.1 README as homepage\n\nCreate a new local directory with a README.md.\nRun echo \"&lt;h1&gt;Page 1&lt;\\h1&gt;\" &gt; page1.html at the command line.\ngit init, git add, and git commit your changes.\nCreate a remote repository on GitHub and link it to your local repository.\nPush your local changes.\nNavigate to Settings in your GitHub repo. Go to GitHub pages and set the source to “main branch”. The page will reload and a link will appear. Go to the link.\nAdd page1 to the end of the link\nAdd the link to page1 in the README. git add, git commit, and git push.\n\n\n\n9.2.2 index as homepage\nThe Urban Institute R Users Group website does not use the README as a home page. Instead it uses and index page.\n\nRun echo \"&lt;h1&gt;Index&lt;\\h1&gt;\" &gt; index.html at the command line.\nRun echo \"&lt;a href=\"https://awunderground.github.io/git-example/page1\"&gt;Page 1&lt;/a&gt;\" &gt; index.html at the command line where url is the link to Page 1 on GitHub pages.\nAdd, commit, and push your code to the main branch.\nAfter a few minutes return to the link provided on GitHub pages.",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Git and Github</span>"
    ]
  },
  {
    "objectID": "09_advanced-git.html#conclusion",
    "href": "09_advanced-git.html#conclusion",
    "title": "9  Advanced Git and Github",
    "section": "9.3 Conclusion",
    "text": "9.3 Conclusion\nGitHub Pages is a great project management tool. It can be integrated perfectly into the Issue &gt; Branch &gt; PR workflow. Branching is useful to allow separate collaborators to work on different features of a codebase simultaneously without interrupting each other. When conflicts do arise, do not fret! Merge conflicts are normal and can be resolved easily.\n\n9.3.1 More resources\n\nGit Cheat Sheet\nHappy Git and GitHub for the UserR\nGit Pocket Guide\nGetting Git Right\nGit Learning Lab\nThe Urban Institute’s Reproducibility Website and its Git and Github page",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Git and Github</span>"
    ]
  },
  {
    "objectID": "09_advanced-git.html#footnotes",
    "href": "09_advanced-git.html#footnotes",
    "title": "9  Advanced Git and Github",
    "section": "",
    "text": "git checkout is another exceedingly common git command. Many resources on the internet may encourage the usage of git checkout &lt;branch name&gt; to switch branches, or git checkout -b &lt;new branch name&gt; to create a new branch and then switch to it. This is okay! However, git checkout also has other capabilities, and that glut of functionality can be confusing to users. This makes git switch the simpler, more modern option.]↩︎",
    "crumbs": [
      "Reproducible Research",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Git and Github</span>"
    ]
  },
  {
    "objectID": "10_functions.html",
    "href": "10_functions.html",
    "title": "10  Advanced R Programming",
    "section": "",
    "text": "10.1 Review",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "10_functions.html#sec-review3",
    "href": "10_functions.html#sec-review3",
    "title": "10  Advanced R Programming",
    "section": "",
    "text": "10.1.1 Relational Data\nWe’ve almost exclusively used data frames up to this point. We leveraged relations within our data to pick subsets of the data with functions like filter().\n\nlibrary(tidyverse)\n\nmsleep |&gt; \n  filter(name == \"Cow\")\n\n# A tibble: 1 × 11\n  name  genus vore  order   conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cow   Bos   herbi Artiod… domesticated           4       0.7       0.667    20\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nImportantly, we almost never used indices or selected data by position, which can lead to errors if the underlying data change. An example of using indices and selecting data by position would be to pick data from row number 5 and column number 4. This idea of using the relations in data reduces the chances of making mistakes and leads to clearer code.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "10_functions.html#programming",
    "href": "10_functions.html#programming",
    "title": "10  Advanced R Programming",
    "section": "10.2 Programming",
    "text": "10.2 Programming\n\n10.2.1 Selecting Data\nThere are other ways to subset data, which are important when working with objects other than data frames. We will focus on [], [[]], and $.\n\nAtomic Vectors\nMuch of our work focuses on four of the six types of atomic vectors: logical, integer, double, and character. [] is useful for subsetting atomic vectors. Consider a vector with the first six letters of the alphabet:\n\nletters_short &lt;- letters[1:6]\n\nWe can use positive integers to subset to the first and fifth letters of the alphabet.\n\nletters_short[c(1, 5)]\n\n[1] \"a\" \"e\"\n\n\nWe can use negative integers to subset to the everything but the first and fifth letters of the alphabet.\n\nletters_short[c(-1, -5)]\n\n[1] \"b\" \"c\" \"d\" \"f\"\n\n\nWe can use Booleans (trues and falses) to subset to the first and fifth letters of the alphabet.\n\nletters_short[c(TRUE, FALSE, FALSE, FALSE, TRUE, FALSE)]\n\n[1] \"a\" \"e\"\n\n\nThis may seem silly, but we have many ways to create Booleans that we can then use to subset a vector.\n\nbooleans &lt;- letters_short %in% c(\"a\", \"e\")\n\nbooleans\n\n[1]  TRUE FALSE FALSE FALSE  TRUE FALSE\n\nletters_short[booleans]\n\n[1] \"a\" \"e\"\n\n\nWe can use a character vector to subset a named vector.\n\nnamed_vector &lt;- c(a = 1, b = 2, c = 3)\n\nnamed_vector\n\na b c \n1 2 3 \n\nnamed_vector[c(\"a\", \"c\")]\n\na c \n1 3 \n\n\nWe are able to select more than one element with [], which will not be true of [[]] and $. One thing to look out for is vector recycling. Let’s go back to letters_short, which is length six, but consider some indices of varying lengths.\n\nletters_short[TRUE]\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\"\n\nletters_short[c(TRUE, FALSE)]\n\n[1] \"a\" \"c\" \"e\"\n\nletters_short[c(TRUE, FALSE, TRUE)]\n\n[1] \"a\" \"c\" \"d\" \"f\"\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWow, R recycles the Booleans. Six is divisible by 1, 2, and 3, so there are many ways to recycle the index to subset letters_short. This is dangerous and can quietly cause analtyic errors.\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCreate letters_short.\nTry subsetting the vectors with indices with length four or five. What happens?\n\n\n\n\n\nLists\n[[]] and $ are useful for subsetting lists. Both can be used to subset data frames, but I recommending avoiding this.\nUnlike [], which returns multiple elements, [[]] and $ can only return a single element and [[]] and $ simplify objects by removing a layer of hierarchy.\n[[]] can select an element by position or name, while $ can only select an element by name. Consider a list with the first six letters of the alphabet.\n\nalphabet &lt;- list(\n  vowels = c(\"a\", \"e\"),\n  consonants = c(\"b\", \"c\", \"d\", \"f\")\n)\n\nWe can use [] to select the first or second element. In both cases, we get back a smaller list.\n\nalphabet[1]\n\n$vowels\n[1] \"a\" \"e\"\n\nclass(alphabet[1])\n\n[1] \"list\"\n\nalphabet[2]\n\n$consonants\n[1] \"b\" \"c\" \"d\" \"f\"\n\nclass(alphabet[2])\n\n[1] \"list\"\n\n\nWe can use [[]] to select the first or second element. Now, we get back a vector instead of a list. [[]] simplified the object by removing a level of hierarchy.\n\nalphabet[[1]]\n\n[1] \"a\" \"e\"\n\nclass(alphabet[[1]])\n\n[1] \"character\"\n\n\nWe can also use [[]] to select an object by name.\n\nalphabet[[\"vowels\"]]\n\n[1] \"a\" \"e\"\n\nclass(alphabet[[\"vowels\"]])\n\n[1] \"character\"\n\n\nWe can use $ to select either vector by name.\n\nalphabet$vowels\n\n[1] \"a\" \"e\"\n\nclass(alphabet$vowels)\n\n[1] \"character\"\n\n\nReferring to objects by name should make for code that is more robust to changing data.\n\nBeforeAfter\n\n\n\nalphabet1 &lt;- list(\n  vowels = c(\"a\", \"e\"),\n  consonants = c(\"b\", \"c\", \"d\", \"f\")\n)\n\nalphabet1[[2]]\n\n[1] \"b\" \"c\" \"d\" \"f\"\n\nalphabet1[[\"consonants\"]]\n\n[1] \"b\" \"c\" \"d\" \"f\"\n\n\n\n\n\nalphabet2 &lt;- list(\n  vowels = c(\"a\", \"e\"),\n  confusing = \"y\",\n  consonants = c(\"b\", \"c\", \"d\", \"f\")\n)\n\nalphabet2[[2]]\n\n[1] \"y\"\n\nalphabet2[[\"consonants\"]]\n\n[1] \"b\" \"c\" \"d\" \"f\"\n\n\n\n\n\nSubsetting lists can be difficult. Fortunately, RStudio has a tool than can help. Click on a list in your global environment. Navigate to the far right and click the list button with a green arrow. This will generate code and add it to the Console.\n\nInterestingly, this tool avoids $ and uses [[]] to pick the vector by name.\n\nalphabet[[\"vowels\"]]\n\n[1] \"a\" \"e\"\n\n\n\n\n\n10.2.2 Control Flow\nLoops are a fundamental programming tool for iteration; however, they are less common in R than in other programming languages. We encourage the map-reduce framework in the tidyverse’s purrr library which we introduce in Section 10.7. Even so, understanding the for loop and while loop is important because you will likely see them in other programmers’ R code, and they are extremely common in other programming languages.\n\nFor Loops\nFor loops have two main pieces: 1. a header and 2. a body. Headers define the number of iterations and potential inputs to the iteration. Bodies are iterated once per iteration. Here is a very simple example:\n\nfor (i in 1:10) {\n  \n  print(i)\n  \n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nWe can use headers several different ways. Like above, we may just want to repeat the values in the index.\n\nfruit &lt;- c(\"apple\", \"banana\", \"cantelope\")\nfor (b in fruit) {\n  \n  print(b)\n  \n}\n\n[1] \"apple\"\n[1] \"banana\"\n[1] \"cantelope\"\n\n\nWe can use the header as an index.\n\nfruit &lt;- c(\"apple\", \"banana\", \"cantelope\")\nfor (i in 1:3) {\n  \n  print(fruit[i])\n  \n}\n\n[1] \"apple\"\n[1] \"banana\"\n[1] \"cantelope\"\n\n\nWe can leverage the index to use results from previous iterations.\n\nresult &lt;- c(1, NA, NA, NA) \nfor (i in 2:4) {\n  \n  result[i] &lt;- result[i - 1] * 2\n  \n}\n\nresult\n\n[1] 1 2 4 8\n\n\nWe’ve now seen three different ways to use the header.\n\nSimply repeat the elements in the header (e.g. print i).\nUse the elements in the header as an index (e.g. select the \\(i^{th}\\) element).\nUse the header to reference past iterations (e.g. i - 1)\n\n\n\n\n\n\n\nDon’t grow vectors!\n\n\n\nIt is tempting to initialize a vector and then grow the vector with a for loop and c(). It is also tempting to initialize a data frame and then grow the data frame with bind_rows(). Because of R’s design, this is computationally very inefficient.\nThis is slow!:\n\nvec &lt;- c(1)\n\nfor (i in 2:10) {\n  \n  vec &lt;- c(vec, i)\n  \n  \n}\n\nvec\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\nIt is essential to pre-allocate vectors and then fill them in. It is also easy to make mistakes when creating indices (e.g. 1:length(x) may end up as c(1, 0)). seq_along() is a helpful alternative to :. The following pre-allocates a vector and then uses the length of the vector to create an index.\n\nnumbers &lt;- vector(mode = \"numeric\", length = 5)\n\nnumbers\n\n[1] 0 0 0 0 0\n\nfor (i in seq_along(numbers)) {\n  \n  numbers[i] &lt;- i\n  \n}\n\nnumbers\n\n[1] 1 2 3 4 5\n\n\nLet’s consider a simple random walk with 100 steps. In this case, the person starts at location zero and random takes one step forward or one step back.\n\nposition &lt;- vector(mode = \"numeric\", length = 100)\n\nset.seed(20230530)\nfor (iter in 2:length(position)) {\n  \n  position[iter] &lt;- position[iter - 1] + sample(x = c(-1, 1), size = 1)\n  \n  \n}\n\nposition\n\n  [1]  0 -1  0  1  2  1  0  1  0  1  2  1  0  1  2  1  2  1  2  1  2  1  2  1  2\n [26]  3  4  5  4  3  2  3  4  3  4  3  2  3  2  3  2  3  2  3  2  3  2  1  2  3\n [51]  2  3  4  3  4  5  6  5  6  5  6  7  6  5  6  7  8  9 10 11 12 13 12 13 12\n [76] 11 10  9 10  9  8  9 10 11 12 13 12 11 12 11 12 13 12 13 14 15 14 15 14 15\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nCreate the following list:\n\n\nalphabet &lt;- list(\n  vowels = c(\"a\", \"e\"),\n  confusing = \"y\",\n  consonants = c(\"b\", \"c\", \"d\", \"f\")\n)\n\n\nWrite a for loop and use str_to_upper() to transform all letters to upper case.\n\n\n\n\n\nWhile Loops\nWhile loops are similar to for loops; however, instead of predetermining the number of iterations in the header, while loops determine a condition in the header and run until that condition is met. This makes them useful for iterating for an unknown number of iterations. Though while loops are less common than for loops, one policy-relevant domain where while loops could be useful is simulation. A while loop could be useful to run a simulation indefinitely until certain conditions are met. Alternatively, both for and while loops could be used to run a simulation a specified number of times.\n\niteration &lt;- 0\nx &lt;- 2\n\nwhile (x &lt; 1000) {\n  \n  iteration &lt;- iteration + 1\n  x &lt;- x * 2\n  \n}\n\nx \n\n[1] 1024\n\niteration\n\n[1] 9\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nWrite the random walk from the for loop above as a while loop. Stop the while loop when position &lt; -10 or position &gt; 10. How many iterations did it take?\n\n\n\n\n\nif, else, and else if\nif_else() and case_when() apply conditional logic to a vector. We most frequently use those functions inside of mutate() to create a new variable or manipulate an existing variable.\nR also has if, else, and else if, which are used to select sections of code to run. This is incredibly useful when programming outside of data manipulation. For example, we can use if to download a file only if it doesn’t already exist.\n\nif (!file.exists(\"data.csv\")) {\n\n  download.file(url = \"web-url.csv\", destfile = \"data.csv\")\n\n}\n\nSelection control flow has two important pieces. First, there is a conditional statement inside (). If the condition is TRUE, then evaluate. If it is FALSE, then don’t evaluate. Second, there is a body contained in {}. Note the formatting in the above example.\nThe conditional statement must be a single TRUE or FALSE. If your statement involves more than one Boolean, then consider using all(), which evaluates to TRUE if everything is TRUE and any(), which evaluates to TRUE if anay element is TRUE.\nLet’s consider a more sophisticated example.\n\nif (distribution == \"normal\") {\n  \n  x &lt;- rnorm(n = 100)\n  \n} else if (distribution == \"poisson\") {\n  \n  x &lt;- rpois(n = 100, lambda = 8)\n  \n} else {\n  \n  stop(\"distribution mast be normal or poissoin\")\n  \n}\n\nThis style of using if, else if, and else is fundamental for including options in custom functions.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "10_functions.html#custom-functions",
    "href": "10_functions.html#custom-functions",
    "title": "10  Advanced R Programming",
    "section": "10.3 Custom Functions",
    "text": "10.3 Custom Functions\n\n10.3.1 Motivation\nCustom functions are an essential building block for good analyses. Custom functions are useful for abiding by the DRY (don’t repeat yourself) principle. Under our conception of DRY, we should create a function any time we do something three times.\nCopying-and-pasting is typically bad because it is easy to make mistakes and we typically want a single source source of truth in a script. Custom functions also promote modular code design and testing.\nThe bottom line: we want to write clear functions that do one and only one thing that are sufficiently tested so we are confident in their correctness.\n\n\n10.3.2 Examples\nLet’s consider a couple of examples from (Barrientos et al. 2021). This paper is a large-scale simulation of formally private mechanisms, which relates to several future chapters of this book.\nDivision by zero, which returns NaN, can be a real pain when comparing confidential and noisy results when the confidential value is zero. This function simply returns 0 when the denominator is 0.\n\n#' Safely divide number. When zero is in the denominator, return 0. \n#'\n#' @param numerator A numeric value for the numerator\n#' @param denominator A numeric value for the denominator\n#'\n#' @return A numeric ratio\n#'\nsafe_divide &lt;- function(numerator, denominator) {\n  \n  if (denominator == 0) {\n    \n    return(0)\n    \n  } else {\n    \n    return(numerator / denominator)\n    \n  }\n}\n\nThis function\n\nImplements the laplace or double exponential distribution, which isn’t included in base R.\nApplies a technique called the laplace mechanism.\n\n\n#' Apply the laplace mechanism\n#'\n#' @param eps Numeric epsilon privacy parameter\n#' @param gs Numeric global sensitivity for the statistics of interest\n#'\n#' @return\n#' \nlap_mech &lt;- function(eps, gs) {\n  \n  # Checking for proper values\n  if (any(eps &lt;= 0)) {\n    stop(\"The eps must be positive.\")\n  }\n  if (any(gs &lt;= 0)) {\n    stop(\"The GS must be positive.\")\n  }\n  \n  # Calculating the scale\n  scale &lt;- gs / eps\n\n  r &lt;- runif(1)\n\n  if(r &gt; 0.5) {\n    r2 &lt;- 1 - r\n    x &lt;- 0 - sign(r - 0.5) * scale * log(2 * r2)\n  } else {\n    x &lt;- 0 - sign(r - 0.5) * scale * log(2 * r)\n  }\n  \n  return(x)\n}\n\n\n\n10.3.3 Basics\nR has a robust system for creating custom functions. To create a custom function, use function():\n\nsay_hello &lt;- function() {\n  \n  \"hello\"\n   \n}\n\nsay_hello()\n\n[1] \"hello\"\n\n\nOftentimes, we want to pass parameters/arguments to our functions:\n\nsay_hello &lt;- function(name) {\n  \n  paste(\"hello,\", name)\n   \n}\n\nsay_hello(name = \"aaron\")\n\n[1] \"hello, aaron\"\n\n\nWe can also specify default values for parameters/arguments:\n\nsay_hello &lt;- function(name = \"aaron\") {\n  \n  paste(\"hello,\", name)\n   \n}\n\nsay_hello()\n\n[1] \"hello, aaron\"\n\nsay_hello(name = \"alex\")\n\n[1] \"hello, alex\"\n\n\nsay_hello() just prints something to the console. More often, we want to perform a bunch of operations and the then return some object like a vector or a data frame. By default, R will return the last unassigned object in a custom function. It isn’t required, but it is good practice to wrap the object to return in return().\nIt’s also good practice to document functions. With your cursor inside of a function, go Insert &gt; Insert Roxygen Skeleton:\n\n#' Say hello\n#'\n#' @param name A character vector with names\n#'\n#' @return A character vector with greetings to name\n#' \nsay_hello &lt;- function(name = \"aaron\") {\n  \n  greeting &lt;- paste(\"hello,\", name)\n  \n  return(greeting)\n  \n}\n\nsay_hello()\n\n[1] \"hello, aaron\"\n\n\nAs you can see from the Roxygen Skeleton template above, function documentation should contain the following:\n\nA description of what the function does\nA description of each function argument, including the class of the argument (e.g. string, integer, dataframe)\nA description of what the function returns, including the class of the object\n\nTips for writing functions:\n\nFunction names should be short but effectively describe what the function does. Function names should generally be verbs while function arguments should be nouns. See the Tidyverse style guide for more details on function naming and style.\nAs a general principle, functions should each do only one task. This makes it much easier to debug your code and reuse functions!\nUse :: (e.g. dplyr::filter() instead of filter()) when writing custom functions. This will create stabler code and make it easier to develop R packages.\n\n\n\n10.3.4 Functions with Multiple Outputs\nWhen return() is reached in a function, return() is evaluated, evaluation ends and R leaves the function.\n\nsow_return &lt;- function() {\n  \n  return(\"The function stops!\")\n  \n  return(\"This never happens!\")\n  \n}\n\nsow_return()\n\n[1] \"The function stops!\"\n\n\nIf the end of a function is reached without calling return(), the value from the last evaluated expression is returned.\nWe prefer to include return() at the end of functions for clarity even though return() doesn’t change the behavior of the function.\nSometimes we want to return more than one vector or data frame. list() is very helpful in these situations because it allows us technically to return a single object (the list) while actually passing out multiple objects (the components of the list).\n\nsummarize_results &lt;- function(x) {\n  \n  mean_x &lt;- mean(x)\n  \n  median_x &lt;- median(x)\n  \n  results &lt;- list(\n    mean = mean_x,\n    median = median_x\n  )\n  \n  return(results)\n  \n}\n\nsummarize_results(x = 1:10)\n\n$mean\n[1] 5.5\n\n$median\n[1] 5.5\n\n\n\n\n10.3.5 Referential Transparency\nR functions, like mathematical functions, should always return the exact same output for a given set of inputs.1 This is called referential transparency. R will not enforce this idea, so you must write good code.\n\nBad!\n\nbad_function &lt;- function(x) {\n  \n  x * y\n  \n}\n\ny &lt;- 2\nbad_function(x = 2)\n\n[1] 4\n\ny &lt;- 3\nbad_function(x = 2)\n\n[1] 6\n\n\n\n\nGood!\n\ngood_function &lt;- function(x, y) {\n  \n  x * y\n  \n}\n  \ny &lt;- 2\ngood_function(x = 2, y = 1)\n\n[1] 2\n\ny &lt;- 3\ngood_function(x = 2, y = 1)\n\n[1] 2\n\n\nBruno Rodriguez has a book and a blog that explore this idea further.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "10_functions.html#debugging",
    "href": "10_functions.html#debugging",
    "title": "10  Advanced R Programming",
    "section": "10.4 Debugging",
    "text": "10.4 Debugging\nR code inside of custom functions can be tougher to troubleshoot than R code outside of custom functions. Fortunately, R has a powerful debugging tool.\nThe debugger requires putting custom functions in their own scripts. This is covered in Section 10.6.\nTo set up the debugger, simply select the red dot to the left of a line of code in a custom function and then source the custom function. After, there should be a red dot next to the defined function in the global environment.2\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 10.1: Setting up the debugger\n\n\n\nNow, when the function is called it will stop at the red dot (the stop point). Importantly, the environment should reflect the environment inside of the function instead of the global environment.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 10.2: Using the debugger\n\n\n\nFinally, RStudio gives several controls for the debugger. There is a button to Continue to the end of the function. There is a button to Stop execution.\nThere is also a button with two brackets and a green arrow. This steps the debugger into another function. This is incredibly useful when functions are nested inside of functions.\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCreate a custom function with at least three different pieces of R code.\nSave the function in a .R script with the same name as the function.\nClick the little red dot to the left of first line of code in the .R script.\nSource the function with the source button at the top right.\nCall the function. You should enter the debugger.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "10_functions.html#benchmarking",
    "href": "10_functions.html#benchmarking",
    "title": "10  Advanced R Programming",
    "section": "10.5 Benchmarking",
    "text": "10.5 Benchmarking\nBenchmarking is the process of estimating the run time of code. Oftentimes, benchmarking is used to compare multiple pieces of code to pick the more performant code. This raises a couple of issues:\n\nComputing environments differ. My MacBook Pro with Apple M1 chips typically outperforms my work computer.\nOther software can slow performance. When I open up Spotify my R processes typically slow down.\n\nWe can’t solve problem 1 with an R package, but we can solve problem 2 by running tests multiple times. library(microbenchmark) makes this very easy.\nSuppose we are interested in the median of a vector of 1 million numbers. We can easily calculate this with median() or quantile(). Suppose we are concerned about computation speed, so lets test the code performance:\n\nlibrary(microbenchmark)\n\nx &lt;- 1:1000000\n\nmicrobenchmark::microbenchmark(\n  median(x),\n  quantile(x, probs = 0.5)\n)\n\nWarning in microbenchmark::microbenchmark(median(x), quantile(x, probs = 0.5)):\nless accurate nanosecond times to avoid potential integer overflows\n\n\nUnit: milliseconds\n                     expr      min       lq     mean   median       uq\n                median(x) 6.057299 6.475171 7.250659 6.542780 7.504128\n quantile(x, probs = 0.5) 3.570567 3.677290 4.103382 3.718167 4.503255\n       max neval cld\n 39.196123   100  a \n  6.304693   100   b\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nLet’s compare %&gt;% and |&gt; to see if they have comparable computation times. Consider this example from Stack Overflow, which shows |&gt; is clearly better.\n\nLoad library(microbenchmark) and add the microbenchmark() function.\nCreate x1 &lt;- 1:1000000, x2 &lt;- 1:1000000, and x3 &lt;- 1:1000000\nTest median(x1), x2 |&gt; median(), and x3 %&gt;% median().3",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "10_functions.html#sec-organizing-an-analysis",
    "href": "10_functions.html#sec-organizing-an-analysis",
    "title": "10  Advanced R Programming",
    "section": "10.6 Organizing an Analysis",
    "text": "10.6 Organizing an Analysis\nWe recommend writing functions for data analysis. We need a plan for how to add custom functions to our workflow built on RStudio projects and Quarto.\nWe typically recommending adding a directory called R or src in a project directory and then sourcing scripts in to Quarto documents. Keeping functions in separate scripts makes the functions easier to use in multiple documents and simplifies the debugging process outlined above.\nWe typically only add one function to an R script in the R/ directory and name the script after the function (without parentheses). Next, we source function scripts at the top of Quarto documents after loading packages with the source(). library(here) is essential if when sourcing from a Quarto document that is in a subdirectory of the project.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "10_functions.html#sec-purrr",
    "href": "10_functions.html#sec-purrr",
    "title": "10  Advanced R Programming",
    "section": "10.7 Iteration with library(purrr)",
    "text": "10.7 Iteration with library(purrr)\nMost functions in R are vectorized. That means a function operates on every element in a vector by default.\n\nteacher_names &lt;- c(\"aaron\", \"alex\", \"alena\")\n\nsay_hello(name = teacher_names)\n\n[1] \"hello, aaron\" \"hello, alex\"  \"hello, alena\"\n\n\nsay_hello() is vectorized. It works without iteration, but let’s just use it as an example to learn about iteration. Let’s start with a simple for loop.\n\n# the vector over which to iterate\nteacher_names &lt;- c(\"aaron\", \"alex\", \"alena\")\n\n# preallocate the output vector\ngreetings &lt;- vector(mode = \"character\", \n                    length = length(teacher_names)\n                    )\nfor (i in seq_along(teacher_names)) {\n  \n  greetings[i] &lt;- say_hello(teacher_names[i])\n  \n}\n\n# print the result\ngreetings\n\n[1] \"hello, aaron\" \"hello, alex\"  \"hello, alena\"\n\n\nThat’s a lot of typing. It is much more common in R to use library(purrr), which replaces apply functions.4 map() iterates over each element of a data structure called .x, applies a function called .f to each element, and then returns a list.\n\nlibrary(purrr)\n\nteacher_names &lt;- c(\"aaron\", \"alex\", \"alena\")\nmap(.x = teacher_names, .f = say_hello)\n\n[[1]]\n[1] \"hello, aaron\"\n\n[[2]]\n[1] \"hello, alex\"\n\n[[3]]\n[1] \"hello, alena\"\n\n\nmap() always returns a list. Sometimes we want to do something called map reduce. Functions like map_chr(), map_dbl(), and map_df() will map reduce. In this case, map_chr() returns a character vector instead of a list:\n\nmap_chr(.x = teacher_names, .f = say_hello)\n\n[1] \"hello, aaron\" \"hello, alex\"  \"hello, alena\"\n\n\nWe can quickly add anonymous functions in map() and its relatives with ~. A tibble is just a list of columns. This function iterates over each column, counts the number of missing values, and then returns a named vector.\n\nmap_dbl(.x = msleep, .f = ~sum(is.na(.x)))\n\n        name        genus         vore        order conservation  sleep_total \n           0            0            7            0           29            0 \n   sleep_rem  sleep_cycle        awake      brainwt       bodywt \n          22           51            0           27            0 \n\n\nThe purrr library also has functions that can iterate over multiple inputs simultaneously. To figure out which purrr function to use ask the following:\n\nHow many inputs do I have? Is it one (map), two (map2), or 3+ (pmap)?\nWhat do I want back? A numeric vector (dbl), character vector (chr), dataframe created by column-binding (dfc), dataframe created by row-binding (dfr), etc?\n\nThe purrr cheatsheet is a great place to go for help using purrr and outlines the different function options. You can also see the purrr documentation for more details.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "10_functions.html#conclusion",
    "href": "10_functions.html#conclusion",
    "title": "10  Advanced R Programming",
    "section": "10.8 Conclusion",
    "text": "10.8 Conclusion\nR has many useful built-in functions, and you can easily install other extremely useful functions by loading in R packages. However, in many cases, it is useful to write your own custom functions. This section illustrated how to write custom R functions with the function() {} syntax. When writing R functions, R’s debugger is extremely helpful to implement “break points” which can help identify errors. Custom functions can be especially powerful when combined with iteration, and the tidyverse’s purrr package offers an elegant way to apply this concept. Lastly, this section covers other programming fundamentals like if else statements, for loops and while loops.\n\n\n\n\nBarrientos, Andrés F., Aaron R. Williams, Joshua Snoke, and Claire McKay Bowen. 2021. “A Feasibility Study of Differentially Private Summary Statistics and Regression Analyses with Evaluations on Administrative and Survey Data.” https://doi.org/10.48550/ARXIV.2110.12055.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "10_functions.html#footnotes",
    "href": "10_functions.html#footnotes",
    "title": "10  Advanced R Programming",
    "section": "",
    "text": "This rule won’t exactly hold if the function contains random or stochastic code. In those cases, the function should return the same output every time if the seed is set with set.seed().↩︎\nAccording to Hadley Wickham, “You can think of an environment as a bag of names. Each name points to an object stored elsewhere in memory.” For more details, see the Environments chapter of Advanced R.↩︎\nDon’t be surprised when microbenchmark::microbenchmark() returns a row with expr median(x2) instead of x2 |&gt; median(). This is expected behavior because the base pipe is implemented as a syntax transformation. You can read more about this in this blog.↩︎\n“Apply functions” refers to a family of base R functions like lapply(). The map() family is similar, but it has more helper functions. You can read more in Advanced R.↩︎",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced R Programming</span>"
    ]
  },
  {
    "objectID": "12_apis.html",
    "href": "12_apis.html",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "",
    "text": "11.1 API Background",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#api-background",
    "href": "12_apis.html#api-background",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "",
    "text": "Application Programming Interface (API)\n\n\n\nAn Application Programming Interface (API) is a set of functions for programmatically accessing and/or processing data. Packages like library(dplyr) and library(ggplot2) have APIs that were carefully designed to be approachable. The dplyr website says, “dplyr is a part of the tidyverse, an ecosystem of packages designed with common APIs and a shared philosophy.”\n\n\n\n\n\n\n\n\nWeb API\n\n\n\nA web API is a way to interact with web-based software through code - usually to retrieve data or use a tool. We will only focus on web APIs in this tutorial. To use web APIs, actions between computers and web applications are typically communicated through URLs that are passed between the computers. Manipulating URLs is key to using web APIs.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#popular-web-apis",
    "href": "12_apis.html#popular-web-apis",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.2 Popular Web APIs",
    "text": "11.2 Popular Web APIs\n\nGoogle Maps APIs\nTwitter API\nGitHub API\nWorld Bank\nCensus API\nOpenFEC\nUrban Institute Education Data Portal\nStanford CoreNLP API\nOpenTripPlanner API\nWorld Health Organization API\nUNdata API",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#why-apis",
    "href": "12_apis.html#why-apis",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.3 Why APIs?",
    "text": "11.3 Why APIs?\n\n11.3.1 Why use APIs?\n\nThe data or method is only available through an API: Census data and IPEDS are available through point-and-click interfaces and APIs, but many data and methods are only available through APIs. For example, data from the GoogleMaps API and methods from the Stanford CoreNLP software are only available in R through APIs.\nAPIs promote reproducibility: Even with good documentation, pointing-and-clicking is tough to reproduce. APIs are useful for writing code to access data in an analysis before processing, modeling, and/or visualizing.\nAPIs enable scalability: It’s simple to point-and-click data for one state or one county. It’s much tougher to point-and-click state or county level data if the data are stored in separate files. APIs can be accessed programtically, which means anything can be iterated to scale.\n\n\n\n11.3.2 Why build APIs?\n\nAPIs enable scalability: APIs enable scalability for data or methods creators. For example, the Urban Institute builds data visualization applications like Explore Your School’s Changing Demographics on top of APIS.\nAPIs create standardization: APIs standardize the communication between applications. By creating a protocol for a store of data or a model, it makes it simpler for downstream applications to access the data or methods of upstream applications.\nDemocratize access to data or methods: Centralized storage and processing on the cloud are cheaper than personal computing on desktops and laptops. APIs allow users to access subsets of data or summary statistics quicker and cheaper than accessing all data and/or processing that data locally. For example, the new Urban Institute prototype summary API endpoints allow users to create summary statistics in API calls, which saves time and local storage.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#technical-background-part-i",
    "href": "12_apis.html#technical-background-part-i",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.4 Technical Background Part I",
    "text": "11.4 Technical Background Part I\n\nServer: Remote computer that stores and processes information for an API. Note: This does not need to be a literal server. Many APIs are serverless and hosted in the cloud.\nClient: Computer requesting information from the API. In our cases, this is your computer.\nHyper-Text Transfer Protocol (HTTP): Set of rules clients use to communicate with servers via the web (etiquette).\nRequest: Contacting a web API with specifics about the information you wish to have returned. In general, this is built up as a URL.\nResponse: A formatted set of information servers return to clients after requests.\n\n\n\n\n\n\nSource: Zapier\n\n\n\n\n\n\nUniform Resource Locators (URL)\n\n\n\nUniform Resource Locators (URL): Text string that specifies a web location, a method for retrieving information from that web location, and additional parameters. We use these every day!",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#process-without-r-code",
    "href": "12_apis.html#process-without-r-code",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.5 Process (without R code)",
    "text": "11.5 Process (without R code)\n\nClient builds up a URL and sends a request to a server\nClient receives a response from the server and checks for an error message\nClient parses the response into a useful object: data frame\n\n\n11.5.1 REQUEST (build up a URL)\nLet’s walk through an example from the Census Bureau.\n\nThe Census Bureau API contains hundreds of distinct data sets. Copy-and-paste https://api.census.gov/data.html into your web browser to see the list of datasets and documentation. Each one of these data sets is an API resource.\nAll Census Bureau API datasets are available from the same host name. Copy-and-paste https://api.census.gov/data/ into your browser (the previous link without .html). This contains information about all of the datasets in the previous link, only now the information is stored as JavaScript Object Notation (JSON) instead of HTML.\nEach dataset is accessed through a base URL which consists of the host name, the year, and the “dataset name”. Copy-and-paste https://api.census.gov/data/2014/pep/natstprc into your web browser. This returns Vintage 2014 Population Estimates: US, State, and PR Total Population and Components of Change. Each base URL corresponds to an API endpoint.\nAt this point, we’re still only seeing metadata. We need to add a query string with a method and parameters. Copy-and-paste https://api.census.gov/data/2014/pep/natstprc?get=STNAME,POP&DATE_=7&for=state:* into your browser. Note the query string begins with ?, includes the get method, and includes three parameters.\n\n\n\n11.5.2 Check for a server error in the response\n\nAll of the URLs in part 1 were correctly specified and all of them returned results. What happens when incorrect URLs are passed to an API? Open a new browser window and copy-and-paste\n\nhttps://api.census.gov/data/2014/pep/natstprc?get=STNAME,POP&DATE_=7&for=state:57\nNote: this call requests information for FIPs 57, which does not exist.\n\n\n11.5.3 Parse the response\nAPIs need to return complicated hierarchical data as text. To do this, most APIs use JavaScript Object Notation (JSON).\n\n\n\n\n\n\nJSON\n\n\n\nJSON is a plain text hierarchical data structure. JSON is not JavaScript code. Instead, it’s a non-rectangular method for storing data that can be accessed by most web applications and programming languages. Lists are made with []. Objects are made with {} and contain key-value pairs. JSON is good at representing non-rectangular data and is standard on the web.\nSome example JSON could be:\n{\n  “Class name”: “Intro to Data Science”,\n  “Class ID”: “PPOL 670”,\n  “Instructors”: [“Aaron R. Williams”, “Alex C. Engler”],\n  “Location”: {\n    “Building”: “Healy Hall”\n    “Room”: “105”\n  }\n}\n\n\n\nWeb APIs can also return Extensible Markup Language (XML) and HyperText Markup Language (HTML), but JSON is definitely most popular. We’ll use library(jsonlite) to parse hierarchical data and turn it into tidy data.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#technical-background-part-ii",
    "href": "12_apis.html#technical-background-part-ii",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.6 Technical Background Part II",
    "text": "11.6 Technical Background Part II\n\nAPI Resource An object in an API. In our case, a resource will almost always be a specific data set.\nAPI endpoint The point where a client communicates with a web API.\nMethod/verb: A verb that specifies the action the client wants to perform on the resource through an API endpoint:\n\nGET - Ask an API to send something to you\nPOST - send something to an API\nHEAD, PUT, DELETE, TRACE, OPTIONS, CONNECT, PATCH\n\nQuery string Part of a URL that assigns values to parameters. Query strings begin with ?. The general form is key=value.\nParameters: Arguments that are passed to APIs. Parameters are separated by &.\nHeaders Meta-information about GET requests. We will only use the User-Agent HTTP Header.\nBody Additional information sent to the server. This isn’t important for GET requests.\nServer response code (HTTP status code)\n\n100s: information responses\n200s: success\n300s: redirection\n400s: client error\n500s: server error",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#r-example-1",
    "href": "12_apis.html#r-example-1",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.7 R example 1",
    "text": "11.7 R example 1\nLet’s walk through the example above using R. First install library(httr) and library(jsonlite) with install.packages(c(\"httr\", \"jsonlite\")).\nhttr contains tools for working with HTTP and URLs. It contains functions for all HTTP methods including GET() and POST().\n\n11.7.1 REQUEST (build up a URL)\nUsing the final URL from the above example, lets query state names and state population in July 2014 for all states, Washington, D.C., and Puerto Rico. Note, it is good practice to create the link outside of GET() because we may need to manipulate the URL string in later examples. Note that * is a wildcard character which requests data for all possible values of the parameter.\n\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(jsonlite)\n\n# make the URL\nurl &lt;- \n  \"https://api.census.gov/data/2014/pep/natstprc?get=STNAME,POP&DATE_=7&for=state:*\"\n\n# use the URL to make a request from the API\npop_json &lt;- GET(url = url)\n\n\n\n11.7.2 Check for a server error in the response\n\nhttp_status(pop_json)\n\n$category\n[1] \"Success\"\n\n$reason\n[1] \"OK\"\n\n$message\n[1] \"Success: (200) OK\"",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#parse-the-response-1",
    "href": "12_apis.html#parse-the-response-1",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.8 3. Parse the response",
    "text": "11.8 3. Parse the response\n\n# get the contents of the response as a text string\npop_json &lt;- content(pop_json, as = \"text\")\n\n# create a character matrix from the JSON\npop_matrix &lt;- fromJSON(pop_json)\n\n# turn the body of the character matrix into a tibble\npop_data &lt;- as_tibble(pop_matrix[2:nrow(pop_matrix), ],\n                      .name_repair = \"minimal\")\n\n# add variable names to the tibble\nnames(pop_data) &lt;- pop_matrix[1, ]\n\npop_data\n\n# A tibble: 52 × 4\n   STNAME               POP      DATE_ state\n   &lt;chr&gt;                &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n 1 Alabama              4849377  7     01   \n 2 Alaska               736732   7     02   \n 3 Arizona              6731484  7     04   \n 4 Arkansas             2966369  7     05   \n 5 California           38802500 7     06   \n 6 Colorado             5355866  7     08   \n 7 Connecticut          3596677  7     09   \n 8 Delaware             935614   7     10   \n 9 District of Columbia 658893   7     11   \n10 Florida              19893297 7     12   \n# ℹ 42 more rows\n\n\nParsing the response can be trickiest step. Here, the data of interest are rectangular and the JSON object is simple to parse. Sometimes, the returned object will be a complicated hierarchical structure, which will demand writing more R code.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#terms-of-service-of-user-agents",
    "href": "12_apis.html#terms-of-service-of-user-agents",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.9 Terms of Service of User Agents",
    "text": "11.9 Terms of Service of User Agents\nAlways read an API’s terms of service to ensure that use of the API conforms to the API’s rules.\nFurthermore, it is a good idea to only run one API request at a time and to identify yourself as a user-agent in the header of the HTTP request. This is simple with user_agent() from library(httr):\n\nGET(url_link, \n    user_agent(\"Georgetown Univ. Student Data Collector (student@georgetown.edu).\"))\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nVisit the Urban Institute Education Data Portal API.\nRead the Terms of Use.\nLet’s pull data from IPEDS about full-time equivalent enrollment in graduate schools in 2016 in Washington, DC (FIP = 11). Create a new .R script called get-ed-data.R.\nRead the instructions for direct API access. Copy-and-paste the generic form of the API call into you .R script.\nNavigate to the documentation for Enrollment - Full time equivalent. Note that you may have to click “Full-time equivalent” under “Enrollment” in the menu on the left-hand side of the page to get to the relevant section. The example URL is “/api/v1/college-university/ipeds/enrollment-full-time-equivalent/{year}/{level_of_study}/” Note: The Urban Institute created an R package that simplifies this entire process. Let’s build our request from scratch just to prove that we can.\nCombine the endpoint URL with the example URL from earlier. This means combining the base URL with the URL and parameters for the endpoint.\nFill in the necessary parameters and filters.\nMake a GET request with GET(). Be sure to include the User-Agent.\nCheck the HTTP status.\nParse the response.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#authentication",
    "href": "12_apis.html#authentication",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.10 Authentication",
    "text": "11.10 Authentication\nMany APIs require authentication. For example, the Census API requires a user-specific API key for all API calls. (You can sign up here) This API key is simply passed as part of the path in the HTTP request.\n\"https://api.census.gov/data/2014/pep/natstprc\n?get=STNAME,POP&DATE_=7&for=state:*&key=your key here\" \nThere are other authentication methods, but this is most common.\nIt is a bad idea to share API credentials. NEVER post a credential on GitHub. A convenient solution is to use a credentials file with the library(dotenv) package as follows:\nFirst, install the package using install.packages(\"dotenv\") in the console and create a file called .env in the directory where your .Rproj is located. You may get a message that files starting with a “.” are reserved for the system, you should hit “ok” to proceed. You can store as many credentials as you want in this file, with each key-value pair on a new line. Note you need to hit enter after the last key-value pair so the file ends with a blank new line.\ncensus_api_key=&lt;key value&gt;\n\nThen, you can access and use credentials as follows:\n\nlibrary(dotenv)\nlibrary(here)\n\nload_dot_env()\ncredential &lt;- Sys.getenv(\"census_api_key\")\n\nurl &lt;- str_glue(\n  paste0(\"https://api.census.gov/data/2014/pep/natstprc\",\n         \"?get=STNAME,POP&DATE_=7&for=state:*&key={credential}\")\n)\n\nNote that you may have to restart your R session to load the file. Be sure to add this .env credentials file to your .gitignore!",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#pagination",
    "href": "12_apis.html#pagination",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.11 Pagination",
    "text": "11.11 Pagination\nA single API call could potentially return an unwieldy amount of information. This would be bad for the server, because the organization would need to pay for lots of computing power. This would also be bad for the client because the client could quickly become overwhelmed by data. To solve this issue, many APIs are paginated. Pagination is simply breaking API responses into subsets.\nFor example, the original example returned information for all states in the United States. When information is requested at the Census tract level, instead of returning information for the entire United States, information can only be requested one state at a time. Getting information for the entire United States will require iterating through each state.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#rate-limiting",
    "href": "12_apis.html#rate-limiting",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.12 Rate Limiting",
    "text": "11.12 Rate Limiting\nRate limiting is capping the number of requests by a client to an API in a given period of time. This is most relevant when results are paginated and iterating requests is necessary. It is also relevant when developing code to query an API–because a developer can burden the API with ultimately useless requests.\nIt is sometimes useful to add Sys.sleep() to R code, to pause the R code to give the API a break from requests between each request. Even 0.5 seconds can be the difference.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#r-example-2-advanced",
    "href": "12_apis.html#r-example-2-advanced",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.13 R example 2 (Advanced)",
    "text": "11.13 R example 2 (Advanced)\nThis example pulls information at the Census tract level. Because of pagination, the example requires a custom function and iterates that function using map_df() from library(purrr). It includes Sys.sleep() to pause the requests between each query.\nThe example pulls the estimated number of males (B01001_002E) and females (B01001_026E) in the 2018 5-year ACS for each Census tract in Alabama and Alaska.\nHere are a few select columns for the 2018 5-year ACS from the Census API documentation page:\n\n\n\nVintage\nDataset Name\nDataset Type\nGeography List\nVariable List\nGroup List\nExamples\n\n\n\n\n2018\nacs&gt;acs5&gt;profile\nAggregate\ngeographies\nvariables\ngroups\nexamples\n\n\n\n\n11.13.1 Write a custom function\nThis function 1. builds a URL and requests from the API, 2. checks for a server error, and 3. parses the response\n\nget_acs &lt;- function(fips, credential) {\n  \n  # build a URL\n  # paste0() is only used because the URL was too wide for the PDF\n  url &lt;- str_glue(\n    paste0(\n      \"https://api.census.gov/data/2018/acs/acs5\",\n      \"?get=B01001_002E,B01001_026E&for=tract:*&in=state:{fips}&key={credential}\"\n    )\n  )\n\n  # use the URL to make a request from the API\n  acs_json &lt;- GET(url = url)\n  \n  # get the contents of the response as a text string\n  acs_json &lt;- content(acs_json, as = \"text\")\n  \n  # create a character matrix from the JSON\n  acs_matrix &lt;- fromJSON(acs_json)\n  \n  # turn the body of the character matrix into a tibble\n  acs_data &lt;- as_tibble(acs_matrix[2:nrow(acs_matrix), ],\n                        .name_repair = \"minimal\")\n  \n  # add variable names to the tibble\n  names(acs_data) &lt;- acs_matrix[1, ]\n  \n  # pause to be polite\n  Sys.sleep(0.5)\n  \n  return(acs_data)\n\n}\n\n\n\n11.13.2 Create a vector of FIPs\nThis could be all states, districts, and territories. It’s only Alabama and Alaska for brevity.\n\nfips &lt;- c(\"01\", \"02\")\n\n\n\n11.13.3 Iterate the functions\nmap_df() iterates get_acs() along the vector of state FIPs and returns a tibble.\n\n# load credentials\ncredential &lt;- Sys.getenv(\"census_api_key\")\n\n# iterate the function over Alabama and Alaska\nmap_df(fips, .f = get_acs, credential = credential)\n\n# A tibble: 1,348 × 5\n   B01001_002E B01001_026E state county tract \n   &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 2409        2800        01    097    006501\n 2 5961        6372        01    097    006502\n 3 3098        2888        01    097    006701\n 4 1837        1993        01    097    006702\n 5 1410        1503        01    097    007201\n 6 1538        1136        01    097    007202\n 7 1586        1311        01    097    007400\n 8 680         705         01    097    007500\n 9 981         997         01    097    007600\n10 537         691         01    097    007700\n# ℹ 1,338 more rows",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#r-packages",
    "href": "12_apis.html#r-packages",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.14 R Packages",
    "text": "11.14 R Packages\nThere are R packages that simplify interacting with many popular APIs. library(tidycensus) (tutorial here) and library(censusapi) (tutorial here) simplifies navigating Census documentation, checking the status code, building URLs for the Census API, and parsing JSON responses. This can save a lot of time and effort! The following code is one iteration of the advanced example from above!\n\nlibrary(censusapi)\n\ngetCensus(\n  name = \"acs/acs5\",\n  vars = c(\"B01001_002E\", \"B01001_026E\"), \n  region = \"tract:*\",\n  regionin = \"state:01\",\n  vintage = 2018,\n  key = credential\n) %&gt;%\n  as_tibble()\n\n# A tibble: 1,181 × 5\n   state county tract  B01001_002E B01001_026E\n   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 01    097    006501        2409        2800\n 2 01    097    006502        5961        6372\n 3 01    097    006701        3098        2888\n 4 01    097    006702        1837        1993\n 5 01    097    007201        1410        1503\n 6 01    097    007202        1538        1136\n 7 01    097    007400        1586        1311\n 8 01    097    007500         680         705\n 9 01    097    007600         981         997\n10 01    097    007700         537         691\n# ℹ 1,171 more rows",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "12_apis.html#conclusion",
    "href": "12_apis.html#conclusion",
    "title": "11  Web Application Programming Interfaces (Web APIs)",
    "section": "11.15 Conclusion",
    "text": "11.15 Conclusion\nR provides robust tools to call APIs. Many R packages, including some developed by data scientists at the Urban Institute, provide more user-friendly interfaces to APIs. Remember to be polite when calling APIs and not to share API credentials publicly.\n\n11.15.1 Popular R API packages and R API packages developed by the Urban Institute\n\ncensusapi\ntidycensus\ntigris\nUrban Institute Education Data Portal\nUrban Institute sedtR package\n\n\n\n11.15.2 Resources\n\nZapier Intro to APIs\nBest practices for API packages\nWhy we built the Education Data Portal\nWhy we built an API for Urban’s Education Data Portal\nHow we built the API for the Education Data Portal\nDemocratizing Big Data Processing for the Education Data Portal\nBuilding R and Stata packages for the Education Data Portal",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Web Application Programming Interfaces (Web APIs)</span>"
    ]
  },
  {
    "objectID": "13_web-scraping.html",
    "href": "13_web-scraping.html",
    "title": "12  Web Scraping",
    "section": "",
    "text": "12.1 Review\nWe explored pulling data from web APIs in DSPP1. With web APIs, stewards are often carefully thinking about how to share information. This will not be the case with web scraping.\nWe also explored extracting data from Excel workbooks in Section 02. We will build on some of the ideas in that section.\nRecall that if we have a list of elements, we can extract the \\(i^{th}\\) element with [[]]. For example, we can extract the third data frame from a list of data frames called data with data[[3]].\nRecall that we can use map() to iterate a function across each element of a vector. Consider the following example:\ntimes2 &lt;- function(x) x * 2\n\nx &lt;- 1:3\n\nmap(.x = x, .f = times2)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "13_web-scraping.html#introduction-and-motivation",
    "href": "13_web-scraping.html#introduction-and-motivation",
    "title": "12  Web Scraping",
    "section": "12.2 Introduction and Motivation",
    "text": "12.2 Introduction and Motivation\nThe Internet is an immense source of information for research. Sometimes we can easily download data of interest in an ideal format with the click of a download button or a single API call.\nBut it probably won’t be long until we need data that require many download button clicks. Or worse, we may want data from web pages that don’t have a download button at all.\nConsider a few examples.\n\nThe Urban Institute’s Boosting Upward Mobility from Poverty project programmatically downloaded 51 .xslx workbooks when building the Upward Mobility Data Tables.\nWe worked with the text of executive orders going back to the Clinton Administration when learning text analysis in DSPP1. Unfortunately, the Federal Register doesn’t publish a massive file of executive orders. So we iterated through websites for each executive order, scraped the text, and cleaned the data.\nThe Urban Institute scraped course descriptions from Florida community colleges to understand opportunities for work-based learning.\nThe Billion Prices Project web scraped millions of prices each day from online retailers. The project used the data to construct real-time price indices that limited political interference and to research concepts like price stickiness.\n\nWe will explore two approaches for gathering information from the web.\n\nIteratively downloading files: Sometimes websites contain useful information across many files that need to be separately downloaded. We will use code to download these files. Ultimately, these files can be combined into one larger data set for research.\nScraping content from the body of websites: Sometimes useful information is stored as tables or lists in the body of websites. We will use code to scrape this information and then parse and clean the result.\n\nSometimes we download many PDF files using the first approach. A related method that we will not cover that is useful for gathering information from the web is extracting text data from PDFs.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "13_web-scraping.html#legal-and-ethical-considerations",
    "href": "13_web-scraping.html#legal-and-ethical-considerations",
    "title": "12  Web Scraping",
    "section": "12.3 Legal and Ethical Considerations",
    "text": "12.3 Legal and Ethical Considerations\nIt is important to consider the legal and ethical implications of any data collection. Collecting data from the web through methods like web scraping raises serious ethical and legal considerations.\n\n12.3.1 Legal1\nDifferent countries have different laws that affect web scraping. The United States has different laws and legal interpretations than countries in Europe, which are largely regulated by the European Union. In general, the United States has more relaxed policies than the European when it comes to gathering data from the web.\nR for Data Science (2e) contains a clear and approachable rundown of legal consideration for gathering information for the web. We adopt their three-part standard of “public, non-personal, and factual”, which relate to terms of service, personally identifiable information, and copyright.\nWe will focus solely on laws in the United States.\n\nTerms of Service\nThe legal environment for web scraping is in flux, but US Courts have created an environment that is legally supportive of gathering public information from the web.\nFirst, we need to understand how many websites bar web scraping. Second, we need to understand when we can ignore these rules.\n\n\n\n\n\n\nTerms of Service\n\n\n\nA terms of service is a list of rules posted by the provider of a website, web service, or software.\n\n\nTerms of Service for many websites bar web scraping.\nFor example, LinkedIn’s Terms of Service says users agree to not “Develop, support or use software, devices, scripts, robots or any other means or processes (including crawlers, browser plugins and add-ons or any other technology) to scrape the Services or otherwise copy profiles and other data from the Services;”\nThis sounds like the end of web scraping, but as Wickham, Çetinkaya-Rundel, and Grolemund (2023) note, Terms of Service end up being a “legal land grab” for companies. It isn’t clear how LinkedIn would legally enforce this. HiQ Labs v. LinkedIn from the United States Court of Appeals for the Ninth Circuit bars Computer Fraud and Abuse Act (CFAA) claims against web scraping public information.2\nWe follow a simple guideline: it is acceptable to scrape information when we don’t need to create an account.\n\n\n\n12.3.2 PII\n\n\n\n\n\n\nPersonal Identifiable Information\n\n\n\nPersonal Identifiable Information (PII) is any information that can be used to directly identify an individual.\n\n\nPublic information on the Internet often contains PII, which raises legal and ethical challenges. We will discuss the ethics of PII later.\nThe legal considerations are trans-Atlantic. The General Data Protection Regulation (GDPR) is a European Union regulation about information privacy. It contains strict rules about the collection and storage of PII. It applies to almost everyone collecting data inside the EU. The GDPR is also extraterritorial, which means its rules can apply outside of the EU under certain circumstances like when an American company gathers information about EU individuals.\nWe will avoid gathering PII, so we don’t need to consider PII.\n\nCopyright\n\n\n\n\n\n\nCopyright Law\n\n\n\n\nCopyright protection subsists, in accordance with this title, in original works of authorship fixed in any tangible medium of expression, now known or later developed, from which they can be perceived, reproduced, or otherwise communicated, either directly or with the aid of a machine or device. Works of authorship include the following categories:\n\n\nliterary works;\n\n\nmusical works, including any accompanying words;\n\n\ndramatic works, including any accompanying music;\n\n\npantomimes and choreographic works;\n\n\npictorial, graphic, and sculptural works;\n\n\nmotion pictures and other audiovisual works;\n\n\nsound recordings; and\n\n\narchitectural works.\n\n\nIn no case does copyright protection for an original work of authorship extend to any idea, procedure, process, system, method of operation, concept, principle, or discovery, regardless of the form in which it is described, explained, illustrated, or embodied in such work.\n\n17 U.S.C.\n\n\nOur final legal concern for gathering information from the Internet is copyright law. We have two main options for avoiding copyright limitations.\n\nWe can avoid copyright protections by not scraping authored content in the protected categories (i.e. literary works and sound recordings). Fortunately, factual data are not typically protected by copyright.\nWe can scrape information that is fair use. This is important if we want to use images, films, music, or extended text as data.\n\nWe will focus on data that are not copyrighted.\n\n\n\n12.3.3 Ethical\nWe now turn to ethical considerations and some of the best-practices for gathering information from the web. In general, we will aim to be polite, give credit, and respect individual information.\n\nBe polite\nIt is expensive and time-consuming to host data on the web. Hosts experience a small burden every time we access a website. This burden is small but can quickly grow with repeated queries. Just like with web APIs, we want to pace the burden of our access to be polite.\n\n\n\n\n\n\nRate Limiting\n\n\n\nRate limiting is the intentional slowing of web traffic for a user or users.\n\n\nWe will use Sys.sleep() in custom functions to slow our web scraping and ease the burden of our web scraping on web hosts.\n\n\n\n\n\n\nrobots.txt\n\n\n\nrobots.txt tells web crawlers and scrapers which URLs the crawler is allowed to access on a website.\n\n\nMany websites contain a robots.txt file. Consider examples from the Urban Institute and White House.\nWe can manually look at the robots.txt. For example, just visit https://www.urban.org/robots.txt or https://www.whitehouse.gov/robots.txt. We can also use library(polite), which will automatically look at the robots.txt.\n\n\nGive Credit\nAcademia and the research profession undervalue the collection and curation of data. Generally speaking, no one gets tenure for constructing even the most important data sets. It is important to give credit for data accessed from the web. Ideally, add the citation to Zotero and then easily add it to your manuscript in Quarto.\nBe sure to make it easy for others to cite data sets that you create. Include an example citation like IPUMS or create a DOI for your data.\nThe rise of generative AI models like GPT-3, Stable Diffusion, DALL-E 2 makes urgent considerations of giving credit. These models consume massive amounts of training data, and it isn’t clear where the training data come from or the legal and ethical implications of the training data.3\nConsider a few current events:\n\nSarah Silverman is suing OpenAI because she “never gave permission for OpenAI to ingest the digital version of her 2010 book to train its AI models, and it was likely stolen from a ‘shadow library’ of pirated works.”\nSomepalli et al. (2023) use state-of-the-art image retrieval models to find that generative AI models like the popular the popular Stable Diffusion model “blatantly copy from their training data.” This is a major problem if the training data are copyrighted. The first page of their paper (here) contains some dramatic examples.\nFinally, this Harvard Business Review article discusses the intellectual property problem facing generative AI.\n\n\n\nRespect Individual Information\nData science methods should adhere to the same ethical standards as any research method. The social sciences have ethical norms about protecting privacy (discussed later) and informed consent.\n\n\n\n\n\n\nDiscussion\n\n\n\nIs it appropriate to collect and share public PII?\nDo these norms apply to data that is already public on the Internet?\n\n\nLet’s consider an example. In 2016, researchers posted data about 70,000 OkCupid accounts. The data didn’t contain names but did contain usernames. The data also contained many sensitive variables including topics like sexual habits and politics.\nThe release drew strong reactions from some research ethicists including Michael Zimmer and Os Keyes.4\nFellegi (1972) defines data privacy as the ability “to determine what information about ourselves we will share with others”. Maybe OkCupid users made the decision to forego confidentiality when they published their accounts. Many institutional ethics committees do not require informed consent for public data.\nRavn, Barnwell, and Barbosa Neves (2020) do a good job developing a conceptual framework that bridges the gap between the view that all public data require informed consent and the view that no public data require informed consent.\nIt’s possible to conceive of a web scraping research project that is purely observational that adheres to the ethical standards of research and contains potentially disclosive information about individuals. Fortunately, researchers can typically use Institutional Review Boards and research ethicists to navigate these questions.\nAs a basic standard, we will avoid collecting PII and use anonymization techniques to limit the risk of re-identification.\nWe will also focus on applications where the host of information crudely shares the information. There are ample opportunities to create value by gathering information from government sources and converting it into more useful formats. For example, the government too often shares information in .xls and .xlsx files, clunky web interfaces, and PDFs.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "13_web-scraping.html#programatically-downloading-data",
    "href": "13_web-scraping.html#programatically-downloading-data",
    "title": "12  Web Scraping",
    "section": "12.4 Programatically Downloading Data",
    "text": "12.4 Programatically Downloading Data\nThe County Health Rankings & Roadmaps is a source of state and local information.\nSuppose we are interested in Injury Deaths at the state level. We can click through the interface and download a .xlsx file for each state.\n\n12.4.1 Downloading a Single File\n\nStart here.\nUsing the interface at the bottom of the page, we can navigate to the page for “Virginia.”\nNext, we can click “View State Data.”\nNext, we can click “Download Virginia data sets.”\n\nThat’s a lot of clicks to get here.\nIf we want to download “2023 Virginia Data”, we can typically right click on the link and select “Copy Link Address”. This should return one of the following two URLS:\nhttps://www.countyhealthrankings.org/sites/default/files/media/document/2023%20County%20Health%20Rankings%20Virginia%20Data%20-%20v2.xlsx\nhttps://www.countyhealthrankings.org/sites/default/files/media/document/2023 County Health Rankings Virginia Data - v2.xlsx\nSpaces are special characters in URLs and they are sometimes encoded as %20. Both URLs above work in the web browser, but only the URL with %20 will work with code.\nAs we’ve seen several times before, we could use read_csv() to directly download the data from the Internet if the file was a .csv.5 We need to download this file because it is an Excel file, which we can do with download.file() provided we include a destfile.\n\ndownload.file(\n  url = \"https://www.countyhealthrankings.org/sites/default/files/media/document/2023%20County%20Health%20Rankings%20Virginia%20Data%20-%20v2.xlsx\", \n  destfile = \"data/virginia-injury-deaths.xlsx\"\n)\n\n\n\n12.4.2 Downloading Multiple Files\nIf we click through and find the links for several states, we see that all of the download links follow a common pattern. For example, the URL for Vermont is\nhttps://www.countyhealthrankings.org/sites/default/files/media/document/2023 County Health Rankings Vermont Data - v2.xlsx\nThe URLs only differ by \"Virginia\" and \"Vermont\". If we can create a vector of URLs by changing state name, then it is simple to iterate downloading the data. We will only download data for two states, but we can imagine downloading data for many states or many counties. Here are three R tips:\n\npaste0() and str_glue() from library(stringr) are useful for creating URLs and destination files.\nwalk() from library(purrr) can iterate functions. It’s like map(), but we use it when we are interested in the side-effect of a function.6\nSometimes data are messy and we want to be polite. Custom functions can help with rate limiting and cleaning data.\n\n\ndownload_chr &lt;- function(url, destfile) {\n\n  download.file(url = url, destfile = destfile)\n\n  Sys.sleep(0.5)\n\n}\n\nstates &lt;- c(\"Virginia\", \"Vermont\")\n\nurls &lt;- paste0(\n  \"https://www.countyhealthrankings.org/sites/default/files/\",\n  \"media/document/2023%20County%20Health%20Rankings%20\",\n  states,\n  \"%20Data%20-%20v2.xlsx\"\n)\n\noutput_files &lt;- paste0(\"data/\", states, \".xlsx\")\n\nwalk2(.x = urls, .y = output_files, .f = download_chr)\n\n\n\n\n\n\n\nExercise 1\n\n\n\nSOI Tax Stats - Historic Table 2 provides individual income and tax data, by state and size of adjusted gross income. The website contains a bulleted list of URLs and each URL downloads a .xlsx file.\n\nUse download.file() to download the file for Alabama.\nExplore the URLs using “Copy Link Address”.\nIterate pulling the data for Alabama, Alaska, and Arizona.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "13_web-scraping.html#web-scraping-with-rvest",
    "href": "13_web-scraping.html#web-scraping-with-rvest",
    "title": "12  Web Scraping",
    "section": "12.5 Web Scraping with rvest",
    "text": "12.5 Web Scraping with rvest\nWe now pivot to situations where useful information is stored in the body of web pages.\n\n12.5.1 Web Design\nIt’s simple to build a website with Quarto because it abstracts away most of web development. For example, Markdown is just a shortcut to write HTML. Web scraping requires us to learn more about web development than when we use Quarto.\nThe user interface of websites can be built with just HTML, but most websites contain HTML, CSS, and JavaScript. The development the interface of websites with HTML, CSS, and JavaScript is called front-end web development.\n\n\n\n\n\n\nHyper Text Markup Language\n\n\n\nHyper Text Markup Language (HTML) is the standard language for creating web content. HTML is a markup language, which means it has code for creating structure and and formatting.\n\n\nThe following HTML generates a bulleted list of names.\n&lt;ul&gt;\n  &lt;li&gt;Alex&lt;/li&gt;\n  &lt;li&gt;Aaron&lt;/li&gt;\n  &lt;li&gt;Alena&lt;/li&gt;\n&lt;/ul&gt;\n\n\n\n\n\n\nCascading Style Sheets\n\n\n\nCascading Style Sheets (CSS) describes hot HTML elements should be styled when they are displayed.\n\n\nFor example, the following CSS adds extra space after sections with ## in our class notes.\n.level2 {\n  margin-bottom: 80px;\n}\n\n\n\n\n\n\nJavaScript\n\n\n\nJavaScript is a programming language that runs in web browsers and is used to build interactivity in web interfaces.\n\n\nQuarto comes with default CSS and JavaScript. library(leaflet) and Shiny are popular tools for building JavaScript applications with R. We will focus on web scraping using HTML and CSS.\nFirst, we will cover a few important HTML concepts. W3Schools offers a thorough introduction. Consider the following simple website built from HTML:\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Hello World!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1 class='important'&gt;Bigger Title!&lt;/h1&gt;\n&lt;h2 class='important'&gt;Big Title!&lt;/h1&gt;\n&lt;p&gt;My first paragraph.&lt;/p&gt;\n&lt;p id='special-paragraph'&gt;My first paragraph.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nAn HTML element is a start tag, some content, and an end tag. Every start tag has a matching end tag. For example, &lt;body and &lt;/body&gt;. &lt;html&gt;, &lt;head&gt;, and &lt;body&gt; are required elements for all web pages. Other HTML elements include &lt;h1&gt;, &lt;h2&gt;, and &lt;p&gt;.\nHTML attributes are name/value pairs that provide additional information about elements. HTML attributes are optional and are like function arguments for HTML elements.\nTwo HTML attributes, classes and ids, are particularly important for web scraping.\n\nHTML classes are HTML attributes that label multiple HTML elements. These classes are useful for styling HTML elements using CSS. Multiple elements can have the same class.\nHTML ids are HTML attributes that label one HTML element. Ids are useful for styling singular HTML elements using CSS. Each ID can be used only one time in an HTML document.\n\nWe can view HTML for any website by right clicking in our web browser and selecting “View Page Source.”7\n\n\n\n\n\n\nExercise 2\n\n\n\n\nInspect the HTML behind this list of “Hello World examples”.\nInspect the HTML behind the Wikipedia page for Jerzy Neyman.\n\n\n\nSecond, we will explore CSS. CSS relies on HTML elements, HTML classes, and HTML ids to style HTML content. CSS selectors can directly reference HTML elements. For example, the following selectors change the style of paragraphs and titles.\np {\n  color: red;\n}\n\nh1 {\n  font-family: wingdings;\n}\nCSS selectors can reference HTML classes. For example, the following selector changes the style of HTML elements with class='important'.\n.important {\n  font-family: wingdings;\n}\nCSS selectors can reference also reference HTML IDs. For example, the following selector changes the style of the one element with id='special-paragraph'\n#special-paragraph {\n  color: pink;\n}\nWe can explore CSS by right clicking and selecting Inspect. Most modern websites have a lot of HTML and a lot of CSS. We can find the CSS for specific elements in a website with the button at the top left of the new window that just appeared.\n\n\n\nInspecting CSS\n\n\n\n\n12.5.2 Tables\nlibrary(rvest) is the main tool for scraping static websites with R. We’ll start with examples that contain information in HTML tables.8\nHTML tables store information in tables in websites using the &lt;table&gt;, &lt;tr&gt;, &lt;th&gt;, and &lt;td&gt;. If the data of interest are stored in tables, then it can be trivial to scrape the information.\nConsider the Wikipedia page for the 2012 Presidential Election. We can scrape all 46 tables from the page with two lines of code. We use the WayBack Machine to ensure the content is stable.\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\ntables &lt;- read_html(\"https://web.archive.org/web/20230814004444/https://en.wikipedia.org/wiki/2012_United_States_presidential_election\") |&gt;\n  html_table()\n\nSuppose we are interested in the table about presidential debates. We can extract that element from the list of tables.\n\ntables[[18]]\n\n# A tibble: 12 × 9\n   `Presidential candidate`     Party `Home state` `Popular vote` `Popular vote`\n   &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;         \n 1 \"Presidential candidate\"     Party Home state   Count          Percentage    \n 2 \"Barack Hussein Obama II\"    Demo… Illinois     65,915,795     51.06%        \n 3 \"Willard Mitt Romney\"        Repu… Massachuset… 60,933,504     47.20%        \n 4 \"Gary Earl Johnson\"          Libe… New Mexico   1,275,971      0.99%         \n 5 \"Jill Ellen Stein\"           Green Massachuset… 469,627        0.36%         \n 6 \"Virgil Hamlin Goode Jr.\"    Cons… Virginia     122,389        0.11%         \n 7 \"Roseanne Cherrie Barr\"      Peac… Utah         67,326         0.05%         \n 8 \"Ross Carl \\\"Rocky\\\" Anders… Just… Utah         43,018         0.03%         \n 9 \"Thomas Conrad Hoefling\"     Amer… Nebraska     40,628         0.03%         \n10 \"Other\"                      Other Other        217,152        0.17%         \n11 \"Total\"                      Total Total        129,085,410    100%          \n12 \"Needed to win\"              Need… Needed to w… Needed to win  Needed to win \n# ℹ 4 more variables: Electoralvote &lt;chr&gt;, `Running mate` &lt;chr&gt;,\n#   `Running mate` &lt;chr&gt;, `Running mate` &lt;chr&gt;\n\n\nOf course, we want to be polite. library(polite) makes this very simple. “The three pillars of a polite session are seeking permission, taking slowly and never asking twice.”\nWe’ll use bow() to start a session and declare our user agent, and scrape() instead of read_html().9\n\nlibrary(polite)\n\nsession &lt;- bow(\n  url = \"https://web.archive.org/web/20230814004444/https://en.wikipedia.org/wiki/2012_United_States_presidential_election\",\n  user_agent = \"Georgetown students learning scraping -- arw109@georgetown.edu\"\n)\n\nsession\n\n&lt;polite session&gt; https://web.archive.org/web/20230814004444/https://en.wikipedia.org/wiki/2012_United_States_presidential_election\n    User-agent: Georgetown students learning scraping -- arw109@georgetown.edu\n    robots.txt: 1 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n\nelection_page &lt;- session |&gt;\n  scrape() \n  \ntables &lt;- election_page |&gt;\n  html_table()\n\ntables[[18]]\n\n# A tibble: 12 × 9\n   `Presidential candidate`     Party `Home state` `Popular vote` `Popular vote`\n   &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;         \n 1 \"Presidential candidate\"     Party Home state   Count          Percentage    \n 2 \"Barack Hussein Obama II\"    Demo… Illinois     65,915,795     51.06%        \n 3 \"Willard Mitt Romney\"        Repu… Massachuset… 60,933,504     47.20%        \n 4 \"Gary Earl Johnson\"          Libe… New Mexico   1,275,971      0.99%         \n 5 \"Jill Ellen Stein\"           Green Massachuset… 469,627        0.36%         \n 6 \"Virgil Hamlin Goode Jr.\"    Cons… Virginia     122,389        0.11%         \n 7 \"Roseanne Cherrie Barr\"      Peac… Utah         67,326         0.05%         \n 8 \"Ross Carl \\\"Rocky\\\" Anders… Just… Utah         43,018         0.03%         \n 9 \"Thomas Conrad Hoefling\"     Amer… Nebraska     40,628         0.03%         \n10 \"Other\"                      Other Other        217,152        0.17%         \n11 \"Total\"                      Total Total        129,085,410    100%          \n12 \"Needed to win\"              Need… Needed to w… Needed to win  Needed to win \n# ℹ 4 more variables: Electoralvote &lt;chr&gt;, `Running mate` &lt;chr&gt;,\n#   `Running mate` &lt;chr&gt;, `Running mate` &lt;chr&gt;\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nInstall and load library(rvest).\nInstall and load library(polite).\nScrape the Presidential debates table from the Wikipedia article for the 2008 presidential election.\n\n\n\n\n\n12.5.3 Other HTML Content\nSuppose we want to scrape every URL in the body of the 2012 Presidential Election webpage. html_table() no longer works.\nWe could manually poke through the source code to find the appropriate CSS selectors. Fortunately, SelectorGadget often eliminates this tedious work by telling you the name of the html elements that you click on.\n\nClick the SelectorGadget gadget browser extension. You may need to click the puzzle piece to the right of the address bar and then click the SelectorGadget browser extension.\nSelect an element you want to scrape. The elements associated with the CSS selector provided at the bottom will be in green and yellow.\n\nIf SelectorGadget selects too few elements, select additional elements. If SelectorGadget selects too many elements, click those elements. They should turn red.\n\nEach click should refine the CSS selector.\nAfter a few clicks, it’s clear we want p a. This should select any element a in p. a is the element for URLs.\nWe’ll need a few more functions to finish this example.\n\nhtml_elements() filters the output of read_html()/scrape() based on the provided CSS selector. html_elements() can return multiple elements while html_element() always returns one element.\nhtml_text2() retrieves text from HTML elements.\nhtml_attrs() retrieves HTML attributes from HTML elements. html_attrs() can return multiple attributes while html_attr() always returns one attribute.\n\n\ntibble(\n  text = election_page |&gt;\n    html_elements(css = \"p a\") |&gt;\n    html_text2(),\n  url = election_page |&gt;\n    html_elements(css = \"p a\") |&gt;\n    html_attr(name = \"href\")\n)\n\n# A tibble: 355 × 2\n   text                  url                                                    \n   &lt;chr&gt;                 &lt;chr&gt;                                                  \n 1 Barack Obama          /web/20230814004444/https://en.wikipedia.org/wiki/Bara…\n 2 Democratic            /web/20230814004444/https://en.wikipedia.org/wiki/Demo…\n 3 Barack Obama          /web/20230814004444/https://en.wikipedia.org/wiki/Bara…\n 4 Democratic            /web/20230814004444/https://en.wikipedia.org/wiki/Demo…\n 5 presidential election /web/20230814004444/https://en.wikipedia.org/wiki/Unit…\n 6 Democratic            /web/20230814004444/https://en.wikipedia.org/wiki/Demo…\n 7 President             /web/20230814004444/https://en.wikipedia.org/wiki/Pres…\n 8 Barack Obama          /web/20230814004444/https://en.wikipedia.org/wiki/Bara…\n 9 running mate          /web/20230814004444/https://en.wikipedia.org/wiki/Runn…\n10 Vice President        /web/20230814004444/https://en.wikipedia.org/wiki/Vice…\n# ℹ 345 more rows\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nSuppose we are interested in examples of early websites. Wikipedia has a list of URLs from before 1995.\n\nAdd the SelectorGadget web extension to your browser.\nUse library(polite) and library(rvest) to scrape() the following URL.\n\nhttps://web.archive.org/web/20230702163608/https://en.wikipedia.org/wiki/List_of_websites_founded_before_1995\n\nWe are interested in scraping the names of early websites and their URLs. Use SelectorGadget to determine the CSS selectors associated with these HTML elements.\nCreate a tibble with a variable called name and a variable called url.\nRemove duplicate rows with distinct() or filter().\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nFind your own HTML table of interest to scrape.\nUse library(rvest) and library(polite) to scrape the table.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "13_web-scraping.html#conclusion",
    "href": "13_web-scraping.html#conclusion",
    "title": "12  Web Scraping",
    "section": "12.6 Conclusion",
    "text": "12.6 Conclusion\nScraping data from websites is a powerful way of collecting data at scale or even when it is not organized in easily downloadable files like CSVs. Iteratively downloading is an elegant alternative to a time-intensive and potentially prohibitive process of going to websites and repeatedly downloading individual data sets. Scraping content from the body of websites is a more sophisticated approach that involves determining website html structure and then using that knowledge to extract key elements of that text. We strongly encourage you to consider the legal and ethnical risks of downloading this data.\n\n\n\n\nFellegi, I. P. 1972. “On the Question of Statistical Confidentiality.” Journal of the American Statistical Association 67 (337): 7–18. https://www.jstor.org/stable/2284695?seq=1#metadata_info_tab_contents.\n\n\nRavn, Signe, Ashley Barnwell, and Barbara Barbosa Neves. 2020. “What Is “Publicly Available Data”? Exploring Blurred PublicPrivate Boundaries and Ethical Practices Through a Case Study on Instagram.” Journal of Empirical Research on Human Research Ethics 15 (1-2): 40–45. https://doi.org/10.1177/1556264619850736.\n\n\nSomepalli, Gowthami, Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023. “Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 6048–58. https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualie, and Model Data. 2nd edition. Sebastopol, CA: O’Reilly.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "13_web-scraping.html#footnotes",
    "href": "13_web-scraping.html#footnotes",
    "title": "12  Web Scraping",
    "section": "",
    "text": "We are not lawyers. This is not official legal advise. If in-doubt, please contact a legal professional.↩︎\nThis blog and this blog support this statement. Again, we are not lawyers and the HiQ Labs v. LinkedIn decision is complicated because of its long history and conclusion in settlement.↩︎\nThe scale of crawling is so great that there is concern about models converging once all models use the same massive training data. Common Crawl is one example. This isn’t a major issue for generating images but model homogeneity is a big concern in finance.↩︎\nWho deserves privacy is underdiscussed and inconsistent. Every year, newspapers across the country FOIA information about government employees and publish their full names, job titles, and salaries.↩︎\nConsequently, code that may once have worked can break, but using read_csv(&lt;file_path&gt;) to access data once it has been downloaded will work consistently.↩︎\nThe only difference between map() and walk() is their outputs. map() returns the results of a function in a list. walk() returns nothing when used without assignment, and we never use walk() with assignment. walk() is useful when we don’t care about the output of functions and are only interested in their “side-effects”. Common functions to use with walk() are ggsave() and write_csv(). For more information on walk(), see Advanced R.↩︎\nWe recommend using Google Chrome, which has excellent web development tools.↩︎\nIf a website is static, that means that the website is not interactive and will remain the same unless the administrator actively makes changes. Hello World examples is an example of a static website.↩︎\nThe polite documentation describes the bow() function as being used to “introduce the client to the host and ask for permission to scrape (by inquiring against the host’s robots.txt file).”↩︎",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html",
    "href": "14_simulation-and-sampling.html",
    "title": "13  Simulation and Sampling",
    "section": "",
    "text": "13.1 Review\nParameters are to populations what statistics are to samples. The process of learning about population parameters from statistics calculated from samples is called statistical inference.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#sec-review5",
    "href": "14_simulation-and-sampling.html#sec-review5",
    "title": "13  Simulation and Sampling",
    "section": "",
    "text": "Population\n\n\n\nA population is the entire set of observations of interest.\nFor example, a population could be everyone residing in France at a point in time. A different population could be every American ages 65 or older.\n\n\n\n\n\n\n\n\nParameter\n\n\n\nA parameter is a numerical quantity that summarizes a population.\nFor example, the population mean and population standard deviation describe important characteristics of many populations.\nMore generally, location parameters, scale parameters, and shape parameters describe many populations.\n\n\n\n\n\n\n\n\nRandom Sample\n\n\n\nA random sample is a random subset of a population.\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nA statistic is a numerical quantity that summarizes a sample.\nFor example, the sample mean and sample standard deviation describe important characteristics of many random samples.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#introduction",
    "href": "14_simulation-and-sampling.html#introduction",
    "title": "13  Simulation and Sampling",
    "section": "13.2 Introduction",
    "text": "13.2 Introduction\nSimulation and sampling are important tools for statistics and data science. After reviewing/introducing basic concepts about probability theory and probability distributions, we will discuss two important applications of simulation and sampling.\n\nMonte Carlo simulation: A class of methods where values are repeatedly sampled/simulated from theoretical distributions that model a data generation process. Theoretical distributions, like the normal distribution, have closed-from representations and a finite number of parameters like mean and variance.\nResampling methods: A class of methods where values are repeatedly sampled from observed data to approximate repeatedly sampling from a population. Bootstrapping is a common resampling method.\n\nMonte Carlo methods and resampling methods have a wide range of applications. Monte Carlo simulation is used by election forecasters to predict electoral outcomes and econometricians to understand the properties of estimators. Resampling methods are used in machine learning and causal inference. Both are fundamental methods for agent-based models including microsimulation.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "href": "14_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "title": "13  Simulation and Sampling",
    "section": "13.3 Fundamentals of Probability Theory",
    "text": "13.3 Fundamentals of Probability Theory\n\n\n\n\n\n\nRandom Variable\n\n\n\n\\(X\\) is a random variable if its value is unknown and/or could change.\n\n\n\\(X\\) could be the outcome from the flip of a coin or the roll of a die. \\(X\\) could also be the amount of rain next July 4th.\n\n\n\n\n\n\nExperiment\n\n\n\nAn experiment is a process that results in a fixed set of possible outcomes.\n\n\n\n\n\n\n\n\nSet\n\n\n\nA set is a collection of objects.\n\n\n\n\n\n\n\n\nSample Space\n\n\n\nA sample space is the set of all possible outcomes for an experiment. We will denote a sample space with \\(\\Omega\\).\n\n\n\n\n\n\n\n\nDiscrete Random Variable\n\n\n\nA set is countable if there is a one-to-one correspondence from the elements of the set to some (finite) or all (countably infinite) positive integers (i.e. 1 = heads and 2 = tails).\nA random variable is discrete if its sample space is countable (finite or countably infinite).\n\n\n\n\n\n\n\n\nContinuous Random Variable\n\n\n\nA random variable is continuous if its sample space is any value in a \\(\\mathbb{R}\\) (real) interval.\nThere are infinite possible values in a real interval so the sample space is uncountable.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#discrete-random-variables",
    "href": "14_simulation-and-sampling.html#discrete-random-variables",
    "title": "13  Simulation and Sampling",
    "section": "13.4 Discrete Random Variables",
    "text": "13.4 Discrete Random Variables\n\n\n\n\n\n\nProbability Mass Function\n\n\n\nA probability mass function (PMF) computes the probability of an event in the sample space of a discrete random variable.\n\\[\np(x) = P(X = x)\n\\tag{13.1}\\]\nwhere \\(0 \\le p(x) \\le 1\\) and \\(\\sum_{x \\in \\Omega} p(x) = 1\\)\n\n\n\n\nCode\ntibble(\n  a = factor(1:6),\n  `P(X = a)` = rep(1 / 6, 6)\n) |&gt;\n  ggplot(aes(a, `P(X = a)`)) +\n  geom_col() +\n  labs(title = \"PMF for rolling a fair die\")\n\n\n\n\n\nFigure 13.1: PMF for rolling a fair die\n\n\n\n\n\n\n\n\nNow we can make statements like \\(P(X = a)\\). For example, \\(P(X = 3) = \\frac{1}{6}\\).\n\n13.4.1 Bernoulli Distribution\nA Bernoulli random variable takes on the value \\(1\\) with probability \\(p\\) and \\(0\\) with probability \\(1 - p\\). It is often used to represent coins. When \\(p = \\frac{1}{2}\\) we refer to the coin as “fair”.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Ber(p)\n\\tag{13.2}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) =\n\\begin{cases}\n1 - p &\\text{ if } x = 0 \\\\\np &\\text{ if } x = 1\n\\end{cases} = p^x(1 - p)^{1-x}\n\\tag{13.3}\\]\n\n\n13.4.2 Binomial Distribution\nA binomial random variable is the number of events observed in \\(n\\) repeated Bernoulli trials.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Bin(n, p)\n\\tag{13.4}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) = {n \\choose x} p^x(1 - p)^{n - x}\n\\tag{13.5}\\]\nWe can calculate the theoretical probability of a given draw from a binomial distribution using this PDF. For example, suppose we have a binomial distribution with \\(10\\) trials and \\(p = \\frac{1}{2}\\). The probability of drawing exactly six \\(1\\)s and four \\(0\\)s is\n\\[\np(X = 6) = \\frac{10!}{6!4!} 0.5^6(1 - 0.5)^{10 - 6} \\approx 0.2051\n\\tag{13.6}\\]\nWe can do similar calculations for each value between \\(0\\) and \\(10\\).\nWe can also take random draws from the distribution. Figure 13.2 shows 1,000 random draws from a binomial distribution with 10 trials and p = 0.5. The theoretical distribution is overlaid in red.\n\n\nCode\ntibble(\n  x = rbinom(n = 1000, size = 10, prob = 0.5)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(breaks = 0:10) +\n  geom_point(data = tibble(x = 0:10, y = map_dbl(0:10, dbinom, size = 10, prob = 0.5)),\n             aes(x, y),\n             color = \"red\") +\n  labs(\n    title = \"1,000 samples of a binomial RV\",\n    subtitle = \"Size = 10; prob = 0.5\",\n    y = NULL\n  ) \n\n\n\n\n\nFigure 13.2: 1,000 random draws from a binomial distribution with 10 trials and p = 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Error\n\n\n\nSampling error is the difference between sample statistics (estimates of population parameters) and population parameters.\n\n\nThe difference between the red dots and black bars in Figure 13.2 is caused by sampling error.\n\n\n13.4.3 Distributions Using R\nMost common distributions have R functions to\n\ncalculate the density of the pdf/pmf for a specific value\ncalculate the probability of observing a value less than \\(a\\)\ncalculate the value associated with specific quantiles of the pdf/pmf\nrandomly sample from the probability distribution\n\nLet’s consider a few examples:\nThe following answers the question: “What is the probability of observing 10 events in 10 trials when p = 0.5?”\n\ndbinom(x = 10, size = 10, prob = 0.5)\n\n[1] 0.0009765625\n\n\n\nThe following answers the question: “What’s the probability of observing 3 or fewer events in 10 trials when p = 0.5”\n\npbinom(q = 3, size = 10, prob = 0.5)\n\n[1] 0.171875\n\n\n\nThe following answers the question: “What is a 10th percentile number of events to see in 10 trials when p = 0.5?\n\nqbinom(p = 0.1, size = 10, prob = 0.5)\n\n[1] 3\n\n\n\nThe following randomly draw ten different binomially distributed random variables.\n\nrbinom(n = 10, size = 10, prob = 0.5)\n\n [1] 6 3 3 1 3 4 6 5 3 4\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nAdd dbinom() for 0, 1, and 2 when size = 10 and prob = 0.5.\nCalculate pbinom() with size = 10 and prob = 0.5. Why do you get the same answer?\nPlug the result from step 1/step 2 into qbinom() with size = 10 and prob = 0.5. Why do you get the answer 2?\nUse rbinom() with size = 10 and prob = 0.5 to sample 10,000 binomially distributed random variables and assign the output to x. Calculate mean(x &lt;= 2). How does the answer compare to step 1/step 2?\n\n\n\n\n\n\n\n\n\nPseudo-random numbers\n\n\n\nComputers use pseudo-random numbers to generate samples from probability distributions. Modern pseudo-random samplers are very random.\nUse set.seed() to make pseudo-random sampling reproducible.\n\n\n\n\n13.4.4 Poisson Random Variable\nA poisson random variable is the number of events that occur in a fixed period of time. For example, a poisson distribution can be used to model the number of visits in an emergency room between 1AM and 2AM.\nWe show that a random variable is poisson-distributed with\n\\[\nX \\sim Pois(\\lambda)\n\\tag{13.7}\\]\nThe parameter \\(\\lambda\\) is both the mean and variance of the poisson distribution. The PMF of a poisson random variable is\n\\[\np(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\tag{13.8}\\]\nFigure 13.3 shows 1,000 draws from a poisson distribution with \\(\\lambda = 10\\).\n\n\nCode\nset.seed(20200905)\n\ntibble(\n  x = rpois(1000, lambda = 10)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(limits = c(0, 29)) +\n  stat_function(fun = dpois, n = 30, color = \"red\", args = list(lambda = 10)) + \n  labs(\n    title = \"1,000 samples of a Poisson RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nFigure 13.3: 1,000 samples of a Poisson RV\n\n\n\n\n\n\n\n\n\n\n13.4.5 Categorical Random Variable\nWe can create a custom discrete probability distribution by enumerating the probability of each event in the sample space. For example, the PMF for the roll of a fair die is\n\\[\np(x) =\n\\begin{cases}\n\\frac{1}{6} & \\text{if } x = 1\\\\\n\\frac{1}{6} & \\text{if } x = 2\\\\\n\\frac{1}{6} & \\text{if } x = 3\\\\\n\\frac{1}{6} & \\text{if } x = 4\\\\\n\\frac{1}{6} & \\text{if } x = 5\\\\\n\\frac{1}{6} & \\text{if } x = 6\n\\end{cases}\n\\tag{13.9}\\]\nThis PMF is visualized in Figure 13.1. We can sample from this PMF with\n\nsample(x = 1:6, size = 1)\n\n[1] 3\n\n\nWe can also sample with probabilities that differ for each event:\n\nsample(\n  x = c(\"rain\", \"sunshine\"), \n  size = 1, \n  prob = c(0.1, 0.9)\n)\n\n[1] \"sunshine\"\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nSample 1,000 observations from a Poisson distribution with \\(\\lambda = 20\\).\nSample 1,000 observations from a normal distribution with \\(\\mu = 20\\) and \\(\\sigma = \\sqrt{20}\\).\nVisualize and compare both distribution.\n\n\n\nWhen \\(\\lambda\\) is sufficiently large, the normal distribution is a reasonable approximation of the poisson distribution.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#continuous-random-variables",
    "href": "14_simulation-and-sampling.html#continuous-random-variables",
    "title": "13  Simulation and Sampling",
    "section": "13.5 Continuous Random Variables",
    "text": "13.5 Continuous Random Variables\n\n\n\n\n\n\nProbability Density Function (PDF)\n\n\n\nA probability density function is a non-negative, integrable function for each real value \\(x\\) that shows the relative probability of values of \\(x\\) for an absolutely continuous random variable \\(X\\).\nWe note PDF with \\(f_X(x)\\).\n\n\n\n\n\n\n\n\nCumulative Distribution Function (CDF)\n\n\n\nA cumulative distribution function (cdf) shows the probability of a random variable \\(X\\) taking on any value less than or equal to \\(x\\).\nWe note CDF with \\(F_X(x) = P(X \\le x)\\)\n\n\nHere is the PDF for a standard normal random variable:\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"PDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\nIf we integrate the entire function we get the CDF.\nCumulative Density Function (CDF): A function of a random variable \\(X\\) that returns the probability that the value \\(X &lt; x\\).\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = pnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"CDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n13.5.1 Uniform Distribution\nUniform random variables have equal probability for every value in the sample space. The distribution has two parameters: minimum and maximum. A standard uniform random has minimum = 0 and maximum = 1.\nWe show that a random variable is uniform distributed with\n\\[\nX \\sim U(a, b)\n\\tag{13.10}\\]\nThe PDF of a uniform random variable is\n\\[\nf(x) =\n\\begin{cases}\n\\frac{1}{b - a} & \\text{if } x \\in [a, b] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\tag{13.11}\\]\nStandard uniform random variables are useful for generating other random processes and imputation.\n\n\nCode\nset.seed(20200904)\n\ntibble(\n  x = runif(1000)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dunif, n = 101, color = \"red\") + \n  labs(\n    title = \"1,000 samples of a standard uniform RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\n\n\n13.5.2 Normal Distribution\nThe normal distribution is the backbone of statistical inference because of the central limit theorem.\nWe show that a random variable is normally distributed with\n\\[\nX \\sim N(\\mu, \\sigma)\n\\tag{13.12}\\]\nThe PDF of a normally distributed random variable is\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right) ^ 2\\right]\n\\tag{13.13}\\]\n\n\n\n\n\n\nFundamental Probability Formula for Intervals\n\n\n\nThe probability that an absolutely continuous random variable takes on any specific value is always zero because the sample space is uncountable. Accordingly, we express the probability of observing events within a region for absolutely continuous random variables.\nIf \\(X\\) has a PDF and \\(a &lt; b\\), then\n\\[\nP(a \\le X \\le b) = P(a \\le X &lt; b) = P(a &lt; X \\le b) = P(a &lt; X &lt; b) = \\int_a^bf(x)dx = F_X(b) - F_X(a)\n\\tag{13.14}\\]\n\n\nThe last portion of this inequality is fundamental to working with continuous probability distributions and is the backbone of much of any intro to statistics course. For example, the probability, \\(P(X &lt; 0)\\) is represented by the blue region below.\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  geom_area(stat = \"function\", fun = dnorm, fill = \"blue\", xlim = c(-4, 0)) +\n  labs(\n    title = \"PDF for a standard normal random variable\",\n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nStudent’s t-distribution and the normal distribution are closely related.\n\nUse pnorm() to calculate \\(P(X &lt; -1)\\) for a standard normal distribution.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 10.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 100.\n\n\n\nObserve how the normal distribution becomes a better approximation for Student’s t-distribution when the degrees of freedom increases.\n\n\n13.5.3 Exponential Distribution\nAn exponential random variable is the wait time between events for a poisson random variable. It is useful for modeling wait time. For example, an exponential distribution can be used to model the wait time between arrivals in an emergency room between 1AM and 2AM. It has one parameter: rate (\\(\\lambda\\)).\nWe show that a random variable is exponentially distributed with\n\\[\nX \\sim Exp(\\lambda)\n\\tag{13.15}\\]\nThe PDF of an exponential random variable is\n\\[\nf(x) = \\lambda\\exp(-\\lambda x)\n\\tag{13.16}\\]\n\n\nCode\ntibble(\n  x = rexp(n = 1000, rate = 1)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_density() +\n  stat_function(fun = dexp, n = 101, args = list(rate = 1), color = \"red\") + \n  labs(\n    title = \"1,000 samples of an exponential RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\n\n\n13.5.4 Other Distributions\n\nGeometric RV: Number of Bernoulli trials up to and including the \\(1^{st}\\) event\nNegative Binomial RV: Number of Bernoulli trials up to and including the \\(r^{th}\\) event\nGamma RV: Time until the \\(\\alpha\\) person arrives",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#parametric-density-estimation",
    "href": "14_simulation-and-sampling.html#parametric-density-estimation",
    "title": "13  Simulation and Sampling",
    "section": "13.6 Parametric Density Estimation",
    "text": "13.6 Parametric Density Estimation\nA key exercise in statistics is selecting a probability distribution to represent data and then learning the parameters of probability distributions from the data. The process is often called model fitting.\nWe are focused on parametric density estimation. Later, we will focus on nonparameteric density estimation. This section will focus on frequentist inference of population parameters from observed data. Later, we will adopt a Bayesian approach to inference.\n\n13.6.1 Maximum Likelihood Estimation\nAll of the probability distributions we have observed have a finite number of parameters. Maximum likelihood estimation is a common method for estimating these parameters.\nThe general process is\n\nPick the probability distribution that fits the observed data.\nIdentify the finite number of parameters associated with the probability distribution.\nCalculate the parameters that maximize the probability of the observed data.\n\n\n\n\n\n\n\nLikelihood\n\n\n\nLet \\(\\vec{x}\\) be observed data and \\(\\theta\\) be a parameter or parameters from a chosen probability distribution. The likelihood is the joint probability of the observed data conditional on values of the parameters.\nThe likelihood of discrete data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n p(x_i|\\theta)\n\\tag{13.17}\\]\nThe likelihood of continuous data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n f(x_i|\\theta)\n\\tag{13.18}\\]\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\n\nMaximum likelihood estimation is a process for estimating parameters for a given distribution that maximizes the log likelihood.\nIn other words, MLEs find the estimated parameters that maximize the probability of observing the observed set of data.\n\n\nWe won’t unpack how to derive the maximum likelihood estimators1 but it is easy to look up most MLEs.\n\nBinomial distribution MLEs\nSuppose we have a sample of data \\(x_1, ..., x_m\\). If the number of trials \\(n\\) is already known, then \\(p\\) is the only parameter for the binomial distribution that needs to be estimated. The MLE for \\(p\\) is \\(\\hat{p} = \\frac{\\sum_{i = 1}^n x_i}{mn}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat{p}\\).\n\nset.seed(20230909)\nx &lt;- rbinom(n = 8, size = 10, prob = 0.3)\n\nx\n\n[1] 4 3 6 3 4 3 3 2\n\n\n\nmle_binom &lt;- sum(x) / (10 * 8)\n\nmle_binom\n\n[1] 0.35\n\n\n\n\nNormal distribution MLEs\n\\(\\mu\\) and \\(\\sigma\\) are the parameters of a normal distribution. The MLEs for a normal distribution are \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i = \\bar{x}\\) and \\(s^2 = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^2\\).2\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i\\) and \\(\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\).\n\nset.seed(20230909)\nx &lt;- rnorm(n = 200, mean = 0, sd = 1)\n\n\nmean_hat &lt;- mean(x)\n\nmean_hat\n\n[1] 0.02825125\n\nsigma2_hat &lt;- mean((x - mean(x)) ^ 2)\n\nsigma2_hat\n\n[1] 0.8119682\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = mean_hat, sd = sqrt(sigma2_hat)))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nExponential distribution MLEs\n\\(\\lambda\\) is the only parameter of an exponential distribution. The MLE for an exponential distribution is \\(\\hat\\lambda = \\frac{1}{\\bar{x}}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\frac{1}{\\bar{x}}\\).\n\nset.seed(20230909)\nx &lt;- rexp(n = 200, rate = 10)\n\nmle_exp &lt;- 1 / mean(x)\n\nmle_exp\n\n[1] 10.58221\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dexp, color = \"red\", args = list(rate = mle_exp))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCreate the vector in the code chunk below:\n\n\n\nCode\nx &lt;- c(\n  30970.787, 10901.544, 15070.015, 10445.772, 8972.258, \n  15759.614, 13341.328, 18498.858, 134462.066, 17498.930, \n  7112.306, 27336.795, 75526.381, 110123.606, 32910.618, \n  16764.452, 21244.380, 18952.455, 954373.470, 4219.635,\n  7078.766, 27657.996, 18337.097, 14566.525, 14220.000, \n  21457.202, 9322.311, 26018.018, 96325.728, 26780.329, \n  25833.356, 10719.360, 8642.935, 29302.623, 10517.174,\n  33831.547, 339077.456, 5805.707, 141505.710, 28168.790, \n  10446.378, 4993.349, 27502.949, 35519.162, 45761.505, \n  26163.096, 72163.668, 15515.435, 69396.895, 84972.590, \n  67248.460, 26966.374, 24624.339, 4779.110, 23330.279,\n  196311.913, 20517.739, 80257.587, 32108.466, 9735.061, \n  20502.579, 2544.004, 165909.040, 20949.512, 16643.695, \n  30267.741, 8359.024, 13355.154, 8425.988, 4491.550,\n  32071.872, 61648.149, 75074.135, 62842.985, 26040.648, \n  68733.979, 63368.710, 11157.211, 5782.610, 3629.674, \n  44399.230, 2852.381, 8200.453, 41249.003, 15006.791,\n  808974.653, 30705.915, 6341.954, 28208.144, 5409.821,\n  54566.805, 10894.864, 4583.550, 31110.875, 43474.872, \n  69059.161, 33054.574, 8789.910, 218887.477, 11051.292, \n  3366.743, 63853.329, 68756.561, 48031.259, 11707.191,\n  26593.634, 8868.942, 19225.309, 27704.670, 10666.549, \n  47151.963, 20343.604, 123932.502, 33030.986, 5412.023, \n  23540.382, 9894.513, 52742.541, 21397.990, 25100.143,\n  23757.882, 48347.300, 4325.134, 23816.776, 11907.656, \n  24179.849, 25967.574, 7531.294, 15131.240, 21595.781, \n  40473.936, 35390.849, 4060.563, 55334.157, 37058.771, \n  34050.456, 17351.500, 7453.829, 48131.565, 10576.746,\n  26450.754, 33592.986, 21425.018, 34729.337, 77370.078, \n  5819.325, 9067.356, 19829.998, 20120.706, 3637.042, \n  44812.638, 22930.229, 29683.776, 76366.822, 15464.594, \n  1273.101, 53036.266, 2846.294, 114076.200, 14492.680, \n  55071.554, 31597.849, 199724.125, 52332.510, 98411.129, \n  43108.506, 6580.620, 12833.836, 8846.348, 7599.796, \n  6952.447, 30022.143, 24829.739, 40784.581, 8997.219,\n  3786.354, 11515.298, 116515.617, 137873.967, 3282.185,\n  107886.676, 13184.850, 51083.235, 2907.886, 51827.538, \n  37564.196, 23196.399, 20169.037, 9020.364, 11118.250, \n  56930.060, 11657.302, 84642.584, 44948.450, 16610.166, \n  5509.231, 4770.262, 15614.233, 5993.999, 22628.114\n)\n\n\n\nVisualize the data with a relative frequency histogram.\nCalculate the MLEs for a normal distribution and add a normal distribution to the visualization in red.\nCalculate the MLEs for a log-normal distribution and add a log-normal distribution to the visualization in blue.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#multivariate-random-variables",
    "href": "14_simulation-and-sampling.html#multivariate-random-variables",
    "title": "13  Simulation and Sampling",
    "section": "13.7 Multivariate Random Variables",
    "text": "13.7 Multivariate Random Variables\nWe’ve explored univariate or marginal distributions thus far. Next, we will focus on multivariate distributions.\n\n\n\n\n\n\nMultivariate Distribution\n\n\n\nA multivariate distribution is a probability distribution that shows the probability (discrete) or relative probability (continuous) of more than one random variable.\nMultivariate distributions require describing characteristics of random variables and the relationships between random variables.\n\n\n\n13.7.1 Multivariate Normal Distribution\nThe multivariate normal distribution is a higher-dimensional version of the normal distribution.\nInstead of a single mean and a single variance, the \\(k\\)-dimensional multivariate normal distribution has a vector of means of length \\(k\\) and a \\(k\\)-by-\\(k\\) variance-covariance matrix3. The vector describes the central tendencies of each dimension of the multivariate distribution and the matrix describe the variance of the distributions and relationships between the distributions.\nWe show that a random vector is multivariate normally distributed with\n\\[\n\\vec{X} \\sim \\mathcal{N}(\\vec\\mu, \\boldsymbol\\Sigma)\n\\tag{13.19}\\]\nThe PDF of a multivariate normally distributed random variable is\n\\[\nf(x) = (2\\pi)^{-k/2}det(\\boldsymbol\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\vec{x} - \\vec\\mu)^T\\boldsymbol\\Sigma^{-1}(\\vec{x} - \\vec\\mu)\\right)\n\\tag{13.20}\\]\nFunctions for working with multi-variate normal distributions from library(MASS). Figure 13.4 shows three different random samples from 2-dimensional multivariate normal distributions.\nSigma1 &lt;- matrix(c(1, 0.8, \n                   0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n  \nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma1) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nSigma2 &lt;- matrix(c(1, 0.2, \n                   0.2, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma2) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nSigma3 &lt;- matrix(c(1, -0.8, \n                   -0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma3) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\n\n\nFigure 13.4: Samples from Multivariate Normal Distributions\n\n\n\n\n\n\n\n(a) Strong Positive Covariance\n\n\n\n\n\n\n\n\n\n\n\n(b) Weak Covariance\n\n\n\n\n\n\n\n\n\n\n\n(c) Strong Negative Covariance",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#monte-carlo-methods",
    "href": "14_simulation-and-sampling.html#monte-carlo-methods",
    "title": "13  Simulation and Sampling",
    "section": "13.8 Monte Carlo Methods",
    "text": "13.8 Monte Carlo Methods\nSimulation methods, including Monte Carlo simulation, are used for policy analysis:\n\nFiveThirtyEight and the New York Times use Monte Carlo simulation to predict the outcomes of elections.\nThe Social Security Administration uses microsimulation to evaluate the distributional impact of Social Security reforms.\nThe Census Bureau uses simulation to understand the impact of statistical disclosure control on released data.\nEconometricians and statisticians use Monte Carlo simulation to demonstrate the properties of estimators.\n\nWe can make probabilistic statements about common continuous random variables because their PDFs are integrable or at least easy enough to approximate with lookup tables. We can make probabilistic statements about common discrete random variables with summation.\nBut we often want to make probabilistic statements about uncommon or complex probability distributions. Maybe the probability distribution of the random variable doesn’t have a tractable integral (i.e. the area under the curve can’t practically be computed). Or maybe there are too many potential outcomes (e.g. rays of light emitting from a light bulb in the Marble Science video linked at the top).\nMonte Carlo: A Monte Carlo method estimates a deterministic quantity using stochastic (random) sampling.\nMonte Carlo but easier this time: A Monte Carlo method takes hundreds or thousands of independent samples from a random variable or variables and then approximates fixed population quantities with summaries of those draws. The quantities could be population parameters like a population mean or probabilities.\nMonte Carlo methods have three major applications:\n\nSampling – Monte Carlo simulation allows for sampling from complex probability distributions. The samples can be used to model real-world events (queues), to model outcomes with uncertain model inputs (election modeling), to generate fake data with known parameters to evaluate statistical methods (model selection when assumptions fail), and to draw from the posteriors of Bayesian models.\nNumerical integration – Integration, as noted above, is important to calculating probabilities and ultimately calculating quantities like expected value or the intervals. Monte Carlo methods can approximate multidimensional integrals that will never be directly solved by computers or simplify estimating probabilities when there are uncountably many potential outcomes (Solitaire).\nOptimization – Monte Carlo methods can be used for complex optimization. We will not focus on optimization.\n\nLet’s explore some examples:\n\n13.8.1 Example 1: Coin Tossing\nWe can calculate the proportion of tosses of a fair coin that we expect to turn up heads by finding the expected value of the binomial distribution and dividing by the number of tosses. But suppose we can’t… Or maybe we wish to confirm our calculations with simulations…\nLet’s try repeated sampling from a binomial distribution to approximate this process:\n\n#' Count the proportion of n tosses that turn up heads\n#'\n#' @param n An integer for the number of tosses\n#'\n#' @return The proportion of n tosses that turn up heads\n#' \ncount_heads &lt;- function(n) {\n  \n  # toss the fair coin n times\n  coin_tosses &lt;- rbinom(n = n, size = 1, prob = 0.5)\n    \n  coin_tosses &lt;- if_else(coin_tosses == 1, \"heads\", \"tails\")\n  \n  # calculate the proportion of heads\n  prop_heads &lt;- mean(coin_tosses == \"heads\")\n  \n  return(prop_heads)\n  \n}\n\nLet’s toss the coin ten times.\n\nset.seed(11)\ncount_heads(n = 10)\n\n[1] 0.3\n\n\nOk, we got 0.3, which we know isn’t close to the expected proportion of 0.5. What if we toss the coin 1 million times.\n\nset.seed(20)\ncount_heads(n = 1000000)\n\n[1] 0.499872\n\n\nOk, that’s more like it.\n\\[\\cdot\\cdot\\cdot\\]\nMonte Carlo simulation works because of the law of large numbers. The law of large numbers states that the probability that the average of trials differs from the expected value converges to zero as the number of trials goes to infinity.\nMonte Carlo simulation basically repeats the ideas behind frequentist inferential statistics. If we can’t measure every unit in a population then we can sample a representative population and estimate parameters about that population.\nThe keys to Monte Carlo simulation are randomness and independent and identically distributed sampling (i.i.d.).\n\n\n13.8.2 Example 2: Bootstrap Sampling\nOn average, a bootstrap sample includes about 63% of the observations from the data that are sampled. This means that an individual bootstrap sample excludes 37% of the observations from the source data!\nSo if we bootstrap sample from a vector of length 100, then \\(\\frac{63}{100}\\) values will end up in the bootstrap sample on average and \\(\\frac{37}{100}\\) of the values will be repeats on average.\nWe can explore this fact empirically with Monte Carlo simulation using repeated samples from a categorical distribution. We will use sample().\n\n#' Calculate the proportion of unique values from a vector of integers included \n#' in a bootstrap sample\n#'\n#' @param integers A vector of integers\n#'\n#' @return The proportion of integers included in the bootstrap sample\n#' \ncount_uniques &lt;- function(integers) {\n  \n  # generate a bootstrap sample\n  samples &lt;- sample(integers, size = length(integers), replace = TRUE)\n  \n  # calculate the proportion of unique values from the original vector\n  prop_unique &lt;- length(unique(samples)) / length(integers)\n  \n  return(prop_unique)\n  \n}\n\nLet’s bootstrap sample 100,000 times.\n\n# pre-allocate the output vector for efficient computation\nprop_unique &lt;- vector(mode = \"numeric\", length = 100000)\nfor (i in seq_along(prop_unique)) {\n  \n  prop_unique[i] &lt;- count_uniques(integers = 1:100)\n  \n}\n\nFinally, calculate the mean proportion and estimate the expected value.\n\nmean(prop_unique)\n\n[1] 0.6337935\n\n\nWe can also calculate a 95% confidence interval using the bootstrap samples.\n\nquantile(prop_unique, probs = c(0.025, 0.975))\n\n 2.5% 97.5% \n 0.57  0.69 \n\n\n\n\n13.8.3 Example 3: \\(\\pi\\)\nConsider one of the examples from Marble Science: Monte Carlo Simulation. Imagine we don’t know \\(\\pi\\) but we know that the equation for the area of a square is \\(r ^ 2\\) and the equation for the area of a circle is \\(\\pi r ^ 2\\). If we know the ratio of the areas of the circle and the square, then we can solve for \\(\\pi\\).\n\\[\n\\frac{\\text{Area of Cirle}}{\\text{Area of Square}} = \\frac{\\pi r ^ 2}{r ^ 2} = \\pi\n\\tag{13.21}\\]\nThis is simply solved with Monte Carlo simulation. Randomly sample a bivariate uniform random variables and count how frequently the values are inside of the square or inside the circle.\n\nexpand_grid(\n  x = seq(0, 4, 0.1),\n  y = seq(0, 2, 0.1)\n) |&gt;\nggplot() +\n  ggforce::geom_circle(aes(x0 = 2, y0 = 1, r = 1), fill = \"blue\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), fill = \"red\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 3, ymin = 0, ymax = 2), fill = NA, color = \"black\") +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\nnumber_of_samples &lt;- 2000000\n\n# sample points in a rectangle with x in [0, 3] and y in [0, 2]\nset.seed(20210907)\nsamples &lt;- tibble(\n  x = runif(number_of_samples, min = 0, max = 3),\n  y = runif(number_of_samples, min = 0, max = 2)\n)\n\n# calculate if (x, y) is in the circle, the square, or neither\nsamples &lt;- samples |&gt;\n  mutate(\n    in_square = between(x, 0, 1) & between(y, 0, 1),\n    in_circle = (x - 2) ^ 2 + (y - 1) ^ 2 &lt; 1\n  ) \n\n# calculate the proportion of samples in each shape\nprop_in_shapes &lt;- samples |&gt;\n  summarize(\n    prop_in_square = mean(in_square), \n    prop_in_circle = mean(in_circle)\n  ) \n\n# calculate the ratio\nprop_in_shapes |&gt;\n  mutate(prop_in_circle / prop_in_square) |&gt;\n  print(digits = 3)\n\n# A tibble: 1 × 3\n  prop_in_square prop_in_circle `prop_in_circle/prop_in_square`\n           &lt;dbl&gt;          &lt;dbl&gt;                           &lt;dbl&gt;\n1          0.166          0.524                            3.15\n\n\nThe answer approximates \\(\\pi\\)!\n\n\n13.8.4 Example 4: Simple Linear Regression\nThe goal of statistical inference is to use data, statistics, and assumptions to infer parameters and probabilities about a population. Typically we engage in point estimation and interval estimation.\nSometimes it is useful to reverse this process to understand and confirm the properties of estimators. That means starting with known population parameters, simulating hundreds or thousands of samples from that population, and then observing point estimates and interval estimates over those samples.\n\nLinear Regression Assumptions\n\nThe population model is of the linear form \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\nThe estimation data come from a random sample or experiment\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) independently and identically distributed (i.i.d.)\n\\(x\\) has variance and there is no perfect collinearity in \\(x\\)\n\n\n\nStatistics\nIf we have one sample of data, we can estimate points and intervals with the following estimators:\nThe residual standard error is\n\\[\n\\hat\\sigma = \\frac{\\sum e_i^2}{(n - 2)}\n\\tag{13.22}\\]\nThe estimate of the slope is\n\\[\n\\hat\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\n\\tag{13.23}\\]\nThe standard error of the estimate of the slope, which can be used to calculate t-statistics and confidence intervals, is\n\\[\n\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2}\n\\tag{13.24}\\]\nThe estimate of the intercept term is\n\\[\n\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1\\bar{x}\n\\tag{13.25}\\]\nThe standard error of the intercept is\n\\[\n\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]}\n\\tag{13.26}\\]\n\n\nMonte Carlo Simulation\nConsider a simple linear regression model with the following population model:\n\\[y = 5 + 15x + \\epsilon\\]\nWe can calculate the above statistics over repeated sampling and confirm their asymptotic properties with Monte Carlo simulation.\nFirst, create 1,000 random samples from the population.\n\nset.seed(20210906)\n\ndata &lt;- map(\n  .x = 1:1000,\n  .f = ~ tibble(\n    x = rnorm(n = 10000, mean = 0, sd = 2),\n    epsilon = rnorm(n = 10000, mean = 0, sd = 10),\n    y = 5 + 15 * x + epsilon\n  )\n)\n\nNext, estimate a simple linear regression model for each draw of the population. This step includes calculating \\(\\hat\\sigma\\), \\(\\hat\\beta_1\\), \\(\\hat\\beta_0\\), \\(\\hat{SE}(\\hat\\beta_1)\\), and \\(\\hat{SE}(\\hat\\beta_0)\\).\n\nestimated_models &lt;- map(\n  .x = data,\n  .f = ~ lm(y ~ x, data = .x)\n)\n\nNext, we extract the coefficients and confidence intervals.\n\ncoefficients &lt;- map_df(\n  .x = estimated_models,\n  .f = tidy,\n  conf.int = TRUE\n)\n\nLet’s look at estimates of the residual standard error. The center of the distribution closely matches the population standard deviation of the error term.\n\nmodel_metrics &lt;- map_df(\n  .x = estimated_models,\n  .f = glance\n) \n\nmodel_metrics |&gt;\n  ggplot(aes(sigma)) +\n  geom_histogram() +\n  labs(title = \"Plot of the estimated residual standard errors\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLet’s plot the coefficients. The centers approximately match the population intercept of 5 and slope of 15.\n\ncoefficients |&gt;\n  ggplot(aes(estimate)) +\n  geom_histogram() +\n  facet_wrap(~term, scales = \"free_x\") +\n  labs(title = \"Coefficients estimates across 10,000 samples\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe standard deviation of the coefficients also matches the standard errors.\n\\[\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2} = \\sqrt\\frac{10^2}{40,000} = 0.05\\]\n\\[\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]} = \\sqrt{10^2\\left[\\frac{1}{10,000} + 0\\right]} = 0.1\\]\n\ncoefficients |&gt;\n  group_by(term) |&gt;\n  summarize(\n    mean(estimate), \n    sd(estimate)\n  )\n\n# A tibble: 2 × 3\n  term        `mean(estimate)` `sd(estimate)`\n  &lt;chr&gt;                  &lt;dbl&gt;          &lt;dbl&gt;\n1 (Intercept)             5.00         0.100 \n2 x                      15.0          0.0482\n\n\nLet’s look at how often the true parameter is inside the 95% confidence interval. It’s close although not exactly 95%.\n\ncoefficients |&gt;\n  filter(term == \"x\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 15 & conf.high &gt;= 15))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1           0.959\n\ncoefficients |&gt;\n  filter(term == \"(Intercept)\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 5 & conf.high &gt;= 5))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1            0.95\n\n\n\n\n\n13.8.5 Example 5: Queuing Example\nSuppose we have a queue at a Social Security field office. Let \\(t\\) be time. When the office opens, \\(t = 0\\) and the queue is empty.\nLet, \\(T_i\\) be the interarrival time and \\(T_i \\sim exp(\\lambda_1)\\)\nLet, \\(S_i\\) be the service time time and \\(S_I \\sim exp(\\lambda_2)\\)\nFrom these two random variables, we can calculate the arrival times, departure times, and wait times for each customer.\n\nThe arrival times are the cumulative sum of the interarrival times.\nThe wait times are zero if a person arrives after the person before them and the difference between the prior person’s departure and the current person’s arrival otherwise.\nThe departure time is arrival time plus the wait time plus the service time.\n\n\nset.seed(19920401)\nqueue &lt;- generate_queue(t = 100, lambda = 1, mu = 1)\n\nqueue\n\n# A tibble: 100 × 5\n   interarrival_time arrival_time service_time wait_time departure_time\n               &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1             0.467        0.467       5.80        0              6.27\n 2             1.97         2.43        0.0892      3.83           6.36\n 3             2.70         5.13        1.26        1.22           7.61\n 4             0.335        5.47        4.85        2.14          12.5 \n 5             0.372        5.84        1.89        6.62          14.3 \n 6             1.72         7.56        0.507       6.78          14.9 \n 7             2.28         9.84        0.932       5.01          15.8 \n 8             0.339       10.2         1.18        5.61          17.0 \n 9             2.54        12.7         1.42        4.25          18.4 \n10             0.572       13.3         0.157       5.10          18.5 \n# ℹ 90 more rows\n\n\n\nflow &lt;- tibble::tibble(\n  time = c(queue$arrival_time, queue$departure_time),\n  type = c(rep(\"arrival\", length(queue$arrival_time)), \n           rep(\"departure\", length(queue$departure_time))), \n  change = c(rep(1, length(queue$arrival_time)), rep(-1, length(queue$departure_time))),\n) |&gt;\n  arrange(time) |&gt; \n  filter(time &lt; 100) |&gt; \n  mutate(queue = cumsum(change) - 1)\n\nflow |&gt;\n  ggplot(aes(time, queue)) +\n  geom_step() +\n  labs(title = \"Simulated queue at the Social Security office\")\n\n\n\n\n\n\n\n\nThis is interesting, but it’s still only one draw from a Monte Carlo simulation. What if we are interested in the distribution of wait times for the fifth customer?\n\n#' Generate wait times at the queue\n#'\n#' @param person_number An integer for the person of interest\n#' @param iterations An integer for the number of Monte Carlo iterations\n#' @param t A t for the maximum time\n#'\n#' @return A vector of wait times\n#' \ngenerate_waits &lt;- function(person_number, iterations, t) {\n  \n  wait_time &lt;- vector(mode = \"numeric\", length = iterations)\n  for (i in seq_along(wait_time)) {\n    \n    wait_time[i] &lt;- generate_queue(t = t, lambda = 1, mu = 1)$wait_time[person_number]\n    \n  }\n  \n  return(wait_time)\n  \n}\n\nset.seed(20200908)\nwait_time &lt;- generate_waits(person_number = 5, iterations = 10000, t = 50)\n\nmean(wait_time)\n\n[1] 1.464371\n\nquantile(wait_time, probs = c(0.025, 0.5, 0.975))\n\n     2.5%       50%     97.5% \n0.0000000 0.9222193 5.8742015 \n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCreate a Monte Carlo simulation of an unfair coin toss where p = 0.6.\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nSuppose we have three independent normally-distributed random variables.\n\\[X_1 \\sim N(\\mu = 0, \\sigma = 1)\\]\n\\[X_2 \\sim N(\\mu = 1, \\sigma = 1)\\]\n\\[X_3 \\sim N(\\mu = 2, \\sigma = 1)\\]\n\nUse Monte Carlo simulation with 10,000 repetitions to estimate how often \\(X_{i1} &lt; X_{i2} &lt; X_{i3}\\).\n\n\n\n\n\n13.8.6 More examples of Monte Carlo simulation\n\nfivethirtyeight 2020 election forecast use\nU.S. Census Bureau simulation of data collection operations\n\n\nMarkov Chain Monte Carlo\nBayesian statisticians estimate posterior distributions of parameters that are combinations of prior distributions and sampling distributions. Outside of special cases, posterior distributions are difficult to identify. Accordingly, most Bayesian estimation uses an extension of Monte Carlo simulation called Markov Chain Monte Carlo or MCMC.\n\n\n\n13.8.7 One Final Note\nMonte Carlo simulations likely underestimate uncertainty. Monte Carlo simulations only capture aleatoric uncertainty and they don’t capture epistemic uncertainty.\nAleatoric uncertainty: Uncertainty due to probabilistic variety\nEpistemic uncertainty: Uncertainty due to a lack of knowledge\nIn other words, Monte Carlo simulations estimates assume the model is correct, which is almost certainly never fully true. Be transparent. Be humble.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#sampling-from-observed-data",
    "href": "14_simulation-and-sampling.html#sampling-from-observed-data",
    "title": "13  Simulation and Sampling",
    "section": "13.9 Sampling from Observed Data",
    "text": "13.9 Sampling from Observed Data\nUntil now, we’ve only discussed sampling from closed-form theoretical distributions. We also called this process simulation. There are many applications where we may want to sample from observed data.\nWe can break these methods into two general approaches:\n\nSampling\nResampling\n\n\n13.9.1 Sampling\n\n\n\n\n\n\nSampling\n\n\n\nSampling is the process of selecting a subset of data. Probability sampling is the process of selecting a sample when the selection uses randomization.\n\n\nSampling has many applications:\n\nReducing costs for the collection of data\nImplementing machine learning algorithms\nResampling\n\n\n\n13.9.2 Resampling\n\n\n\n\n\n\nResampling\n\n\n\nResampling is the process of repeatedly sampling from observed data to approximate the generation of new data.\n\n\nThere are at least three popular resampling methods:\n\nCross Validation: Partitioning the data and shuffling the partitions to understand the accuracy of predictive models.\nBootstrap sampling: Repeated sampling with replacement to estimate sampling distributions from observed data.\nJackknife: Leave-one-out sampling to estimate the bias and standard error of a statistic.\n\nWe focused on cross-validation for machine learning and predictive modeling in data science for public policy. We will use this approach again for predictive modeling.\nWe will also learn about bootstrap sampling when we discuss nonparametric statistics.\n\n\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical Inference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "14_simulation-and-sampling.html#footnotes",
    "href": "14_simulation-and-sampling.html#footnotes",
    "title": "13  Simulation and Sampling",
    "section": "",
    "text": "(Casella and Berger 2002) offers a robust introduction to deriving maximum likelihood estimators.↩︎\nNote that the MLE for variance is biased.↩︎\nCorrelation may be more familiar than covariance. Sample correlation is standardized sample covariance. \\(Corr(\\vec{x}, \\vec{y}) = \\frac{Cov(\\vec{x}, \\vec{y})}{S_{\\vec{x}}S_{\\vec{y}}}\\). Correlation is also between -1 and 1 inclusive. Covariance can take on any real value.↩︎",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation and Sampling</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html",
    "href": "15_microsimulation.html",
    "title": "14  Microsimulation",
    "section": "",
    "text": "14.1 Motivation\nIt is often important to ask “what would be the policy impact of…” on a population or subpopulation.\nOne common approach is to look at representative units. For example, we could construct one observation that is representative (e.g. median family structure, median income, etc.) and pass its values into a calculator. Then, we could extrapolate this experience to other observations.\nAnother common approach is to look at aggregated data. For example, we could look at county-level insurance coverage in Medicaid expansion and non-expansion states and then extrapolate to other health care expansions.\nOrcutt (1957) suggested a radically different approach. Instead of using a representative unit or aggregated data to project outcomes, model outcomes for individual units and aggregate the results. Potential units-of-analysis include people, households, and firms. Models include anything from simple accounting rules to complex behavioral and demographic models.\nIt took decades for this approach to see wide adoption because of data and computational limitations, but it is now commonplace in policy evaluation.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#calculators",
    "href": "15_microsimulation.html#calculators",
    "title": "14  Microsimulation",
    "section": "14.2 Calculators",
    "text": "14.2 Calculators\nWe’ll first look at calculators, which are an important tool for representative unit methods and microsimulation.\n\nThe Tax Policy Center’s Marriage Calculator can be used to calculate tax marriage penalties and benefits.\nPolicyEngine contains scores of calculators for taxes and benefits. For example, this calculator evaluates marginal tax rates in California accounting for taxes and benefits.\n\nThese two examples show the value of using calculators to explore potentially harmful marriage disincentives like marriage penalties, benefits cliffs, and extremely high marginal tax rates.\n\n14.2.1 Example 1\nSuppose we are interested in creating a new tax credit that is very simple. Its only parameters are number of children and total family income. It has the following characteristics:\n\nIgnore the first $20,000 of family income\nCreate a maximum benefit of $3,000 for one child, $4,500 for two children, and $6,000 for three or more children\nReduce the benefit by $0.10 for every dollar of income in excess of $20,000.\n\nWe first create an R function that implements this policy proposal.\n\n#' Calculate the benefit from the new tax credit\n#'\n#' @param num_children A numeric for the number of children\n#' @param family_income A numeric for family income\n#'\n#' @return Numeric benefit in dollars\n#' \nnew_tax_credit &lt;- function(num_children, family_income) {\n  \n  modified_income &lt;- pmax(family_income - 20000, 0)\n  \n  benefit &lt;- dplyr::case_when(\n    num_children &gt;= 3 ~ pmax(0, 6000 - 0.1 * modified_income),\n    num_children == 2 ~ pmax(0, 4500 - 0.1 * modified_income),\n    num_children == 1 ~ pmax(0, 3000 - 0.1 * modified_income),\n    TRUE ~ 0\n  )\n\n  return(benefit)\n  \n}\n\nWe can apply the calculator to representative cases:\n\nnew_tax_credit(num_children = 1, family_income = 34000)\n\n[1] 1600\n\nnew_tax_credit(num_children = 4, family_income = 12000)\n\n[1] 6000\n\n\nWe can also apply the calculator to many potential values and generate benefit plots:\n\n\nCode\nexpand_grid(\n  family_income = seq(0, 100000, 100),\n  num_children = 1:3\n) |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = num_children, \n      .y = family_income, \n      .f = new_tax_credit\n    )\n  ) |&gt;\n  ggplot(aes(family_income, benefit, color = factor(num_children))) +\n  geom_line() +\n  scale_x_continuous(labels = scales::label_dollar()) +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  labs(\n    x = \"Family income\",\n    y = \"Benefit\",\n    color = \"Number of children\"\n  )\n\n\n\n\n\n\n\n\nFigure 14.1: Benefits for different income levels and numbers of children\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCreate a new, well-documented function called alternative_tax_credit() based on new_tax_credit() that models the following proposed tax credit:\n\nIgnore the first $20,000 if family income.\nCreate a maximum benefits of $4,000 for one child, $4,500 for two children, and $5,000 for three children.\nReduces the benefit by $.05 for every dollar of income in excess of $20,000.\n\nUse library(ggplot2), expand_grid(), and map2_dbl() to create a chart like the one in the notes for this alternative tax credit.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#microsimulation",
    "href": "15_microsimulation.html#microsimulation",
    "title": "14  Microsimulation",
    "section": "14.3 Microsimulation",
    "text": "14.3 Microsimulation\n\n\n\n\n\n\nMicrosimulation\n\n\n\nMicrosimulation is a tool for projection that starts with individual observations (i.e. people or households) and then aggregates those individuals.\n\n\nMicrosimulation requires many assumptions and significant investment, but is useful for a few reasons:\n\nIt can be used to project heterogeneous outcomes and to look at the distribution of outcomes instead of just typical outcomes.\nIt can be used to evaluate “what-if” situations by comparing baseline projections with counterfactual projections.\n\nMicrosimulation is widely used in government, not-for-profits, and academia:\n\nThe Congressional Budget Office’s Long-Term (CBOLT) model is used for long-term fiscal forecasts.\nThe CBO uses HISIM2 to forecast health insurance coverage and premiums for people under age 65.\nThe Urban-Brookings Tax Policy Center evaluates most major tax proposals with its tax microsimulation model.\n\nMicrosimulation is also widely cited in popular publications around important debates:\n\nThe Urban Institute’s HIPSM was cited in the majority opinion of the Supreme Court case King v. Burwell, which upheld the Patient Protection and Affordable Care Act.\nTPC’s tax model is regularly cited in the New York Times, Wall Street Journal, and Washington Post when TPC evaluates candidates’ tax plans.\n\nMicrosimulation models range from simple calculators applied to representative microdata to very complex dynamic models.\n\n14.3.1 Basic Microsimulation\n\n\n\n\n\n\nAccounting Rules\n\n\n\nAccounting rules are the basic calculations associated with government law and programs like taxes, Social Security benefits, and Medicare.\nAccounting rules are sometimes called arithmetic rules because they are typically limited to addition, subtraction, multiplication, division, and simple if-else logic.\n\n\nLet’s start with a very simple algorithm for microsimulation modeling:\n\nConstruct a sample that represents the population of interest.\nApply accounting rules.\nAggregate.\n\nThe simplest microsimulation models essentially apply tax models similar to TurboTax to a representative set of microdata.\nWe can summarize output from microsimulation models with summary statistics that demonstrate the distribution of outcomes. For example, it is common to look at deciles or key percentiles to understand the heterogeneity of changes.\nWe can construct a baseline simulation by applying current law for step 2. Next, we can construct an alternative or counterfactual simulation by changing step 2 to a proposed policy. Finally, we can difference current law and the counterfactual to estimate the impact of a policy.\n\n\n14.3.2 Example 2\nLet’s consider a simple example where we apply the benefit calculator from earlier to families from the 2022 Annual Social and Economic Supplement to the Current Population Survey.\nTo keep things simple, we only consider families related to the head of household (and ignore other families in the household). Furthermore, we will ignore observations weights.1\n\n\nCode\n# if file doesn't exist in data, then download\nif (!file.exists(here(\"data\", \"cps_microsim.csv\"))) {\n  \n  cps_extract_request &lt;- define_extract_cps(\n    description = \"2018-2019 CPS Data\",\n    samples = \"cps2022_03s\",\n    variables = c(\"YEAR\", \"NCHILD\", \"FTOTVAL\")\n  )\n  \n  submitted_extract &lt;- submit_extract(cps_extract_request)\n  \n  downloadable_extract &lt;- wait_for_extract(submitted_extract)\n  \n  data_files &lt;- download_extract(\n    downloadable_extract,\n    download_dir = here(\"data\")\n  )\n  \n  cps_data &lt;- read_ipums_micro(data_files)\n  \n  cps_data |&gt;\n    filter(PERNUM == 1) |&gt;\n    mutate(\n      FTOTVAL = zap_labels(FTOTVAL),\n      NCHILD = zap_labels(NCHILD)\n    ) |&gt;\n    select(SERIAL, YEAR, ASECWT, NCHILD, FTOTVAL) |&gt;\n    rename_with(tolower) |&gt;\n    write_csv(here(\"data\", \"cps_microsim.csv\"))\n  \n}\n\n\n\nasec &lt;- read_csv(here(\"data\", \"cps_microsim.csv\"))\n\nRows: 59148 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): serial, year, asecwt, nchild, ftotval\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproposal1 &lt;- asec |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    )\n  )\n\narrange(proposal1, desc(benefit))\n\n# A tibble: 59,148 × 6\n   serial  year asecwt nchild ftotval benefit\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1    118  2022   865.      4    1552    6000\n 2    159  2022  1098.      3       1    6000\n 3    405  2022   933.      3   16478    6000\n 4   1914  2022   614.      4    9300    6000\n 5   3269  2022   391.      3    5400    6000\n 6   4632  2022  2624.      3       0    6000\n 7   4692  2022  1746.      3   15500    6000\n 8   4812  2022  1468.      4   12000    6000\n 9   5494  2022   662.      3   18000    6000\n10   5596  2022  1708.      3   10002    6000\n# ℹ 59,138 more rows\n\n\n\n\n14.3.3 Distributional Analysis\nAggregate analysis and representative unit analysis often mask important heterogeneity. The first major advantage of microsimulation is the ability to apply distributional analysis.\n\n\n\n\n\n\nDistributional Analysis\n\n\n\nDistributional analysis is the calculation and interpretation of statistics outside of the mean, median, and total. The objective is to understand a range of outcomes instead of typical outcomes.\n\n\nHere, we expand step 3 from the basic microsimulation algorithm to include a range of statistics. The most common statistics are percentiles or outcomes for ntiles.\n\n\n14.3.4 Example 3\nConsider the previous example. Let’s summarize the mean benefit by family income decile. We can use the ntile() function to construct ntiles(). We can use min() and max() in summarize() to define the bounds of the ntiles.\n\ndistributional_table &lt;- proposal1 |&gt;\n  mutate(ftotval_decile = ntile(ftotval, n = 10)) |&gt;\n  group_by(ftotval_decile) |&gt;\n  summarize(\n    min_income = min(ftotval),\n    max_income = max(ftotval),\n    mean_benefit = mean(benefit)\n  )\n\ndistributional_table\n\n# A tibble: 10 × 4\n   ftotval_decile min_income max_income mean_benefit\n            &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1              1     -19935      14160       1019. \n 2              2      14160      25200        921. \n 3              3      25200      37432        863. \n 4              4      37433      50100        622. \n 5              5      50100      65122        266. \n 6              6      65124      84000         59.6\n 7              7      84000     108006          0  \n 8              8     108010     143210          0  \n 9              9     143221     205154          0  \n10             10     205163    2320191          0  \n\n\n\n\nCode\ndistributional_table |&gt;\n  ggplot(aes(ftotval_decile, mean_benefit)) +\n  geom_col() +\n  geom_text(aes(label = scales::label_dollar()(mean_benefit)), vjust = -1) +\n  scale_x_continuous(breaks = 1:10) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1)),\n    labels = scales::label_dollar() \n  ) +\n  labs(\n    title = \"Distribution of Average Benefits Under the Benefit Proposal\",\n    x = \"Familiy Income Decile\",\n    y = \"Average Benefit\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nApply the calculator from the earlier exercise to the CPS data.\nAggregate outcomes for the 5th, 15th, and 25th percentiles.\nAggregate outcomes for the bottom 10 vintiles (a vintile is one twentieth of the population).\n\n\n\n\n\n14.3.5 Counterfactual Analysis\n\n\n\n\n\n\nCounterfactual\n\n\n\nA counterfactual is a situation that would be true under different circumstances.\n\n\nThe second major advantage of microsimulation is the ability to implement counterfactuals and evaluate “what-if” situations. Consider a few examples:\n\nWhat could happen to the distribution of post-tax income if the top marginal tax rate is increased by 5 percentage points?\nWhat could happen to median Social Security benefits in 2030 if the retirement age is increased by 2 months every year beginning in 2024?\nWhat could happen to total student loan balances if federal student loan interest accumulation is paused for 6 more months.\n\nWe update our microsimulation algorithm to include counterfactual analysis.\n\nConstruct a sample that represents the population of interest.\nApply accounting rules that reflect current circumstances. This is the baseline microsimulation.\nApply accounting rules that reflect counterfactual circumstances. This is the counterfactual microsimulation.\nAggregate results with a focus on the difference between the baseline microsimulation and the counterfactual simulation.\n\n\n\n14.3.6 Example 4\nLet’s pretend the new_tax_credit() is current law. It is our baseline. Suppose a legislator proposes reforms to the law. This is our counterfactual. Here are the proposed changes:\n\nEliminate benefits for families with zero income to promote work.\nEliminate the $20,000 income exclusion to reduce benefits for higher earners.\n\n\n#' Calculate the benefit from the new tax credit\n#'\n#' @param num_children A numeric for the number of children\n#' @param family_income A numeric for family income\n#'\n#' @return Numeric benefit in dollars\nnewer_tax_credit &lt;- function(num_children, family_income) {\n  \n  dplyr::case_when(\n    family_income == 0 ~ 0,\n    num_children &gt;= 3 ~ pmax(0, 6000 - 0.1 * family_income),\n    num_children == 2 ~ pmax(0, 4500 - 0.1 * family_income),\n    num_children == 1 ~ pmax(0, 3000 - 0.1 * family_income),\n    TRUE ~ 0\n  )\n\n}\n\n\nproposal2 &lt;- asec |&gt;\n  mutate(\n    benefit_baseline = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    ),\n    benefit_counteractual = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = newer_tax_credit\n    )\n  )\n\nproposal2 |&gt;\n  mutate(benefit_change = benefit_counteractual - benefit_baseline) |&gt;\n    mutate(ftotval_decile = ntile(ftotval, n = 10)) |&gt;\n  group_by(ftotval_decile) |&gt;\n  summarize(\n    min_income = min(ftotval),\n    max_income = max(ftotval),\n    mean_benefit = mean(benefit_change)\n  )\n\n# A tibble: 10 × 4\n   ftotval_decile min_income max_income mean_benefit\n            &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1              1     -19935      14160       -374. \n 2              2      14160      25200       -436. \n 3              3      25200      37432       -557. \n 4              4      37433      50100       -467. \n 5              5      50100      65122       -242. \n 6              6      65124      84000        -59.6\n 7              7      84000     108006          0  \n 8              8     108010     143210          0  \n 9              9     143221     205154          0  \n10             10     205163    2320191          0  \n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nRun the baseline and counterfactual simulations.\nCreate a bar chart representing the change in benefits.\n\n\n\n\n\n14.3.7 Extrapolation\n\n\n\n\n\n\nExtrapolation\n\n\n\nExtrapolation is the extension of microsimulation models to unobserved time periods. Most often, microsimulation models are extrapolated into the future.\n\n\nMost microsimulation models incorporate time. For example, many models look at a few or many years. We now add a new step 2 to the microsimulation algorithm, where we we can project demographic and economic outcomes into the future or past before applying accounting rules and aggregating the results.\n\nConstruct a sample that represents the population of interest.\nExtrapolate the population of interest over multiple time periods.\nApply accounting rules.\nAggregate.\n\nSome microsimulation models treat time as continuous. More often, microsimulation treats time as discrete. For example, models represent time every month or every year.\nExtrapolation adds uncertainty and assumptions to microsimulation. It is important to be clear about what microsimulation is and isn’t.\n\n\n\n\n\n\nProjection\n\n\n\nA projection explores what could happen under a given set of assumptions. It is always correct under the assumptions.\nStats Canada\n\n\n\n\n\n\n\n\nForecast\n\n\n\nA forecast attempts to predict the most-likely future.\nStats Canada\n\n\nMost microsimulation models are projections, not forecasts.\n\n\n14.3.8 Transitions\n\n\n\n\n\n\nStatic Microsimulation\n\n\n\nStatic microsimulation models do not subject units to individual transitions between time periods \\(t - 1\\) and \\(t\\) or to behavioral responses.\n\n\nStatic models typically only deal with one time period or they deal with multiple time periods but reweight the data to match expected totals and characteristics over time. Basically, individual decisions affect the distribution of outcomes but have little impact on overall outcomes.\n\n\n\n\n\n\nDynamic Microsimulation\n\n\n\nDynamic microsimulation models subject individual units to transitions between time periods \\(t - 1\\) and \\(t\\). This is sometimes referred to as “aging” the population. Dynamic microsimulation models sometimes subject individual units to behavioral responses.\n\n\nTransitions from period \\(t - 1\\) to period \\(t\\) are key to dynamic microsimulation. Transitions can be deterministic or stochastic. An example of a deterministic transition is an individual always joining Medicare at age 65. An example of a stochastic transition is an unmarried individual marrying at age 30 with probability \\(p_1\\) and remaining unmarried with probability \\(p_2\\). Stochastic transitions are connected to the idea of Monte Carlo simulation.\nTransition models are fundamental to stochastic transitions. Transition models include transition probability models (categorical variables) and processes based on probability distributions (continuous variables).\nHere are are few examples of transitions that could be modeled:\n\nIf an individual will acquire more education.\nIf an individual will work.\nIf an individual will marry, divorce, or widow.\nIf an individual will have children.\nIf an individual will retire.\nIf an individual will die.\n\nTransition models are often taken from existing literature or estimated on panel data. The data used to estimate these models must have at least two time periods.\nIt is common to extrapolate backwards, which is also known as backcasting, to evaluate microsimulation models against history. Backcasting can add important longitudinal information to records like detailed earnings histories for calculating Social Security benefits. Backcasting also offers opportunities to benchmark model projections against observed history.\n\n\n14.3.9 Example 5\nLet’s extrapolate our 2022 CPS data to 2023 and 2024 using transition models for number of children and family total income.\nThe transition model for number of children is very simple. 15% of families lose a child, 80% of families observe no change, and 5% of families gain one child.\n\n#' Extrapolate the number of children\n#'\n#' @param num_children A numeric for the number of children in time t\n#'\n#' @return A numeric for the number in time t + 1\n#'\nchildren_hazard &lt;- function(num_children) {\n  \n  change &lt;- sample(x = c(-1, 0, 1), size = 1, prob = c(0.15, 0.8, 0.05))\n  \n  pmax(num_children + change, 0)\n  \n}\n\nThe transition model for family income is very simple. The proportion change is drawn from a normal distribution with \\(\\mu = 0.02\\) and \\(\\sigma = 0.03\\).\n\n#' Extrapolate family income\n#'\n#' @param num_children A numeric for family income in time t\n#'\n#' @return A numeric for family income in time t + 1\n#'\nincome_hazard &lt;- function(family_income) {\n  \n  change &lt;- rnorm(n = 1, mean = 0.02, sd = 0.03)\n  \n  family_income + family_income * change\n  \n}\n\nFor simplicity, we combine both transition models into one function.\n\n#' Extrapolate the simple CPS\n#'\n#' @param data A tibble with nchild and ftotval\n#'\n#' @return A data frame in time t + 1\n#'\nextrapolate &lt;- function(data) {\n  \n  data |&gt;\n    mutate(\n      nchild = map_dbl(.x = nchild, .f = children_hazard),\n      ftotval = map_dbl(.x = ftotval, .f = income_hazard),\n      year = year + 1\n    )\n\n}\n\nFinally, we extrapolate.\n\n# make the stochastic results reproducible\nset.seed(20230812)\n\n# extrapolate using t to create t + 1\nasec2023 &lt;- extrapolate(asec)\nasec2024 &lt;- extrapolate(asec2023)\n\n# combine\nasec_extrapolated &lt;- bind_rows(\n  asec,\n  asec2023,\n  asec2024\n)\n\nThe benefit and income amounts in new_tax_credit() are not indexed for inflation. Let’s see how benefits change over time with the extrapolated data.\n\nasec_extrapolated |&gt;\n  mutate(\n    benefit = map2_dbl(\n      .x = nchild, \n      .y = ftotval, \n      .f = new_tax_credit\n    )\n  ) |&gt;\n  group_by(year) |&gt;\n  summarize(\n    mean_ftotval = mean(ftotval),\n    total_nchild = sum(nchild),\n    mean_benefit = mean(benefit)\n  )\n\n# A tibble: 3 × 4\n   year mean_ftotval total_nchild mean_benefit\n  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1  2022       97768.        46296         375.\n2  2023       99705.        45615         379.\n3  2024      101700.        44848         379.\n\n\nEven though incomes grew and the number of children declined, it turns out that enough families went from zero children to one child under the transition probability model for children that average benefits remained about the same.\n\n\n14.3.10 Beyond Accounting Rules\n\n\n\n\n\n\nBehavioral Responses\n\n\n\nA behavioral response is an assumed reaction to changing circumstances in a microsimulation model.\nFor example, increasing Social Security benefits may crowd out retirement savings (Chetty et al. 2014). In other words, the existence of Social Security may induce a person to save less for retirement.\n\n\nUntil now, we’ve only considered accounting rules within each time period. Accounting rules are appealing because they don’t require major assumptions, but they are typically insufficient. It is often necessary to consider how units will respond to a changing environment.\nBehavioral responses are often elasticities estimated with econometric analysis. Behavioral responses are also a huge source of assumptions and uncertainty for microsimulation.\nConsider a microsimulation model that models retirement savings. When modeling a proposal to expand Social Security benefits, an analyst may use parameters estimated in (Chetty et al. 2014) to model reductions in savings in accounts like 401(k) accounts. These are behavioral responses. These estimates are uncertain, so the analyst could add sensitivity analysis to model low, medium, and high rates of crowding out.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#building-a-representative-population",
    "href": "15_microsimulation.html#building-a-representative-population",
    "title": "14  Microsimulation",
    "section": "14.4 Building a Representative Population",
    "text": "14.4 Building a Representative Population\nBuilding a starting sample and constructing data for estimating transition models is difficult. We briefly outline a few techniques of interest.\n\n14.4.1 Synthetic Starting Data\nWe’ve adopted a cross-sectional starting population. Some microsimulation models adopt a synthetic2 approach where every observation is simulated from birth.\n\n\n14.4.2 Data Linkage and Imputation\nMany microsimulation models need more variables than are included in any one source of information. For example, retirement models often need demographic information, longitudinal earnings information, and tax information. Many microsimulation models rely on data linkage and data imputation techniques to augment their starting data.\n\n\n\n\n\n\nData Linkage\n\n\n\nData linkage is the process of using distance-based rules or probabilistic models to connect an observation in one source of data to an observation in another source of data.\n\n\n\n\n\n\n\n\nData imputation\n\n\n\nData imputation is the process of using models to predict values where data is unobserved. The data could be missing because of nonresponse or because the information was not gathered in the data collection process.\n\n\n\n\n14.4.3 Validation\n\n\n\n\n\n\nValidation\n\n\n\nValidation is the process of reviewing results to determine their validity. Techniques include comparing statistics, visual comparisons, and statistical comparisons like the Kolmogorov-Smirnov test for the equivalence of two distributions.\n\n\nVisual and statistical validation are essential to evaluating the quality of a microsimulation model. If validation looks poor, then a modeler can redo other parts of the microsimulation workflow or they can reweight or align the data.\n\n\n14.4.4 Reweighting\n\n\n\n\n\n\nReweighting\n\n\n\nReweighting is the process of adjusting observation weights in a data set so aggregate weighted statistics from the data set hit specified targets.\n\n\nSuppose a well-regarded source of information says mean income is $50,000 but a microsimulation model estimates mean income of $45,000. We can use reweighting to plausibly adjust the weights in the microsimulation model so mean income is $50,000.\nTechniques include post-stratification and calibration. Kolenikov (2016) offers a good introduction.\n\n\n14.4.5 Alignment\n\n\n\n\n\n\nAlignment\n\n\n\nAlignment is the process of adjusting model coefficients or predicted values so aggregated outputs align with specified aggregate targets. These targets are aggregate outcomes like total income or state-level tax revenue.\n\n\nSuppose a well-regarded source of information says mean income is $50,000 but a microsimulation model estimates mean income of $45,000. We can adjust the intercept in linear regression models or adjust predicted values so mean income is $50,000. Li and O’Donoghue (2014) offers a good introduction.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#assumptions",
    "href": "15_microsimulation.html#assumptions",
    "title": "14  Microsimulation",
    "section": "14.5 Assumptions",
    "text": "14.5 Assumptions\nMicrosimulation is only as useful as its assumptions. We will review a few key assumptions present in many microsimulation models.\n\n14.5.1 Open vs. Closed\n\n\n\n\n\n\nClosed Model\n\n\n\nA closed microsimulation model models the life cycle of all units in the model. For example, two existing units marry.\n\n\n\n\n\n\n\n\nOpen Model\n\n\n\nAn open microsimulation model allows for the on-demand generation of new, but mature, units in the model.\n\n\n\n\n14.5.2 Independence\nAnother important assumption deals with the relationship between units in the model. Should observations be treated as wholly independent or do they interact? For example, if someone takes a job is it more difficult for another individual to take the job?\nInteractions can be explicit or implicit.\n\n\n\n\n\n\nExplicit Interaction\n\n\n\nExplicit interaction allows the actions of one unit to affect other units during extrapolation or behavioral models.\nFor example, models of marriage markets may account for changing economic circumstances among potential matches.\n\n\n\n\n\n\n\n\nImplicit Interaction\n\n\n\nImplicit interaction allows the actions of one unit to affect other units in post-processing.\nFor example, reweighting and alignment techniques allow outcomes for one unit to affect other units through intercepts in models and new weights.\n\n\n\n\n14.5.3 Markov Assumption\n\n\n\n\n\n\nMarkov Assumption\n\n\n\nThe Markov assumptions states that the only factors affecting a transition from period \\(t - 1\\) to period \\(t\\) are observable in \\(t - 1\\).\nThe Markov assumption only considers memory or history to the extent that it is observable in period \\(t - 1\\). For example, \\(t - 2\\) may affect educational attainment or college savings in \\(t - 1\\), which can affect the transition from \\(t - 1\\) to \\(t\\), but \\(t - 2\\) will never be explicitly included.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#uncertainty",
    "href": "15_microsimulation.html#uncertainty",
    "title": "14  Microsimulation",
    "section": "14.6 Uncertainty",
    "text": "14.6 Uncertainty\n\n\n\n\n\n\nAletoric Uncertainty\n\n\n\nAleatoric uncertainty is uncertainty due to probabilistic randomness.\n\n\n\n\n\n\n\n\nEpistemic Uncertainty\n\n\n\nEpistemic uncertainty is uncertainty due to lack of knowledge of the underlying system.\n\n\nThe microsimulation field has a poor track record of quantifying uncertainty. Most estimates do not contain standard errors or even crude distributions of outcomes.\nMicrosimulation models have many sources of uncertainty. The starting data and data used for model estimation are often samples with sampling error. Transition models fail to capture all sources of variation. The models often intentionally include Monte Carlo error.\nMcClelland, Khitatrakun, and Lu (2020) explored adding confidence intervals to microsimulation models using normal approximations and bootstrapping methods3. They find that normal approximations work well in most cases unless policy changes affect a small number of returns.\nAcross microsimulations for five policy approaches, they estimate modest confidence intervals. This makes sense. First, microsimulation aggregates many units that have their own sources of uncertainty. This is different than earlier examples of Monte Carlo simulation that focused on one observation at a time. Aggregation reduces variance.4\nSecond, the authors only consider uncertainty because of sampling variation. This fails to capture aleatoric uncertainty from statistical matching, imputation, and projection. Furthermore, these methods fail to capture epistemic uncertainty. The confidence intervals, in other words, assume the model is correct.\nGiven these shortcomings, it is important to be clear about assumptions, transparent about implementations, and humble about conclusions. All useful models are wrong. The hope is to be as little wrong as possible.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#microsimulation-model-examples",
    "href": "15_microsimulation.html#microsimulation-model-examples",
    "title": "14  Microsimulation",
    "section": "14.7 Microsimulation Model Examples",
    "text": "14.7 Microsimulation Model Examples\nFirst, let’s outline a few characteristics of microsimulation models.\n\n\n\n\n\n\nCompiled Programming Languages\n\n\n\nCompiled programming languages have an explicit compiling step where code is converted to assembly language and ultimately binary code.\nC++, Fortran, and Java are examples of compiled programming languages.\n\n\n\n\n\n\n\n\nScripting Programming Languages\n\n\n\nScripting programming languages, also known as interpreted programming languages, do not have a compiling step.\nR, Python, and Julia are examples of scripting programming languages.\n\n\nAruoba and Fernndez-Villaverde (2018) benchmark several programming languages on the same computing task. Lower-level, compiled programming languages dominate higher-level, scripting programming languages like R and Python. Julia is the lone bright spot that blends usability and performance.\nMany microsimulation models are written in lower-level, compiled programming languages. Fortran may seem old, but there is a reason it has stuck around for microsimulation.\n\n\n\n\n\n\nGeneral Microsimulation Models\n\n\n\nGeneral microsimulation models contain a wide range of behaviors and population segments.\n\n\n\n\n\n\n\n\nSpecialized Microsimulation Models\n\n\n\nSpecialized microsimulation models focus on a limit set of behaviors or population segments. ~ Stats Canada\n\n\n\n14.7.1 Simulating the 2020 Census\n\nName: Simulating the 2020 Census\nAuthors: Diana Elliott, Steven Martin, Jessica Shakesprere, Jessica Kelly\nGeneral or specific: Specific\nLanguage: Python\nPurpose: The authors simulate various Decennial Census response factors and evaluate the distribution of responses to the Decennial Census.\n\n\n\n14.7.2 TPC Microsimulation in the Cloud\n\nName: TPC Microsimulation in the Cloud\nAuthors: The TPC microsimulation team, Jessica Kelly, Kyle Ueyama, Alyssa Harris\nGeneral or specific: Specific\nLanguage: Fortran\nPurpose: The authors take TPC’s tax microsimulation model and move it to the cloud. This allows TPC to reverse the typical microsimulation process. Instead of describing policies and observing the outcomes, they can describe desirable outcomes and then use grid search to back out policies that achieve those outcomes.\n\n\n\n14.7.3 Modeling Income in the Near Term (MINT)5\n\nName: Modeling Income in the Near Term (MINT)\nAuthors: Karen E. Smith and many other people\nGeneral or specific: General\nLanguage: SAS6\nPurpose: The Social Security Administration uses MINT to evaluate the distributional impact of various Social Security policy proposals.\n\n\n\n\n\nAruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A Comparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien Nielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and Crowd-Out in Retirement Savings Accounts: Evidence from Denmark*.” The Quarterly Journal of Economics 129 (3): 1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response Adjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary Alignment Methods in Microsimulation Models.” Journal of Artificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020. “Estimating Confidence Intervals in a Tax Microsimulation Model.” International Journal of Microsimulation 13 (2): 2–20. https://doi.org/10.34196/IJM.00216.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.” The Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "15_microsimulation.html#footnotes",
    "href": "15_microsimulation.html#footnotes",
    "title": "14  Microsimulation",
    "section": "",
    "text": "Most data for microsimulation models are collected through complex surveys. Accordingly, most microsimulation models need to account for weights to calculate estimates that represent the entire population of interest.↩︎\nSynthetic will carry many meanings this semester.↩︎\nTheir implementation of bootstrapping is clever and uses replicate weights to simplify computation and manage memory.↩︎\nThe standard error of the mean reduces at a rate of \\(\\sqrt{n}\\) as the sample size increases.↩︎\nMINT comically models income and many other variables for at least 75 years into the future.↩︎\nThe Dynamic Simulation of Income Model (DYNASIM) is a related to MINT and is written in Fortran.↩︎",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Microsimulation</span>"
    ]
  },
  {
    "objectID": "16_nonparametric-1.html",
    "href": "16_nonparametric-1.html",
    "title": "15  Nonparametric Curve Fitting",
    "section": "",
    "text": "15.1 Review",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Curve Fitting</span>"
    ]
  },
  {
    "objectID": "16_nonparametric-1.html#sec-review8",
    "href": "16_nonparametric-1.html#sec-review8",
    "title": "15  Nonparametric Curve Fitting",
    "section": "",
    "text": "Statistical models serve three major purposes\n\nSummary\nInference\nPrediction\n\nWe adopted nonparametric techniques like KNN, decision trees, and random forests for making accurate predictions.\nIn certain applications, these data-driven approaches work well.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Curve Fitting</span>"
    ]
  },
  {
    "objectID": "16_nonparametric-1.html#nonparametric-statistics",
    "href": "16_nonparametric-1.html#nonparametric-statistics",
    "title": "15  Nonparametric Curve Fitting",
    "section": "15.2 Nonparametric Statistics",
    "text": "15.2 Nonparametric Statistics\n\n\n\n\n\n\nParametric Statistics\n\n\n\nParametric statistics requires the form of the population distribution to be completely specified except for a finite number of parameters.\nFor example, for a given analysis, an analyst could require/assume that the population be normally distributed. The normal distribution has two parameters: mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)).\n\n\n\n\n\n\n\n\nNonparametric Statistics\n\n\n\nNonparametric statistics1 require a minimal number of assumptions about the form of the population distribution.\nHiggins (2004) offers two key examples of minimal assumptions:\n\nThe data are from a population with a continuous distribution.\nThe population depends on location2 and scale3 parameters.\n\n\n\nThere are nonparametric tests for many common types of statistical inference. For example,\n\n1-sample location parameter test (a non-parametric analog to a 1-sample t-test).\n2-sample location parameter test (a non-parametric analog to a 2-sample t-test).\nK-sample location parameter test (a non-parametric analog to ANOVA).\n2-sample scale parameter test (a non-parametric analog to a 2-sample F-test).\n\nNonparametric statistics can also be grouped by their statistical ideas.4 Many of these ideas are incredibly simple and could be, as my professor used to say, derived while sitting on the beach.\n\nSmoothing: Fitting curves without specifying the functional forms of the curves. This can be done in one dimension to estimate a probability distribution function and in multiple dimensions for estimating smoothed conditional means.\nPermutation: Exhaustively reconfigure the data to create a sampling distribution for a statistic.\nBootstrap: Repeatedly sample from the data and calculate a statistic to create a sampling distribution for a statistic.\n\nThis set of notes focuses on techniques for estimating probability density functions (PDFs) with smoothing when the data don’t conform to common PDFs like the normal distribution or exponential distribution. The next set of notes will focus on the permutation and bootstrap for estimating sampling distributions (PDFs for statistics).",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Curve Fitting</span>"
    ]
  },
  {
    "objectID": "16_nonparametric-1.html#estimating-probability-density-functions",
    "href": "16_nonparametric-1.html#estimating-probability-density-functions",
    "title": "15  Nonparametric Curve Fitting",
    "section": "15.3 Estimating Probability Density Functions",
    "text": "15.3 Estimating Probability Density Functions\nSo far, we’ve explored parametric techniques for estimating densities and probability density functions. We looked at data and fit common probability distributions with a finite number of parameters to the data. For instance, for a normal distribution the sufficient statistics or finite number of parameters are the mean and standard deviation. Unfortunately, common distributions may not fit the data.\nIn this section, we will look at data-driven nonparametric approaches to estimating probability density functions.\nSuppose we have have a random sample of data \\(x_1, x_2, ..., x_n\\) for one variable and the data come from a population with a continuous probability density function. We represent the PDF as \\(f_X(x)\\). Our goal is to estimate \\(f_X(x)\\) with \\(\\hat{f_X}(x)\\) using a data-driven approach and without specify a model or functional form.\nWe will discuss two approaches to this problem:\n\nRelative frequency histogram\nKernel density estimators (KDEs)\n\nWe can use relative frequency histograms and KDEs to 1. visualize data and 2. generate new data with random sampling.\n\n15.3.1 Relative Frequency Histograms\n\n\n\n\n\n\nRelative Frequency Histogram\n\n\n\nRelative frequency histograms are histograms normalized such that the bars sum to 1 that can be used to estimate PDFs.\nNote: PDFs must integrate to 1.\n\n\nThe procedure for creating a relative frequency histogram is fairly simple:\n\nPartition the range of the data into \\(k\\) bins. The widths are called binwidths and typically the bins have equal binwidths.\nCount the number of observations in each bin.\nDivide the counts by n * binwidth to normalize the histogram\n\nLet’s consider a formal definition from (Higgins 2004).\nPartition the range of the data into bins such that \\(a_1 &lt; a_2 &lt; ... &lt; a_k\\). Let \\(a_{i + 1} - a_i\\) be the binwidth. The density estimate for a value \\(x\\) in an interval \\(a_i &lt; x \\le a_{i + 1}\\) is\n\\[\n\\hat{f}(x) = \\frac{\\text{number of observations} \\in (a_i, a_{i + 1}]}{n \\cdot (a_{i + 1} - a_i)}\n\\tag{15.1}\\]\n\nExample\nConsider three random samples of sizes \\(n = 40\\), \\(n = 100\\), and \\(n = 10,000\\) from standard normal distribution. In the code below, we store these samples in an a tibble with two columns, one for the name of the sample set and one for the observation in the sample.\n\nset.seed(20230617)\ndata &lt;- bind_rows(\n  `n=40` = tibble(x = rnorm(n = 40)),\n  `n=100` = tibble(x = rnorm(n = 100)),\n  `n=10,000` = tibble(x = rnorm(n = 10000)),\n  .id = \"set\"\n) |&gt;\n  mutate(set = factor(set, levels = c(\"n=40\", \"n=100\", \"n=10,000\")))\n\nLet’s use library(ggplot2) and the after_stat() function to create relative frequency histograms for the three random samples. Remember, the relative frequency histograms are \\(\\hat{f}(x)\\).\nIn this example, we know \\(f_X(x)\\). It is the PDF for \\(X \\sim N(\\mu = 0, \\sigma = 1)\\). Next, superimpose the known PDF on top of \\(\\hat{f_X}(x)\\).\n\ndata |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = 0, sd = 1)) +\n  facet_wrap(~ set)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 15.1: Relative frequency histograms fit to sample data from standard normal distributions with three different sample sizes.\n\n\n\n\n\n\n\n\n\n\nNumber of Bins and Binwidth\nHistograms and relative frequency histograms are very sensitive to the selection of binwidths. geom_histogram() defaults to 30 bins, but this can be overridden with the binwidth and bins arguments.\nSeveral papers have developed rules for determining the number of bins or binwidth including Scott and Sain (2005).\nAn important takeaway is that increasing \\(k\\), the number of bins, can lead to noisier bars but more precision in the estimate, while decreasing \\(k\\) results in less noisy bars but imprecise estimates. Figure 15.2 recreates Figure 15.1 with fewer bins.\n\ndata |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 10) +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = 0, sd = 1)) +\n  facet_wrap(~ set)\n\n\n\n\nFigure 15.2: Relative frequency histograms fit to sample data from standard normal distributions with three different sample sizes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nRun hist(cars$speed, plot = FALSE) and look at the results.\nWrite R code to convert $counts into $density based on the process of generating a relative frequency histogram described above.\n\n\n\n\n\n\n15.3.2 Kernel Density Estimation\nWe will introduce kernel density estimators by considering two related concepts.\n\nConcept #1\nWe were able to simplify and fit complex PDFs using mixture distributions. In fact, as \\(k\\), the number of mixtures in the mixture distribution increased, we were able to fit increasingly complex probability distributions. Unfortunately, this process created overparameterized models.\nKernel density estimation, a nonparametric approach for estimating PDFs, takes a slightly different approach. It places a simple PDF (called a kernel) at every data point in the data and then combines these simple PDFs into a complex PDF. We no longer need to estimate many parameters and specify the number of clusters, but the KDE is computationally expensive and requires storing all of the original data.\n\n\nConcept #2\nRelative frequency histograms have a couple of issues:\n\nThey are very sensitive to binwidths\nThey are not smooth\n\n\n\n\n\n\n\nKernel Density Estimators\n\n\n\nKernel Density Estimators are weighted relative frequencies of the number of values near some value. The counts are normalized so the entire KDE integrates to 1. KDEs are a data-driven method for estimating PDFs.\nKernel density estimators are smoothed histograms. Fundamentally, they use a weighted average of data points near \\(x_0\\) to generate \\(\\hat{f}(x_0)\\). Here, \\(x_0\\) is the point where we are estimating the probability density.\n\n\n\n\n\n\n\n\nKernel\n\n\n\nA kernel is a weighting function that maps real values to real positive values. Kernels typically integrate to 1 and are symmetric.\nUnder this definition, kernels are PDFs like the normal density function and uniform density function.\n\n\n\n\nVisual KDE Process\nKernel density estimation is a data-driven approach to fitting non-parametric PDFs. Let’s walk through a visual demonstration of a KDE where x = c(1, 4, 4.5, 5, 6, 8, 10).\n\nPick a kernel type (normal, triangle, uniform)\nAdd kernels centered at every observation with a specified bandwidth\nDivide each kernel by \\(n\\) so the distribution is normalized\nTo find the KDE at value \\(x_0\\), sum the normalized kernels at \\(x_0\\)\n\nFirst, we need to pick a kernel. We will mostly use normal kernels, but there are many potential kernels including the triangular kernel and uniform kernel. Figure 15.3 shows the PDFs for a few kernels.\n\nCode\ntibble(x = 0) |&gt;\n  ggplot() +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = 0, sd = 1)) +\n  scale_x_continuous(limits = c(-3, 3)) +\n  scale_y_continuous(limits = c(0, 0.6))\n\ntibble(x = 0) |&gt;\n  ggplot() +\n  stat_function(fun = dunif, color = \"red\", args = list(min = -0.5, max = 0.5)) +\n  scale_x_continuous(limits = c(-3, 3)) +\n  scale_y_continuous(limits = c(0, 1.2))\n\ntibble(x = seq(-3, 3, 0.1)) |&gt;\n  mutate(d = 1 - abs(x)) |&gt;\n  mutate(d = pmax(d, 0)) |&gt;\n  ggplot() +\n  geom_line(aes(x, d), color = \"red\") +\n  scale_x_continuous(limits = c(-3, 3)) +\n  scale_y_continuous(limits = c(0, 1.2))\n\n\n\n\nFigure 15.3: Simple Kernels for Kernel Desnity Estimation\n\n\n\n\n\n\n\n(a) Normal kernel\n\n\n\n\n\n\n\n\n\n\n\n(b) Uniform kernel (Boxcar function)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Triangular kernel\n\n\n\n\n\n\n\n\n\n\n\nSecond, we add kernels centered at every observation with a specified bandwidth. Figure 15.4 shows seven normal PDFs centered at our example data points.\nFor the sake of simplicity and demonstration, we’ll typically use default bandwidths. Think of bandwidths like binwidths for the relative frequency histograms we discussed in Section 15.3.1.2. Using smaller bandwidths leads to noisier KDEs but more precision. Increasing the bandwidth leads to smoother KDEs but less precision. The KDE Wikipedia page provides a more in depth discussion of bandwidth selection.\n\n\nCode\ndata &lt;- expand_grid(\n  x = c(1, 4, 4.5, 5, 6, 8, 10),\n  grid = seq(-1, 13, 0.1)\n)\n\ndata_simple &lt;- distinct(data, x)\n\nbw &lt;- stats::bw.nrd0(data_simple$x)\n\ndata &lt;- data |&gt;\n  mutate(density = dnorm(x = grid, mean = x, sd = bw))\n\ndata |&gt;\n  ggplot() +\n  geom_line(\n    data = data,\n    aes(x = grid, y = density, group = factor(x)),\n    alpha = 0.5,\n    color = \"red\"\n  ) +\n  scale_y_continuous(limits = c(0, 0.35)) +\n  labs(\n    x = \"x\"\n  )\n\n\n\n\n\nFigure 15.4: Add PDFs centered at each observation in the data\n\n\n\n\n\n\n\n\nThird, we need to adjust these normal PDFs so the KDE integrates to 1. Otherwise, the KDE will not be a PDF. To do this, simply divide each PDF by the number of data points. Figure 15.5 shows each kernel divided by 7.\n\n\nCode\ndata &lt;- expand_grid(\n  x = c(1, 4, 4.5, 5, 6, 8, 10),\n  grid = seq(-1, 13, 0.1)\n)\n\ndata_simple &lt;- distinct(data, x)\n\nbw &lt;- stats::bw.nrd0(data_simple$x)\n\ndata &lt;- data |&gt;\n  mutate(density = dnorm(x = grid, mean = x, sd = bw) / nrow(data_simple))\n\ndata |&gt;\n  ggplot() +\n  geom_line(\n    data = data,\n    aes(x = grid, y = density, group = factor(x)),\n    alpha = 0.5,\n    color = \"red\"\n  ) +\n  scale_y_continuous(limits = c(0, 0.35)) +  \n  labs(\n    x = \"x\"\n  )\n\n\n\n\n\nFigure 15.5: The KDE (black) is the Sum of the Normal Distributions (red) Normalized by the Number of Observations\n\n\n\n\n\n\n\n\nFinally, for any observation \\(x_0\\), sum up the red lines. This sum is the KDE at point \\(x_0\\).\nTypically, we calculate the KDE for many values of \\(x_0\\) (e.g. a sequence from -2 to 15 by increments of 0.1) and then visualize the result. The black line in Figure 15.6 is the estimated KDE across the range of the data.\n\n\nCode\ndata &lt;- expand_grid(\n  x = c(1, 4, 4.5, 5, 6, 8, 10),\n  grid = seq(-1, 13, 0.1)\n)\n\ndata_simple &lt;- distinct(data, x)\n\nbw &lt;- stats::bw.nrd0(data_simple$x)\n\ndata &lt;- data |&gt;\n  mutate(density = dnorm(x = grid, mean = x, sd = bw) / nrow(data_simple))\n\nkde_x0 &lt;- tibble(\n  x0 = density(x = data_simple$x, n = 10000)$x,\n  f_hat_x0 = density(x = data_simple$x, n = 10000)$y\n) |&gt;\n  slice_min(abs(x0 - 4))\n  \nkde_plot &lt;- data |&gt;\n  ggplot() +\n  geom_line(\n    data = data,\n    aes(x = grid, y = density, group = factor(x)),\n    alpha = 0.5,\n    color = \"red\"\n  ) +\n  geom_density(\n    data = data_simple,\n    aes(x = x)\n  ) +\n  geom_point(\n    data = kde_x0,\n    aes(x = x0, y = f_hat_x0)\n  ) +\n  geom_segment(\n    data = kde_x0,\n    aes(x = 4, y = 0, xend = 4, yend = f_hat_x0),\n    linetype = \"dashed\"\n  ) +\n  annotate(\"text\", x = 3, y = 0.15,\n           label = \"hat(f)(x[0])\", parse = TRUE) +\n  scale_y_continuous(limits = c(0, 0.35)) +\n  labs(\n    x = \"x\"\n  )\n\nkde_plot\n\n\n\n\n\nFigure 15.6: The KDE (black) is the Sum of the Normal Distributions (red) Normalized by the Number of Observations\n\n\n\n\n\n\n\n\n\n\nMathematical KDE Process\n\nPick a kernel type (normal, triangle, uniform)\nPick a bandwidth\nAdd kernels centered at every observation\nDivide each kernel by \\(n\\) so the distribution is normalized\nTo find the KDE at value \\(x_0\\), sum the normalized kernels at \\(x_0\\)\n\nLet \\(w(z)\\) be a kernel (symmetric probability density function) centered at \\(z\\). This kernel can give more weight to observations near \\(z\\) and less weight to observations far from \\(z\\).\nSuppose we use a normal kernel and one data point \\(x_i\\). \\(\\mu = 0\\) because the kernel is centered at \\(z\\). We’ll always use \\(\\sigma = 1\\). If we assume \\(x_0 = x_i\\), then our weighting function is \\(W(x_0) = \\frac{1} {\\sqrt{2\\pi}} exp(-\\frac{x_0^2}{2})\\).\nWhen we evaluate the KDE at point \\(x_0\\), \\(x_0 = x_i\\) won’t necessarily be true. So we will need to shift our weighting function. We simply need to modify our function to be \\(W(x_0 - x_i) = \\frac{1} {\\sqrt{2\\pi}} exp(-\\frac{(x_0 - x_i)^2}{2})\\).5\nWe may also want to adjust the widths of our kernels. When using the normal kernel, we leave \\(\\sigma = 1\\) and adjust the bandwidth. The bandwidth controls the weight given to points near \\(x_0\\) when estimating the density. Bandwidths are like binwidths in relative frequency histograms. If \\(\\triangle\\) is the bandwidth, then our weighting function is \\(W(\\frac{x_0 -x_i}{\\triangle})\\).\nRecall that a kernel must integrate to 1. Because \\(\\triangle\\) changes the shape of the curve, and therefore the area under the curve, we must account for that by dividing the curve by \\(\\triangle\\). This makes our weight function \\(\\frac{1}{\\triangle} W(\\frac{x - x_i}{\\triangle})\\). Note that as \\(\\triangle\\) increases, the PDF curves become wider with less density at any given point.\nWe now have a building block \\(\\frac{1}{\\triangle} W(\\frac{x-x_i}{\\triangle})\\). Let’s finally relax our assumption that we have one observation and assume we have \\(n\\) values of \\(x_i\\).We can take the summation of \\(n\\) of these distributions to create a kernel density estimate. Again, because the KDE must integrate to 1, we must divide the function by \\(n\\).\nThus, the kernel density estimate (KDE) is\n\\[\n\\hat{f}(x_0) = \\frac{1}{n\\triangle} \\sum_{i = 1}^n w\\left(\\frac{x_0 - x_i}{\\triangle}\\right)\n\\tag{15.2}\\]\nBased on Equation 15.2, we need to pick a weighting function (\\(W(z)\\)) and a bandwidth (\\(\\triangle\\)). We will almost always use the normal distribution and bw.nrd0 bandwidth.6 This means we can define a much simpler process:\n\nPlace a normal distribution on each observation in the data with a bandwidth determined by bw.nrd0\nDivide each kernel by \\(n\\) so the distribution is normalized\nTo find the KDE at value \\(x_0\\), sum the normalized kernels at \\(x_0\\)\n\n\n\nCode\nkde_plot\n\n\n\n\n\nFigure 15.7: The KDE (black) is the Sum of the Normal Distributions (red) Normalized by the Number of Observations\n\n\n\n\n\n\n\n\n\n\nKDE’s in R\nThere are several ways to use kernel density estimators in R. We will explore geom_density() and density().\nThe use of geom_density() is nearly identical to geom_histogram(). The function has a bw argument to change the bandwidth and a kernel argument to change the kernel.\n\ndata &lt;- expand_grid(\n  x = c(1, 4, 4.5, 5, 6, 8, 10)\n)\n\ndata |&gt;\n  ggplot(aes(x = x)) +\n  geom_density()\n\ndata |&gt;\n  ggplot(aes(x = x)) +\n  geom_density(kernel = \"triangular\")\n\n\n\nFigure 15.8: geom_density() with different kernels\n\n\n\n\n\n\n\n(a) Gaussian kernel\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Triangular kernel\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post offers a good walkthrough of the KDE process in plain language.\n\n\n\n\n\n\nExercise 2\n\n\n\nThe log-normal distribution is a distribution whose natural log is normally distributed. It is useful for modeling variables with a right skew like income and wealth.\n\nSample 100 observations from a log-normal distribution using rlnorm() with meanlog = 0 and sdlog = 1.\nCreate a relative frequency histogram for the variable.\nAdd a KDE with geom_density() in red.\nUse stat_function(), like in Figure 15.1, to add the theoretical PDF to the visualization in blue. You should now have three layers.\nDuplicate the code and set n to 10000. Is something off?",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Curve Fitting</span>"
    ]
  },
  {
    "objectID": "16_nonparametric-1.html#nonparametric-curve-smoothing",
    "href": "16_nonparametric-1.html#nonparametric-curve-smoothing",
    "title": "15  Nonparametric Curve Fitting",
    "section": "15.4 Nonparametric Curve Smoothing",
    "text": "15.4 Nonparametric Curve Smoothing\nWe now move to bivariate data where we have \\(x_1, x_2, ..., x_n\\) and \\(y_1, y_2, ..., y_n\\). Our goal is to flexibly estimate \\(f(x)\\) with \\(\\hat{f}(x)\\).7\n\\(\\hat{f}(x)\\) is an estimate of the conditional mean \\(E\\left[Y|X=x\\right]\\). Recall that linear regression is a parametric approach, with a finite number of parameters, for estimating this conditional mean. When we plug in a specific value, \\(x_0\\), we get back the conditional mean for \\(y\\) at the specific value \\(x_0\\).\nWe’re focused on nonparametric approaches to estimate \\(E\\left[Y|X=x\\right]\\). We want to make a minimal number of assumptions and we don’t want to specify a functional form. This is a very data-driven approach, which is advantageous when the data follow a clear pattern but the functional form isn’t easy to specify with just main effects or low-order polynomials.\n\n15.4.1 Approaches\nThere are three major nonparametric approaches to curve smoothing.\n\nKernel methods. Kernel methods are related to kernel density estimation for estimating probability density functions. Here, we need to estimate the joint PDF \\(f_{X,Y}(x, y)\\) and the marginal PDF \\(f_X(x)\\). Next, integrate out \\(f_X(x)\\) to approximate \\(E\\left[Y|X=x\\right]\\). This approach can be highly biased in the tails of \\(x\\) and in sparse regions of \\(x\\).(Hastie, Tibshirani, and Friedman 2009)\nRegression splines. Regression splines break \\(x\\) into ordered regions and then fit \\(\\hat{f}(x)\\) as a series of piece-wise low-order polynomial regressions. Special conditions ensure that \\(\\hat{f}(x)\\) is continuous where splines meet.\nLocal linear regression and locally estimated scatter plot smoothing (LOESS). These methods use weighted linear regression estimated on observations near \\(x_0\\), where \\(x_0\\) is a specific point in the domain where we want to estimate \\(\\hat{f}\\)\n\n\n\n15.4.2 K-Nearest Neighbors\nK-Nearest Neighbors (KNN) average is an estimate of the regression function/conditional mean \\(E\\left[Y|X=x\\right]\\).\nLet \\(N_k(x_0)\\) be the \\(k\\) closest observations to \\(x_0\\). Closeness is often measured with Euclidean distance. Let \\(Ave\\) denote the mean. Then\n\\[\n\\hat{f}(x_0) = Ave\\left(y_i | x_i \\in N_k(x_0)\\right)\n\\tag{15.3}\\]\nis an estimate of the conditional mean of \\(y\\) at point \\(x_0\\).\nConsider an example with simulated data.\n\nset.seed(20201004)\n\nx &lt;- runif(n = 100, min = 0, max = 10)\n\ndata1 &lt;- bind_cols(\n  x = x,\n  y = 10 * sin(x) + x + 20 + rnorm(n = length(x), mean = 0, sd = 2)\n)\n\n\n\nCode\nlibrary(tidymodels)\n\n# create a recipe\nknn_rec &lt;-\n  recipe(formula = y ~ x, data = data1)\n\n# create a knn model specification\nknn_mod &lt;- \n  nearest_neighbor(neighbors = 3) |&gt; \n  set_engine(engine = \"kknn\") |&gt; \n  set_mode(mode = \"regression\")\n\n# create a workflow\nknn_wf &lt;-\n  workflow() |&gt; \n  add_recipe(knn_rec) |&gt; \n  add_model(knn_mod)\n\n# fit the knn model specification on the training data\nknn_fit &lt;- knn_wf |&gt; \n  fit(data = data1)\n\npredictions_grid &lt;- tibble(\n  x = seq(0, 10, 0.1),\n  predict(object = knn_fit, new_data = tibble(x = seq(0, 10, 0.1)))\n)\n\n# visualize the data\nggplot() +\n  geom_point(data = data1, aes(x = x, y = y), alpha = 0.25) +\n  geom_path(data = predictions_grid, aes(x = x, y = .pred), color = \"red\") +\n  labs(\n    title = \"Example 1\",\n    subtitle = \"Training Data with Predictions\"\n  )\n\n\n\n\n\nFigure 15.9: KNN average on simulated data with \\(k = 3\\).\n\n\n\n\n\n\n\n\nKNN Average has a couple of issues.\n\n\\(\\hat{f}(x)\\) is often a discontinuous stepwise function.\nAll observations in \\(N_k(x_0)\\) are given the same weight even though some values are nearer to \\(x_0\\) than other values.\n\n\n\n15.4.3 Local Linear Regression and LOESS\nLocal linear regression fits a separate weighted linear regression model for each \\(x_0\\) on a subset of the data. Only the closest observations are used. LOESS, a specific local linear regression model, is a curve smoothing approach for data visualization.\nThe conditional mean for LOESS is a fairly simple weighted linear regression model.\n\\[\n\\hat{f}(x_0) = \\hat{l}(x_0)\n\\]\nAll we need to do is calculate weights for the weighted linear regression. Span, the \\(k\\) closest observations considered for each linear regression model is the only hyperparameter for LOESS. We will use the tricube weight function using the following procedure:\n\nCalculate the distance from \\(x_0\\) to all observations in the data.\nPick the \\(k\\) closest values.\nDivide all chosen distances by the maximum distance so that all distances are in \\([0, 1]\\).\nApply the tricube function, \\((1 - u ^ 3) ^ 3\\) to the scaled distances.\n\nOnce the weights are calculated, simply plug them into a linear regression model using the \\(k\\) closest observations to \\(x_0\\).\nLet’s look at the math that explains the algorithm. The LOESS conditional mean uses weighted least squares to find \\(\\hat{l}(x_0)\\) that minimizes\n\\[\n\\sum_{x_i \\in N_k(x_o)} \\left[y_i - l(x_i)\\right]^2W\\left(\\frac{|x_0 - x_i|}{\\triangle_{x_o}}\\right)\n\\]\n\n\\(x_i \\in N_k(x_o)\\) is the \\(k\\) closest observations to \\(x_0\\)\n\\(l(x_i)\\) is a weighted linear regression model\n\\(|x_0 - x_i|\\) is a vector of distances\n\\(\\triangle_{x_o}\\) is the maximum distance in the vector \\(|x_0 - x_i|\\)\n\\(W()\\) is a kernel\n\nFigure 15.10 demonstrates LOESS on simulated data.\n\n\nCode\ntricube &lt;- function(u) {\n  \n  (1 - u ^ 3) ^ 3\n  \n}\n\ndistance &lt;- function(x, x0, k) {\n  \n  distances &lt;- abs(x - x0)\n\n  sorted_distances &lt;- distances |&gt;\n    sort()\n  \n  ids &lt;- which(distances &lt;= max(sorted_distances[1:k]))\n  \n  chosen_distances &lt;- distances[ids]\n  \n  scaled_distances &lt;- chosen_distances / max(chosen_distances)\n  \n  list(\n    scaled_distances = scaled_distances,\n    ids = ids\n  )\n  \n}\n\nmy_loess &lt;- function(data, k, x0) {\n  \n  distances &lt;- distance(x = data$x, x0 = x0, k = k)\n  \n  weights &lt;- tricube(u = distances$scaled_distances)\n  \n  fit &lt;- lm(y ~ x, data[distances$ids, ], weights = weights)\n  \n  # return coefficient \n  list(\n    coefficient = coef(fit),\n    y_0 = predict(fit, newdata = tibble(x = x0))\n  )\n  \n}\n\npoints &lt;- tibble(\n  x = c(2, 4, 8),\n  y = map_dbl(x, ~my_loess(data1, k = 10, x0 = .x)$y_0)\n)\nline2 &lt;- tibble(\n  x = seq(0, 4, 0.1),\n  y = map_dbl(x, ~sum(c(1, .x) * my_loess(data1, k = 10, x0 = 2)$coefficient))\n)\nline4 &lt;- tibble(\n  x = seq(2, 6, 0.1),\n  y = map_dbl(x, ~sum(c(1, .x) * my_loess(data1, k = 10, x0 = 4)$coefficient))\n)\nline8 &lt;- tibble(\n  x = seq(6, 10, 0.1),\n  y = map_dbl(x, ~sum(c(1, .x) * my_loess(data1, k = 10, x0 = 8)$coefficient))\n)\n\n\ndata1 |&gt; \n  ggplot(aes(x, y)) + \n  geom_point() +\n  geom_smooth(method = \"loess\", span = 0.1, method.args = list(degree = 1, surface = \"direct\"), se = FALSE) +\n  geom_point(\n    data = points,\n    aes(x, y),\n    color = \"red\"\n  ) +\n  geom_line(data = line2, aes(x, y), linetype = 2) +\n  geom_line(data = line4, aes(x, y), linetype = 2) +\n  geom_line(data = line8, aes(x, y), linetype = 2)\n\n\n\n\n\nFigure 15.10: LOESS fit on simulated data. Red points are predicted values a \\(x_0 \\in \\{2, 4, 8\\}\\). Dashed lines are fitted local linear regression models.\n\n\n\n\n\n\n\n\nIn general, LOESS is very poor for extrapolating beyond the range of \\(x\\) and even in places where data are sparse. In practice, local linear regression models and LOESS are mostly used for visualizing trends in scatter plots using a smoothed conditional mean line.\n\n\n15.4.4 Code\n\ngeom_smooth()\nLOESS is not limited to simple linear regression models. For example, geom_smooth() includes a 2nd-order polynomial for each local regression model by default. Let’s fit a\n\ndata1 |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nFigure 15.11: geom_smooth() demonstrated on simulated data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse geom_smooth() to fit a LOESS model to the cars data with x = speed and y = dist.\nUse geom_smooth() to fit a linear regression model to the cars data with x = speed and y = dist. Make the line red.\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nloess()\nloess() can be used to fit a local linear regression model. It contains many arguments than be used to fine-tune the fit include span, degree, and surface.\n\ndata1_loess &lt;- loess(formula = y ~ x, data = data1, degree = 1)\n\ndata1_loess\n\nCall:\nloess(formula = y ~ x, data = data1, degree = 1)\n\nNumber of Observations: 100 \nEquivalent Number of Parameters: 2.76 \nResidual Standard Error: 5.446 \n\n\nOnce we have a LOESS object, we can use predict() to calculate the conditional mean.\n\npredict(data1_loess, newdata = tibble(x = c(1, 5, 8)))\n\n       1        2        3 \n25.78555 22.38631 30.15598 \n\n\nLOESS is nonparametric and fits a new model for each unique \\(x_0\\). LOESS is memory-based meaning that almost all computation is done when predict() is run. This is computationally expensive and LOESS will not work well with large data sets.\n\n\n\n\n\n\nExercise 4\n\n\n\n\nGenerate the simulated data from above.\nGenerate the plot Figure 15.11.\nUse loess() and predict() to generate a conditional mean for \\(x_0 = 6\\).\nAdd the predicted values to the data visualization using geom_point() and color = \"red\".\n\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n\n\nHiggins, James J. 2004. An Introduction to Modern Nonparametric Statistics. Pacific Grove, CA: Brooks/Cole.\n\n\nScott, David W., and Stephan R. Sain. 2005. “Multidimensional Density Estimation.” In, 24:229–61. Elsevier. https://doi.org/10.1016/S0169-7161(04)24009-3.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Curve Fitting</span>"
    ]
  },
  {
    "objectID": "16_nonparametric-1.html#footnotes",
    "href": "16_nonparametric-1.html#footnotes",
    "title": "15  Nonparametric Curve Fitting",
    "section": "",
    "text": "R.A. Fisher, a key developer of frequentist statistics, developed an early nonparametric test called Fisher’s Exact Test. The use of nonparametric statistics has increased with swelling computing power.↩︎\nA location parameter controls the horizontal shift of a distribution. Location parameters include the mean, median, and mode.↩︎\nA scale parameter controls the spread of a distribution. Scale parameters include variance and standard deviation.↩︎\nMethods based on binomial distribution are also common but we will skip them.↩︎\nRecall your algebra class where, to move the vertex of a parabola in the x-axis some distance \\(h\\), you updated the formula \\(y = x^2\\) to \\(y = (x-h)^2\\). The change to the normal function is analogous. In fact, we can use this idea to shift any kernel.↩︎\n\\(\\text{bandwidth} = 0.9 min\\left(\\hat{\\sigma}, \\frac{IQR}{1.34}\\right)\\)↩︎\nNote that \\(f_x(x)\\) above meant a probability density function. In this case, \\(f(x)\\) is a function that relates \\(y_1, y_2, ..., y_n\\) to \\(x_1, x_2, ..., x_n\\).↩︎",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Nonparametric Curve Fitting</span>"
    ]
  },
  {
    "objectID": "17_nonparametric-2.html",
    "href": "17_nonparametric-2.html",
    "title": "16  Nonparametric Inference",
    "section": "",
    "text": "16.1 Review\nStatistical inference is a procedure for making judgements about population parameters using data and assumptions.\nWe’ve learned about many types of tests for statistical inference. For example, consider the following tests:\nDifferent tests apply in different applied situations. For example, Higgins (2004) identifies at least four approaches to testing hypotheses:\nWe use library(infer) for statistical inference in R.1\nlibrary(infer) has a more powerful workflow for other types of tests built around four verbs:\nThe four verbs can be piped together as we will see in later examples. For now, let’s focus on t_test(), which is a short cut for running t-tests in R.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Nonparametric Inference</span>"
    ]
  },
  {
    "objectID": "17_nonparametric-2.html#sec-review9",
    "href": "17_nonparametric-2.html#sec-review9",
    "title": "16  Nonparametric Inference",
    "section": "",
    "text": "One-sample t-test to determine if a population mean is different from a key value like 0.\nTwo-sample t-test to determine if two population means are different from one another.\nANOVA to determine if multiple population means are different from other means.\nTwo-sample F-tests to determine if two population variances are different from one another.\n\n\n\nUse normal-theory methods like t-tests. Unfortunately, the assumptions of these tests may not be met.\nTransform data and use normal-theory methods like t-tests. Unfortunately, the assumptions of these tests may still not be met.\nUse tests based on other common distributions like the exponential distribution. There may be insufficient data to determine the form or the data may come from a difficult distribution with unknown parameters or characteristics.\nUse nonparametric statistics.\n\n\n\n\nspecify() is used to specify the variable or variables of interest.\nhypothesize() is used to declare a null hypothesis.\ngenerate() is used to generate data from the null hypothesis using parametric and nonparametric approaches.\ncalculate() is used to calculate the distribution of statistics from data created by generate() to form the null hypothesis.\n\n\n\n16.1.1 Example 1\nWe simulate some data with 8 observations from \\(X \\sim N(\\mu = 0, \\sigma = 1)\\) and 8 observations from \\(X \\sim N(\\mu = 1, \\sigma = 1)\\).\n\nset.seed(20230402)\n\nsim_data &lt;- bind_rows(\n  tibble(group = \"1\", x = rnorm(n = 8, mean = 0, sd = 1)),\n  tibble(group = \"2\", x = rnorm(n = 8, mean = 1, sd = 1))\n)\n\n\n1-Sample T-Test\nLet’s start with group 1 and test if \\(\\mu\\) is statistically significantly different than 0:\n\\[H_0: \\mu = 0\\]\n\\[H_a: \\mu \\ne 0\\]\n\nsim_data |&gt;\n  filter(group == \"1\") |&gt;\n  t_test(response = x, mu = 0)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     -1.21     7   0.264 two.sided     -0.421    -1.24    0.398\n\n\nBased on the p-value and t-statistic, we have insufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) level.\n\nNow, let’s work with group 2 using the same hypotheses.\n\nsim_data |&gt;\n  filter(group == \"2\") |&gt;\n  t_test(response = x, mu = 0)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      3.01     7  0.0196 two.sided      0.954    0.205     1.70\n\n\nBased on the p-value and t-statistic, there is sufficient evidence to reject the null hypothesis in favor of the alternative hypothesis at the \\(\\alpha = 0.05\\) significance level. It is likely that the population mean, \\(\\mu\\), for group 2 is different than 0.\n\n\n2-Sample T-Test\nNext, let’s implement a two-sample t-test to test if the population means from group 1 and group 2 are different.\n\\[H_0: \\mu_1 = \\mu_2\\]\n\\[H_a: \\mu_1 \\ne \\mu_2\\]\n\nsim_data |&gt;\n  t_test(\n    formula = x ~ group, \n    order = c(\"1\", \"2\"),\n    alternative = \"two-sided\"\n  )\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     -2.93  13.9  0.0111 two.sided      -1.37    -2.38   -0.367\n\n\nBased on the p-value and t-statistic, there is sufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) significance level. It is likely that the population means are different.\n\n\n\n\n\n\nWarning\n\n\n\nImportant assumptions support the inferences in Example 1. We will unpack these assumptions later.\n\n\n\n\n\n16.1.2 Example 2\nLet’s consider a real example from Ashraf, Karlan, and Yin (2006). They created a commitment savings product for a Philippine bank called SEED (Save, Earn, Enjoy Deposits). They evaluated the effect of the commitment savings product on the level of household savings (among other outcomes) using a randomized control trial (RCT). The authors shared their data on the Harvard Dataverse. J-PAL offers a brief summary.\nseed.csv contains simplified data from their analysis data.\n\nseed &lt;- read_csv(here(\"data\", \"seed.csv\"))\n\nglimpse(seed)\n\nRows: 1,777\nColumns: 5\n$ group2    &lt;chr&gt; \"treatment\", \"control\", \"treatment\", \"control\", \"control\", \"…\n$ group3    &lt;chr&gt; \"treatment\", \"marketing\", \"treatment\", \"marketing\", \"control…\n$ totbal    &lt;dbl&gt; 121.32, 125.19, 529.39, 101.80, 118.04, 532.69, 598.41, 548.…\n$ newtotbal &lt;dbl&gt; 4.62, 0.00, 4139.34, 105.94, 0.00, 362.74, 2323.50, 373.13, …\n$ balchange &lt;dbl&gt; -116.70, -125.19, 3609.95, 4.14, -118.04, -169.95, 1725.09, …\n\n\n\n1-Sample T-Test\nLet’s start with a simple 1-sample T-Test to see if the population mean for change in balance after one year of the SEED accounts is statistically significantly different than zero. Our null and alternative hypotheses are\n\\[H_0: \\mu_{\\text{balance change}} = 0\\]\n\\[H_a: \\mu_{\\text{balance change}} \\ne 0\\]\n\nseed |&gt;\n  t_test(response = balchange, mu = 0)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      2.72  1776 0.00652 two.sided       293.     81.9     503.\n\n\nWe reject the null hypothesis in favor of the alternative hypothesis. We conclude that the mean change in bank account balances is different than zero.\nWe make inferences with data and assumptions. Did our assumptions hold in this case? The assumptions for a 1-sample T-Test are\n\n\\(x_1, x_2, ..., x_n\\) is an IID random sample from a population.\nThe population is normally distributed or the sample size is large enough for the Central Limit Theorem to apply.\n\nBoth assumptions are reasonable. We can assume that the sample is identically distributed because the sample was drawn from one population: clients of the Green Bank of Caraga. It also seems reasonable to assume that a household that received the commitment savings product did not somehow influence a control household’s level of savings. Thus the IID assumption is reasonable.\nThe study assessed more than 1,000 households, suggesting that we need not be concerned with the distribution’s normality.\nThe average doesn’t represent the experience of the average person. Maybe we want to run a statistical test on the median. Unfortunately, normal-theory statistics doesn’t give us a tool for this. This could be a big challenge based on key percentiles for the change in account balance:\n\nseed |&gt;\n  summarize(\n    q = seq(0, 1, 0.1), \n    balchange = quantile(balchange, probs = q)\n  )\n\n# A tibble: 11 × 2\n       q balchange\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1   0     -2169. \n 2   0.1    -526. \n 3   0.2    -280  \n 4   0.3    -191. \n 5   0.4    -117. \n 6   0.5     -63.0\n 7   0.6       0  \n 8   0.7       0  \n 9   0.8      20.9\n10   0.9     323. \n11   1    114001. \n\n\n\n\n2-Sample T-Test\nNext, let’s run a 2-sample t-test to compare the means from the control and treatment group.2\n\\[H_0: \\mu_{control} = \\mu_{treatment}\\]\n\\[H_a: \\mu_{control} \\ne \\mu_{treatment}\\]\n\nseed |&gt;\n  t_test(\n    formula = balchange ~ group2, \n    order = c(\"treatment\", \"control\"),\n    alternative = \"two-sided\"\n  )\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      1.56 1064.   0.118 two.sided       350.    -89.0     788.\n\n\nThere is insufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) significance level. The mean change in account balance is not statistically significantly different for the treatment and control groups.\nWe make inferences with data and assumptions. Did our assumptions hold in this case? The assumptions for a 2-sample T-Test are\n\n\\(x_1, x_2, ..., x_n\\) is an IID random sample from population 1. \\(y_1, y_2, ..., y_n\\) is an IID random sample from a population 2.\nThe populations are normally distributed or the sample size is large enough for the Central Limit Theorem to apply.\nThe populations have equal variances (possibly unknown).\n\nAssumptions 1 and 2 hold, but we don’t know if assumption 3 holds. There are statistical tests for the equivalence of two population variances. In this case, we don’t even need the tests. The two samples have dramatically different variances.\n\nseed |&gt;\n  group_by(group2) |&gt;\n  summarize(var(balchange))\n\n# A tibble: 2 × 2\n  group2    `var(balchange)`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 control           5514806.\n2 treatment        37127545.\n\n\nMany applications of 2-sample t-tests incorrectly ignore assumption 3 but situations where we want to make inferences about the difference in means when population variances differ is widespread. This is known as the Behrens-Fisher problem.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nSimulate two-sample data with 8 observations each like in Section 16.1.1 using the log-normal distribution with rlnorm().\nFirst, run a 2-sample t-test where the simulated data have very similar means.\nSecond, run a 2-sample t-test where the simulated data have very different means.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Nonparametric Inference</span>"
    ]
  },
  {
    "objectID": "17_nonparametric-2.html#permutation-test",
    "href": "17_nonparametric-2.html#permutation-test",
    "title": "16  Nonparametric Inference",
    "section": "16.2 Permutation Test",
    "text": "16.2 Permutation Test\n\n\n\n\n\n\nCombination\n\n\n\nA combination is the exhaustive reshuffling of \\(n\\) objects into two groups without regard for order.\nLet \\(k\\) be the number of objects in the first group. There are \\(C(n, k) = {n \\choose k} = \\frac{n!}{(n - k)!k!}\\) combinations.\nConsider the objects A, B, C, D. Suppose we want to combine them into two groups of equal size. There are \\(C(4, 2) = {4 \\choose 2} = 6\\) combinations.\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nA, B\nC, D\n\n\nA, C\nB, D\n\n\nA, D\nB, C\n\n\nB, C\nA, D\n\n\nB, D\nA, C\n\n\nC, D\nA, B\n\n\n\n\n\nThe permutation test is typically used to compare parameters from two or more populations. The algorithm for a permutation test is as follows:\n\nDetermine some parameter of interest for two or more populations (like the mean or median). The groups can be treatment and control groups.\nCalculate the difference in statistics for each group. This is the test statistic or observed statistic.\nDetermine every possible combination of assignment of each observation to the groups (e.g. treatment and control group).\nFor each combination:\n\nCalculate the difference in the determined statistic (e.g. mean or median) between the two groups.\nStore that difference in a vector. This vector is called the permutation distribution.\n\nDetermine the p-value by comparing the test statistic to the permutation distribution.\n\nFor a gentle (and alpaca-themed) visualized introduction to this content, see this blog.\n\n\n\n\n\n\nPermutation Distribution\n\n\n\nThe permutation distribution is a vector of statistics calculated on combinations3 of the data.\n\n\n\n\n\n\n\n\nPermutation principle\n\n\n\nThe permutation principle states that the permutation distribution is an appropriate reference distribution and can be used to determine p-values and statistical significance. (Higgins 2004)4\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the data used in a permutation test are selected randomly or come from a designed experiment, then we can make inferences about the population. Otherwise, we can only make inferences about the sample.\n\n\n\n16.2.1 Example 1 (Revisited)\nLet’s revisit Example 1 using permutation methods. In general we use library(infer) to implement permutations tests. Here we implement the method from scratch for demonstration.\n\n2-Sample Difference of Means\n\n#' Implement a permutation test for means\n#'\n#' @param x A numeric vector\n#' @param group_size The group size of group 1\n#'\n#' @return A permutation distribution \n#' \npermute_means &lt;- function(x, group_size = 8) {\n  \n  # combn() returns a matrix with group_size rows\n  # and a column for every combination of integers associated with x\n  indices &lt;- combn(x = 1:length(x), m = group_size)\n  \n  index &lt;- 1:ncol(indices)\n\n  # 1. extract a vector of indices stored as a column in the indices matrix\n  # 2. extract only the observations in x included in the vector of indices\n  # 3. take the mean of those observations\n  dist &lt;- map_dbl(\n    .x = index, \n    .f = ~ mean(x[indices[, .x]]) - mean(x[-indices[, .x]])\n  )\n  \n  return(dist)\n  \n}\n\npermute_means() permutes the indices of the values in x to permute the values of x. In this case, with group_size = 8 the first column has indices 1, 2, 3, 4, 5, 6, 7, 8 and the last column has indices 9, 10, 11, 12, 13, 14, 15, 16. The remaining columns express every possible permutation of the values 1:16 into two groups of size 8.\n\n# create the permutation distribution\npermutation_dist &lt;- permute_means(sim_data$x, 8)\n\n# visualize the permutation distribution\ntibble(x = permutation_dist) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# calculate the test statistic\nD &lt;- mean(sim_data[sim_data$group == \"1\", ]$x) -\n  mean(sim_data[sim_data$group == \"2\", ]$x)\n\n# calculate a p-value\nmean(D &gt; permutation_dist)\n\n[1] 0.006915307\n\n\nThe p-value is very small. There is sufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) level. It is likely that the population means of the two samples are different.\n\n\n2-Sample Difference of Medians\n\n#' Implement a permutation test for medians\n#'\n#' @param x A numeric vector\n#' @param group_size The group size of group 1\n#'\n#' @return A permutation distribution \n#' \npermute_medians &lt;- function(x, group_size = 8) {\n  \n  indices &lt;- combn(1:length(x), group_size)\n  \n  index &lt;- 1:ncol(indices)\n\n  dist &lt;- map_dbl(\n    .x = index, \n    .f = ~ median(x[indices[, .x]]) - median(x[-indices[, .x]])\n  )\n  \n  return(dist)\n  \n}\n\n# calculate the permutation distribution\npermutation_dist &lt;- permute_medians(sim_data$x, 8)\n\n# visualize the permutation distribution\ntibble(x = permutation_dist) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# calculate the test statistic\nD &lt;- median(sim_data[sim_data$group == \"1\", ]$x) -\n  median(sim_data[sim_data$group == \"2\", ]$x)\n\n# calculate the p-value\nmean(D &gt; permutation_dist)\n\n[1] 0.02206682\n\n\nThe p-value is small. There is sufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) level. It is likely that the population medians (location parameters) of the two samples are different.\n\n\n\n16.2.2 Example 2 (Revisited)\nLet \\(n\\) be the number of observations in the data and \\(k\\) be the number of observations in the larger group. Then there are \\({n \\choose k} = \\frac{n!}{k!(n - k)!}\\) combinations of the data. This can be computationally very expensive.\nConsider the data from example 2, which contains 1,777 observations. That’s 935 observations in the control group and 842 observations in the treatment group.\n\nchoose(nrow(seed), sum(seed$group2 == \"control\"))\n\n[1] Inf\n\n\nFortunately, sampling permutations can give a decent approximation of the full permutation test.\n\n2-Sample Difference in Means\nWe will now use library(infer) to calculate the observed statistic, generate the null distribution, and calculate the p-value.\nRecall our null and alternative hypotheses.\n\\[H_0: \\mu_{control} = \\mu_{treatment}\\]\n\\[H_a: \\mu_{control} \\ne \\mu_{treatment}\\]\n\n# calculate the test statistic\npoint_estimate &lt;- seed |&gt;\n  specify(balchange ~ group2) |&gt;\n  calculate(stat = \"diff in means\", order = c(\"control\", \"treatment\"))\n\n# generate the permutation distribution\nperm_dist &lt;- seed |&gt;\n  specify(balchange ~ group2) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 5000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", order = c(\"control\", \"treatment\"))\n\n# visualize the p-value\nperm_dist |&gt;\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n\n\n\n\n\n\n# calculate the p-value\nperm_dist |&gt;\n  get_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.109\n\n\nOnce again, there is insufficient evidence to reject the null hypothesis at the \\(\\alpha = 0.05\\) significance level. The mean change in account balance is not statistically significantly different for the treatment and control groups.\n\n\n2-Sample Difference in Medians\nUsing normal-theory statistics, we don’t have clear strategies for statistical tests about the median. This is simple with nonparametric statistics.\n\n# calculate the test statistic\npoint_estimate &lt;- seed |&gt;\n  specify(balchange ~ group2) |&gt;\n  calculate(stat = \"diff in medians\", order = c(\"control\", \"treatment\"))\n\n# generate the permutation distribution\nperm_dist &lt;- seed |&gt;\n  specify(balchange ~ group2) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 5000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in medians\", order = c(\"control\", \"treatment\"))\n\n# visualize the p-value\nperm_dist |&gt;\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n\n\n\n\n\n\n# calculate the p-value\nperm_dist |&gt;\n  get_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0012\n\n\nWe have sufficient evidence to reject the null hypothesis in favor if the alternative hypothesis at the \\(\\alpha = 0.05\\) level. It is likely that the population median for group 1 is different than the population median for group 2.\nInterestingly, the median change for the treatment group is slightly less negative than the control group.\n\nseed |&gt;\n  group_by(group2) |&gt;\n  summarize(median = median(balchange))\n\n# A tibble: 2 × 2\n  group2    median\n  &lt;chr&gt;      &lt;dbl&gt;\n1 control    -84.6\n2 treatment  -38.1\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nUse the same log-normal data simulated in the earlier exercise.\nFirst, run a 2-sample permutation test for the difference in means using very similar population means.\nSecond, run a 2-sample permutation test for the difference in means using very different population means.\n\n\n\n\n\n\n16.2.3 Other techniques\nStatistical tests are often selected because of their coverage probabilities and power.\n\n\n\n\n\n\nCoverage probability\n\n\n\nCoverage probability is how close the stated level of confidence is to the actual level of confidence.5\nFor example, a given statistical test, sampling procedure, and population may generate confidence intervals that contain the true parameter of interest 93% of the time for a 95% confidence interval.\n\n\n\n\n\n\n\n\nPower\n\n\n\nPower is the probability of rejecting \\(H_0\\) if it is false.\n\n\nWe want tests with correct coverage probabilities and high statistical power. If the assumptions are met then parametric tests often have more power than nonparametric tests. The power advantage can often flip when assumptions start failing.\nSome nonparametric tests, which are often variations on the permutation test, have more power than the permutation test. Often, the Wilcoxon Rank Sum test, which is uses ranks instead of values, has higher power. Another test to explore is the Kruskal-Wallis test.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Nonparametric Inference</span>"
    ]
  },
  {
    "objectID": "17_nonparametric-2.html#nonparametric-bootstrap",
    "href": "17_nonparametric-2.html#nonparametric-bootstrap",
    "title": "16  Nonparametric Inference",
    "section": "16.3 Nonparametric Bootstrap",
    "text": "16.3 Nonparametric Bootstrap\nWe will investigate one more technique for density estimation focused on estimating sampling distributions.\n\n\n\n\n\n\nNonparametric Bootstrap Sample\n\n\n\nThe nonparametric bootstrap sample is a random sample with replacement.\n\n\n\n\n\n\n\n\nSampling Distribution\n\n\n\nA sampling distribution of a statistic is the distribution we would observe if a statistic was calculated on many different samples from the same population distribution.\n\n\nSampling distributions are fundamental to statistics. We frequently use the normal distribution or Student’s t distribution to make inferences about population means and population regression coefficients.\n\n\n\n\n\n\nConfidence interval\n\n\n\nGiven a sampling procedure and a population, the target parameter is in the confidence interval X% of the time over many random samples.6\n\n\nThe idea of repeated sampling is fundamental to sampling distributions, confidence intervals, and statistical inference in frequentist statistics. The nonparametric bootstrap leverages the idea of repeated sampling to allow for statistical inference under a minimal number of assumptions. This makes it possible to perform inference:\n\nWhen a known sampling distribution for a statistic isn’t available\nWhen the sample is too small to apply the central limit theorem\n\n\n16.3.1 Basic Bootstrap\nSuppose we are interested in an unknown parameter \\(\\theta\\).\n\nCompute \\(\\hat\\theta\\) from the original data.\nTake \\(B\\) bootstrap samples of size \\(n\\) from the original data.\nCalculate \\(\\hat\\theta_{b,i}\\) on each bootstrap sample.\n\nThe vector \\(\\hat{\\vec\\theta_b}\\) is the bootstrap distribution and it has multiple uses. Importantly, it is a nonparametric estimate of the sampling distribution of \\(\\hat\\theta\\).\n\n\n16.3.2 Example\n\nB &lt;- 5000\nb &lt;- vector(mode = \"numeric\", length = B)\n\nfor (i in seq_along(b)) {\n  \n  x_boot &lt;- sample(seed$balchange, replace = TRUE)\n  \n  b[i] &lt;- median(x_boot)\n  \n}\n\ntibble(b) |&gt;\n  ggplot(aes(b)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can use this permutation distribution to calculate bias (Section 16.3.3), standard errors (Section 16.3.4), and confidence intervals (Section 16.3.6).\n\n\n16.3.3 Bias\nSometimes estimators of population parameters are biased.7 Bootstrapping can be used to estimate the bias in an estimator.\nLet \\(\\theta\\) be the parameter of interest and \\(\\hat\\theta\\) be an estimator. \\(\\hat\\theta\\) is unbiased if \\(E[\\hat\\theta] = \\theta\\).\nAccordingly, \\(bias(\\hat\\theta) = E[\\hat\\theta] - \\theta\\).\n\\(E[\\hat\\theta]\\) is just the sample mean of the sampling distribution.\n\\[\n\\hat{bias}(\\hat\\theta) = \\bar{\\hat{\\theta^*}} - \\hat\\theta\n\\]\nwhere \\(\\bar{\\hat\\theta^*} = \\frac{1}{b} \\sum_{b = 1}^B \\hat\\theta_b\\).\nBasically, to estimate bias we subtract our estimate from the mean of the sampling distribution, which is our bootstrap distribution. This estimate of bias can be used to correct for bias.\nIf \\(\\frac{|bias|}{se} \\le 0.25\\), then it is likely unnecessary to correct for bias (Rizzo 2008). We will next see how to estimate \\(se\\).\n\n\n16.3.4 Standard errors\nSuppose we want to estimate the standard error of an estimator \\(\\hat\\theta\\). To do this, simply take the standard deviation of the bootstrap replicates.\n\\[\n\\hat{se}(\\hat\\theta^*) = \\sqrt{\\frac{1}{B-1} \\sum_{b = 1}^B \\left(\\hat\\theta_b - \\bar{\\hat\\theta^*}\\right)^2}\n\\]\nwhere \\(\\bar{\\hat\\theta^*} = \\frac{1}{b} \\sum_{b = 1}^B \\hat\\theta_b\\).\nHistorically, the bootstrap was used to estimate the standard errors of estimates. The jackknife, another resampling method, can be used to estimate bias and standard errors. We won’t focus on this. Instead, we will estimate confidence intervals (Chernick and LaBudde 2011).\n\n\n16.3.5 Example\nFirst, let’s repeat the SEED example from above using the boot() function. boot() expects a custom function with the argument x for the data and the argument i for the index for statistics. We useR = 5000 for B &lt;- 5000 in the earlier example.\n\nlibrary(boot)\n\nseed_bootstrap &lt;- boot(\n  data = seed$balchange, \n  statistic = function(x, i) median(x[i]), \n  R = 5000\n)\n\nseed_bootstrap\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = seed$balchange, statistic = function(x, i) median(x[i]), \n    R = 5000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1*   -63.04 -0.9602388    7.817336\n\n\nlibrary(boot) and boot() are useful when we want to see bias. They are also useful when we are interested in statistics not included in library(infer). Otherwise, we will use library(infer).\n\nbootstrap_distribution &lt;- seed |&gt;\n  specify(response = balchange) |&gt;\n  hypothesize(null = \"point\", med = 0) |&gt;\n  generate(reps = 5000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"median\")\n\nbootstrap_distribution |&gt;\n  visualize()\n\n\n\n\n\n\n\n\n\n\n16.3.6 Confidence intervals\nUnder certain assumptions it is simple to construct confidence intervals for parameters like the population mean.\nThere are several ways to calculate confidence intervals using bootstrapping:\n\nPercentile method\nBCA\nt-pivot\n\nApproach three doesn’t always work but probably has the most desirable properties. Approach two is a suitable backup. Approach one often has the worst properties, but is simple and will be sufficient in many applications.\n\nPercentile Method\nFor each bootstrap sample indexed by \\(b = 1, ..., B\\),\n\nCompute \\(\\hat\\theta\\) from the original data.\nTake \\(B\\) bootstrap samples of size \\(n\\) from the data.\nCompute \\(\\hat\\theta_b\\) for each bootstrap sample.\nFor \\(\\alpha = 0.05\\), find the 2.5th and 97.th percentiles of \\(\\hat{\\vec\\theta}\\).\n\n\n\n\n16.3.7 Example\n\npoint_estimate &lt;- seed |&gt;\n  specify(response = balchange) |&gt;\n  calculate(stat = \"median\")\n\nbootstrap_distribution |&gt;\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\n\n\n\n\n\n\nbootstrap_distribution |&gt;\n  get_p_value(obs_stat = point_estimate, direction = \"two-sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\nbootstrap_distribution |&gt;\n  get_confidence_interval(\n    point_estimate = point_estimate,\n    level = 0.95,\n    type = \"se\"\n  )\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    -78.2    -47.8\n\n\nThe confidence intervals are comparable. We’re good to go.\nStudents can take multiple statistics classes and never learn how to calculate a 95% confidence interval for a median. It’s simple with bootstrapping.\nIn this case, it’s striking that the balance for the median SEED account declined over the period and there is sufficient evidence that the median is statistically significantly different than $0. In fact, it’s negative!",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Nonparametric Inference</span>"
    ]
  },
  {
    "objectID": "17_nonparametric-2.html#more-methods",
    "href": "17_nonparametric-2.html#more-methods",
    "title": "16  Nonparametric Inference",
    "section": "16.4 More Methods",
    "text": "16.4 More Methods\nM-Estimation and quantile regression are two nonparametric techniques that we will not discuss that are common public policy analysis.\nM-Estimation is useful for estimating conditional means in the presence of outliers. Quantile regression is useful for estimating conditional quantiles like the conditional median.\n\n\n\n\n\n\nExercise 3\n\n\n\n\nStart with the seed data. Drop observations where group3 == \"marketing\".\nUse library(infer) and bootstrapping to test if the median is statistically significantly different than zero.\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nlibrary(infer) allows for tests about the median but not other percentiles.\n\nStart with the full seed data.\nUse boot() and boot.ci() to test if the 75th percentile for balchange is different than 0.\n\n\n\n\n\n\n\nAshraf, N., D. Karlan, and W. Yin. 2006. “Tying Odysseus to the Mast: Evidence From a Commitment Savings Product in the Philippines.” The Quarterly Journal of Economics 121 (2): 635–72. https://doi.org/10.1162/qjec.2006.121.2.635.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001. “Interval Estimation for a Binomial Proportion.” Statistical Science 16 (2). https://doi.org/10.1214/ss/1009213286.\n\n\nChernick, Michael R., and Robert A. LaBudde. 2011. An Introduction to Bootstrap Methods with Applications to r. Hoboken, N.J: Wiley.\n\n\nHiggins, James J. 2004. An Introduction to Modern Nonparametric Statistics. Pacific Grove, CA: Brooks/Cole.\n\n\nRizzo, Maria L. 2008. Statistical Computing with r. Chapman & Hall/CRC Computer Science and Data Analysis Series. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Nonparametric Inference</span>"
    ]
  },
  {
    "objectID": "17_nonparametric-2.html#footnotes",
    "href": "17_nonparametric-2.html#footnotes",
    "title": "16  Nonparametric Inference",
    "section": "",
    "text": "Base R has excellent tools for statistical inference. library(infer) is useful because it plays well with library(tidyverse). We can just use t_test() for a one-sample t-test.↩︎\nThe RCT actually contains a treatment group, a control group that received marketing materials, and a proper control group. In this case, I’ve lumped the marketing group in with the treatment group.↩︎\nA permutation is the exhaustive reshuffling of \\(n\\) objects into \\(m\\) groups with regard for order. We should probably call the permutation test the combination test. Combinations are the exhaustive reshuffling of \\(n\\) objects into \\(m\\) groups without regard for order. Using permutations generates the same results as combinations but takes much longer because many of the configurations are duplicated.↩︎\nThe permutation test is considered an exact test because the null model will never test more as significant more than the \\(\\alpha\\) level of the test.↩︎\nBrown, Cai, and DasGupta (2001) explores the coverage probabilities of different techniques for interval estimation for a binomial proportion. Reading this paper made me question everything I knew about confidence intervals.↩︎\nConfidence intervals are about a process. There is a distinct difference between confidence and probabilities. It is incorrect to say, “the true parameter is in the interval X% of the time.” This is what a credible interval is and we will learn more about credible intervals when we learn Bayesian statistics.↩︎\nRecall that \\(\\bar{X}\\) is an unbiased estimator of the population mean \\(\\mu\\). On the other hand, \\(\\hat\\sigma^2\\) is a biased estimator of variance \\(\\sigma^2\\).↩︎",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Nonparametric Inference</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html",
    "href": "19_predictive-modeling-motivation.html",
    "title": "17  Predictive Modeling Motivation",
    "section": "",
    "text": "17.1 Problem Structure\nThere are many policy applications where it is useful to make accurate predictions on “unseen” data. Unseen data means any data not used to develop a model for predictions.\nThe broad process of developing models to make predictions of a pre-defined variable is predictive modeling or supervised machine learning. The narrower process of developing a specific model with data is called fitting a model, learning a model, or estimating a model. The pre-defined variable the model is intended to predict is the outcome variable and the remaining variables used to predict the outcome variable are predictors.1\nIn most applications we have modeling data that include the outcome of interest and implementation data that do not include the outcome of interest. Our objective is to fit, or estimate, some \\(\\hat{f}(\\vec{x})\\) using the modeling data to predict the missing outcome in the implementation data.\nOur fitted models never perfectly predict the outcome, \\(y\\). Therefore, we end up with \\(y = \\hat{f}(\\vec{x}) + \\epsilon\\). A major effort in predictive modeling is improving \\(\\hat{f}(\\vec{x})\\) to reduce \\(\\epsilon\\).",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#problem-structure",
    "href": "19_predictive-modeling-motivation.html#problem-structure",
    "title": "17  Predictive Modeling Motivation",
    "section": "",
    "text": "Figure 17.1: Modeling Data and Implementation Data for predicting the presence of lead in homes\n\n\n\n\n\n\n\n\n\nModeling Data\n\n\n\nModeling data contain the outcome variable of interest, and predictors for fitting models to predict the outcome variable of interest.\nModeling data are often historical data or high-cost data.\n\n\n\n\n\n\n\n\nImplementation Data\n\n\n\nImplementation data contain predictors for predicting the outcome of interest but don’t necessarily contain the outcome of interest.\nThe implementation data needs to contain all of the predictors used in the modeling data, and the implementation data can’t be too different from the modeling data.\n\n\n\n\n\n\n\n\nFigure 17.2: Using modeling data to fit a model and make predctions on the implementation data",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#example-marbles",
    "href": "19_predictive-modeling-motivation.html#example-marbles",
    "title": "17  Predictive Modeling Motivation",
    "section": "17.2 Example: Marbles",
    "text": "17.2 Example: Marbles\n\n17.2.1 Implementation Data\n\n\n\nSize\nColor\n\n\n\n\nSmall\n?\n\n\nBig\n?\n\n\nBig\n?\n\n\n\n\n\n17.2.2 Modeling Data\n\n\n\nSize\nColor\n\n\n\n\nSmall\nRed\n\n\nSmall\nRed\n\n\nSmall\nRed\n\n\nSmall\nRed\n\n\nSmall\nBlue\n\n\nBig\nRed\n\n\nBig\nBlue\n\n\nBig\nBlue\n\n\nBig\nBlue\n\n\nBig\nBlue\n\n\n\n\n\n17.2.3 Predictive Model\n\nChallengeSolution\n\n\nHow can we predict color in the implementation data?\n\n\nA simple decision tree works well here! Predictive modeling! Machine learning!\n\nIf a marble is small, then predict Red\nIf a marble is big, then predict Blue\n\nMost predictive modeling is repeating this type of learning in higher-dimensional applications where it is tougher to see the model with a glance.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#applications",
    "href": "19_predictive-modeling-motivation.html#applications",
    "title": "17  Predictive Modeling Motivation",
    "section": "17.3 Applications",
    "text": "17.3 Applications\nPredictive modeling has many applications in public policy. Predictive models can be used to improve the allocation of scarce resources, scale survey responses or hand-labelled data sets, and forecast or nowcast.\n\n17.3.1 Targeting Interventions\n\n\n\n\n\n\nTargeting Interventions\n\n\n\nUsing predictive modeling to target the person, household, or place most in need of a policy intervention. Effective targeting can maximize the impact of a limited intervention.\n\n\n\n\n17.3.2 Amplified Asking\n\n\n\n\n\n\nAmplified Asking\n\n\n\nAmplified asking (Salganik 2018) uses predictive modeling trained on a small amount of survey data from one data source and applies it to a larger data source to produce estimates at a scale or geographic granularity not possible with either data set on its own.\n\n\n\n\n17.3.3 Scaled Labeling\n\n\n\n\n\n\nScaled Labeling\n\n\n\nThe process of labeling a subset of data and then using predictive models to label the full data.\nThis is most effective when labeling the data is time consuming or expensive.\n\n\n\n\n17.3.4 Forecasting and Nowcasting\n\n\n\n\n\n\nForecasting and Nowcasting\n\n\n\nA forecast attempts to predict the most-likely future. A nowcast attempts to predict the most-likely present in situations where information is collected and reported with a lag.\n\n\n\n\n17.3.5 Imputation\nBroadly, imputation is the task of using models to assign, or impute, missing values in some dataset. Broadly, imputation can be used for a variety of applications including trying to reduce the consequences of omitted variable bias, generating synthetic data to minimize disclosure risks, and data fusion. Amplified asking and scaled labeling are also examples of imputation tasks.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#case-studies",
    "href": "19_predictive-modeling-motivation.html#case-studies",
    "title": "17  Predictive Modeling Motivation",
    "section": "17.4 Case Studies",
    "text": "17.4 Case Studies\n\n17.4.1 Preventing Lead Poisoning\nConsider “Predictive Modeling for Public Health: Preventing Childhood Lead Poisoning” by Potash et al. (2015).\nThe authors use historical data in Chicago from home lead inspectors and observable characteristics about homes and neighborhoods to predict if homes contain lead.\n\nMotivation: Lead poisoning is terrible for children, but Chicago has limited lead inspectors.\nImplementation data: Observed characteristics about homes and neighborhoods in Chicago.\nModeling data: Historical characteristics about homes and neighborhoods in Chicago and results from home lead inspections.\nObjective: Predict the probability of a home containing lead. 1. Prioritize sending inspectors to the homes of the most at-risk children before the child suffers from elevated blood lead level (BLL). 2. Create a risk score for children that can be used by parents and health providers.\nTools: Cross validation and regularized logistic regression, linear SVC, and Random Forests.\nResults: The precision of the model was two to five times better than the baseline scenario of randomly sending inspectors. The advantage faded over time and in situations where more inspectors were available.\n\n\n\n17.4.2 Measuring Wealth in Rwanda\nConsider “Calling for Better Measurement: Estimating an Individual’s Wealth and Well-Being from Mobile Phone Transaction Records” by Blumenstock (n.d.) and “Predicting poverty and wealth from mobile phone metadata” by Blumenstock, Cadamuro, and On (2015).\n\nMotivation: Measuring wealth and poverty in developing nations is challenging and expensive.\nImplementation data: 2005-2009 phone metadata from about 1.5 million phone customers in Rwanda.\nModeling data: A follow-up phone stratified survey of 856 phone customers with questions about asset ownership, home ownership, and basic welfare indicators linked to the phone metadata.\nObjective: Use the phone metadata to accurately predict variables about asset ownership, home ownership, and basic welfare indicators. One of their outcomes variables is the first principal component of asset variables.\nTools: 5-fold cross-validation, elastic net regression and regularized logistic regression, deterministic finite automation.\nResults: The results in the first paper were poor, but follow-up work resulted in models with AUC as high as 0.88 and \\(R^2\\) as high as 0.46.\n\n\n\n17.4.3 Collecting Land-Use Reform Data\nConsider “How Urban Piloted Data Science Techniques to Collect Land-Use Reform Data” by Zheng (2020).\n\nMotivation: Land-use reform and zoning are poorly understood at the national level. The Urban Institute set out to create a data set from newspaper articles that reflects land-use reforms.\nImplementation data: About 76,000 newspaper articles\nModeling data: 568 hand-labelled newspaper articles\nObjective: Label the nature of zoning reforms on roughly 76,000 newspaper articles.\nTools: Hand coding, natural language processing, cross-validation, and random forests\nResults: The results were not accurate enough to treat as a data set but could be used to target subsequent hand coding and investigation.\n\n\n\n17.4.4 Google Flu Trends\nConsider the high-profile Google Flu Trends model by Ginsberg et al. (2009).\n\nMotivation: The CDC tracks flu-like illnesses and releases data on about a two-week lag. Understanding flu-like illnesses on a one-day lag can improve public health responses.\nImplementation data: Real-time counts of Google search queries.\nModeling data: 2003-2008 CDC data about flu-like illnesses and counts for 50 million of the most common Google search queries in each state.\nObjective: Predict regional-level flu outbreaks on a one-day lag using Google searches to make predictions.\nTools: Logistic regression and the top 45 influenza related illness query terms.\nResults: The in-sample results were very promising. Unfortunately, the model performed poorly when implemented. Google implemented Google Flu Trends but later discontinued the model. The two major issues were:\n\nDrift: The model decayed over time because of changes to search\nAlgorithmic confounding: The model overestimated the 2009 flu season because of fear-induced search traffic during the Swine flu outbreak\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nWhat other policy applications do you know for making accurate predictions on unseen data?\n\n\n\n\n\n\n\nBlumenstock, Joshua. n.d. “Calling for Better Measurement: Estimating an Individual’s Wealth and Well-Being from Mobile Phone Transaction Records.” Center for Effective Global Action. https://escholarship.org/uc/item/8zs63942.\n\n\nBlumenstock, Joshua, Gabriel Cadamuro, and Robert On. 2015. “Predicting Poverty and Wealth from Mobile Phone Metadata.” Science 350 (6264): 1073–76. https://doi.org/10.1126/science.aac4420.\n\n\nGinsberg, Jeremy, Matthew H. Mohebbi, Rajan S. Patel, Lynnette Brammer, Mark S. Smolinski, and Larry Brilliant. 2009. “Detecting Influenza Epidemics Using Search Engine Query Data.” Nature 457 (7232): 1012–14. https://doi.org/10.1038/nature07634.\n\n\nPotash, Eric, Joe Brew, Alexander Loewi, Subhabrata Majumdar, Andrew Reece, Joe Walsh, Eric Rozier, Emile Jorgenson, Raed Mansour, and Rayid Ghani. 2015. “KDD ’15: The 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.” In, 2039–47. Sydney NSW Australia: ACM. https://doi.org/10.1145/2783258.2788629.\n\n\nSalganik, Matthew J. 2018. Bit by Bit: Social Research in the Digital Age. Princeton: Princeton University Press.\n\n\nZheng, Vivian. 2020. “How Urban Piloted Data Science Techniques to Collect Land-Use Reform Data.” https://urban-institute.medium.com/how-urban-piloted-data-science-techniques-to-collect-land-use-reform-data-475409903b88.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "19_predictive-modeling-motivation.html#footnotes",
    "href": "19_predictive-modeling-motivation.html#footnotes",
    "title": "17  Predictive Modeling Motivation",
    "section": "",
    "text": "The are many names for outcome variables including target and dependent variable. There are many names for predictors including features and independent variables.↩︎",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Predictive Modeling Motivation</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html",
    "href": "20_predictive-modeling-concepts.html",
    "title": "18  Predictive Modeling Concepts and Regression",
    "section": "",
    "text": "18.1 Regression\nPredictive modeling is split into two approaches: regression and classification.\nThis chapter will focus on regression. A later chapter focuses on classification.\nWith regression, our general goal is to fit \\(\\hat{f}(\\vec{x})\\) using modeling data that will minimize predictive errors on unseen data. There are many algorithms for fitting \\(\\hat{f}(\\vec{x})\\). For regression, most of these algorithms fit conditional means; however, some use conditional quantiles like the conditional median.\nFamilies of predictive models:",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictive Modeling Concepts and Regression</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#regression",
    "href": "20_predictive-modeling-concepts.html#regression",
    "title": "18  Predictive Modeling Concepts and Regression",
    "section": "",
    "text": "Regression\n\n\n\nPredictive modeling with a numeric outcome.\nNote: Regression and ordinary least squares linear regression (which we often informally refer to simply as “linear regression”) are different ideas. We will use many algorithms for regression applications that are not linear regression.\n\n\n\n\n\n\n\n\nClassification\n\n\n\nPredictive modeling with a categorical outcome. The output of these models can be predicted classes of a categorical variable or predicted probabilities (e.g. 0.75 for “A” and 0.25 for “B”).\n\n\n\n\n\n\nlinear\ntrees\nnaive\nkernel\nNN\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nName all of the algorithms you know for each family of predictive model.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictive Modeling Concepts and Regression</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#regression-algorithms",
    "href": "20_predictive-modeling-concepts.html#regression-algorithms",
    "title": "18  Predictive Modeling Concepts and Regression",
    "section": "18.2 Regression Algorithms",
    "text": "18.2 Regression Algorithms\nLet’s explore a few algorithms for generating predictions in regression applications. Consider some simulated data with 1,000 observations, one predictor, and one outcome.\n\nlibrary(tidymodels)\n\nset.seed(20201004)\n\nx &lt;- runif(n = 1000, min = 0, max = 10)\n\ndata1 &lt;- bind_cols(\n  x = x,\n  y = 10 * sin(x) + x + 20 + rnorm(n = length(x), mean = 0, sd = 2)\n)\n\nFor reasons explained later, we split the data into a training data set with 750 observations and a testing data set with 250 observations.\n\nset.seed(20201007)\n\n# create a split object\ndata1_split &lt;- initial_split(data = data1, prop = 0.75)\n\n# create the training and testing data\ndata1_train &lt;- training(x = data1_split)\ndata1_test  &lt;- testing(x = data1_split)\n\nFigure 18.1 visualizes the training data.\n\n# visualize the data\ndata1_train |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = 0.25) +\n  labs(title = \"Example 1 Data\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 18.1: Simulated training data\n\n\n\n\n\n\n18.2.1 Linear Regression\n\nFind the line that minimizes the sum of squared errors between the line and the observed data.\n\nNote: Linear regression is linear in its coefficients but can fit non-linear patterns in the data with the inclusion of higher order terms (for example, \\(x^2\\)).\nFigure 18.2 shows four different linear regression models fit to the training data. Degree is the magnitude of the highest order term included in the model.\nFor example, “Degree = 1” means \\(\\hat{y}_i = b_0 + b_1x_i\\) and “Degree = 3” means \\(\\hat{y}_i = b_0 + b_1x_i + b_2x_i^2 + b_3x_i^3\\).\n\n\nCode\nlin_reg_model1 &lt;- linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nlin_reg_model2 &lt;- linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ poly(x, degree = 2, raw = TRUE), data = data1_train)\n\nlin_reg_model3 &lt;- linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ poly(x, degree = 3, raw = TRUE), data = data1_train)\n\nlin_reg_model4 &lt;- linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ poly(x, degree = 4, raw = TRUE), data = data1_train)\n\n# create a grid of predictions\nnew_data &lt;- tibble(x = seq(0, 10, 0.1))\n\npredictions_grid &lt;- tibble(\n  x = seq(0, 10, 0.1),\n  `Degree = 1` = predict(object = lin_reg_model1, new_data = new_data)$.pred,\n  `Degree = 2` = predict(object = lin_reg_model2, new_data = new_data)$.pred,\n  `Degree = 3` = predict(object = lin_reg_model3, new_data = new_data)$.pred,\n  `Degree = 4` = predict(object = lin_reg_model4, new_data = new_data)$.pred\n) |&gt;\n  pivot_longer(-x, names_to = \"model\", values_to = \".pred\")\n\nggplot() +\n  geom_point(data = data1_train, aes(x = x, y = y), alpha = 0.25) +\n  geom_path(data = predictions_grid, aes(x = x, y = .pred), color = \"red\") +\n  facet_wrap(~model) +\n  labs(\n    title = \"Example 1: Data with Predictions (Linear Regression)\",\n    subtitle = \"Prediction in red\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 18.2: Fitting the non-linear pattern in the data requires higher order terms\n\n\n\n\n\n\n\n18.2.2 KNN\n\nFind the \\(k\\) closest observations using only the predictors. Closeness is typically measured with Euclidean distance.\nTake the mean of the outcome variables for the \\(k\\) closest observations.\n\n\n\nCode\n# create a knn model specification\nknn_mod1 &lt;- \n  nearest_neighbor(neighbors = 1) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nknn_mod5 &lt;- \n  nearest_neighbor(neighbors = 5) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nknn_mod50 &lt;- \n  nearest_neighbor(neighbors = 50) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nknn_mod500 &lt;- \n  nearest_neighbor(neighbors = 500) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\n# create a grid of predictions\nnew_data &lt;- tibble(x = seq(0, 10, 0.1))\n\npredictions_grid &lt;- tibble(\n  x = seq(0, 10, 0.1),\n  `KNN with k=1` = predict(object = knn_mod1, new_data = new_data)$.pred,\n  `KNN with k=5` = predict(object = knn_mod5, new_data = new_data)$.pred,\n  `KNN with k=50` = predict(object = knn_mod50, new_data = new_data)$.pred,\n  `KNN with k=500` = predict(object = knn_mod500, new_data = new_data)$.pred\n) |&gt;\n  pivot_longer(-x, names_to = \"model\", values_to = \".pred\")\n\n# visualize the data\nggplot() +\n  geom_point(data = data1_train, aes(x = x, y = y), alpha = 0.25) +\n  geom_path(data = predictions_grid, aes(x = x, y = .pred), color = \"red\") +\n  facet_wrap(~model) +\n  labs(\n    title = \"Example 1: Data with Predictions (KNN)\",\n    subtitle = \"Prediction in red\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nChanging k leads to models that under and over fit to the data\n\n\n\n\n\n\n18.2.3 Regression Trees\n\nConsider all binary splits of all predictors to split the data into two groups. Choose the split that results in groups with the lowest MSE for the outcome variable within each group.\nRepeat step 1 recursively until a stopping parameter is reached (cost complexity, minimum group size, tree depth).\n\nDecision Tree: The Obama-Clinton Divide from the New York Times is a clear example of a regression tree.\n\n\nCode\nreg_tree_mod1 &lt;- \n  decision_tree(cost_complexity = 0.1) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nreg_tree_mod2 &lt;- \n  decision_tree(cost_complexity = 0.01) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nreg_tree_mod3 &lt;- \n  decision_tree(cost_complexity = 0.001) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\nreg_tree_mod4 &lt;- \n  decision_tree(cost_complexity = 0.0001) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  fit(formula = y ~ x, data = data1_train)\n\n# create a grid of predictions\nnew_data &lt;- tibble(x = seq(0, 10, 0.1))\n\npredictions_grid &lt;- tibble(\n  x = seq(0, 10, 0.1),\n  `Regression Tree with cp=0.1` = predict(object = reg_tree_mod1, new_data = new_data)$.pred,\n  `Regression Tree with cp=0.01` = predict(object = reg_tree_mod2, new_data = new_data)$.pred,\n  `Regression Tree with cp=0.001` = predict(object = reg_tree_mod3, new_data = new_data)$.pred,\n  `Regression Tree with cp=0.0001` = predict(object = reg_tree_mod4, new_data = new_data)$.pred\n) |&gt;\n  pivot_longer(-x, names_to = \"model\", values_to = \".pred\")\n\n# visualize the data\nggplot() +\n  geom_point(data = data1_train, aes(x = x, y = y), alpha = 0.25) +\n  geom_path(data = predictions_grid, aes(x = x, y = .pred), color = \"red\") +\n  facet_wrap(~model) +\n  labs(\n    title = \"Example 1: Data with Predictions (Regression Trees)\",\n    subtitle = \"Prediction in red\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 18.3: Changing the cost complexity parameter leads to models that are under of over fit to the data\n\n\n\n\n\nRegression trees generate a series of binary splits that can be visualized. Consider the simple tree from Figure 18.3 where cp=0.1.\n\n\nCode\nlibrary(rpart.plot)\n\nrpart.plot(reg_tree_mod1$fit, roundint = FALSE)\n\n\n\n\n\n\n\n\n\nLinear regression is a parameteric approach. It requires making assumptions about the functional form of the data and reducing the model to a finite number of parameters.\nKNN and regression trees are nonparametric approaches to predictive modeling because they do not require an assumption about the functional form of the model. The data-driven approach of nonparametric models is appealing because it requires fewer assumptions, but it often requires more data and care to not overfit the modeling data.\nRegression trees are the simplest tree-based algorithms. Other tree-based algorithms, like gradient-boosted trees and random forests, often have far better performance than regression trees.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nUse library(tidymodels) to fit a KNN model to the data1_train. Pick any plausible value of \\(k\\).\nMake predictions on the testing data.\nEvaluate the model using rmse().",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictive Modeling Concepts and Regression</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#error",
    "href": "20_predictive-modeling-concepts.html#error",
    "title": "18  Predictive Modeling Concepts and Regression",
    "section": "18.3 Error",
    "text": "18.3 Error\nDifferent applications call for different error metrics. Root mean squared error (RMSE) and mean squared error (MSE) are popular metrics for regression.\n\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}\n\\tag{18.1}\\]\n\\[\nMSE = \\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\n\\tag{18.2}\\]\nLet’s consider the MSE for a single observation \\((x_0, y_0)\\) and think about the model that generates \\(\\hat{y_0}\\), which we will denote \\(\\hat{f}(x_0)\\).\n\\[\nMSE = (y_0 - \\hat{f}(x_0))^2\n\\tag{18.3}\\]\nAssuming \\(y_i\\) independent and \\(y_i - \\hat{y}_i \\sim N(0, \\sigma^2)\\), we can decompose the expected value of MSE into three types of error: irreducible error, bias error, and variance error. Understanding the types of error will inform modeling decisions.\n\\[\nE[MSE] = E[y_0 - \\hat{f}(x_0)]^2 = Var(\\epsilon) + [Bias(\\hat{f}(x_0))]^2 + Var(\\hat{f}(x_0))\n\\tag{18.4}\\]\n\\[\nE[MSE] = E[y_0 - \\hat{f}(x_0)]^2 = \\sigma^2 + (\\text{model bias})^2 + \\text{model variance}\n\\tag{18.5}\\]\n\n\n\n\n\n\nIrreducible error (\\(\\sigma^2\\))\n\n\n\nError that can’t be reduced regardless of model quality. This is often caused by factors that affect the outcome of interest that aren’t measured or included in the data set.\n\n\n\n\n\n\n\n\nBias error\n\n\n\nDifference between the expected prediction of a predictive model and the correct value. Bias error is generally the error that comes from approximating complicated real-world problems with relatively simple models.\n\n\nHere, the model on the left does a poor job fitting the data (high bias) and the model on the right does a decent job fitting the data (low bias).\n\n\nCode\nset.seed(20200225)\n\nsample1 &lt;- tibble(\n  x = runif(100, min = -10, max = 10),\n  noise = rnorm(100, mean = 10, sd = 10),\n  y = -(x ^ 2) + noise\n)\n\ngrid.arrange(\n  sample1 |&gt;\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\",\n                se = FALSE) +\n    theme_void() +\n    labs(title = \"High Bias\") +\n    theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\")),\n  \n  sample1 |&gt;\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(se = FALSE,\n                span = 0.08) +\n    theme_void() +\n    labs(title = \"Low Bias\") +\n    theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\")),\n  ncol = 2\n)\n\n\n\n\n\nBias\n\n\n\n\n\n\n\n\n\n\nVariance error\n\n\n\nHow much the predicted value (\\(\\hat{y}\\)) and fitted model (\\(\\hat{f}\\)) change for a given data point when using a different sample of data to fit the model.\n\n\nHere, the model on the left does not change much as the training data change (low variance) and the model on the right changes a lot when the training data change (high variance).\n\n\nCode\nset.seed(20200226)\n\nsample2 &lt;- tibble(\n  sample_number = rep(c(\"Sample 1\", \"Sample 2\", \"Sample 3\", \"Sample 4\"), 100),\n  x = runif(400, min = -10, max = 10),\n  noise = rnorm(400, mean = 10, sd = 10),\n  y = -(x ^ 2) + noise\n)\n\ngrid.arrange(\n  sample2 |&gt;\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\",\n                se = FALSE,\n                alpha = 0.5) +\n    facet_wrap(~sample_number) +\n    theme_void() +\n    labs(title = \"Low Variance\") +\n    theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\")),\n  \n  sample2 |&gt;\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(se = FALSE,\n                span = 0.08,\n                alpha = 0.5) +\n    facet_wrap(~sample_number) +\n    theme_void() +\n    labs(title = \"High Variance\") +\n    theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\")),\n  ncol = 2\n)\n\n\n\n\n\nVariance\n\n\n\n\nAs can be seen in the error decomposition and plots above, for any given amount of total prediction error, there is a trade-off between bias error and variance error. When one type of error is reduced, the other type of error increases.\nThe bias-variance trade-off can generally be reconceived as\n\nunderfitting-overfitting tradeoff\nsimplicity-complexity tradeoff\n\n\n\n\n\n\n\n\n\n\nOur objective is to make accurate predictions on unseen data. It is important to make sure that models make accurate predictions on the modeling data and the implementation data.\n\n\n\n\n\n\nIn-sample error\n\n\n\nThe predictive error of a model measured on the data used to estimate the predictive model.\n\n\n\n\n\n\n\n\nOut-of-sample error\n\n\n\nThe predictive error of a model measured on the data not used to estimate the predictive model. Out-of-sample error is generally greater than the in-sample error.\n\n\n\n\n\n\n\n\nGeneralizability\n\n\n\nHow well a model makes predictions on unseen data relative to how well it makes predictions on the data used to estimate the model.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictive Modeling Concepts and Regression</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#spending-data",
    "href": "20_predictive-modeling-concepts.html#spending-data",
    "title": "18  Predictive Modeling Concepts and Regression",
    "section": "18.4 Spending Data",
    "text": "18.4 Spending Data\nPredictive modelers strategically “spend data” to create accurate models that generalize to the implementation data. We will focus on two strategies: the training-testing split and v-fold cross validation.\n\n\n\n\n\n\nTraining data\n\n\n\nA subset of data used to develop a predictive model. The share of data committed to a training set depends on the number of observations, the number of predictors in a model, and heterogeneity in the data. 0.8 is a common share.\n\n\n\n\n\n\n\n\nTesting data\n\n\n\nA subset of data used to estimate model performance. The testing set usually includes all observations not included in the testing set. Do not look at these data until the very end and only estimate the out-of-sample error rate on the testing set once. If the error rate is estimated more than once on the testing data, it will underestimate the error rate.\n\n\n\n\n\n\n\n\nFigure 18.4\n\n\n\n\n\n\n\n\n\n\\(v\\)-fold Cross-Validation\n\n\n\nBreak the data into \\(v\\) equal-sized, exclusive groups. Train the model on data from \\(v - 1\\) folds (analysis data) and measure the error metric (i.e. RMSE) on the left-out fold (assessment data). Repeat this process \\(v\\) times, each time leaving out a different fold. Average the \\(v\\) error metrics.\n\\(v\\)-fold cross-validation is sometimes called \\(k\\)-fold cross-validation.\n\n\nStrategies for spending data differ based on the application and the size and nature of the data. Ideally, the training-testing split is made at the beginning of the modeling process. The \\(v\\)-fold cross-validation is used on the testing data for comparing\n\napproaches for feature/target engineering\nalgorithms\nhyperparameter tuning\n\n\n\n\n\n\n\nData Leakage\n\n\n\nWhen information that won’t be available when the model makes out-of-sample predictions is used when estimating a model. Looking at data from the testing set creates data leakage. Data leakage leads to an underestimate of out-of-sample error.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFeature and target engineering is a common source of data leakage.\nFor example, regularized regression models expect variables to be normalized (divide each predictor by the sample mean and divide by the sample standard deviation). A naive approach would be to calculate the means and standard deviations once one the full data set and use them on the testing data or use them in cross validation. This causes data leakage and biased underestimates of the out-of-sample error rate.\nNew means and standard deviations need to be estimated every time a model is fit including within each iteration of a resampling method! This is a lot of work, but library(tidymodels) makes this simple.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictive Modeling Concepts and Regression</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#predictive-modeling-pipelines",
    "href": "20_predictive-modeling-concepts.html#predictive-modeling-pipelines",
    "title": "18  Predictive Modeling Concepts and Regression",
    "section": "18.5 Predictive Modeling Pipelines",
    "text": "18.5 Predictive Modeling Pipelines\nThe rest of this chapter focuses on a predictive modeling pipeline applied to the simulated data. This pipeline is a general approach to creating predictive models and can change based on the specifics of an application. We will demonstrate the pipeline using KNN and regression trees.\n\nProblem Formulation\nSplit data into training and testing data\nExploratory Data Analysis\nSet up Resampling for Model Selection\nCreate Candidate Models\nTest and Choose the “Best” Model\nOptional: Expand the Model Fit\nEvaluate the Final Model\nOptional: Implement the Final Model\n\n\n18.5.1 Example: KNN\n\n1. Split data into training and testing data\n\nset.seed(20201007)\n\n# create a split object\ndata1_split &lt;- initial_split(data = data1, prop = 0.75)\n\n# create the training and testing data\ndata1_train &lt;- training(x = data1_split)\ndata1_test  &lt;- testing(x = data1_split)\n\n\n\n2. Exploratory Data Analysis\n\n\n\n\n\n\nWarning\n\n\n\nOnly perform EDA on the training data.\nExploring the testing data will lead to data leakage.\n\n\n\ndata1_train |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point(alpha = 0.25) +\n  labs(title = \"Example 1 Data\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3. Set up Resampling for Model Selection\nWe use 10-fold cross validation for hyperparameter tuning and model selection. The training data are small (750 observations and one predictor), so we will repeat the entire cross validation process five times.\nEstimating out-of-sample error rates with cross-validation is an uncertain process. Repeated cross-validation improves the accuracy of our estimated error rates.\n\ndata1_folds &lt;- vfold_cv(data = data1_train, v = 10, repeats = 5)\n\n\n\n4. Create Candidate Models\nWe have three main levers for creating candidate models:\n\nFeature and target engineering. Feature and target engineering is generally specified when creating a recipe with recipe() and step_*() functions.\nSwitching algorithms. Algorithms are specified using functions from library(parsnip).\nHyperparameter tuning. Hyperparameter tuning is typically handled by using tune() when picking an algorithm and then creating a hyperparameter tuning grid.\n\nIt is common to normalize (subtract the sample mean and divide by the sample standard deviation) predictors when using KNN.\n\nknn_recipe &lt;-\n  recipe(formula = y ~ ., data = data1_train) |&gt;\n  step_normalize(all_predictors())\n\nWe use nearest_neighbor() to create a KNN model and we use tune() as a placeholder for k. Note that we can tune many hyperparameters for a given model. The Regression Tree example later in this chapter illustrates this concept.\n\nknn_mod &lt;-\n  nearest_neighbor(neighbors = tune()) |&gt;\n  set_engine(engine = \"kknn\") |&gt;\n  set_mode(mode = \"regression\")\n\nWe use grid_regular() to specify a hyperparameter tuning grid for potential values of \\(k\\).\n\nknn_grid &lt;- grid_regular(neighbors(range = c(1, 99)), levels = 10)\n\nknn_grid\n\n# A tibble: 10 × 1\n   neighbors\n       &lt;int&gt;\n 1         1\n 2        11\n 3        22\n 4        33\n 5        44\n 6        55\n 7        66\n 8        77\n 9        88\n10        99\n\n\nFinally, we combine the recipe and model objects into a workflow().\n\nknn_workflow &lt;-\n  workflow() |&gt;\n  add_recipe(recipe = knn_recipe) |&gt;\n  add_model(spec = knn_mod)\n\n\n\n5. Test and Choose the “Best” Model\ntune_grid() fits the models to the repeated cross validation with differing hyperparameters and captures RMSE against each evaluation data set.\n\nknn_res &lt;-\n  knn_workflow |&gt;\n  tune_grid(\n    resamples = data1_folds,\n    grid = knn_grid,\n    metrics = metric_set(rmse)\n  )\n\nWe can quickly extract information from the tuned results object with functions like collect_metrics(), show_best(), and autoplot().\n\nknn_res |&gt;\n  collect_metrics()\n\n# A tibble: 10 × 7\n   neighbors .metric .estimator  mean     n std_err .config              \n       &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1         1 rmse    standard    2.86    50  0.0325 Preprocessor1_Model01\n 2        11 rmse    standard    2.06    50  0.0209 Preprocessor1_Model02\n 3        22 rmse    standard    2.01    50  0.0200 Preprocessor1_Model03\n 4        33 rmse    standard    2.00    50  0.0193 Preprocessor1_Model04\n 5        44 rmse    standard    2.01    50  0.0197 Preprocessor1_Model05\n 6        55 rmse    standard    2.03    50  0.0205 Preprocessor1_Model06\n 7        66 rmse    standard    2.07    50  0.0221 Preprocessor1_Model07\n 8        77 rmse    standard    2.11    50  0.0242 Preprocessor1_Model08\n 9        88 rmse    standard    2.17    50  0.0266 Preprocessor1_Model09\n10        99 rmse    standard    2.24    50  0.0291 Preprocessor1_Model10\n\nknn_res |&gt;\n  show_best()\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        33 rmse    standard    2.00    50  0.0193 Preprocessor1_Model04\n2        22 rmse    standard    2.01    50  0.0200 Preprocessor1_Model03\n3        44 rmse    standard    2.01    50  0.0197 Preprocessor1_Model05\n4        55 rmse    standard    2.03    50  0.0205 Preprocessor1_Model06\n5        11 rmse    standard    2.06    50  0.0209 Preprocessor1_Model02\n\nknn_res |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n6. Optional: Expand the Model Fit\nSometimes we will pick the best predictive model specification (feature and target engineering + algorithm + hyperparameters) and fit the model on all of the training data.\nFirst, finalize the workflow with the “best” hyperparameters from the tuned results.\n\nfinal_wf &lt;- \n  knn_workflow |&gt; \n  finalize_workflow(select_best(knn_res))\n\nSecond, use last_fit() to fit the model on all of the data.\n\nfinal_fit &lt;- \n  final_wf |&gt;\n  last_fit(data1_split) \n\n\n\n\n\n\n\nNote\n\n\n\nselect_best() takes a narrow view of model selection and picks the model with the lowest error rate.\nIt is important to consider other elements when picking a “best.” Other considerations include parsimony, cost, and equity.\n\n\n\n\n7. Evaluate the Final Model\nThe final fit object contains the out-of-sample error metric from the testing data. This metric is our best estimate of model performance on unseen data.\nIn this case, the testing data error is slightly higher than the training data error, which makes sense.\n\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2.04  Preprocessor1_Model1\n2 rsq     standard       0.922 Preprocessor1_Model1\n\n\n\n\n8. Optional: Implement the Final Model\nfinal_fit and pedict() can be used to apply the model to new, unseen data.\nOnly implement the model if it achieves the objectives of the final model.\n\n\n\n18.5.2 Example: Regression Trees\nWe’ll next work with a subset of data from the Chicago data set from library(tidymodels).\n\nchicago_small &lt;- Chicago |&gt;\n  select(\n    ridership,\n    Clark_Lake,\n    Quincy_Wells,\n    Irving_Park,\n    Monroe,\n    Polk,\n    temp,\n    percip,\n    Bulls_Home,\n    Bears_Home,\n    WhiteSox_Home,\n    Cubs_Home,\n    date\n  ) |&gt;\n  mutate(weekend = wday(date, label = TRUE) %in% c(\"Sat\", \"Sun\")) |&gt;\n  glimpse()\n\nRows: 5,698\nColumns: 14\n$ ridership     &lt;dbl&gt; 15.732, 15.762, 15.872, 15.874, 15.423, 2.425, 1.467, 15…\n$ Clark_Lake    &lt;dbl&gt; 15.561, 15.720, 15.558, 15.745, 15.602, 2.413, 1.374, 9.…\n$ Quincy_Wells  &lt;dbl&gt; 8.371, 8.351, 8.359, 7.852, 7.621, 0.911, 0.414, 4.807, …\n$ Irving_Park   &lt;dbl&gt; 3.744, 3.853, 3.861, 3.843, 3.878, 1.735, 1.164, 2.903, …\n$ Monroe        &lt;dbl&gt; 5.672, 6.013, 5.786, 5.959, 5.769, 1.044, 0.530, 3.165, …\n$ Polk          &lt;dbl&gt; 2.481, 2.436, 2.526, 2.450, 2.573, 0.006, 0.000, 1.065, …\n$ temp          &lt;dbl&gt; 19.45, 30.45, 25.00, 22.45, 27.00, 24.80, 18.00, 32.00, …\n$ percip        &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.…\n$ Bulls_Home    &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Bears_Home    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ WhiteSox_Home &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Cubs_Home     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ date          &lt;date&gt; 2001-01-22, 2001-01-23, 2001-01-24, 2001-01-25, 2001-01…\n$ weekend       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FA…\n\n\n\n0. Problem Formulation\nThe variable ridership is the outcome variable. It measures ridership at the Clark/Lake L station in Chicago. The variables Clark_Lake, Quincy_Wells, Irving_Park, Monroe, and Polk measure ridership at several stations 14 days before the ridership variable.\nThe objective is predict ridership to better allocate staff and resources to the Clark/Lake L station.\n\n\n1. Split data into training and testing data\n\nset.seed(20231106)\n\n# create a split object\nchicago_split &lt;- initial_split(data = chicago_small, prop = 0.8)\n\n# create the training and testing data\nchicago_train &lt;- training(x = chicago_split)\nchicago_test  &lt;- testing(x = chicago_split)\n\n\n\n2. Exploratory Data Analysis\nThis chapter in “Feature and Target Engineering” contains EDA for the data set of interest.\n\n\n3. Set up Resampling for Model Selection\nWe use 10-fold cross validation for hyperparameter tuning and model selection. The training data are larger in the previous example, so we skip repeats.\n\nchicago_folds &lt;- vfold_cv(data = chicago_train, v = 10)\n\n\n\n4. Create Candidate Models\nWe will use regression trees for this example. For feature and target engineering, we will simply remove the date column.\n\nrpart_recipe &lt;-\n  recipe(formula = ridership ~ ., data = chicago_train) |&gt;\n  step_rm(date)\n\nWe specify a regression tree with the rpart engine.\n\nrpart_mod &lt;-\n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune(),\n    min_n = tune()\n  ) |&gt;\n  set_engine(engine = \"rpart\") |&gt;\n  set_mode(mode = \"regression\")\n\nWe use grid_regular() to specify a hyperparameter tuning grid for potential values of the cost-complexity parameter, tree depth parameter, and minimum n.\n\nrpart_grid &lt;- grid_regular(\n  cost_complexity(range = c(-5, -1)), \n  tree_depth(range = c(3, 15)), \n  min_n(),\n  levels = 10\n)\n\nrpart_grid\n\n# A tibble: 1,000 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1       0.00001            3     2\n 2       0.0000278          3     2\n 3       0.0000774          3     2\n 4       0.000215           3     2\n 5       0.000599           3     2\n 6       0.00167            3     2\n 7       0.00464            3     2\n 8       0.0129             3     2\n 9       0.0359             3     2\n10       0.1                3     2\n# ℹ 990 more rows\n\n\nFinally, we combine the recipe and model objects into a workflow().\n\nrpart_workflow &lt;-\n  workflow() |&gt;\n  add_recipe(recipe = rpart_recipe) |&gt;\n  add_model(spec = rpart_mod)\n\n\n\n5. Test and Choose the “Best” Model\n\nrpart_res &lt;-\n  rpart_workflow |&gt;\n  tune_grid(resamples = chicago_folds,\n            grid = rpart_grid,\n            metrics = metric_set(rmse))\n\nrpart_res |&gt;\n  collect_metrics()\n\n# A tibble: 1,000 × 9\n   cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1       0.00001            3     2 rmse    standard    2.44    10  0.0414\n 2       0.0000278          3     2 rmse    standard    2.44    10  0.0414\n 3       0.0000774          3     2 rmse    standard    2.44    10  0.0414\n 4       0.000215           3     2 rmse    standard    2.44    10  0.0414\n 5       0.000599           3     2 rmse    standard    2.44    10  0.0414\n 6       0.00167            3     2 rmse    standard    2.45    10  0.0418\n 7       0.00464            3     2 rmse    standard    2.47    10  0.0456\n 8       0.0129             3     2 rmse    standard    2.67    10  0.0392\n 9       0.0359             3     2 rmse    standard    2.67    10  0.0392\n10       0.1                3     2 rmse    standard    3.02    10  0.0383\n# ℹ 990 more rows\n# ℹ 1 more variable: .config &lt;chr&gt;\n\nrpart_res |&gt;\n  show_best()\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1       0.00001            5    40 rmse    standard    2.37    10  0.0374\n2       0.0000278          5    40 rmse    standard    2.37    10  0.0374\n3       0.00001            5    35 rmse    standard    2.37    10  0.0374\n4       0.0000278          5    35 rmse    standard    2.37    10  0.0374\n5       0.0000774          5    35 rmse    standard    2.37    10  0.0369\n# ℹ 1 more variable: .config &lt;chr&gt;\n\nrpart_res |&gt;\n  select_best()\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config                \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                  \n1         0.00001          5    40 Preprocessor1_Model0921\n\nrpart_res |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n6. Optional: Expand the Model Fit\nSometimes we will pick the best predictive model specification (feature and target engineering + algorithm + hyperparameters) and fit the model on all of the training data.\nFirst, finalize the workflow with the “best” hyperparameters from the tuned results.\n\nfinal_wf &lt;- \n  rpart_workflow |&gt; \n  finalize_workflow(select_best(rpart_res))\n\nSecond, use last_fit() to fit the model on all of the data.\n\nfinal_fit &lt;- \n  final_wf |&gt;\n  last_fit(chicago_split) \n\n\n\n7. Evaluate the Final Model\nThe final fit object contains the out-of-sample error metric from the testing data. This metric is our best estimate of model performance on unseen data.\nIn this case, the testing data error is slightly higher than the training data error, which makes sense.\n\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2.34  Preprocessor1_Model1\n2 rsq     standard       0.869 Preprocessor1_Model1\n\n\n\n\n8. Optional: Implement the Final Model\nIf we think an expected error of around 2,340 people is appropriate, we can implement this model for the Chicago Transit Authority.\nInstead, we’re going back to the drawing board in our class lab to see if we can do better.1",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictive Modeling Concepts and Regression</span>"
    ]
  },
  {
    "objectID": "20_predictive-modeling-concepts.html#footnotes",
    "href": "20_predictive-modeling-concepts.html#footnotes",
    "title": "18  Predictive Modeling Concepts and Regression",
    "section": "",
    "text": "Pretend we haven’t already touched the testing data!↩︎",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Predictive Modeling Concepts and Regression</span>"
    ]
  },
  {
    "objectID": "21_supervised-classification.html",
    "href": "21_supervised-classification.html",
    "title": "19  Classification",
    "section": "",
    "text": "19.1 Types of Classification",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "21_supervised-classification.html#types-of-classification",
    "href": "21_supervised-classification.html#types-of-classification",
    "title": "19  Classification",
    "section": "",
    "text": "Binary Classification\n\n\n\nBinary classification: Predicting one of two classes. For example, rat burrow or no rat burrow, lead paint or no lead paint, or insured or uninsured. Classes are often recoded to 1 and 0 as in logistic regression.\n\n\n\n\n\n\n\n\nMulticlass Classification:\n\n\n\nMulticlass classification: Predicting one of three or more classes. For example, single filer, joint filer, or head of household; or on-time, delinquent, or defaulted. Classes can be recoded to integers for models like multinomial logistic regression, but many of the best models can handle factors.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "21_supervised-classification.html#metrics",
    "href": "21_supervised-classification.html#metrics",
    "title": "19  Classification",
    "section": "19.2 Metrics",
    "text": "19.2 Metrics\nClassification problems require a different set of error metrics and diagnostics than regression problems. Assume a binary classifier for the following definitions. Let an event be the outcome 1 in a binary classification problem and a non-event be the outcome 0.\n\n\n\n\n\n\nTrue positive\n\n\n\nTrue positive: Correctly predicting an event. Predicting \\(\\hat{y_i} = 1\\) when \\(y_i = 1\\)\n\n\n\n\n\n\n\n\nTrue negative\n\n\n\nTrue negative: Correctly predicting a non-event. Predicting \\(\\hat{y_i} = 0\\) when \\(y_i = 0\\)\n\n\n\n\n\n\n\n\nFalse positive\n\n\n\nFalse positive: Incorrectly predicting an event for a non-event. Predicting \\(\\hat{y_i} = 1\\) when \\(y_i = 0\\)\n\n\n\n\n\n\n\n\nFalse negative\n\n\n\nFalse negative: Incorrectly predicting a non-event for an event. Predicting \\(\\hat{y_i} = 0\\) when \\(y = 1\\)\n\n\n\n\n\n\n\n\nConfusion Matrix\n\n\n\nConfusion matrix: A simple matrix that compares predicted outcomes with actual outcomes.\n\n\nAs these notes will show, there are many ways to combine true positives, false positives, true negatives, and false negatives into a given metric. It is important to understand a use-case in which a predictive algorithm will be used. That should inform the specific metric selected.\n\nExample Confusion Matrix\n\n\n\n\n\n\n\n\n\n\nTrue Value\n\n\n\n\n\n\\(y=1\\)\n\\(y=0\\)\n\n\nPredicted Value\n\\(\\hat{y}=1\\)\nTrue Positive (TP)\nFalse Positive (FP)\n\n\n\n\\(\\hat{y}=0\\)\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\n\n\n\n\n\nAccuracy:\n\n\n\nAccuracy: The sum of the values on the main diagonal of the confusion matrix divided by the total number of predictions (\\(\\frac{TP + TN}{total}\\)).\n\n\n\n19.2.1 Example 1\nConsider the following set of true values and predicted values from a binary classification problem:\n\n\n\n\n\ntrue_value\npredicted_value\n\n\n\n\n0\n0\n\n\n0\n0\n\n\n0\n0\n\n\n1\n0\n\n\n1\n1\n\n\n1\n1\n\n\n0\n1\n\n\n0\n1\n\n\n1\n1\n\n\n1\n1\n\n\n\n\n\nThe confusion matrix for this data:\n\n\n\n\n\n\n\n\n\n\n\nTrue Value\n\n\n\n\n\n\\(y=1\\)\n\\(y=0\\)\n\n\nPredicted Value\n\\(\\hat{y}=1\\)\n4\n2\n\n\n\n\\(\\hat{y}=0\\)\n1\n3\n\n\n\n\n\n19.2.2 Example 2\nA test for breast cancer is 99.1% accurate across 1,000 tests. Is it a good test?\n\n\n\n\n\n\n\n\n\n\n\nTrue Value\n\n\n\n\n\n\\(y=1\\)\n\\(y=0\\)\n\n\nPredicted Value\n\\(\\hat{y}=1\\)\n1\n4\n\n\n\n\\(\\hat{y}=0\\)\n5\n990\n\n\n\nThis test only accurately predicted one cancer case. In fact, a person was more likely not to have cancer given a positive test than to have cancer. This example demonstrates the base rate fallacy and the accuracy paradox. Both are the results of high class imbalance. Clearly we need more sophisticated way of evaluating classifiers than just accuracy.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "21_supervised-classification.html#more-metrics",
    "href": "21_supervised-classification.html#more-metrics",
    "title": "19  Classification",
    "section": "19.3 More metrics",
    "text": "19.3 More metrics\n\n\n\n\n\n\n\n\n\n\n19.3.1 Most Important Metrics\n\n\n\n\n\n\nAccuracy\n\n\n\nAccuracy: How often the classifier is correct. \\(\\frac{TP +TN}{total}\\). All else equal, we want to maximize accuracy.\n\n\n\n\n\n\n\n\nPrecision\n\n\n\nPrecision: How often the classifier is correct when it predicts events. \\(\\frac{TP}{TP+FP}\\). All else equal, we want to maximize precision.\n\n\n\n\n\n\n\n\nRecall\n\n\n\nRecall/Sensitivity/True Positive Rate: How often the classifier is correct when there is an event. \\(\\frac{TP}{TP+FN}\\). All else equal, we want to maximize recall/sensitivity.\n\n\n\n\n19.3.2 Other Metrics\n\n\n\n\n\n\nSpecificity\n\n\n\nSpecificity: How often the classifier is correct when there is a non-event. \\(\\frac{TN}{TN+FP}\\). All else equal, we want to maximize specificity.\n\n\n\n\n\n\n\n\nFalse Positive Rate\n\n\n\nFalse Positive Rate: 1 - Specificity\nNote that False Positive Rate is sometimes referred to as FPR.\n\n\n\n\n19.3.3 Example 2 continued\n\n\n\n\n\n\n\n\n\n\n\nTrue Value\n\n\n\n\n\n\\(y=1\\)\n\\(y=0\\)\n\n\nPredicted Value\n\\(\\hat{y}=1\\)\n1\n4\n\n\n\n\\(\\hat{y}=0\\)\n5\n990\n\n\n\n\n\nPrecision: \\(\\frac{TP}{TP + FP} = \\frac{1}{1 + 4} = \\frac{1}{5}\\)\nRecall/Sensitivity: \\(\\frac{TP}{TP + FN} = \\frac{1}{1 + 5} = \\frac{1}{6}\\)\nThe breast cancer test has poor precision and recall.\nSpecificity: \\(\\frac{TN}{FP + TN} = \\frac{990}{4 + 990} = \\frac{990}{994}\\)\nFalse Positive Rate (FPR): \\(1 - Specificity = \\frac{4}{994}\\)\nThe breast cancer cancer test also has poor False Positive Rate.\n\n\n19.3.4 Using Thresholds to Predict Classes:\nMost algorithms for classification generate predicted classes and probabilities of predicted classes. A predicted probability of 0.99 for an event is very different than a predicted probability of 0.51.\nTo generate class predictions, usually a threshold is used. For example, if \\(\\hat{P}(event) &gt; 0.5\\) then predict event. It is common to adjust the threshold to values other than 0.5. As 0.5 decreases, marginal cases shift from \\(\\hat{y} = 0\\) to \\(\\hat{y} = 1\\) because the threshold for an event decreases.\nAs the threshold decreases (we are predicting more events and fewer non-events):\n\nthe true positive rate increases and the false positive rate increases\nprecision decreases and sensitivity/recall increases\nsensitivity increases and specificity decreases\n\nIn general, the goal is to create a model that has a high true positive rate and a low false positive rate, high precision and high recall, and high sensitivity and high specificity. Changing the threshold is a movement along these tradeoffs. Estimating “better” models is often a way to improve these tradeoffs. Of course, there is some amount of irreducible error.\n\n\n\n\n\n\nReceiver Operating Curve (ROC)\n\n\n\nReceiver Operating Characteristics (ROC) curve: A curve that shows the trade-off between the false positive rate and true positive rate as different threshold probabilities are used for classification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArea Under the Curve\n\n\n\nArea Under the Curve (AUC): A one-number summary of an ROC curve where 0.5 is random guessing and the rules of thumb are 0.7 is acceptable, 0.8 is good, and 0.9 is excellent.\nAUC is also sometimes referred to as AUROC (Area under the Receiver Operating Curve).\n\n\n\n\n# A tibble: 3 × 4\n  Quality    .metric .estimator .estimate\n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 Acceptable roc_auc binary         0.733\n2 Good       roc_auc binary         0.841\n3 Excellent  roc_auc binary         0.925\n\n\n\nROC Curves and AUC\n\n\n\n19.3.5 Relative costs\nTrue positives, true negatives, false positives, and false negatives can carry different costs, and it is important to consider the relative costs when creating models and interventions.\nA false positive for a cancer test could result in more diagnostic tests. A false negative for a cancer test could lead to untreated cancer and severe health consequences. The relative differences in these outcomes should be considered.\nA false positive for a rat burrow is a wasted trip for an exterminator. A false negative for an rat burrow is an untreated rat burrow. The difference in these outcomes is small, especially compared to the alternative of the exterminator guessing which alleys to visit.\n\n\n19.3.6 Multiclass metrics\nConsider a multiclass classification problem with three unique levels (“a”, “b”, “c”)\n\n\n\n\n\ntrue_value\npredicted_value\n\n\n\n\na\na\n\n\na\na\n\n\na\na\n\n\na\na\n\n\nb\nb\n\n\nb\na\n\n\nb\nb\n\n\nb\nc\n\n\nc\nc\n\n\nc\nb\n\n\nc\na\n\n\nc\nc\n\n\n\n\n\n\nCreate a confusion matrix:\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Value\n\n\n\n\n\n\n\n\n\\(y=a\\)\n\\(y=b\\)\n\\(y=c\\)\n\n\nPredicted Value\n\\(\\hat{y}=a\\)\n4\n1\n1\n\n\n\n\\(\\hat{y}=b\\)\n0\n2\n2\n\n\n\n\\(\\hat{y}=c\\)\n0\n1\n2\n\n\n\n\nAccuracy still measures how often the classifier is correct. In multiclass classification problem, the correct predictions are on the diagonal.\nAccuracy: \\(\\frac{4 + 2 + 2}{12} = \\frac{8}{12} = \\frac{2}{3}\\)\nThere are multiclass extensions of precision, recall/sensitivity, and specificity. They are beyond the scope of this class.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "21_supervised-classification.html#r-code",
    "href": "21_supervised-classification.html#r-code",
    "title": "19  Classification",
    "section": "19.4 R Code",
    "text": "19.4 R Code\n\n19.4.1 Example 1\nExample 1 uses data about penguins from the Palmer Archipelago in Antarctica. The data include measurements about three different species of penguins. This example only considers two classes and does not use resampling methods because only one model is estimated.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\n# drop to two species\npenguins_small &lt;- penguins %&gt;%\n  filter(species %in% c(\"Adelie\", \"Gentoo\")) %&gt;%\n  mutate(species = factor(species))\n\n# look at missing data\nmap_dbl(.x = penguins_small, .f = ~ sum(is.na(.x)))\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0 \n\n# drop missing values\npenguins_small &lt;- penguins_small %&gt;%\n  drop_na()\n\n\nStep 1. Split the data into training and testing data\n\nset.seed(20201013)\n\n# create a split object\npenguins_small_split &lt;- initial_split(data = penguins_small, prop = 0.8)\n\n# create the training and testing data\npenguins_small_train &lt;- training(x = penguins_small_split) \npenguins_small_test &lt;- testing(x = penguins_small_split)\n\nrm(penguins_small)\n\n\n\nStep 2. EDA\n\npenguins_small_train %&gt;%\n  ggplot(aes(x = flipper_length_mm, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nStep 3. Estimate a Model\n\n# create a recipe\ncart_rec &lt;- \n  recipe(formula = species ~ ., data = penguins_small_train)\n\n# create a cart model object\ncart_mod &lt;- \n  decision_tree() %&gt;%\n  set_engine(engine = \"rpart\") %&gt;%\n  set_mode(mode = \"classification\")\n\ncart_wf &lt;- workflow() %&gt;%\n  add_recipe(cart_rec) %&gt;%\n  add_model(cart_mod)\n  \n# fit the model\ncart_fit &lt;- cart_wf %&gt;%\n  fit(data = penguins_small_train)\n\n# create a tree\nrpart.plot::rpart.plot(x = cart_fit$fit$fit$fit)\n\n\n\n\n\n\n\n\n\n\nStep 4. Evaluate a Model\n\n# predict the predicted class and the predicted probability of each class\npredictions &lt;- bind_cols(\n  penguins_small_test,\n  predict(object = cart_fit, new_data = penguins_small_test),\n  predict(object = cart_fit, new_data = penguins_small_test, type = \"prob\")\n)\n\nselect(predictions, species, starts_with(\".pred\"))\n\n# A tibble: 53 × 4\n   species .pred_class .pred_Adelie .pred_Gentoo\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n 1 Adelie  Adelie                 1            0\n 2 Adelie  Adelie                 1            0\n 3 Adelie  Adelie                 1            0\n 4 Adelie  Adelie                 1            0\n 5 Adelie  Adelie                 1            0\n 6 Adelie  Adelie                 1            0\n 7 Adelie  Adelie                 1            0\n 8 Adelie  Adelie                 1            0\n 9 Adelie  Adelie                 1            0\n10 Adelie  Adelie                 1            0\n# ℹ 43 more rows\n\n\n\nCreate a confusion matrix:\n\nconf_mat(data = predictions,\n         truth = species,\n         estimate = .pred_class)\n\n          Truth\nPrediction Adelie Gentoo\n    Adelie     27      1\n    Gentoo      0     25\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nExerciseAnswer\n\n\n“Adelie” is the “event”.\nCalculate the accuracy.\n\n\n\\[Accuracy = \\frac{TP + TN}{total} = \\frac{27 + 25}{53} = \\frac{52}{53} \\approx 0.981\\]\n\naccuracy(data = predictions,\n         truth = species,\n         estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.981\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nExerciseAnswer\n\n\nCalculate the precision.\n\n\n\\[Precision = \\frac{TP}{TP + FP} = \\frac{27}{27 + 1} = \\frac{27}{28} \\approx 0.964\\]\n\nprecision(data = predictions,\n          truth = species,\n          estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision binary         0.964\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nExerciseAnswer\n\n\nCalculate the sensitivity.\n\n\n\\[Sensitivity = \\frac{27}{27 + 0} = \\frac{27}{27} = 1\\]\n\nrecall(data = predictions,\n       truth = species,\n       estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 recall  binary             1\n\n\n\n\n\n\n\n\n\n\nStep 5. Make a New Prediction\n\n\n\nSource: Lescroël, A. L.; Ballard, G.; Grémillet, D.; Authier, M.; Ainley, D. G. (2014)\n\n\n\n\nnew_penguins &lt;- tribble(\n  ~island, \n  ~bill_length_mm,\n  ~bill_depth_mm,\n  ~flipper_length_mm,\n  ~body_mass_g,\n  ~sex,\n  ~year,\n  \"Torgersen\",\n  40, \n  19, \n  190, \n  4000, \n  \"male\", \n  2008\n)\n\npredict(object = cart_fit, new_data = new_penguins)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 Adelie     \n\npredict(object = cart_fit, new_data = new_penguins, type = \"prob\")\n\n# A tibble: 1 × 2\n  .pred_Adelie .pred_Gentoo\n         &lt;dbl&gt;        &lt;dbl&gt;\n1            1            0\n\n\nNote that the decision tree has one split, on flipper length. Below the threshold, all penguins are Adelie. Consequently, the probability associated with .pred_Adelie is 100% or 1.0. You can see this with the code below:\n\npenguins_small_train |&gt; \n  filter(flipper_length_mm &lt; 206) |&gt;\n  group_by(species) |&gt; \n  count()\n\n# A tibble: 1 × 2\n# Groups:   species [1]\n  species     n\n  &lt;fct&gt;   &lt;int&gt;\n1 Adelie    117",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "22_ensembling.html",
    "href": "22_ensembling.html",
    "title": "20  Ensembling",
    "section": "",
    "text": "20.1 Motivation\nA team of mediocre predictive models can often outperform one excellent predictive model.\nThis chapter introduces ensembling, the process of combining predictions from multiple models into one prediction. We will focus on ensembling trees.\nThis chapter also introduces an extended example of predicting a low response rate on the US Census Bureau’s decennial census planning database.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ensembling</span>"
    ]
  },
  {
    "objectID": "22_ensembling.html#planning-database",
    "href": "22_ensembling.html#planning-database",
    "title": "20  Ensembling",
    "section": "20.2 Planning Database",
    "text": "20.2 Planning Database\n\nMotivation: The decennial census is used to allocate hundreds of billions of dollars and to redistrict political power. Ensuring an accurate count is imperative. The Census Bureau uses the planning database (PDB) and a modeled low-response score (LRS) to plan outreach to improve response rates.\nImplementation data: Demographic and housing information for the American Community Survey at the census tract and census block group levels.\nModeling data: Demographic and housing information for the American Community Survey and the low-response score for the 2010 Decennial Census at the census tract and census block group levels.\nObjective: Predict which areas will have the lowest response rates to the 2020 Decennial Census.\nTools: Linear regression based on the top 25 predictors from gradient-boosted trees.\nResults: Unclear but we’ll see more in these notes!\n\n\n20.2.1 Hard to Count Score\nThe US Census Bureau develops planning databases to prepare for decennial censuses. This database has many uses, but one use is to predict the census tracts and census block groups that will have the worst response rates for the decennial census.\nThis exercise began with the Hard to Count Score (Antonio Bruce, Robinson, and Sanders 2001; Anotnio Bruce and Robinson 2003). The authors used theory to develop an index of twelve variables correlated with non response and under counting to sort census tracts and plan for the next decennial census.\n\nRenter occupied units\nUnmarried\nVacant units\nMulti-unit structures\nBelow poverty\nNo high school graduate\nDifferent housing unit 1 year ago\nPublic assistance\nUnemployed\nCrowded units\nLinguistically isolated households\nNo phone service\n\nThis is clearly a predictive task but the authors brought a traditional social sciences approach.\n\n\n20.2.2 Kaggle Competition\nIn 2012, the US Census Bureau crowd sourced a predictive modeling competition on Kaggle (Erdman and Bates 2014, 2017). The competition was motivated by the America COMPETES act. Kaggle competitions allow teams to compete in predictive modeling tasks for cash prizes.\nThe Census Bureau’s Kaggle Competition tried to answer the question, “Which statistical model best predicts 2010 Census mail return rates?”\nParticipants use the 2012 Block-Group-Level Planning Database and were evaluated using mean squared error weighted by 2010 population.1\n244 teams and individuals competed. Bill Bame, a software develop from Maryland, won the competition with a mean squared error of 2.60. The top three finished use ensembled tree-based models like random forests and boosted trees (Erdman and Bates 2014).\n\n\n20.2.3 The Low-Response Score\nThe best models in the Kaggle competition did not directly meet the needs of the US Census Bureau because the models were “black box” and included auxiliary data.\nInstead, the Census Bureau took the 25 “most important” variables from the winning model and used multiple linear regression to construct and evaluate the low response score (Erdman and Bates 2014, 2017). This increased the MSE of the model used to fit the LRS but increased the interpretability.\nFollow-up work seeks to improve the model performance of an interpretable model for this exact implementation.\n\n\n20.2.4 LRS Set Up\nWe will work with the tract-level PDB instead of the block-group level PDB to simplify computation. The PDB contains more than 500 variables. For now, we only consider the top 25 variables to further simplify computation.\n\npdb_small &lt;- read_csv(here::here(\"data\", \"pdb_small.csv\"))\n\nRows: 69650 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): State_name, County_name\ndbl (24): Low_Response_Score, non_return_rate, Renter_Occp_HU_ACS_13_17, Pop...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(20231114)\npdb_split &lt;- initial_split(pdb_small, prop = 0.8)\n\npdb_train &lt;- training(pdb_split)\n\nLastly, we set up \\(v\\)-fold cross validation. We only use five folds because some of the models take a long time to fit to the data.\n\npdb_folds &lt;- vfold_cv(data = pdb_train, v = 5)\n\n\n\n20.2.5 Linear Regression\nWe first consider multiple linear regression. We need a recipe, a model, and a workflow to fit the model five times and evaluate its predictive accuracy.\nThe data set contains a few variables that shouldn’t be included in the model. We use add_role() to turn them into “ids” and then step_rm(has_role(\"id\")) to remove them from consideration.\n\nlm_rec &lt;- recipe(non_return_rate ~ ., pdb_train) |&gt;\n  add_role(State_name, County_name, Low_Response_Score, new_role = \"id\") |&gt;\n  step_rm(has_role(\"id\"))\n\nlm_mod &lt;- linear_reg() |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  set_engine(engine = \"lm\")\n\nlm_wf &lt;- workflow() |&gt;\n  add_recipe(lm_rec) |&gt;\n  add_model(lm_mod)\n\nlm_resamples &lt;- lm_wf |&gt;\n  fit_resamples(resamples = pdb_folds)\n\nThe model has good, but not great performance.\n\nlm_resamples |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   5.33      5 0.0580  Preprocessor1_Model1\n2 rsq     standard   0.485     5 0.00906 Preprocessor1_Model1\n\n\nWe can use last_fit() to fit the model to all of the training data and extract_fit_parsnip() to examine the estimated coefficients.\n\nlm_wf |&gt;\n  last_fit(pdb_split) |&gt;\n  extract_fit_parsnip()\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                (Intercept)     Renter_Occp_HU_ACS_13_17  \n               25.852142128                  0.000527581  \n        Pop_18_24_ACS_13_17       Female_No_HB_ACS_13_17  \n                0.001415666                  0.000738697  \n   NH_White_alone_ACS_13_17         Pop_65plus_ACS_13_17  \n               -0.000940139                 -0.001967564  \nRel_Child_Under_6_ACS_13_17              Males_ACS_13_17  \n                0.001897582                  0.000884922  \n MrdCple_Fmly_HHD_ACS_13_17          Pop_25_44_ACS_13_17  \n               -0.002668457                  0.001216952  \n Tot_Vacant_Units_ACS_13_17            College_ACS_13_17  \n                0.002089677                 -0.000344927  \n      Med_HHD_Inc_ACS_13_17          Pop_45_64_ACS_13_17  \n               -0.000097679                  0.001730143  \n     HHD_Moved_in_ACS_13_17           Hispanic_ACS_13_17  \n                0.003871744                 -0.000371730  \n      Single_Unit_ACS_13_17    Diff_HU_1yr_Ago_ACS_13_17  \n               -0.002632994                 -0.000848577  \n         Pop_5_17_ACS_13_17       NH_Blk_alone_ACS_13_17  \n                0.001696031                  0.000546687  \n    Sngl_Prns_HHD_ACS_13_17        Not_HS_Grad_ACS_13_17  \n               -0.004322504                 -0.000210310  \n  Med_House_Value_ACS_13_17  \n                0.000009283  \n\n\n\n\n20.2.6 Regression Trees\nNext, we consider regression trees.\n\ndt_rec &lt;- recipe(non_return_rate ~ ., pdb_train) |&gt;\n  add_role(State_name, County_name, Low_Response_Score, new_role = \"id\") |&gt;\n  step_rm(has_role(\"id\"))\n\ndt_mod &lt;- decision_tree(cost_complexity = 0.001) |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  set_engine(engine = \"rpart\")\n\ndt_wf &lt;- workflow() |&gt;\n  add_recipe(dt_rec) |&gt;\n  add_model(dt_mod)\n\ndt_resamples &lt;- dt_wf |&gt;\n  fit_resamples(resamples = pdb_folds)\n\nAgain, the model has good, but not great performance.\n\ndt_resamples |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   5.30      5 0.0334  Preprocessor1_Model1\n2 rsq     standard   0.490     5 0.00384 Preprocessor1_Model1",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ensembling</span>"
    ]
  },
  {
    "objectID": "22_ensembling.html#ensembling",
    "href": "22_ensembling.html#ensembling",
    "title": "20  Ensembling",
    "section": "20.3 Ensembling",
    "text": "20.3 Ensembling\nSimple predictive models like linear regression, KNN, and regression trees are intuitive but are often outperformed by more complicated tree-based, ensembled predictive models like random forests and XGBoost.\n\n\n\n\n\n\nEnsembling\n\n\n\nEnsembling combine predictions from multiple models into one prediction.\n\n\nBagging, boosting, and stacking are three strategies for ensembling. We will cover bagging and boosting in this chapter. Random forests are an implementation bagging. XGBoost is an implementation of boosting.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ensembling</span>"
    ]
  },
  {
    "objectID": "22_ensembling.html#bagging",
    "href": "22_ensembling.html#bagging",
    "title": "20  Ensembling",
    "section": "20.4 Bagging",
    "text": "20.4 Bagging\n\n\n\n\n\n\nBagging\n\n\n\nBagging, short for bootstrap aggregation, is a general purpose ensembling method that reduces the variance of a predictive model by training the model on many bootstrap samples2 and then ensembling the predictions.\nThe mean is typically used to ensemble in regression applications and majority vote is typically use to ensemble in classification applications.\n\n\nAveraging a set of observations to reduce variance is a fundamental statistical idea. Recall the standard error of the sample mean. Suppose we have a set of data \\(x_1, x_2, ...x_n\\). Let each each observations be independent and have variance \\(\\sigma^2\\). The variance of the mean \\(\\bar{x}\\) is \\(\\frac{\\sigma^2}{n}\\). Averaging the observations reduces variance. Averaging even more observations further reduces variance but at a diminishing rate.\nIt is possible to reduce variance error by training many predictive models on different sets of data and then ensembling the predictions.\nWe typically don’t have enough data to train models on partitions of the data, so we use bootstrap sampling.\n\n\n\n\n\n\nBagging Algorithm\n\n\n\n\nbootstrap sample from the data \\(B\\) times.\nfor the \\(b^{th}\\) bootstrap sample, fit a predictive model.\nAverage the predicted values.\n\n\n\nIf \\(\\hat{f}^b(\\vec{x})\\) is the model trained on the \\(b^{th}\\) bootstrap sample, then\n\\[\\hat{f}_\\text{bagged}(\\vec{x}) = \\frac{1}{B} \\sum_{b = 1}^B \\hat{f}^b(\\vec{x})\\]\nBagging works particularly well with high-variance models like regression trees.\nWe will first demonstrate bagged trees, which use the following algorithm:\n\n\n\n\n\n\nBagged Trees Algorithm\n\n\n\n\nBootstrap sampling from the data \\(B\\) times.\nFor the \\(b^{th}\\) bootstrap sample, fit a regression tree.\nTo make a prediction, calculate the mean of the \\(B\\) predicted values.\n\n\n\nBagged trees have four hyperparameters, but three of the hyperparameters are from regression trees:\n\nThe number of trees\nThe cost-complexity parameter\nThe maximum tree depth\nThe minimum number of observations in a terminal node\n\nIn practice, the number of trees does not need to be tuned and using too many trees will not lead to a model that is overfit. Typically, the number of trees can be set in the 100 to 500 range based on computational limitations.\nConsider the simulated data and the predicted values for different numbers of trees.\n\n\nCode\nlibrary(tidymodels)\n\nset.seed(20201004)\n\nx &lt;- runif(n = 1000, min = 0, max = 10)\n\ndata1 &lt;- bind_cols(\n  x = x,\n  y = 10 * sin(x) + x + 20 + rnorm(n = length(x), mean = 0, sd = 2)\n)\n\nset.seed(20201007)\n\n# create a split object\ndata1_split &lt;- initial_split(data = data1, prop = 0.75)\n\n# create the training and testing data\ndata1_train &lt;- training(x = data1_split)\ndata1_test  &lt;- testing(x = data1_split)\n\nggplot() +\n  geom_point(data = data1_train, aes(x = x, y = y), alpha = 0.25) +\n  labs(\n    title = \"Example 1 Data\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nSimulated data\n\n\n\n\nThe syntax for bagged trees is slightly different than other models and requires the use of library(baguette). Here, times is the number of bootstraps to use for the algorithm. When times = 1, then the algorithm is one regression tree trained on a bootstrap sample of the data.\nNotice how the predicted value line smooths out because of the averaging of many trees.\n\nlibrary(baguette)\n\nbagged_trees1 &lt;- bag_tree(cost_complexity = 0.001) |&gt;\n  set_engine(\"rpart\", times = 1) |&gt; # 1 ensemble member\n  set_mode(\"regression\") |&gt; \n  fit(y ~ ., data = data1)\n\nbagged_trees2&lt;- bag_tree(cost_complexity = 0.001) |&gt;\n  set_engine(\"rpart\", times = 25) |&gt; # 25 ensemble members \n  set_mode(\"regression\") |&gt; \n  fit(y ~ ., data = data1)\n\nbagged_trees3 &lt;- bag_tree(cost_complexity = 0.001) |&gt;\n  set_engine(\"rpart\", times = 100) |&gt; # 100 ensemble members \n  set_mode(\"regression\") |&gt; \n  fit(y ~ ., data = data1)\n\nbagged_trees4 &lt;- bag_tree(cost_complexity = 0.001) |&gt;\n  set_engine(\"rpart\", times = 500) |&gt; # 500 ensemble members \n  set_mode(\"regression\") |&gt; \n  fit(y ~ ., data = data1)\n\n  # create a grid of predictions\nnew_data &lt;- tibble(x = seq(0, 10, 0.1))\n\npredictions_grid &lt;- tibble(\n  x = seq(0, 10, 0.1),\n  `Trees = 1` = predict(object = bagged_trees1, new_data = new_data)$.pred,\n  `Trees = 25` = predict(object = bagged_trees2, new_data = new_data)$.pred,\n  `Trees = 100` = predict(object = bagged_trees3, new_data = new_data)$.pred,\n  `Trees = 500` = predict(object = bagged_trees4, new_data = new_data)$.pred\n) |&gt;\n  pivot_longer(-x, names_to = \"model\", values_to = \".pred\")\n\nggplot() +\n  geom_point(data = data1_train, aes(x = x, y = y), alpha = 0.25) +\n  geom_path(data = predictions_grid, aes(x = x, y = .pred), color = \"red\") +\n  facet_wrap(~model) +\n  labs(\n    title = \"Example 1: Data with Predictions (Bagged Regression Trees)\",\n    subtitle = \"Prediction in red\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s use bagged trees to predict the low response rate from the Census PDB example.\n\nbagged_trees_mod &lt;- bag_tree(cost_complexity = 0.001) |&gt;\n  set_engine(engine = \"rpart\", times = 60) |&gt;\n  set_mode(mode = \"regression\") \n\nbagged_trees_mod_wf &lt;- workflow() |&gt;\n  add_recipe(recipe = dt_rec) |&gt;\n  add_model(spec = bagged_trees_mod)\n\nbagged_trees_resamples &lt;- bagged_trees_mod_wf |&gt;\n  fit_resamples(resamples = pdb_folds)\n\ncollect_metrics(bagged_trees_resamples)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   5.04      5 0.0389  Preprocessor1_Model1\n2 rsq     standard   0.541     5 0.00472 Preprocessor1_Model1",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ensembling</span>"
    ]
  },
  {
    "objectID": "22_ensembling.html#random-forests",
    "href": "22_ensembling.html#random-forests",
    "title": "20  Ensembling",
    "section": "20.5 Random Forests",
    "text": "20.5 Random Forests\n\nAveraging many highly correlated quantities does not lead to as large a reduction in variance as averaging many uncorrelated quantities. (James et al. 2017)\n\nThe models fit on bootstrap samples of data are often highly correlated. Random forests, one of the most popular bagging algorithms, aims to decorrelate trees and reduce variance error more than simple bagging.\nRandom forests are identical to bagged trees, except each time a split is considered in a regression tree, RF only considers a subset of predictors instead of all predictors. By intentionally making each tree a little worse, the algorithm typically makes better predictions.\n\n\n\n\n\n\nRandom Forests Algorithm\n\n\n\n\nBootstrap sampling from the data \\(trees\\) times.\nFor the \\(i^{th}\\) bootstrap sample, fit a regression tree.\nWhen fitting the regression tree, only consider mtry predictors for each split. Stop splitting the data if min_n is reached in a node.\nTo make a prediction, calculate the mean of the \\(trees\\) predicted values.\n\n\n\nmtry, the number of predictors considered at each split, typically defaults to \\(\\lfloor\\sqrt{p}\\rfloor\\), where \\(p\\) is the total number of predictors. mtry is a hyperparameter that can be tuned and the optimal value typically depends on the number of “useful” predictors in data set.\n\n\n\n\n\n\nRegularization\n\n\n\nRegularization is a change to a loss function or algorithm intended to intentionally reduce the intensity of a model fit to reduce variance error.\nLASSO regression (L-1 regularization) and Ridge regression (L-2 regularization) are two popular regularization techniques.\n\n\nRandom forests are regularized models, and have similarities to linear regression with ridge regression, even though it is a non-parametric model.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nWe will skip random forests for the simulated data example. Why doesn’t it make sense to use a random forest model in this case?\n\n\n\nAs \\(mtry \\to p\\), RF converges with bagged trees.\nLet’s use random forests to predict the low response rate from the Census PDB example.\n\n\nCode\nrf_rec &lt;- recipe(non_return_rate ~ ., pdb_train) |&gt;\n  add_role(State_name, County_name, Low_Response_Score, new_role = \"id\") |&gt;\n  step_rm(has_role(\"id\"))\n\nrf_mod &lt;- rand_forest(\n  mtry = tune(),\n  min_n = tune(),\n  trees = 200\n  ) |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  set_engine(\n    engine = \"ranger\", \n    importance = \"impurity\",\n    num.threads = 4\n  )\n\nrf_wf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_mod)\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(1, 15)),\n  min_n(range = c(1, 15)),\n  levels = 5\n)\n\nrf_resamples &lt;- tune_grid(\n  rf_wf,\n  resamples = pdb_folds,\n  grid = rf_grid\n)\n\ncollect_metrics(rf_resamples)\n\nshow_best(rf_resamples)\n\nautoplot(rf_resamples)\n\n\n\nrf_rec &lt;- recipe(non_return_rate ~ ., pdb_train) |&gt;\n  add_role(State_name, County_name, Low_Response_Score, new_role = \"id\") |&gt;\n  step_rm(has_role(\"id\"))\n\n# we selected these hyperparameters with tuning and cross validation\nrf_mod &lt;- rand_forest(\n  trees = 200,\n  mtry = 11,\n  min_n = 4\n) |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  set_engine(\n    engine = \"ranger\", \n    importance = \"impurity\",\n    num.threads = 4\n  )\n\nrf_wf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_mod)\n\nrf_resamples &lt;- rf_wf |&gt;\n  fit_resamples(resamples = pdb_folds)\n\nThe random forest results are great.\n\nrf_resamples |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.51      5 0.0461  Preprocessor1_Model1\n2 rsq     standard   0.631     5 0.00571 Preprocessor1_Model1\n\n\n\n20.5.1 Interpretability\nIt is important to understand the process for fitting models, how the nature of the data affects fitted models, and how fitted models generate predictions. Models that lack these features are often called “black box.”\n\n\n\n\n\n\nGlobal Interpretability\n\n\n\nThe ability to understand overall model behavior for a fitted predictive modeling algorithm.\n\n\n\n\n\n\n\n\nLocal Interpretability\n\n\n\nThe ability to understand individual predictions for a fitted predictive modeling algorithm.\n\n\nLinear regression is easy to interpret because the fitting process is simple and linear regression models have a finite number of clear regression coefficients. Regression trees result in a clear tree that often simple enough to include in the New York Times.\nThe shift from linear regression and regression trees, which are locally and globally interpretable to ensembled models sacrifices model interpretability. Random forests return neither simple regression coefficients or a clear, visualized tree.\nInterpretable Machine Learning is a good resource for learning more.\n\n\n20.5.2 Variable Importance\nLinear regression returns coefficients and regression trees return trees that can be visualized. Bagged trees and random forests return neither.\n\n\n\n\n\n\nVariable Importance\n\n\n\nVariable importance is the contribution each predictor has to a fitted model.\n\n\nVariable importance is an alternative approach that aims to capture the why of a model and prediction. For linear regression, variable importance is typically just the absolute value of the estimated coefficient.\nFor regression with random forests, variable importance is the reduction in mean squared error caused by the addition of each predictor across all of the trees. It can be imprecise for categorical predictors, but is a valuable tool for understanding a random forest model.\nHere we fit the random forest model on all of the training data and then visualize the variable importance.\n\nrf_final &lt;- rf_wf |&gt;\n  last_fit(pdb_split) \n\nrf_final |&gt;\n  extract_fit_parsnip() |&gt;\n  vip(num_features = 20) \n\n\n\n\n\n\n\n\nIt is common to rescale variable importance for random forests so the most important variable has a value of 1.\n\nrf_final |&gt;\n  extract_fit_parsnip() |&gt;\n  vip(num_features = 20) %&gt;%\n  .$data |&gt;\n  mutate(\n    Importance = Importance / max(Importance),\n    Variable = fct_reorder(.x = Importance, .f = Variable)\n  ) |&gt;\n  ggplot(aes(Importance, Variable)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n20.5.3 Parallelization\n\n\n\n\n\n\nParallelization\n\n\n\nParallelization breaks large computing tasks into smaller computing tasks and then executes those tasks on separate processors or cores at the same time.\nCoordinating the parallelization requires extra computation but concurrently running the task on multiple cores can often save time.\n\n\nComputers have a limited number of cores. The following code counts the number of available cores.\n\nparallel::detectCores()\n\n[1] 10\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not use all available cores. Only use a subset of available cores so other computations (e.g. Spotify and email) can continue on your computer.\n\n\nWe will focus on two forms of parallelization. First, parallelization is simple with bagged models because trees are independently grow. Simply include num.threads in the set_engine() call.\n\nset_engine(\n    engine = \"ranger\", \n    importance = \"impurity\",\n    num.threads = 4\n  )\n\nParallelization is more difficult with boosted models, which are sequentially trained. The hyperparameter tuning later in this chapter uses a more complex socket parallelization with the following setup. This should detect a backend built into library(tidymodels) and parallelize the hyperparameter tuning with an entire model fit allocated to each core.\n\nlibrary(doFuture)\nlibrary(doParallel)\n\n# detect the number of cores\nall_cores &lt;- parallel::detectCores(logical = FALSE)\n\n# set up the parallel backend using futures\nregisterDoFuture()\n\n# set up socket parallelization\ncl &lt;- makeCluster(all_cores - 4L)\n\n# plan for parallelization\nplan(cluster, workers = cl)",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ensembling</span>"
    ]
  },
  {
    "objectID": "22_ensembling.html#boosting",
    "href": "22_ensembling.html#boosting",
    "title": "20  Ensembling",
    "section": "20.6 Boosting",
    "text": "20.6 Boosting\n\nBoosting appears to dominate bagging on most problems, and became the preferred choice. (Hastie, Tibshirani, and Friedman 2009)\n\n\n\n\n\n\n\nBoosting\n\n\n\nBoosting is a general purpose ensembling method that reduces the bias of a predictive model by sequentially fitting predictive models using information from the previous iteration of the sequence. For regression, the \\(i^{th}\\) predictive model is typically fit on the residuals from the \\((i - 1)^{th}\\) predictive model.\n\n\n\n\n\n\n\n\nBoosting Algorithm for Regression\n\n\n\n\nSet \\(\\hat{f}(\\vec{x}) = 0\\) and \\(r_i = y_i\\) where \\(r_i\\) is the \\(i^{th}\\) residual and \\(y_i\\) is the observed value for the \\(i^{th}\\) observation in the training data.\nFor \\(b = 1, 2, ..., B\\),\n\nFit a predictive model \\(\\hat{f}^b\\) to the data \\((\\mathbf{X}, \\vec{r})\\).\nUpdate \\(\\hat{f}\\) by adding a weighted model\n\n\n\\[\\hat{f}(\\vec{x}) \\leftarrow \\hat{f}(\\vec{x}) + \\lambda\\hat{f}^b(\\vec{x})\\]\n(c) Update the residuals\n\\[r_i \\leftarrow r_i - \\lambda\\hat{f}^b(\\vec{x})\\]\n\nOutput the final model\n\n\\[\\hat{f}(\\vec{x}) = \\sum_{b = 1}^B \\lambda\\hat{f}^b(\\vec{x})\\]",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ensembling</span>"
    ]
  },
  {
    "objectID": "22_ensembling.html#xgboost",
    "href": "22_ensembling.html#xgboost",
    "title": "20  Ensembling",
    "section": "20.7 XGBoost",
    "text": "20.7 XGBoost\nXGBoost is a specific, tree-based boosting algorithm that frequently wins predictive modeling competitions.\n\n\n\n\n\n\nXGBoost Algorithm for Regression\n\n\n\n\nSet \\(\\hat{f}(\\vec{x}) = 0\\) and \\(r_i = y_i\\) where \\(r_i\\) is the \\(i^{th}\\) residual and \\(y_i\\) is the observed value for the \\(i^{th}\\) observation in the training data.\nFor \\(b = 1, 2, ..., B\\),\n\nFit a tree with \\(d\\) splits \\(\\hat{f}^b\\) to the data \\((\\mathbf{X}, \\vec{r})\\).\nUpdate \\(\\hat{f}\\) by adding a new tree\n\n\n\\[\\hat{f}(\\vec{x}) \\leftarrow \\hat{f}(\\vec{x}) + \\lambda\\hat{f}^b(\\vec{x})\\]\n(c) Update the residuals\n\\[r_i \\leftarrow r_i - \\lambda\\hat{f}^b(\\vec{x})\\]\n\nOutput the final model\n\n\\[\\hat{f}(\\vec{x}) = \\sum_{b = 1}^B \\lambda\\hat{f}^b(\\vec{x})\\]\n\n\nConsider a few lessons from James et al. (2017):\n\nSetting \\(B\\) too large can slowly lead to overfitting for XGBoost. Bagging and random forests do not overfit with large \\(B\\).\n\n\\(\\lambda\\) is the learning rate and is typically near 0.01 or 0.001. Smaller values of \\(\\lambda\\) require very large \\(B\\).\n\\(d\\), the number of splits in each tree, can be set as low as \\(d = 1\\). In this case, each tree is only a single split or “stump.”\n\nThe performance of XGBoost is very sensitive to hyperparameter tuning. In general, fitting the model is computationally very expensive.\nXGBoost does not parallelize as easily as random forests because trees are fit sequentially instead of independently. It is possible to parallelize hyperparameter tuning.\n\n\nCode\nlibrary(doFuture)\nlibrary(doParallel)\n\n# detect the number of cores\nall_cores &lt;- parallel::detectCores(logical = FALSE)\n\n# set up the parallel backend using futures\nregisterDoFuture()\n\n# set up socket parallelization\ncl &lt;- makeCluster(all_cores - 4L)\n\n# plan for parallelization\nplan(cluster, workers = cl)\n\nxgb_spec &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(), mtry = tune(),         ## randomness\n  learn_rate = tune()                          ## step size\n) |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\nset.seed(1)\nxgb_grid &lt;- grid_latin_hypercube(\n  tree_depth(),\n  min_n(range = c(2, 10)),\n  loss_reduction(range = c(-5, -3)),\n  sample_size = sample_prop(),\n  mtry(range = c(1, 10)),\n  learn_rate(range = c(-5, -1)),\n  size = 30\n)\n\nxgb_wf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(xgb_spec)\n\nxgb_resamples &lt;- tune_grid(\n  xgb_wf,\n  resamples = pdb_folds,\n  grid = xgb_grid\n)\n\ncollect_metrics(xgb_resamples)\n\n\n\n# hyperparameters selected with cross validation and tuning\nxgb_spec &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = 12, \n  min_n = 8,\n  loss_reduction = 0.00007778243,                     \n  sample_size = 0.7604136, \n  mtry = 8,         \n  learn_rate = 0.01237174241                      \n) |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\nxgb_wf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(xgb_spec)\n\nxgb_resamples &lt;- xgb_wf |&gt;\n  fit_resamples(resamples = pdb_folds)\n\nThese results are the best.\n\ncollect_metrics(xgb_resamples)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.39      5 0.0499  Preprocessor1_Model1\n2 rsq     standard   0.649     5 0.00612 Preprocessor1_Model1",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ensembling</span>"
    ]
  },
  {
    "objectID": "22_ensembling.html#final-comparison",
    "href": "22_ensembling.html#final-comparison",
    "title": "20  Ensembling",
    "section": "20.8 Final Comparison",
    "text": "20.8 Final Comparison\nFinally, let’s compare the RMSE and \\(r\\)-squared for linear regression, regression trees, bagged trees, random forests (with hyperparameter tuning), and XGBoost (with hyperparameter tuning).\n\nbind_rows(\n  `Linear regression` = collect_metrics(lm_resamples) |&gt;\n    filter(.metric == \"rmse\"),\n  `Regression trees` = collect_metrics(dt_resamples) |&gt;\n    filter(.metric == \"rmse\"),  \n  `Bagged trees` = collect_metrics(bagged_trees_resamples) |&gt;\n    filter(.metric == \"rmse\"),\n  `Random forest` = collect_metrics(rf_resamples) |&gt;\n    filter(.metric == \"rmse\"),\n  `XGBoost` = collect_metrics(xgb_resamples) |&gt;\n    filter(.metric == \"rmse\"),\n  .id = \"model\"\n)\n\n# A tibble: 5 × 7\n  model             .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 Linear regression rmse    standard    5.33     5  0.0580 Preprocessor1_Model1\n2 Regression trees  rmse    standard    5.30     5  0.0334 Preprocessor1_Model1\n3 Bagged trees      rmse    standard    5.03     5  0.0380 Preprocessor1_Model1\n4 Random forest     rmse    standard    4.51     5  0.0461 Preprocessor1_Model1\n5 XGBoost           rmse    standard    4.39     5  0.0499 Preprocessor1_Model1\n\nbind_rows(\n  `Linear regression` = collect_metrics(lm_resamples) |&gt;\n    filter(.metric == \"rsq\"),  \n  `Regression trees` = collect_metrics(dt_resamples) |&gt;\n    filter(.metric == \"rsq\"),    \n  `Bagged trees` = collect_metrics(bagged_trees_resamples) |&gt;\n    filter(.metric == \"rsq\"),\n  `Random forest` = collect_metrics(rf_resamples) |&gt;\n    filter(.metric == \"rsq\"),  \n  `XGBoost` = collect_metrics(xgb_resamples) |&gt;\n    filter(.metric == \"rsq\"),\n  .id = \"model\"\n)\n\n# A tibble: 5 × 7\n  model             .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 Linear regression rsq     standard   0.485     5 0.00906 Preprocessor1_Model1\n2 Regression trees  rsq     standard   0.490     5 0.00384 Preprocessor1_Model1\n3 Bagged trees      rsq     standard   0.541     5 0.00455 Preprocessor1_Model1\n4 Random forest     rsq     standard   0.631     5 0.00571 Preprocessor1_Model1\n5 XGBoost           rsq     standard   0.649     5 0.00612 Preprocessor1_Model1\n\n\n\n\n\n\nBruce, Anotnio, and Gregory Robinson. 2003. “The Planning Database: Its Development and Use as an Effective Tool in Census 2000.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b2edb90180a9132f64f8287af2db92c031b5d40b.\n\n\nBruce, Antonio, Gregory Robinson, and Monique V. Sanders. 2001. “Hard-to-Count Scores and Broad Demographic Groups Associated with Patterns of Response Rates in Census 2000.” Proceedings of the Social Statistics Section, American Statistical Association.\n\n\nErdman, Chandra, and Nancy Bates. 2014. “The u.s. Census Bureau Mail Return Rate Challenge: Crowdsourcing to Develop a Hard-to-Count Score.” https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/rrs2014-08.pdf.\n\n\n———. 2017. “The Low Response Score (LRS).” Public Opinion Quarterly 81 (1): 144–56. https://doi.org/10.1093/poq/nfw040.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. An introduction to statistical learning: with applications in R. Corrected at 8th printing. Springer texts in statistics. New York Heidelberg Dordrecht London: Springer. https://doi.org/10.1007/978-1-4614-7138-7.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ensembling</span>"
    ]
  },
  {
    "objectID": "22_ensembling.html#footnotes",
    "href": "22_ensembling.html#footnotes",
    "title": "20  Ensembling",
    "section": "",
    "text": "The Kaggle competition website says mean absolute error weighed by 2010 population.↩︎\nThis is the same bootstrap sampling used for nonparametric statistical inference.↩︎",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ensembling</span>"
    ]
  },
  {
    "objectID": "23_regularization.html",
    "href": "23_regularization.html",
    "title": "21  Regularization",
    "section": "",
    "text": "21.1 Motivation\nThe previous chapter introduced the idea of regularization in the context of random forests. Regularization is a powerful technique for parametric regression models.\nThis section takes the PDB examples and explores three types of regularized regression models: ridge regression, LASSO regression, and elastic net regression.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "23_regularization.html#linear-regression",
    "href": "23_regularization.html#linear-regression",
    "title": "21  Regularization",
    "section": "21.2 Linear regression",
    "text": "21.2 Linear regression\nOrdinary least squares (OLS) linear regression generates coefficients that minimize the sum of squared residuals.\n\\[\\min(SSE) = \\min\\left(\\sum_{i = 1}^n (y_i - \\hat{y_i}) ^ 2\\right)\\]\nIt is impossible to solve for OLS when \\(p &gt; n\\) where \\(p\\) is the number of predictors and \\(n\\) is the number of observations. Additionally, OLS is unstable when data contain multicollinearity.\nLet’s fit a linear regression model on the PDB data set.\n\npdb_small &lt;- read_csv(here::here(\"data\", \"pdb_small.csv\"))\n\nset.seed(20231114)\npdb_split &lt;- initial_split(pdb_small, prop = 0.8)\n\npdb_train &lt;- training(pdb_split)\n\n\nlm_mod &lt;- linear_reg() |&gt;\n  set_mode(mode = \"regression\") |&gt;\n  set_engine(engine = \"lm\")\n\nlm_rec &lt;- recipe(non_return_rate ~ ., data = pdb_train) |&gt;\n  add_role(State_name, County_name, Low_Response_Score, new_role = \"id\") |&gt;\n  step_rm(has_role(\"id\"))\n\nlm_wf &lt;- workflow() |&gt;\n  add_model(spec = lm_mod) |&gt;\n  add_recipe(recipe = lm_rec)\n\nlm_wf |&gt;\n  fit(data = pdb_train) |&gt;\n  extract_fit_parsnip() |&gt;\n  # vi() |&gt;\n  tidy() |&gt;\n  print(n = Inf)\n\n# A tibble: 23 × 5\n   term                           estimate   std.error statistic   p.value\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                 25.9        0.109          238.   0        \n 2 Renter_Occp_HU_ACS_13_17     0.000528   0.000160         3.30 9.55e-  4\n 3 Pop_18_24_ACS_13_17          0.00142    0.000112        12.7  7.89e- 37\n 4 Female_No_HB_ACS_13_17       0.000739   0.000367         2.02 4.39e-  2\n 5 NH_White_alone_ACS_13_17    -0.000940   0.0000495      -19.0  3.84e- 80\n 6 Pop_65plus_ACS_13_17        -0.00197    0.000149       -13.2  1.47e- 39\n 7 Rel_Child_Under_6_ACS_13_17  0.00190    0.000355         5.34 9.17e-  8\n 8 Males_ACS_13_17              0.000885   0.000147         6.01 1.92e-  9\n 9 MrdCple_Fmly_HHD_ACS_13_17  -0.00267    0.000251       -10.7  1.81e- 26\n10 Pop_25_44_ACS_13_17          0.00122    0.000142         8.56 1.13e- 17\n11 Tot_Vacant_Units_ACS_13_17   0.00209    0.0000922       22.7  3.97e-113\n12 College_ACS_13_17           -0.000345   0.0000738       -4.68 2.93e-  6\n13 Med_HHD_Inc_ACS_13_17       -0.0000977  0.00000169     -57.9  0        \n14 Pop_45_64_ACS_13_17          0.00173    0.000160        10.8  2.51e- 27\n15 HHD_Moved_in_ACS_13_17       0.00387    0.000178        21.7  3.18e-104\n16 Hispanic_ACS_13_17          -0.000372   0.0000532       -6.99 2.77e- 12\n17 Single_Unit_ACS_13_17       -0.00263    0.0000800      -32.9  1.64e-235\n18 Diff_HU_1yr_Ago_ACS_13_17   -0.000849   0.0000978       -8.68 4.23e- 18\n19 Pop_5_17_ACS_13_17           0.00170    0.000145        11.7  9.76e- 32\n20 NH_Blk_alone_ACS_13_17       0.000547   0.0000565        9.68 3.92e- 22\n21 Sngl_Prns_HHD_ACS_13_17     -0.00432    0.000208       -20.7  3.62e- 95\n22 Not_HS_Grad_ACS_13_17       -0.000210   0.000138        -1.53 1.26e-  1\n23 Med_House_Value_ACS_13_17    0.00000928 0.000000224     41.4  0",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "23_regularization.html#regularization",
    "href": "23_regularization.html#regularization",
    "title": "21  Regularization",
    "section": "21.3 Regularization",
    "text": "21.3 Regularization\nRegularization/Penalization: To reduce the magnitude of parameters (coefficients).\nRegularization, or penalization, allows linear regression to work with very wide data, to generate stable estimates for data with multicollinearity, and to perform feature selection.\nFor regression, the idea is to add a penalty \\(P\\) to the optimization routine:\n\\[\\min(SSE + P) = \\min\\left(\\sum_{i = 1}^n (y_i - \\hat{y_i}) ^ 2 + P\\right)\\]",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "23_regularization.html#ridge-regression",
    "href": "23_regularization.html#ridge-regression",
    "title": "21  Regularization",
    "section": "21.4 Ridge Regression",
    "text": "21.4 Ridge Regression\n\\[\\min(SSE + P) = \\min\\left(\\sum_{i = 1}^n (y_i - \\hat{y_i}) ^ 2 + \\lambda\\sum_{j = 1}^p\\beta_j^2\\right)\\]\nRidge regression adds an L2 penalty to the optimization routine. The model has one hyperparameter, \\(\\lambda\\), which determines how much penalty to add. There is no penalty when \\(\\lambda = 0\\) (just OLS).\nAll variables should be centered and scaled (standardized) before estimation. Thus the coefficients will be in standardized units.\nRidge regression reduces coefficients but it does not eliminate coefficients.\n\n21.4.1 Ridge regression reduces but does not eliminate coefficients\n\nfit_ridge &lt;- function(data, penalty) {\n  \n  ridge_rec &lt;- recipe(non_return_rate ~ ., data = pdb_train) |&gt;\n    add_role(State_name, County_name, Low_Response_Score, new_role = \"id\") |&gt;\n    step_rm(has_role(\"id\")) |&gt;\n    step_normalize(all_predictors())\n  \n  ridge_mod &lt;- linear_reg(penalty = penalty, mixture = 0) |&gt;\n    set_mode(mode = \"regression\") |&gt;\n    set_engine(engine = \"glmnet\")  \n  \n  ridge_wf &lt;- workflow() |&gt;\n    add_recipe(recipe = ridge_rec) |&gt;\n    add_model(spec = ridge_mod)\n  \n  ridge_wf |&gt;\n    fit(data = pdb_train) |&gt;\n    extract_fit_parsnip() |&gt;\n    tidy() |&gt;\n    mutate(penalty = penalty)\n  \n}\n\nridge_fit &lt;- seq(0, 50, 1) |&gt;\n  map_dfr(.f = ~ fit_ridge(pdb_train, .x))\n\nggplot() +\n  geom_line(\n    data = filter(ridge_fit, term != \"(Intercept)\"),\n    mapping = aes(penalty, estimate, group = term),\n    alpha = 0.4\n  ) +\n  geom_point(\n    data = filter(ridge_fit, term != \"(Intercept)\", penalty == 0),\n    mapping = aes(penalty, estimate),\n    color = \"red\"\n  )\n\n\n\n\n\n\n\n\nNote that regularized regression models expect standardized predictors (mean = 0 and standard deviation = 1). This changes this starting estimated parameters.",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "23_regularization.html#lasso-regression",
    "href": "23_regularization.html#lasso-regression",
    "title": "21  Regularization",
    "section": "21.5 LASSO Regression",
    "text": "21.5 LASSO Regression\n\\[\\min(SSE + P) = \\min\\left(\\sum_{i = 1}^n (y_i - \\hat{y_i}) ^ 2 + \\lambda\\sum_{j = 1}^p|\\beta_j|\\right)\\]\nLeast Absolute Shrinkage and Selection Operator (LASSO) regression adds an L1 penalty to the optimization routine. The model has one hyperparameter, \\(\\lambda\\), which determines how much penalty to add. There is no penalty when \\(\\lambda = 0\\) (just OLS).\nAll variables should be centered and scaled (standardized) before estimation. Thus the coefficients will be in standardized units.\nLASSO regression can regularize coefficients all the way to zero.\n\n21.5.1 LASSO regression eliminates coefficients\n\nfit_lasso &lt;- function(data, penalty) {\n  \n  lasso_rec &lt;- recipe(non_return_rate ~ ., data = pdb_train) |&gt;\n    add_role(State_name, County_name, Low_Response_Score, new_role = \"id\") |&gt;\n    step_rm(has_role(\"id\")) |&gt;\n    step_normalize(all_predictors())\n  \n  lasso_mod &lt;- linear_reg(penalty = penalty, mixture = 1) |&gt;\n    set_mode(mode = \"regression\") |&gt;\n    set_engine(engine = \"glmnet\")  \n  \n  lasso_wf &lt;- workflow() |&gt;\n    add_recipe(recipe = lasso_rec) |&gt;\n    add_model(spec = lasso_mod)\n  \n  lasso_wf |&gt;\n    fit(data = pdb_train) |&gt;\n    extract_fit_parsnip() |&gt;\n    tidy() |&gt;\n    mutate(penalty = penalty)\n  \n}\n\nlasso_fit &lt;- seq(0, 3.5, 0.1) |&gt;\n  map_dfr(.f = ~ fit_lasso(pdb_train, .x))\n\nggplot() +\n  geom_line(\n    data = filter(lasso_fit, term != \"(Intercept)\"),\n    mapping = aes(penalty, estimate, group = term),\n    alpha = 0.4\n  ) +\n  geom_point(\n    data = filter(lasso_fit, term != \"(Intercept)\", penalty == 0),\n    mapping = aes(penalty, estimate),\n    color = \"red\"\n  )",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "23_regularization.html#elastic-net-regression",
    "href": "23_regularization.html#elastic-net-regression",
    "title": "21  Regularization",
    "section": "21.6 Elastic Net Regression",
    "text": "21.6 Elastic Net Regression\n\\[\\min(SSE + P) = \\min\\left(\\sum_{i = 1}^n (y_i - \\hat{y_i}) ^ 2 + \\lambda_1\\sum_{j = 1}^p\\beta_j^2 + \\lambda_2\\sum_{j = 1}^p|\\beta_j|\\right)\\]\nElastic net regression combines ridge regression and LASSO regression. It has two hyperparameters, \\(\\lambda_1\\) and \\(\\lambda_2\\). Sometimes the hyperparameters are \\(\\lambda\\) and mixture, which determines how much of \\(\\lambda\\) to apply to each penalty (i.e. mixture = 0 is ridge regression and mixture = 1 is LASSO regression).\nAll variables should be centered and scaled (standardized) before estimation. Thus the coefficients will be in standardized units.\nElastic net regression can perform feature selection, but in a less dramatic fashion than LASSO regression.\n\n21.6.1 Elastic net blends Ridge regression and LASSO regression",
    "crumbs": [
      "Supervised Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html",
    "href": "27_advanced-unsupervised-ml.html",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "",
    "text": "22.1 Review 1\nWe’ll start with a review of multivariate normal distributions. In particular, this exercise demonstrates the impact of the variance-covariance matrix on the shape of multivariate normal distributions.\nlibrary(mvtnorm)\n\nsigma1 &lt;- matrix(\n  c(1, 0,\n    0, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\nsigma2 &lt;- matrix(\n  c(1, 0.8,\n    0.8, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\nbind_rows(\n  independent = tibble(\n    V1 = rnorm(n = 1000),\n    V2 = rnorm(n = 1000)\n  ),\n  sigma1 = rmvnorm(\n    n = 1000, \n    sigma = sigma1\n  ) |&gt;\n    as_tibble(),\n  sigma2 = rmvnorm(\n    n = 1000, \n    sigma = sigma2\n  ) |&gt;\n    as_tibble(),\n  .id = \"source\"\n) |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point() +\n  facet_wrap(~ source)",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#sec-review6a",
    "href": "27_advanced-unsupervised-ml.html#sec-review6a",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "",
    "text": "Exercise 1\n\n\n\n\nLoad library(mvtnorm).\nCopy and paste the following code. This code will not obviously not run as is. We will add tibbles to independent, sigma1 and sigma2 in the steps below.\n\n\nbind_rows(\n  independent = ,\n  var_covar1 = ,\n  var_covar2 = ,\n  .id = \"source\"\n)\n\n\nCreate a tibble with V1 and V2. For both variables, use rnorm() to sample 1,000 observations from a standard normal distribution. Add the results to independent.\nUsing the following variance-covariance matrix, sample 1,000 observations from a multivariate-normal distribution. Add the results for sigma1 and use as_tibble().\n\n\nsigma1 &lt;- matrix(\n  c(1, 0,\n    0, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\n\nUsing the following variance-covariance matrix, sample 1,000 observations from a multivariate-normal distribution. Add the results for sigma2 and use as_tibble().\n\n\nsigma2 &lt;- matrix(\n  c(1, 0.8,\n    0.8, 1), \n  nrow = 2, ncol = 2, byrow = TRUE\n)\n\n\nCreate a scatter plot with V1 on the x-axis and V2 on the y-axis. Facet based on source.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#a-new-type-of-random-variable",
    "href": "27_advanced-unsupervised-ml.html#a-new-type-of-random-variable",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "22.2 A New Type of Random Variable",
    "text": "22.2 A New Type of Random Variable\nWe learned about common univariate and multivariate distributions. For each of the distributions, there are well-defined and straightforward ways to sample values from the distribution. We can also manipulate these distributions to calculate probabilities.\nThe real world is complicated, and we will quickly come across data where we struggle to find a common probability distributions.\nFigure Figure 22.1 shows a relative frequency histogram for the duration of eruptions at Old Faithful in Yellowstone National Park.\n\n# faithful is a data set built into R\nfaithful |&gt;\n  ggplot(aes(eruptions, y = after_stat(density))) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 22.1: Distribution of waiting times between eruptions at the Old Faithful geyser in Yellowstone National Park.\n\n\n\n\n\nThis distribution looks very complicated. But what if we break this distribution into pieces? In this case, what if we think of the distribution as a combination of two normal distributions?\n\n\nCode\n# show geyser as two normal distribution\nlibrary(mclust)\n\ngmm_geyser &lt;- Mclust(\n  data = dplyr::select(faithful, eruptions), \n  G = 2\n)\n\n\nbind_cols(\n  faithful,\n  cluster = gmm_geyser$classification\n) |&gt;\n  ggplot(aes(eruptions, y = after_stat(density), \n             fill = factor(cluster))) +\n  geom_histogram() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Variable\n\n\n\nA latent variable is a variable that isn’t directly observed but can be inferred through other variables and modeling. Sometimes the latent variable is meaningful but unobserved. Sometimes it isn’t meaningful.\nLatent variables are sometimes called hidden variables.\n\n\nBreaking complex problems into smaller pieces is good. These latent variables will allow us to do some cools things:\n\nSimply express complicated probability distributions\nMake inferences about complex populations\nCluster data\n\nIn this set of notes, we’ll use latent variables to\n\nConstruct mixture distributions\nCluster data\n\nLet’s consider a “data generation story” different than anything we considered in Chapter 5. Instead of sampling directly from one known probability distribution, we will sample in two stages (Hastie, Tibshirani, and Friedman 2009).\n\nSample from a discrete probability distribution with \\(k\\) unique values (i.e. Bernoulli distribution when \\(k = 2\\) and categorical distribution when \\(k &gt; 2\\)).\nSample from one of \\(k\\) different distributions conditional on the outcome of step 1.\n\nThis new sampling procedure aligns closely with the idea of hierarchical sampling and hierarchical models. It is also sometimes called ancestral sampling (Bishop 2006, 430).\nThis two-step approach dramatically increases the types of distributions at our disposal because we are no longer limited to individual common univariate distributions like a single normal distribution or a single uniform distribution. The two-step approach is also the foundation of two related tools:\n\nMixture distributions: Distributions expressed as the linear combination of other distributions. Mixture distributions can be very complicated distributions expressed in terms of simple distributions with known properties.\nMixture modeling: Statistical inference about sub-populations made only with pooled data without labels for the sub populations.\n\nWith mixture distributions, we care about the overall distribution and don’t care about the latent variables.\nWith mixture modeling, we use the overall distribution to learn about the latent variables/sub populations/clusters in the data.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#mixture-distributions",
    "href": "27_advanced-unsupervised-ml.html#mixture-distributions",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "22.3 Mixture Distributions",
    "text": "22.3 Mixture Distributions\n\n\n\n\n\n\nMixture Distribution\n\n\n\nA mixture distribution is a probabilistic model that is a linear combination of common probability distributions.\nA discrete mixture distribution can be expressed as\n\\[\np_{mixture}(x) = \\sum_{k = 1}^K \\pi_kp(x)\n\\]\nwhere \\(K\\) is the number of mixtures and \\(\\pi_k\\) is the weight of each PMF included in the mixture distribution.\nA continuous mixture distribution can be expressed as\n\\[\np_{mixture}(x) = \\sum_{k = 1}^K \\pi_kf(x)\n\\]\nwhere \\(K\\) is the number of mixtures and \\(\\pi_k\\) is the weight of each PDF included in the mixture distribution.\n\n\n\n22.3.1 Example 1\nLet’s consider a concrete example with a Bernoulli distribution and two normal distributions.\n\nSample \\(X \\sim Bern(p = 0.25)\\)\nSample from \\(Y \\sim N(\\mu = 0, \\sigma = 2)\\) if \\(X = 0\\) and \\(Y \\sim (\\mu = 4, \\sigma = 2)\\) if \\(X = 1\\).\n\nNow, let’s sample from a Bernoulli distribution and then sample from one of two normal distributions using R code.\n\ngenerate_data &lt;- function(n) {\n  \n  step1 &lt;- sample(x = c(0, 1), size = n, replace = TRUE, prob = c(0.75, 0.25))\n  \n  step1 &lt;- sort(step1)\n  \n  step2 &lt;- c(\n    rnorm(n = sum(step1 == 0), mean = 0, sd = 2),\n    rnorm(n = sum(step1 == 1), mean = 5, sd = 1)\n  )\n  \n  tibble::tibble(\n    x = step1,\n    y = step2\n  )\n\n}\n\nset.seed(1)\n\ngenerate_data(n = 1000) |&gt;\n  ggplot(aes(x = y, y = after_stat(density))) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThis marginal distribution looks complex but the process of creating the marginal distribution is simple.\nIn fact, consider this quote from Bishop (2006) (Page 111):\n\nBy using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.\n\n\n\n\n\n\n\nComponent\n\n\n\nA component is each common probability distribution that is combined to create a mixture distribution. For example, a mixture of two Gaussian distributions has two components.\n\n\n\n\n\n\n\n\nMixing Coefficient\n\n\n\nA mixing coefficient is the probability associated with a component with a component in a mixture distribution. Mixing coefficients must sum to 1.\nWe’ll use \\(\\pi_k\\) for population mixing coefficients and \\(p_k\\) for sample mixing coefficients. Mixing coefficients are also called mixing weights and mixing probabilities.\n\n\nMixture distributions are often overparameterized, which means they have an excessive number of parameters. For a univariate mixture of normals with \\(k\\) components, we have \\(k\\) means, \\(k\\) standard deviations, and \\(k\\) mixing coefficients.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nSample 1,000 observations from a mixture of three normal distributions with the following parameters:\n\n\n\\(p_1 = p_2 = p_3\\)\n\\(\\mu_1 = -3\\), \\(\\mu_2 = 0\\), \\(\\mu_3 = 3\\)\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3 = 1\\)\n\n\nCreate a relative frequency histogram of the values.\n\n\n\n\n\n22.3.2 Example 2\nSuppose we used statistical inference to infer some parameters for the geysers example above. We will describe how to estimate these paramaters later.\n\n\\(p_1 =\\) 0.3485696 and \\(p_2 =\\) 0.6514304\n\\(\\bar{x_1} =\\) 2.0189927 and \\(\\bar{x_2} =\\) 4.2737083\n\\(s_1 =\\) 0.2362355and \\(s_2 =\\) 0.4365146\n\nThe mixture density is\n\\[\nf_{mixture}(x) = p_1f(x|\\mu = \\bar{x_1}, \\sigma = s_1) + p_2f(x|\\mu = \\bar{x_2},\\sigma=s_2)\n\\tag{22.1}\\]\n\ngeyser_density &lt;- function(x, model) {\n  \n  probs &lt;- model$parameters$pro\n  \n  d1 &lt;- dnorm(\n    x, \n    mean =  model$parameters$mean[1], \n    sd = sqrt(model$parameters$variance$sigmasq[1])\n  )\n  \n  d2 &lt;- dnorm(\n    x, \n    mean =  model$parameters$mean[2], \n    sd = sqrt(model$parameters$variance$sigmasq[2])\n  )\n  \n  probs[1] * d1 + probs[2] * d2\n  \n}\n\nmm &lt;- tibble(\n  x = seq(0, 5, 0.01),\n  f_x = map_dbl(x, geyser_density, model = gmm_geyser)\n) \n\nggplot() +\n  geom_histogram(data = faithful, mapping = aes(x = eruptions, y = after_stat(density))) +\n  geom_line(data = mm, mapping = aes(x, f_x), color = \"red\") + \n  labs(\n    title = \"\",\n    subtitles = \"Observed data in black, inferred distribution in red\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#sec-review6b",
    "href": "27_advanced-unsupervised-ml.html#sec-review6b",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "22.4 Review #2",
    "text": "22.4 Review #2\n\n22.4.1 Multivariate Normal Distribution\nThe multivariate normal distribution is a higher-dimensional version of the univariate normal distribution. The MVN distribution has a vector of means of length \\(k\\) and a \\(k\\)-by-\\(k\\) variance-covariance matrix.\nWe show that a random vector is multivariate normally distributed with\n\\[\n\\vec{X} \\sim \\mathcal{N}(\\vec\\mu, \\boldsymbol\\Sigma)\n\\tag{22.2}\\]\nThe PDF of a multivariate normally distributed random variable is\n\\[\nf(x) = (2\\pi)^{-k/2}det(\\boldsymbol\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\vec{x} - \\vec\\mu)^T\\boldsymbol\\Sigma^{-1}(\\vec{x} - \\vec\\mu)\\right)\n\\tag{22.3}\\]\n\n\n22.4.2 K-Means Clustering\nK-Means Clustering is a heuristic-based approach to finding latent groups in data. The algorithm assigns each observation to one and only one group through a two step iteration that minimizes the Euclidean distance between observations and centroids for each group.\n\nSetupStep 1Step 2Step 3Step 4Step 5\n\n\nConsider the following data set.\n\n\nCode\ndata &lt;- tibble(x = c(1, 2, 1, 4, 7, 10, 8),\n               y = c(5, 4, 4, 3, 7, 8, 5))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Randomly place K centroids in your n-dimensional vector space\n\n\nCode\ncentroids &lt;- tibble(x = c(2, 5),\n                  y = c(5, 5),\n                  cluster = c(\"a\", \"b\"))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Calculate the nearest centroid for each point using a distance measure\n\n\nCode\ncentroids &lt;- tibble(x = c(2, 5),\n                  y = c(5, 5),\n                  cluster = c(\"a\", \"b\"))\n\nggplot() +\n  geom_point(data = data, aes(x, y), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  geom_line(aes(x = c(4, 2), y = c(3, 5)), linetype = \"dashed\") +  \n  geom_line(aes(x = c(4, 5), y = c(3, 5)), linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Assign each point to the nearest centroid\n\n\nCode\ndata$cluster &lt;- c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\")\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids, aes(x, y, color = cluster), size = 4) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Recalculate the position of the centroids based on the means of the assigned points\n\n\nCode\ncentroids2 &lt;- data %&gt;%\n  group_by(cluster) %&gt;%\n  summarize(x = mean(x), y = mean(y))\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids, aes(x, y), size = 4, alpha = 0.25) +\n  geom_point(data = centroids2, aes(x, y, color = cluster), size = 4) +  \n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Repeat steps 2-4 until no points change cluster assignments\n\n\nCode\ndata$cluster &lt;- c(\"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\")\n\nggplot() +\n  geom_point(data = data, aes(x, y, color = cluster), size = 2) +\n  geom_point(data = centroids2, aes(x, y, color = cluster), size = 4) +  \n  scale_x_continuous(limits = c(0, 10)) +\n  scale_y_continuous(limits = c(0, 10)) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nUse library(tidyclust) to cluster the faithful data into three clusters.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#mixture-modelingmodel-based-clustering",
    "href": "27_advanced-unsupervised-ml.html#mixture-modelingmodel-based-clustering",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "22.5 Mixture Modeling/Model-Based Clustering",
    "text": "22.5 Mixture Modeling/Model-Based Clustering\nUntil now, we’ve assumed that we’ve known all parameters when working with mixture distributions. What if we want to learn these parameters/make inferences about these parameters?\nThe process of making inferences about latent groups is related to K-Means Clustering. While K-Means Clustering is heuristic based, mixture modeling formalize the process of making inferences about latent groups using probability models. Gaussian mixture models (GMM) are a popular mixture model.\n\n\n\n\n\n\nMixture Modeling\n\n\n\nMixture modeling is the process of making inferences about sub populations using data that contain sub population but no labels for the sub populations.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#gaussian-mixture-modeling-gmm",
    "href": "27_advanced-unsupervised-ml.html#gaussian-mixture-modeling-gmm",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "22.6 Gaussian Mixture Modeling (GMM)",
    "text": "22.6 Gaussian Mixture Modeling (GMM)\n\n\n\n\n\n\nGaussian Mixture Modeling (GMM)\n\n\n\nGaussian mixture modeling (GMM) is mixture modeling that uses normal and multivariate normal distributions.\n\n\n\n\n\n\n\n\nHard Assignment\n\n\n\nHard assignment assigns an observation in a clustering model to one and only one group.\n\n\n\n\n\n\n\n\nSoft Assignment\n\n\n\nSoft assignment assigns an observation in a clustering model to all groups with varying weights or probabilities.\n\n\n\n\n\n\n\n\nResponsibilities\n\n\n\nSoft assignments are quantified with responsibilities. Responsibilities are the probability that a given observation belongs to a given group. The soft assignments for an observation sum to 1.\nWe quantified responsibilities with \\(\\pi_k\\) for mixture distributions. Responsibilities are parameters we will infer during mixture modeling.\n\n\nThere are two main differences between K-Means Clustering and GMM.\n\nInstead of calculating Euclidean distance from each observation to each group centroid, we use multivariate normal distributions to calculate the probability that an observation belongs to each group.\n\nObservations close to the means of a mixture will have a high relative probability of belonging to that mixture.\nObservations far from the means of a mixture will have a low relative probability of belonging to that mixture.\n\nInstead of simply updating \\(k\\) group centroids, we must update \\(k\\) multivariate normal distributions. This requires calculating a vector of means and a variance-covariance matrix for each of the \\(k\\) groups.\n\n\n22.6.1 Example 3\nThe parameters in example 2 were estimated using GMM. Let’s repeat a similar exercise with the faithful using eruptions and waiting instead of just eruptions. We’ll assume there are three groups.\n\n# fit GMM\ngmm2_geyser &lt;- Mclust(faithful, G = 3)\n\nLet’s plot the multivariate normal distributions. Figure 22.2 shows the centroids (stars) and shapes (ellipses) of the distributions in black. The colors represent hard assignments to groups and the size of the points represent the uncertainty of the assignments with larger points having more uncertainty.\n\n# plot fitted model\nplot(gmm2_geyser, what = \"uncertainty\")\n\n\n\n\n\n\n\nFigure 22.2: Uncertainty plot from a GMM\n\n\n\n\n\nWe can also summarize the model with library(broom).\n\nlibrary(broom)\n\naugment(gmm2_geyser)\n\n# A tibble: 272 × 4\n   eruptions waiting .class .uncertainty\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1      3.6       79 1          2.82e- 2\n 2      1.8       54 2          8.60e-13\n 3      3.33      74 1          3.26e- 3\n 4      2.28      62 2          3.14e- 7\n 5      4.53      85 3          1.17e- 2\n 6      2.88      55 2          3.09e- 3\n 7      4.7       88 3          2.99e- 3\n 8      3.6       85 1          2.39e- 2\n 9      1.95      51 2          5.23e-12\n10      4.35      85 3          5.52e- 2\n# ℹ 262 more rows\n\ntidy(gmm2_geyser)\n\n# A tibble: 3 × 5\n  component  size proportion mean.eruptions mean.waiting\n      &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1         1    40      0.166           3.79         77.5\n2         2    97      0.356           2.04         54.5\n3         3   135      0.478           4.46         80.8\n\nglance(gmm2_geyser)\n\n# A tibble: 1 × 7\n  model     G    BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 EEE       3 -2314. -1126.    11     NA   272\n\n\n\n\n22.6.2 mclust\nThe previous example uses library(mclust)1 and library(broom).\nMclust() is the main function for fitting Gaussian Mixture Models. The function contains several different types of models for the variances of the multivariate normal distributions. The defaults are sensible. G is the number of groups. If G isn’t specified, then Mclust() will try 1:9 and pick the G with the lowest BIC (defined below).\nplot() with what = \"uncertainty\" creates a very useful data visualization for seeing the multivariate normal distributions and classifications for low-dimensional GMM.\nglance(), tidy(), and augment() from library(broom) return important information about the assignments, groups, and model diagnostics.\n\n\n22.6.3 Estimation\nSuppose we have \\(n\\) observations, \\(k\\) groups, and \\(p\\) variables. A single GMM will have\n\nan \\(n\\) by \\(k\\) matrix of responsibilities\n\\(k\\) vectors of means of length \\(p\\)\n\\(k\\) \\(p\\) by \\(p\\) variance-covariance matrices\n\nWe want the maximum likelihood estimates for all of the parameters in the model. For technical reasons, it is very difficult to get these estimates using popular methods like stochastic gradient descent.\nInstead, we will use expectations maximization (EM) to find the parameters. We also used EM for K-Means clustering.\n\n\n\n\n\n\n\nRandomly initialize all of the parameters. Calculate the log-likelihood.\nE-Step: Update the responsibilities assuming the means and variance-covariance matrices are known.\nM-Step: Estimate new means and variance-covariance matrices assuming the responsibilities are known. The means and variance-covariance matrices are calculated using weighted MLE where the responsibilities are the weights.\nCalculate the log-likelihood. Go back to step 2 if the log-likelihood improves by at least as much as the stopping threshold.\n\n\n\n\nThis algorithm is computationally efficient, but it is possible for it to find a local maximum log-likelihood without finding the global maximum log-likelihood.\nFor a more mathematical description of this process, see Elements of Statistical Learning Section 6.8 (Hastie, Tibshirani, and Friedman 2009). A highly descriptive comparison to kmeans (with Python code) can be seen here.\n\n\n22.6.4 Example 4\nLet’s consider a policy-relevant example using data from the Small Area Health Insurance Estimates (SAHIE) Program.\nFirst, we pull the 2016 county-level estimates of the uninsured rate. We label a state as an expansion state if it expanded data before 2015-01-01. We use this date with 2016 data because of policy lags.\n\nlibrary(censusapi)\n\nsahie &lt;- getCensus(\n  name = \"timeseries/healthins/sahie\",\n  key = Sys.getenv(\"CENSUS_KEY\"),\n  vars = c(\"GEOID\", \"PCTUI_PT\"),\n  region = \"county:*\",\n  time = 2016\n) |&gt;\n  as_tibble()\n\nNext, we pull data from the Kaiser Family Foundation about the expansion dates of Medicaid under the Patient Protection and Affordable Care Act.\n\nstates &lt;- tribble(\n  ~state, ~state_fips, ~implementation_date,\n  \"Alabama\", \"01\", NA,\n  \"Alaska\", \"02\", \"2015-09-15\",\n  \"Arizona\", \"04\", \"2014-01-01\",\n  \"Arkansas\", \"05\", \"2014-01-01\",\n  \"California\", \"06\", \"2014-01-01\",\n  \"Colorado\", \"08\", \"2014-01-01\",\n  \"Connecticut\", \"09\", \"2014-01-01\",\n  \"Delaware\", \"10\", \"2014-01-01\",\n  \"District of Columbia\", \"11\", \"2014-01-01\",\n  \"Florida\", \"12\", NA,\n  \"Georgia\", \"13\", NA,\n  \"Hawaii\", \"15\", \"2014-01-01\",\n  \"Idaho\", \"16\", \"2020-01-01\",\n  \"Illinois\", \"17\", \"2014-01-01\",\n  \"Indiana\", \"18\", \"2015-02-01\",\n  \"Iowa\", \"19\", \"2014-01-01\",\n  \"Kansas\", \"20\", NA,\n  \"Kentucky\", \"21\", \"2014-01-01\", \n  \"Louisiana\", \"22\", \"2016-07-01\",\n  \"Maine\", \"23\", \"2018-07-02\",\n  \"Maryland\", \"24\", \"2014-01-01\",\n  \"Massachusetts\", \"25\", \"2014-01-01\",\n  \"Michigan\", \"26\", \"2014-04-01\",\n  \"Minnesota\", \"27\", \"2014-01-01\",\n  \"Mississippi\", \"28\", NA,\n  \"Missouri\", \"29\", \"2021-07-01\",\n  \"Montana\", \"30\", \"2016-01-01\",\n  \"Nebraska\", \"31\", \"2020-10-01\",\n  \"Nevada\", \"32\", \"2014-01-01\", \n  \"New Hampshire\", \"33\", \"2014-08-15\",\n  \"New Jersey\", \"34\", \"2014-01-01\",\n  \"New Mexico\", \"35\", \"2014-01-01\",\n  \"New York\", \"36\", \"2014-01-01\", \n  \"North Carolina\", \"37\", NA,\n  \"North Dakota\", \"38\", \"2014-01-01\", \n  \"Ohio\", \"39\", \"2014-01-01\",\n  \"Oklahoma\", \"40\", \"2021-07-01\", \n  \"Oregon\", \"41\", \"2014-01-01\", \n  \"Pennsylvania\", \"42\", \"2015-01-01\", \n  \"Rhode Island\", \"44\", \"2014-01-01\", \n  \"South Carolina\", \"45\", NA,\n  \"South Dakota\", \"46\", \"2023-07-01\", \n  \"Tennessee\", \"47\", NA,\n  \"Texas\", \"48\", NA,\n  \"Utah\", \"49\", \"2020-01-01\",\n  \"Vermont\", \"50\", \"2014-01-01\",\n  \"Virginia\", \"51\", \"2019-01-01\", \n  \"Washington\", \"53\", \"2014-01-01\",\n  \"West Virginia\", \"54\", \"2014-01-01\",\n  \"Wisconsin\", \"55\", NA,\n  \"Wyoming\", \"56\", NA\n) %&gt;%\n  mutate(implementation_date = ymd(implementation_date))\n\nsahie &lt;- left_join(\n  sahie, \n  states,\n  by = c(\"state\" = \"state_fips\")\n) |&gt;\n  filter(!is.na(PCTUI_PT)) |&gt; \n  mutate(expanded = implementation_date &lt; \"2015-01-01\") %&gt;%\n  mutate(expanded = replace_na(expanded, FALSE))\n\nWe use GMM to cluster the data.\n\nuni &lt;- select(sahie, PCTUI_PT)\n\nset.seed(1)\nuni_mc &lt;- Mclust(uni, G = 2)\n\nglance(uni_mc)\n\n# A tibble: 1 × 7\n  model     G     BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 V         2 -18542. -9251.     5     NA  3141\n\n\nWhen we compare .class to expansion, we see that the model does an good job of labeling counties’ expansion status without observing counties’ expansion status.\n\nbind_cols(\n  sahie,\n  augment(uni_mc)\n) |&gt;\n  count(expanded, .class)\n\nNew names:\n• `PCTUI_PT` -&gt; `PCTUI_PT...5`\n• `PCTUI_PT` -&gt; `PCTUI_PT...9`\n\n\n# A tibble: 4 × 3\n  expanded .class     n\n  &lt;lgl&gt;    &lt;fct&gt;  &lt;int&gt;\n1 FALSE    1        465\n2 FALSE    2       1486\n3 TRUE     1       1042\n4 TRUE     2        148\n\n\n\n\n22.6.5 BIC\nLikelihood quantifies how likely observed data are given a set of parameters. If \\(\\theta\\) is a vector of parameters, then \\(L(\\theta |x) = f(x |\\theta)\\) is the likelihood function.\nWe often don’t know the exact number of latent groups in the data. We need a way to compare models with varying numbers of groups. Simply picking the model with the maximum likelihood will lead to models with too many groups.\nThe Bayesian information criterion (BIC) is an alternative to likelihoods that penalizes models for having many parameters. Let \\(L\\) be the likelihood, \\(m\\) the number of free parameters, and \\(n\\) the number of observations.\n\\[\nBIC = -2log(L) + mlog(n)\n\\tag{22.4}\\]\nWe will choose models that minimize BIC. Ideally, we will use v-fold cross validation for this process.\n\n\n22.6.6 Example 5\nThe Mclust() function will try G = 1:9 when G isn’t specified. Mclust() will also try 14 different variance models for the mixture models.\n\n\n\n\n\n\nImportant\n\n\n\nWe want to minimize BIC but library(mclust) is missing a negative sign. So we want to maximize the BIC plotted by library(mclust). You can read more here.\n\n\nWe can plot the BICs with plot() and view the optimal model with glance().\n\nfaithful_gmm &lt;- Mclust(faithful)\n\nplot(faithful_gmm, what = \"BIC\")\n\n\n\n\n\n\n\nglance(faithful_gmm)\n\n# A tibble: 1 × 7\n  model     G    BIC logLik    df hypvol  nobs\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 EEE       3 -2314. -1126.    11     NA   272",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#bernoulli-mixture-modeling-bmm",
    "href": "27_advanced-unsupervised-ml.html#bernoulli-mixture-modeling-bmm",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "22.7 Bernoulli Mixture Modeling (BMM)",
    "text": "22.7 Bernoulli Mixture Modeling (BMM)\nLet’s consider a data generation story based on the Bernoulli distribution. Now, each variable, \\(X_1, X_2, ..., X_D\\), is draw from a mixture of \\(K\\) Bernoulli distributions.\n\\[\nX_d  = \\begin{cases}\nBern(p_1) \\text{ with probability }\\pi_1 \\\\\nBern(p_2) \\text{ with probability }\\pi_2 \\\\\n\\vdots \\\\\nBern(p_K) \\text{ with probability }\\pi_K\n\\end{cases}\n\\tag{22.5}\\]\nLet \\(i\\) be an index for each mixture that contributes to the random variable. The probability mass function of the random variable is written as\n\\[\nP(X_d) = \\Pi_{i = 1}^Kp_i^{x_i} (1 - p_i)^{1 - x_i}\n\\tag{22.6}\\]\nLet’s consider a classic example from Bishop (2006) and Murphy (2022). The example uses the MNIST database, which contains 70,000 handwritten digits. The digits are stored in 784 variables, from a 28 by 28 grid, with values ranging from 0 to 255, which indicate the darkness of the pixel.\nTo prepare the data, we divide each pixel by 255 and then turn the pixels into indicators with values under 0.5 as 0 and values over 0.5 as 1. Figure Figure 22.3 visualizes the first four digits after reading in the data and applying pre-processing.\n\nsource(here::here(\"R\", \"visualize_digit.R\"))\n\nmnist &lt;- read_csv(here::here(\"data\", \"mnist_binary.csv\"))\n\nglimpse(dplyr::select(mnist, 1:10))\n\nRows: 60,000\nColumns: 10\n$ label    &lt;dbl&gt; 5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4…\n$ pix_28_1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_5 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pix_28_9 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nvisualize_digit(mnist, 1)\nvisualize_digit(mnist, 2)\nvisualize_digit(mnist, 3)\nvisualize_digit(mnist, 4)\n\n\n\n\n\n\n\n\n\n\n\n(a) 5\n\n\n\n\n\n\n\n\n\n\n\n(b) 0\n\n\n\n\n\n\n\n\n\n\n\n(c) 4\n\n\n\n\n\n\n\n\n\n\n\n(d) 1\n\n\n\n\n\n\n\nFigure 22.3: First Four Digits\n\n\n\nThe digits are labelled in the MNIST data set but we will ignore the labels and use Bernoulli Mixture Modeling to learn the latent labels or groups. We will treat each pixel as its own Bernoulli distribution and cluster observations using mixtures of 784 Bernoulli distributions. This means each cluster will contain \\(784\\) parameters.\n\n22.7.1 Two Digit Example\nLet’s start with a simple example using just the digits “1” and “8”. We’ll use library(flexmix) by Leisch (2004). library(flexmix) is powerful but uses different syntax than we are used to.\n\nThe function flexmix() expects a matrix.\nThe formula expects the entire matrix on the left side of the ~.\nWe specify the distribution used during the maximization (M) step with model = FLXMCmvbinary().\n\n\nlibrary(flexmix)\n\nLoading required package: lattice\n\nmnist_18 &lt;- mnist |&gt;\n  filter(label %in% c(\"1\", \"8\")) |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nThe starting assignments are random, so we set a seed.\n\nset.seed(20230612)\nmnist_18_clust &lt;- flexmix(\n  formula = mnist_18 ~ 1, \n  k = 2, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 100)\n)\n\nThe MNIST data are already labelled, so we can compare our assignments to the labels if we convert the “soft assignments” to “hard assignments”. Note that most applications won’t have labels.\n\nmnist |&gt;\n  filter(label %in% c(\"1\", \"8\")) |&gt;  \n  bind_cols(cluster = mnist_18_clust@cluster) |&gt;\n  count(label, cluster)\n\n# A tibble: 4 × 3\n  label cluster     n\n  &lt;dbl&gt;   &lt;int&gt; &lt;int&gt;\n1     1       1   482\n2     1       2  6260\n3     8       1  5610\n4     8       2   241\n\n\nFigure 22.4 shows the estimated \\(p_i\\) for each pixel for each cluster. The figure shows 784 \\(p_i\\) for \\(k = 1\\) and 784 \\(p_i\\) for \\(k = 2\\). We see that the estimated parameters closely resemble the digits.\nOf course, each digit can differ from these images because everyone writes differently. In some ways, these are average digits across many version of the digits.\n\nmeans_18 &lt;- rbind(\n  t(parameters(mnist_18_clust, component = 1)),\n  t(parameters(mnist_18_clust, component = 2))\n) |&gt;\n  as_tibble() |&gt;\n  mutate(label = NA)\n\nvisualize_digit(means_18, 1)\nvisualize_digit(means_18, 2)\n\n\n\n\n\n\n\n\n\n\n\n(a) 8\n\n\n\n\n\n\n\n\n\n\n\n(b) 1\n\n\n\n\n\n\n\nFigure 22.4: Estimated Parameters for Each Cluster\n\n\n\nThe BMM does a good job of labeling the digits and recovering the average shape of the digits.\n\n\n22.7.2 Ten Digit Example\nLet’s now consider an example that uses all 10 digits.\nIn most applications, we won’t know the number of latent variables. First, we sample 1,0002 digits and run the model with \\(k = 2, 3, ..., 12\\). We’ll calculate the BIC for each hyperparameter and pick the \\(k\\) with lowest BIC.\n\nset.seed(20230613)\nmnist_sample &lt;- mnist |&gt;\n  slice_sample(n = 1000) |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nsteps &lt;- stepFlexmix(\n  formula = mnist_sample ~ 1, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 100, minprior = 0),\n  k = 2:12, \n  nrep = 1\n)\n\n\\(k = 7\\) provides the lowest BIC. This is probably because digits like 3 and 8 are very similar.\n\nsteps\n\n\nCall:\nstepFlexmix(formula = mnist_sample ~ 1, model = FLXMCmvbinary(), \n    control = list(iter.max = 100, minprior = 0), k = 2:12, nrep = 1)\n\n   iter converged  k k0    logLik      AIC      BIC      ICL\n2    43      TRUE  2  2 -196191.7 395521.4 403221.6 403227.9\n3    30      TRUE  3  3 -188722.8 382153.6 393706.4 393713.9\n4    32      TRUE  4  4 -182949.0 372176.0 387581.4 387585.1\n5    27      TRUE  5  5 -178955.2 365758.4 385016.4 385019.7\n6    35      TRUE  6  6 -175448.7 360315.5 383426.1 383428.5\n7    37      TRUE  7  7 -171697.0 354381.9 381345.1 381347.8\n8    37      TRUE  8  8 -171282.5 355123.1 385938.8 385941.1\n9    38      TRUE  9  9 -169213.3 352554.6 387223.0 387224.9\n10   25      TRUE 10 10 -165521.6 346741.2 385262.2 385263.7\n11   34      TRUE 11 11 -162919.3 343106.5 385480.1 385481.8\n12   26      TRUE 12 12 -162253.5 343345.0 389571.1 389572.7\n\n\nNext, we run the BMM on the full data with \\(k = 7\\).\n\nmnist_full &lt;- mnist |&gt;\n  dplyr::select(-label) |&gt;\n  as.matrix()\n\nmnist_clust &lt;- flexmix(\n  formula = mnist_full ~ 1, \n  k = 7, \n  model = FLXMCmvbinary(), \n  control = list(iter.max = 200, minprior = 0)\n)\n\nThe MNIST data are already labelled, so we can compare our assignments to the labels if we convert the “soft assignments” to “hard assignments”. Note that most applications won’t have labels. The rows of the table are the digits. The columns of the table are the clusters. We can see, for example, that most of the 0’s are clustered in cluster 5.\n\nlabels &lt;- mnist |&gt;\n  bind_cols(cluster = mnist_clust@cluster)\n\ntable(labels$label, labels$cluster)\n\n   \n       1    2    3    4    5    6    7\n  0    5  357  282  289 4875    1  114\n  1   36  288   40   35    0 6319   24\n  2  114  166  652   73   50  163 4740\n  3  263  473 4786   80   41  260  228\n  4 3384 1779    4  139    7   53  476\n  5  325 2315 2367  173  109   59   73\n  6    9   86   41 4365   42  128 1247\n  7 3560 2395   21    0   25  234   30\n  8  257 2582 2369   57   32  445  109\n  9 3739 1797  109    5   26  136  137\n\n\nFigure 22.5 shows the estimated \\(p_i\\) for each pixel for each cluster. The following visualize the \\(784K\\) parameters that we estimated. It shows 784 \\(p_i\\) for \\(k = 1, 2, ..., 7\\) clusters. We see that the estimated parameters closely resemble the digits.\nmeans &lt;- rbind(\n  t(parameters(mnist_clust, component = 1)),\n  t(parameters(mnist_clust, component = 2)),\n  t(parameters(mnist_clust, component = 3)),\n  t(parameters(mnist_clust, component = 4)),\n  t(parameters(mnist_clust, component = 5)),\n  t(parameters(mnist_clust, component = 6)),\n  t(parameters(mnist_clust, component = 7))\n) |&gt;\n  as_tibble() |&gt;\n  mutate(label = NA)\n\nvisualize_digit(means, 1)\nvisualize_digit(means, 2)\nvisualize_digit(means, 3)\nvisualize_digit(means, 4)\nvisualize_digit(means, 5)\nvisualize_digit(means, 6)\nvisualize_digit(means, 7)\n\n\n\n\n\n\n\n\n\n\n\n(a) 3, 7, and 9\n\n\n\n\n\n\n\n\n\n\n\n(b) 5, 7, and 8\n\n\n\n\n\n\n\n\n\n\n\n(c) 3\n\n\n\n\n\n\n\n\n\n\n\n(d) 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) 0\n\n\n\n\n\n\n\n\n\n\n\n(f) 1\n\n\n\n\n\n\n\n\n\n\n\n(g) 2\n\n\n\n\n\n\n\nFigure 22.5: Estimated Parameters for Each Cluster\n\n\n\nThe example with all digits doesn’t result in 10 distinct mixtures but it does a fairly good job of structuring finding structure in the data. Without labels and considering the variety of messy handwriting, this is a useful model.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#considerations",
    "href": "27_advanced-unsupervised-ml.html#considerations",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "22.8 Considerations",
    "text": "22.8 Considerations\nMixture modeling is difficult for a couple of reasons:\n\nWe need to assume a model. It can be difficult to assume a multivariate distribution that fits the data in all dimensions of interest.\nThe models are overparameterized and can take a very long time to fit.\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n\n\nLeisch, Friedrich. 2004. “FlexMix: A General Framework for Finite Mixture Models and Latent Class Regression in R.” Journal of Statistical Software 11 (8). https://doi.org/10.18637/jss.v011.i08.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press.",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "27_advanced-unsupervised-ml.html#footnotes",
    "href": "27_advanced-unsupervised-ml.html#footnotes",
    "title": "22  Mixture Distributions and Mixture Modeling",
    "section": "",
    "text": "library(tidyclust) currently doesn’t support mixture modeling. I hope this will change in the future.↩︎\nThis is solely to save computation time.↩︎",
    "crumbs": [
      "Unsupervised Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Mixture Distributions and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html",
    "href": "28_geospatial.html",
    "title": "23  Geospatial Analysis with sf",
    "section": "",
    "text": "23.1 Motivation",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#motivation",
    "href": "28_geospatial.html#motivation",
    "title": "23  Geospatial Analysis with sf",
    "section": "",
    "text": "Many data are inherently spatial. We need tools to manipulate, explore, communicate, and model spatial data.\nPoint-and-click tools suffer many of the drawbacks outlined in earlier note sets.\nProprietary geospatial tools are prohibitively expensive.\nEveryone loves maps.",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#librarysf-and-geospatial-data",
    "href": "28_geospatial.html#librarysf-and-geospatial-data",
    "title": "23  Geospatial Analysis with sf",
    "section": "23.2 library(sf) and geospatial data",
    "text": "23.2 library(sf) and geospatial data\nGeospatial data are multidimensional. The outline of a simple rectangular state like Kansas may have a few vertices and a complex state like West Virginia may have many more vertices. Furthermore, geospatial data have auxiliary information about bounding boxes and projections. Fortunately, library(sf) can store all of this information in a rectangular tibble that works well with library(dplyr) and library(ggplot2).\nlibrary(sf) stores geospatial data, which are points, linestrings, or polygons in a geometry column. Consider this example:\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\namtrak_stations_file &lt;- \n  here(\"data\", \"amtrak_stations.geojson\")\n\n# download Amtrak data\nif(!file.exists(amtrak_stations_file)) {\n  \n  download.file(\n    url = \"https://opendata.arcgis.com/datasets/628537f4cf774cde8aa9721212226390_0.geojson\",\n    destfile = amtrak_stations_file\n  )\n  \n}\n\n# read sf data\namtrak_stations &lt;- st_read(amtrak_stations_file, quiet = TRUE)\n\n# print the geometry column\n# NOTE: geometry columns are \"sticky\" -- geometry is returned even though it \n# isn't include in select()\namtrak_stations |&gt;\n  select(stationnam)\n\nSimple feature collection with 1096 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -124.2883 ymin: 25.84956 xmax: -68.67001 ymax: 49.27376\nGeodetic CRS:  WGS 84\nFirst 10 features:\n              stationnam                   geometry\n1               Alma, MI POINT (-84.64484 43.39173)\n2             Albany, NY  POINT (-73.80919 42.7445)\n3   Abbotsford-Colby, WI POINT (-90.31467 44.92856)\n4           Aberdeen, MD POINT (-76.16326 39.50845)\n5            Absecon, NJ POINT (-74.50148 39.42405)\n6        Albuquerque, NM  POINT (-106.648 35.08207)\n7  Antioch-Pittsburg, CA  POINT (-121.816 38.01771)\n8            Arcadia, MO POINT (-90.62441 37.59217)\n9      Atlantic City, NJ   POINT (-74.4399 39.3627)\n10           Ardmore, OK POINT (-97.12552 34.17247)\n\n\n\n23.2.1 Points\n\n\n\n\n\n\nPoints\n\n\n\nPoints are zero-dimensional geometries. Points are coordinates and are frequently represented by a single longitude and latitude.\n\n\nLet’s map the Amtrak stations from the above example:\n\n# create a map\namtrak_stations |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(\n    title = \"Example of point data\",\n    caption = \"Amtrak stations from opendata.arcgis.com\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n23.2.2 Linestrings\n\n\n\n\n\n\nLinestrings\n\n\n\nLinestrings are one-dimensional geometries. Linestrings are a sequence of points connected by lines.\n\n\nHere is an example of Amtrak train routes from the Bureau of Transportation Statistics:\n\namtrak_routes_file &lt;- here(\"data\", \"Amtrak_Routes.geojson\")\n\n# load sf data\namtrak_routes &lt;- st_read(amtrak_routes_file, quiet = TRUE)\n\n# create map with lines data\namtrak_routes |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(\n    title = \"Example of line data\",\n    caption = \"Amtrak routes from Bureau of Transportation Statistics\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n23.2.3 Polygons\n\n\n\n\n\n\nPolygons\n\n\n\nPolygons are two-dimensional geometries. Polygons are a sequence of points connected by lines that form a closed shape.\n\n\nHere is a simple example with of the Continental United States:\n\n# load and subset states data\nstates &lt;- tigris::states(cb = TRUE, progress_bar = FALSE) |&gt;\n  filter(!STATEFP %in% c(\"78\", \"69\", \"66\", \"60\", \"72\", \"02\", \"15\"))\n\nRetrieving data for the year 2021\n\nstates |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = \"Example of polygon data\") +\n  theme_void()\n\n\n\n\n\n\n\n\nMultipoints, multilinestrings, and multipolygons are additional geometries based on points, linestrings, and polygons. The sf documentation outlines many geometry types.\n\n\n\n\n\n\nSimple features\n\n\n\nSimple features is a standard for storing and accessing geometric features outlined in the ISO 19125 standard.\n\n\nlibrary(sf) implements simple features in R and introduces sf data.",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#geom_sf",
    "href": "28_geospatial.html#geom_sf",
    "title": "23  Geospatial Analysis with sf",
    "section": "23.3 geom_sf()",
    "text": "23.3 geom_sf()\ngeom_sf() plots sf data. The function automatically references the geometry column and does not require any aesthetic mappings. geom_sf() works well with layers and it is simple to combine point, line, and polygon data.\ngeom_sf() works like geom_point() for point data, geom_line() for linestring data, and geom_area() for polygon data (e.g. fill controls the color of shapes and color controls the border colors of shapes for polygons).\n\namtrak_map &lt;- ggplot() +\n  geom_sf(\n    data = states,\n    fill = NA\n  ) +\n  geom_sf(\n    data = amtrak_routes, \n    color = \"blue\"\n  ) +\n  geom_sf(\n    data = amtrak_stations, \n    color = \"red\", \n    size = 0.75,\n    alpha = 0.5\n  ) +\n  labs(title = \"Amtrak stations and train routes\") +\n  theme_void()\n\namtrak_map",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#getting-and-loading-spatial-data",
    "href": "28_geospatial.html#getting-and-loading-spatial-data",
    "title": "23  Geospatial Analysis with sf",
    "section": "23.4 Getting and loading spatial data",
    "text": "23.4 Getting and loading spatial data\n\n23.4.1 Shapefiles\nShapefiles are a proprietary file format created by ESRI, the company that creates ArcGIS. Shapefiles are popular because ESRI dominated the GIS space for a long time. A shapefile is actually usually three or more binary files.\nst_read() reads shapefiles into R in the sf format. Simply point the function to the file ending in .shp. Note that the other binary files associated with the shapefile must also be located in the same directory as the .shp file.\n\n\n23.4.2 GeoJSON\n.geojson is an open source file type for storing geospatial data. It is plain text, which means it plays well with version control.\nst_read() also reads GeoJSON data. Point the function at a file ending in .geojson to read the data.\n\n\n23.4.3 .csv files\nLots of geographic information is stored in .csv files–especially for point data where it is sensible to have a longitude and latitude columns. Loading point data from .csv files requires two steps. First, read the file with read_csv(). Second, use st_as_sf() to convert the tibble into an sf object and specify the columns with longitude and latitude with the coords argument:\nst_as_sf(data, coords = c(\"lon\", \"lat\"))\nNote: It is trickier (but far less common) to load line, polygon, and multipolygon data from .csvs.\n\n\n23.4.4 library(tigris)\nlibrary(tigris) is an exceptional package that downloads and provides TIGER/Line shapefiles from the US Census Bureau. TIGER stands for Topologically Integrated Geographic Encoding and Referencing.\nThe package provides lots of data with simple functions (full list here) like counties() to access counties, tracts() to access census tracts, and roads() to access roads. The state and county arguments accept names and FIPS codes.\nlibrary(tigris) has a new shift option that elides Alaska and Hawaii next to the Continental United States.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nUsing library(tigris), pull roads data for DC with state = \"DC\" and county = \"District of Columbia\".\nCreate a map with geom_sf() and theme_void().\n\n\n\nTIGER line files are high-resolution and follow legal boundaries. Sometimes the maps are counterintuitive. For example, the outline of Michigan will include the Great Lakes, which is uncommon. Cartographic boundary files are quicker to download and are clipped to the coastline, which better aligns with expectations.\n\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nlibrary(patchwork)\n\nmi &lt;- states(progress_bar = FALSE) |&gt;\n  filter(STUSPS == \"MI\") |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = \"TIGER/Line\") +\n  theme_void() \n\nRetrieving data for the year 2021\n\nmi_cb &lt;- states(cb = TRUE, progress_bar = FALSE) |&gt;\n  filter(STUSPS == \"MI\") |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = \"Cartographic Boundary\") +  \n  theme_void()\n\nRetrieving data for the year 2021\n\nmi + mi_cb\n\n\n\n\n\n\n\n\nlibrary(rgeoboundaries)\nThe rgeoboundaries package is a client for the geoBoundaries API, providing country political administrative boundaries for countries around the world. This package can be installed from GitHub using the remotes package as follows\nThe rgeoboundaries package can provide boundaries for countries at different administrative division levels. For example, here we obtain the adm1 boundaries (the first subnational level) for Bangladesh. The type argument of the geoboundaries() function can be set to obtain a simplified version of the boundaries.\n\nlibrary(rgeoboundaries)\n\nbangladesh &lt;- geoboundaries(\n  country = \"Bangladesh\", \n  adm_lvl = \"adm1\",\n  type = \"SSCGS\" # Simplified Single Country Globally Standardized\n) \n\nggplot(data = bangladesh) +\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nWe want to read in and map the locations of World Bank development projects in Bangladesh downloaded from AidData, which includes geographic and other information about development projects.\n\nCopy the below code into your script to read in aiddata_bangladesh.csv with read_csv().\n\n\naiddata &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/awunderground/awunderground-data/\",\n    \"main/aiddata/aiddata_bangladesh.csv\"\n    )\n  )\n\nRows: 2648 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (15): project_id, project_location_id, place_name, location_type_code, l...\ndbl  (6): precision_code, geoname_id, latitude, longitude, location_class, g...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nUse st_as_sf() to convert the .csv to sf.\nUse st_set_crs(value = 4326) to set the CRS (we will discuss below).\nAdd a basemap of adm1 boundaries for Bangladesh using the bangladesh object created above.\nMap the Bangladesh development project data with color = status.\n\n\n\n\n\n23.4.5 library(tidycensus)\nlibrary(tidycensus), which was created by the creator of library(tigris), is also a valuable source of geographic data. Simply include geometry = TRUE in functions like get_acs() to pull the shapes data as sf. The state and county arguments accept names and FIPS codes.\nlibrary(tidycensus) sometimes requires the same Census API Key we used in the API tutorial (sign up here). You should be able to install your API key into your version of R with census_api_key(\"your-key-string\", install = TRUE). To obtain your keystring, you can use library(dotenv) and Sys.getenv(&lt;key name in .env file&gt;).\nlibrary(tidycensus) has two big differences from library(tigris): 1. it can pull Census data with the geographic data, and 2. it only provides cartographic boundary files that are smaller and quicker to load and more familiar than TIGER/Line shapefiles by default.\n\nlibrary(tidycensus)\n\ndc_income &lt;- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\", \n  state = \"DC\", \n  county = \"District of Columbia\", \n  geometry = TRUE,\n  year = 2019,\n  progress = FALSE\n)\n\nGetting data from the 2015-2019 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nBoth library(tigris) and library(tidycensus) have a year parameter that determines the year of the data obtained in functions like tidycensus::get_acs() or tigris::states(). This parameter currently defaults to 2020 for get_acs() and tigris functions. tidycensus::get_acs() also notably defaults to pulling 5-year ACS data. We recommend reading the documentation for these functions to understand the parameter options and their default values.",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#choropleths",
    "href": "28_geospatial.html#choropleths",
    "title": "23  Geospatial Analysis with sf",
    "section": "23.5 Choropleths",
    "text": "23.5 Choropleths\nChoropleths are maps that use fill to display the variation in a variable across geographies.\n\n\nGetting data from the 2017-2021 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nCopy the code that pulls income in DC by census tract under library(tidycensus).\nTry to recreate the above choropleth using fill.\nHint: Use scale_fill_gradient() with low = \"#cfe8f3\" and high = \"#062635\".",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#spatial-concepts",
    "href": "28_geospatial.html#spatial-concepts",
    "title": "23  Geospatial Analysis with sf",
    "section": "23.6 Spatial concepts",
    "text": "23.6 Spatial concepts\nGeospatial work requires making an assumption about the shape of the Earth and a decision about how to project three-dimensions on to a two-dimensional surface. Note: the Earth is an ellipsoid where the diameter from pole-to-pole is smaller than from equator to equator. In other words, it is swollen at the equator.\n\n\n\n\n\n\nGeographic coordinate reference systems\n\n\n\nGeographic coordinate reference systems represent geographies with a three-dimensional representation of the Earth. (i.e., The data have not been projected from what we think of as a globe to what we think of as a map). Data are typically stored as longitude and latitude.\n\n\n\n\n\n\n\n\nProjected coordinate reference system\n\n\n\nProjected coordinate reference system represent geographies with a two-dimensional representation of the Earth. A projected CRS is the combination of a geographic CRS and a projection. Data are typically stored in feet or meters, which is useful for distance-based spatial operations like calculating distances and creating buffers.\n\n\n\n\n\n\n\n\nProjection\n\n\n\nA projection is a mathematical transformation that converts three-dimensional coordinates for a spheroid/ellipsoid into a two-dimensions.\n\n\n\n23.6.1 Popular projections\nAll projections are wrong, but all projections are wrong in different ways with different uses.\nCylindrical projections maintain lines of longitude and latitude but distort areas and distances. Conic projections distort longitudes and latitudes but maintain areas. Azimuthal projections maintain distances but struggle to project large areas.\n\nMercator projection\nThe Mercator projection is widespread because it maintains straight lines for longitude and latitude. This was useful for navigators on the open sea hundreds of years ago. This is less useful in the 21st century. The Mercator projection is conformal, so while it maintains angles, it seriously distorts area. To see this, play the Mercator Puzzle.\n\n\n\n\n\n\nFigure 23.1: Mercator\n\n\n\nSource: Encyclopædia Britannica\n\n\nAlbers Equal Area Projection\nThe Albers Equal Area projection, which is a conic projection, doesn’t maintain angles, but it is an equal area projection. It is the default projection of the Urban Institute.\n\n\n\n\n\n\nFigure 23.2: Albers\n\n\n\n\n\nState Plane Coordinate Systems (SPCS)\nThe State Plane Coordinate System is a collection of 124 coordinate systems for specific areas of the United States. States with east-west directions, like Tennessee use the Lambert conformal conic projection. North-south states like Illinois use the transverse Mercator projection. SPCS is not a projection, but the coordinate systems are projected. This site has a thorough introduction.\n\n\n\n\n\n\nFigure 23.3: State Planes\n\n\n\nSource: NOAA\n\n\nEPSG Codes\nA CRS can be uniquely identified by EPSG codes and proj4 strings. EPSG comes from the European Petrol Survey Group, which no longer exists. EPSG codes are 4-6 numbers. Always check to see if a CRS is specified after loading spatial data. Here are some defaults:\n\nThe EPSG code will always be 4326 for GeoJSONs.\n.csv files with longitude and latitude will typically be 4326.\nR should read the CRS from .prj file when reading shapefiles. If it fails, open the .prj file and use this tool to identify the EPSG code.\n\nUse st_crs() to see the CRS. Use st_set_crs() to set the CRS. Use st_transform() to transform the CRS. Note that st_set_crs() simply adds or updates CRS information - it does not transform the data. When using multiple different geospatial datasets for mapping (e.g. layering points and polygons), they should have the same CRS prior to mapping.\nepsg.io and spatialreference.org are useful for finding and learning more about EPSG codes.\n\n\nDatum\n\n\n\n\n\n\nDatum\n\n\n\nA datum is a reference point for measuring locations on the surface of the Earth. The datum defines an anchor point for coordinate systems and thus allows a unique set of longitudes and latitudes to fully define the surface of the Earth.\n\n\nThe invention of GPS has standardized datums. Geodetic datums like the North American Datum of 1983 (NAD83) and the World Geodetic System of 1984 (WGS84) now dominate. In fact, all GPS measurements are based on WGS84. This blog describes the importance of datums.\n\n\nlibrary(crsuggest)\nlibrary(crsuggest) can simplify the process of picking a CRS.\n\nlibrary(crsuggest)\n\nUsing the EPSG Dataset v10.019, a product of the International Association of Oil & Gas Producers. \nPlease view the terms of use at https://epsg.org/terms-of-use.html.\n\nsuggest_crs(amtrak_stations)\n\n# A tibble: 10 × 6\n   crs_code crs_name                        crs_type crs_gcs crs_units crs_proj4\n   &lt;chr&gt;    &lt;chr&gt;                           &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 6350     NAD83(2011) / Conus Albers      project…    6318 m         +proj=ae…\n 2 5072     NAD83(NSRS2007) / Conus Albers  project…    4759 m         +proj=ae…\n 3 5071     NAD83(HARN) / Conus Albers      project…    4152 m         +proj=ae…\n 4 5070     NAD83 / Conus Albers            project…    4269 m         +proj=ae…\n 5 5069     NAD27 / Conus Albers            project…    4267 m         +proj=ae…\n 6 6925     NAD83(2011) / Kansas LCC (ftUS) project…    6318 us-ft     +proj=lc…\n 7 6924     NAD83(2011) / Kansas LCC        project…    6318 m         +proj=lc…\n 8 6923     NAD83 / Kansas LCC (ftUS)       project…    4269 us-ft     +proj=lc…\n 9 6922     NAD83 / Kansas LCC              project…    4269 m         +proj=lc…\n10 6467     NAD83(2011) / Kansas North (ft… project…    6318 us-ft     +proj=lc…\n\nvirginia_stations &lt;- amtrak_stations |&gt;\n  filter(state == \"VA\")\n\nsuggest_crs(virginia_stations)\n\n# A tibble: 10 × 6\n   crs_code crs_name                        crs_type crs_gcs crs_units crs_proj4\n   &lt;chr&gt;    &lt;chr&gt;                           &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 6593     NAD83(2011) / Virginia North (… project…    6318 us-ft     +proj=lc…\n 2 6592     NAD83(2011) / Virginia North    project…    6318 m         +proj=lc…\n 3 3686     NAD83(NSRS2007) / Virginia Nor… project…    4759 us-ft     +proj=lc…\n 4 3685     NAD83(NSRS2007) / Virginia Nor… project…    4759 m         +proj=lc…\n 5 32146    NAD83 / Virginia North          project…    4269 m         +proj=lc…\n 6 32046    NAD27 / Virginia North          project…    4267 us-ft     +proj=lc…\n 7 2924     NAD83(HARN) / Virginia North (… project…    4152 us-ft     +proj=lc…\n 8 2853     NAD83(HARN) / Virginia North    project…    4152 m         +proj=lc…\n 9 2283     NAD83 / Virginia North (ftUS)   project…    4269 us-ft     +proj=lc…\n10 6488     NAD83(2011) / Maryland (ftUS)   project…    6318 us-ft     +proj=lc…\n\n\nThe top suggestion for virginia_stations is CRS == 6593. If we look up this CRS we see it has geodetic datum NAD83 and is based on the Lambert Conformal Conic projection used for SPCS. If we look up the best SPCS for Northern Virginia, we get CRS == 3686, which is the third recommendation.\nThe differences between these two recommendations are not significant.\n\n\nBottom line\nThat’s a lot of technical information. When mapping in the US\n\nUse CRS = 4326 when you load the data to understand the locations you are mapping. This is not a projection but plotting the data acts like a projection.\nUse CRS = 5070 if you are mapping the entire Continental US. Other useful EPSG codes are available here.\nUse the recommended state plane coordinate system for state and local maps.\n\nHere’s the map from earlier with EPSG 4326 on the left and EPSG 5070 (Alber’s Equal Area Conic Projection) on the right:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCopy-and-paste the AidData exercise from exercise 2.\nRepeat the mapping with a EPSG code that makes sense for Bangladesh using st_transform() (Hint: you can identify the EPSG code using suggest_crs(aiddata)).\nCopy-and-paste the AidData exercise from exercise 2 again.\nRepeat the mapping with a EPSG code that makes sense for Bangladesh using coord_sf(crs = ####) where #### is the EPSG code from step 2 (Hint: coord_sf() can be added to your mapping code following a + ). This skips the need for st_transform() when making maps.",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#spatial-operations",
    "href": "28_geospatial.html#spatial-operations",
    "title": "23  Geospatial Analysis with sf",
    "section": "23.7 Spatial operations",
    "text": "23.7 Spatial operations\nWe often want to manipulate spatial data or use spatial data for calculations. This section covers a few common operations.\n\n23.7.1 Aggregation\nSometimes we want to aggregate smaller geographies into larger geographies. This is simple with a group_by() and summarize() workflow. Suppose we want to combine North Dakota and South Dakota into Dakota.\n\n# states to combine\ndmv_names &lt;- c(\"South Dakota\", \"North Dakota\")\n\n# add a projection\nstates &lt;- states |&gt;\n  st_transform(crs = 5070)\n\n# aggregate states and make a map\nstates |&gt;\n  mutate(new_NAME = if_else(NAME %in% dmv_names, \"Dakota\", NAME)) |&gt;\n  group_by(new_NAME) |&gt;\n  summarize() |&gt;\n  ggplot() +\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n23.7.2 Spatial Joins\nSpatial joins are joins like left_join() but the join is based on geography instead of “by” variables. Note: both geographies must have the same CRS. Like with any join, it is important to track the number of rows before and after joins and to note that joins may be one-to-one, one-to-many.\nst_join() performs a left spatial join in R. st_intersects means observations will join if the geographies in x touch the geographies in y. The sf package offers a number of different geometric confirmations that can be used for spatial joins, such as st_covered_by (identifies if x is copletely within y), st_within (identifies if x is within a specified distance of y) and many more. The sf cheat sheet provides a good outline of the different options.\nSuppose we want to count the number of Amtrak stations in each state.\n\n# set states CRS to 4326 to match the Amtrak data\namtrak_stations &lt;- st_transform(amtrak_stations, crs = 5070)\n\n# dimension before join\ndim(amtrak_stations)\n\n[1] 1096   12\n\n# spatial join using intersection\namtrak &lt;- st_join(states, amtrak_stations, join = st_intersects)\n\n# dimension after join -- lose international stations\ndim(amtrak)\n\n[1] 1080   21\n\n# count the number of stations per state\namtrak |&gt;\n  as_tibble() |&gt; # converting from sf to tibble speeds calculations\n  group_by(NAME) |&gt;\n  summarize(number_of_stations = n()) |&gt;\n  arrange(desc(number_of_stations))\n\n# A tibble: 49 × 2\n   NAME         number_of_stations\n   &lt;chr&gt;                     &lt;int&gt;\n 1 California                  184\n 2 Oregon                       87\n 3 New York                     80\n 4 Pennsylvania                 63\n 5 Michigan                     54\n 6 Washington                   50\n 7 Illinois                     41\n 8 Wisconsin                    37\n 9 Florida                      31\n10 Texas                        31\n# ℹ 39 more rows\n\n\n\n\n23.7.3 Buffers\nAdding buffers to points, lines, and polygons is useful for counting shapes near other shapes. For instance, we may want to count the number of housing units within 500 meters of a metro station or the number of schools more than 5 miles from a fire station.\nSuppose we want to count the number of Amtrak stations within 5 kilometers of each Amtrak station. We can buffer the Amtrak station points, join the unbuffered data to the buffered data, and then count.\n\n# add a buffer of 5 kilometers to each station\n# the units package is useful for setting buffers with different units\namtrak_stations_buffered &lt;- st_buffer(\n  amtrak_stations, \n  dist = units::set_units(5, \"km\")\n)\n\n# spatial join the unbuffered shapes to the buffer shapes\namtrak_stations_joined &lt;- st_join(\n  amtrak_stations_buffered,\n  amtrak_stations, \n  join = st_intersects\n)\n\n# count the station names\namtrak_stations_joined |&gt;\n  as_tibble() |&gt;\n  count(stationnam.x, sort = TRUE)\n\n# A tibble: 1,007 × 2\n   stationnam.x                   n\n   &lt;chr&gt;                      &lt;int&gt;\n 1 Monterey, CA                  27\n 2 Yosemite National Park, CA    17\n 3 Boston, MA                     9\n 4 Salem, OR                      9\n 5 Seaside, OR                    8\n 6 Manchester Center, VT          6\n 7 Eugene, OR                     5\n 8 Las Vegas, NV                  5\n 9 Oakland, CA                    5\n10 Palm Springs, CA               5\n# ℹ 997 more rows\n\n\n\n\n23.7.4 Distances\nEuclidean distance is common for calculating straight line distances but does not make sense for calculating distances on the surface of an ellipsoid like Earth.\n\\[d(\\vec{p}, \\vec{q}) = \\sqrt{\\sum_{i = 1}^n(q_i - p_i)^2}\\]\nInstead, it is common to use Haversine distance which accounts for the curvature in the globe.\n\n\n\n\n\n\nFigure 23.4: Haversine Distance\n\n\n\nSuppose we want to find the closest and furthest Amtrak stations from Washington DC’s Union Station.\n\n# create a data frame with just Union Station\nunion_station &lt;- amtrak_stations |&gt;\n  filter(state == \"DC\")\n\nno_union_station &lt;- amtrak_stations |&gt;\n  filter(state != \"DC\") \n\n# calculate the distance from Union Station to all other stations\namtrak_distances &lt;- st_distance(union_station, no_union_station)\n\n# find the closest station\namtrak_stations |&gt;\n  slice(which.min(amtrak_distances)) |&gt;\n  select(stationnam, city)\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1618448 ymin: 1915016 xmax: 1618448 ymax: 1915016\nProjected CRS: NAD83 / Conus Albers\n      stationnam       city                geometry\n1 Alexandria, VA Alexandria POINT (1618448 1915016)\n\n# find the further station\namtrak_stations |&gt;\n  slice(which.max(amtrak_distances)) |&gt;\n  select(stationnam, city)\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -2328052 ymin: 2299661 xmax: -2328052 ymax: 2299661\nProjected CRS: NAD83 / Conus Albers\n   stationnam    city                 geometry\n1 Fortuna, CA Fortuna POINT (-2328052 2299661)",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#geospatial-modeling",
    "href": "28_geospatial.html#geospatial-modeling",
    "title": "23  Geospatial Analysis with sf",
    "section": "23.8 Geospatial modeling",
    "text": "23.8 Geospatial modeling\nCross-sectional regression models assume that the error term is independently and identically distributed, which in turn means the dependent variable is independently and identically distributed. This is often a reasonable assumption.\nThe assumption of independence often falls apart with spatial data. If number of coal power plants in a state is an independent variable and atmospheric carbon dioxide in a state is the dependent variable, then it doesn’t make much sense to assume that North Carolina and South Carolina are independent. If South Carolina has many coal burning power plants, then the emissions could affect atmospheric carbon dioxide in North Carolina.\nSpatial regression methods attempt to account for this dependence between observations.\nSpatial autocorrelation: Correlation between observations that are geographically close.\nProcess:\n\nEstimate a non-spatial regression model.\nUse tests of spatial autocorrelation like Moran’s I, Geary’s c, or Getis and Ord’s G-statistic on the residuals to test for spatial autocorrelation.\nIf spatial autocorrelation exists, the use a spatial error model or a spatial lag model.",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#parting-thoughts",
    "href": "28_geospatial.html#parting-thoughts",
    "title": "23  Geospatial Analysis with sf",
    "section": "23.9 Parting Thoughts",
    "text": "23.9 Parting Thoughts\nThis note set works entirely with vector spatial data. Vector data consists of vertices turned into points, lines, and polygons.\nSome spatial data are raster data, which are stored as pixels or grid cells. For example, a raster data set may have an even one square mile grid over the entire United States with data about the amount of soy crops within each pixel. It is common for satellite data to be converted to rasters. This website contains good example of raster data.",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "28_geospatial.html#more-resources",
    "href": "28_geospatial.html#more-resources",
    "title": "23  Geospatial Analysis with sf",
    "section": "23.10 More Resources",
    "text": "23.10 More Resources\n\nDrawing Beautiful Maps with sf - part 1\nDrawing Beautiful Maps with sf - part 2\nR Spatial Workshop Notes\nAnalyzing US Census Data by Kyle Walker – Chapter 6\nsf Cheat Sheet",
    "crumbs": [
      "Other Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Geospatial Analysis with sf</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aruoba, S. Boragan, and Jesus Fernndez-Villaverde. 2018. “A\nComparison of Programming Languages in Economics: An Update.” https://www.sas.upenn.edu/~jesusfv/Update_March_23_2018.pdf.\n\n\nAshraf, N., D. Karlan, and W. Yin. 2006. “Tying Odysseus to the\nMast: Evidence From a Commitment Savings Product in the\nPhilippines.” The Quarterly Journal of Economics 121\n(2): 635–72. https://doi.org/10.1162/qjec.2006.121.2.635.\n\n\nBarrientos, Andrés F., Aaron R. Williams, Joshua Snoke, and Claire McKay\nBowen. 2021. “A Feasibility Study of Differentially Private\nSummary Statistics and Regression Analyses with Evaluations on\nAdministrative and Survey Data.” https://doi.org/10.48550/ARXIV.2110.12055.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nBlumenstock, Joshua. n.d. “Calling for Better Measurement:\nEstimating an Individual’s Wealth and Well-Being from\nMobile Phone Transaction Records.” Center for Effective\nGlobal Action. https://escholarship.org/uc/item/8zs63942.\n\n\nBlumenstock, Joshua, Gabriel Cadamuro, and Robert On. 2015.\n“Predicting Poverty and Wealth from Mobile Phone Metadata.”\nScience 350 (6264): 1073–76. https://doi.org/10.1126/science.aac4420.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial Proportion.”\nStatistical Science 16 (2). https://doi.org/10.1214/ss/1009213286.\n\n\nBruce, Anotnio, and Gregory Robinson. 2003. “The Planning\nDatabase: Its Development and Use as an Effective Tool in Census\n2000.” https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b2edb90180a9132f64f8287af2db92c031b5d40b.\n\n\nBruce, Antonio, Gregory Robinson, and Monique V. Sanders. 2001.\n“Hard-to-Count Scores and Broad Demographic Groups Associated with\nPatterns of Response Rates in Census 2000.” Proceedings of\nthe Social Statistics Section, American Statistical Association.\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical\nInference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.\n\n\nChernick, Michael R., and Robert A. LaBudde. 2011. An Introduction\nto Bootstrap Methods with Applications to r. Hoboken, N.J: Wiley.\n\n\nChetty, Raj, John N. Friedman, Søren Leth-Petersen, Torben Heien\nNielsen, and Tore Olsen. 2014. “Active Vs. Passive Decisions and\nCrowd-Out in Retirement Savings Accounts: Evidence from\nDenmark*.” The Quarterly Journal of Economics 129 (3):\n1141–1219. https://doi.org/10.1093/qje/qju013.\n\n\nErdman, Chandra, and Nancy Bates. 2014. “The u.s. Census Bureau\nMail Return Rate Challenge: Crowdsourcing to Develop a Hard-to-Count\nScore.” https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/rrs2014-08.pdf.\n\n\n———. 2017. “The Low Response Score (LRS).” Public\nOpinion Quarterly 81 (1): 144–56. https://doi.org/10.1093/poq/nfw040.\n\n\nEubank, Nick. 2016. “Embrace Your Fallibility: Thoughts on Code\nIntegrity.” https://www.nickeubank.com/wp-content/uploads/2016/06/Eubank_EmbraceYourFallibility.pdf.\n\n\nFellegi, I. P. 1972. “On the Question of Statistical\nConfidentiality.” Journal of the American Statistical\nAssociation 67 (337): 7–18. https://www.jstor.org/stable/2284695?seq=1#metadata_info_tab_contents.\n\n\nGinsberg, Jeremy, Matthew H. Mohebbi, Rajan S. Patel, Lynnette Brammer,\nMark S. Smolinski, and Larry Brilliant. 2009. “Detecting Influenza\nEpidemics Using Search Engine Query Data.” Nature 457\n(7232): 1012–14. https://doi.org/10.1038/nature07634.\n\n\nHastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. Springer Series in Statistics. New York, NY:\nSpringer.\n\n\nHiggins, James J. 2004a. An Introduction to Modern Nonparametric\nStatistics. Pacific Grove, CA: Brooks/Cole.\n\n\n———. 2004b. An Introduction to Modern Nonparametric Statistics.\nPacific Grove, CA: Brooks/Cole.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2017. An introduction to statistical learning: with applications in\nR. Corrected at 8th printing. Springer texts in statistics. New\nYork Heidelberg Dordrecht London: Springer. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nKolenikov, Stas J. 2016. “Post-Stratification or a Non-Response\nAdjustment?” Survey Practice 9 (3): 1–12. https://doi.org/10.29115/SP-2016-0014.\n\n\nLeisch, Friedrich. 2004. “FlexMix: A General Framework for Finite\nMixture Models and Latent Class Regression in\nR.” Journal of Statistical\nSoftware 11 (8). https://doi.org/10.18637/jss.v011.i08.\n\n\nLi, Jinjing, and Cathal O’Donoghue. 2014. “Evaluating Binary\nAlignment Methods in Microsimulation Models.” Journal of\nArtificial Societies and Social Simulation 17 (1): 15. https://doi.org/10.18564/jasss.2334.\n\n\nMcClelland, Robert, Surachai Khitatrakun, and Chenxi Lu. 2020.\n“Estimating Confidence Intervals in a Tax Microsimulation\nModel.” International Journal of Microsimulation 13 (2):\n2–20. https://doi.org/10.34196/IJM.00216.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An\nIntroduction. Adaptive Computation and Machine Learning Series.\nCambridge, Massachusetts: The MIT Press.\n\n\nOrcutt, Guy H. 1957. “A New Type of Socio-Economic System.”\nThe Review of Economics and Statistics 39 (2): 116. https://doi.org/10.2307/1928528.\n\n\nPeng, Roger. 2018. “Teaching r to New Users - from Tapply to the\nTidyverse.” https://simplystatistics.org/posts/2018-07-12-use-r-keynote-2018/.\n\n\nPotash, Eric, Joe Brew, Alexander Loewi, Subhabrata Majumdar, Andrew\nReece, Joe Walsh, Eric Rozier, Emile Jorgenson, Raed Mansour, and Rayid\nGhani. 2015. “KDD ’15: The 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining.” In, 2039–47.\nSydney NSW Australia: ACM. https://doi.org/10.1145/2783258.2788629.\n\n\nRavn, Signe, Ashley Barnwell, and Barbara Barbosa Neves. 2020.\n“What Is “Publicly Available Data”?\nExploring Blurred PublicPrivate Boundaries and Ethical\nPractices Through a Case Study on Instagram.” Journal of\nEmpirical Research on Human Research Ethics 15 (1-2): 40–45. https://doi.org/10.1177/1556264619850736.\n\n\nRizzo, Maria L. 2008. Statistical Computing with r. Chapman\n& Hall/CRC Computer Science and Data Analysis Series. Boca Raton:\nChapman & Hall/CRC.\n\n\nRodrigues, Bruno. 2022. Modern R with the tidyverse. https://modern-rstats.eu.\n\n\nSalganik, Matthew J. 2018. Bit by Bit: Social Research in the\nDigital Age. Princeton: Princeton University Press.\n\n\nScott, David W., and Stephan R. Sain. 2005. “Multidimensional\nDensity Estimation.” In, 24:229–61. Elsevier. https://doi.org/10.1016/S0169-7161(04)24009-3.\n\n\nSomepalli, Gowthami, Singla, Micah Goldblum, Jonas Geiping, and Tom\nGoldstein. 2023. “Diffusion Art or Digital Forgery? Investigating\nData Replication in Diffusion Models.” Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 6048–58. https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.”\nJournal of Computational and Graphical Statistics 19 (1): 3–28.\nhttps://doi.org/10.1198/jcgs.2009.07098.\n\n\n———. 2014. “Tidy Data.” https://doi.org/10.18637/jss.v059.i10.\n\n\n———. n.d. The tidyverse style guide. https://style.tidyverse.org/index.html.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualie, and Model\nData. 2nd edition. Sebastopol, CA: O’Reilly.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. 1st ed.\nPaperback; O’Reilly Media. http://r4ds.had.co.nz/.\n\n\nZheng, Vivian. 2020. “How Urban Piloted Data Science Techniques to\nCollect Land-Use Reform Data.” https://urban-institute.medium.com/how-urban-piloted-data-science-techniques-to-collect-land-use-reform-data-475409903b88.",
    "crumbs": [
      "References"
    ]
  }
]