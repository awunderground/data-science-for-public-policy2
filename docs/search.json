[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Public Policy Part II",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "05_simulation-and-sampling.html#sec-review5",
    "href": "05_simulation-and-sampling.html#sec-review5",
    "title": "1  Simulation and Sampling",
    "section": "1.1 Review",
    "text": "1.1 Review\n\n\n\n\n\n\nPopulation\n\n\n\nA population is the entire set of observations of interest.\nFor example, a population could be everyone residing in France at a point in time. For example, a population could be every American ages 65 or older.\n\n\n\n\n\n\n\n\nParameter\n\n\n\nA parameter is a numerical quantity that summarizes a population.\nFor example, the population mean and population standard deviation describe important characteristics of many populations.\nMore generally, location parameters, scale parameters, and shape parameters describe many populations.\n\n\n\n\n\n\n\n\nRandom Sample\n\n\n\nA random sample is a random subset of a population.\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nA statistic is a numerical quantity that summarizes a sample.\nFor example, the sample mean and sample standard deviation describe important characteristics of many random samples.\n\n\nParameters are to populations what statistics are to samples. The process of learning about population parameters from statistics calculated from samples is called statistical inference."
  },
  {
    "objectID": "05_simulation-and-sampling.html#introduction",
    "href": "05_simulation-and-sampling.html#introduction",
    "title": "1  Simulation and Sampling",
    "section": "1.2 Introduction",
    "text": "1.2 Introduction\nSimulation and sampling are important tools for statistics and data science. After reviewing/introducing basic concepts about probability theory and probability distributions, we will discuss two important applications of simulation and sampling.\n\nMonte Carlo simulation: A class of methods where values are repeatedly sampled/simulated from theoretical distributions that model a data generation process. Theoretical distributions, like the normal distribution, have closed-from representations and a finite number of parameters like mean and variance.\nResampling methods: A class of methods where values are repeatedly sampled from observed data to approximate repeatedly sampling from a population. Bootstrapping is a common resampling method.\n\nMonte Carlo methods and resampling methods have a wide range of applications. Monte Carlo simulation is used by election forecasters to predict electoral outcomes and econometricians to understand the properties of estimators. Resampling methods are used in machine learning and causal inference. Both are fundamental methods for agent-based models including microsimulation."
  },
  {
    "objectID": "05_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "href": "05_simulation-and-sampling.html#fundamentals-of-probability-theory",
    "title": "1  Simulation and Sampling",
    "section": "1.3 Fundamentals of Probability Theory",
    "text": "1.3 Fundamentals of Probability Theory\n\n\n\n\n\n\nRandom Variable\n\n\n\n\\(X\\) is a random variable if its value is unknown and/or could change.\n\n\n\\(X\\) could be the outcome from the flip of a coin or the roll of a die. \\(X\\) could also be the amount of rain next July 4th.\n\n\n\n\n\n\nExperiment\n\n\n\nAn experiment is a process that results in a fixed set of possible outcomes.\n\n\n\n\n\n\n\n\nSet\n\n\n\nA set is a collection of objects.\n\n\n\n\n\n\n\n\nSample Space\n\n\n\nA sample space is the set of all possible outcomes for an experiment. We will denote a sample space with \\(\\Omega\\).\n\n\n\n\n\n\n\n\nDiscrete Random Variable\n\n\n\nA set is countable if there is a one-to-one correspondence from the elements of the set to some (finite) or all (countably infinite) positive integers (i.e. 1 = heads and 2 = tails).\nA random variable is discrete if its sample space is countable (finite or countably infinite).\n\n\n\n\n\n\n\n\nContinuous Random Variable\n\n\n\nA random variable is continuous if its sample space is any value in a \\(\\mathbb{R}\\) (real) interval.\nThere are infinite possible values in a real interval so the sample space is uncountable."
  },
  {
    "objectID": "05_simulation-and-sampling.html#discrete-random-variables",
    "href": "05_simulation-and-sampling.html#discrete-random-variables",
    "title": "1  Simulation and Sampling",
    "section": "1.4 Discrete Random Variables",
    "text": "1.4 Discrete Random Variables\n\n\n\n\n\n\nProbability Mass Function\n\n\n\nA probability mass function (PMF) computes the probability of an event in the sample space of a discrete random variable.\n\\[\np(x) = P(X = x)\n\\tag{1.1}\\]\nwhere \\(0 \\le p(x) \\le 1\\) and \\(\\sum_{x \\in \\Omega} p(x) = 1\\)\n\n\n\n\nCode\ntibble(\n  a = factor(1:6),\n  `P(X = a)` = rep(1 / 6, 6)\n) |&gt;\n  ggplot(aes(a, `P(X = a)`)) +\n  geom_col() +\n  labs(title = \"PMF for rolling a fair die\")\n\n\n\n\nFigure 1.1: PMF for rolling a fair die\n\n\n\n\n\nNow we can make statements like \\(P(X = a)\\). For example, \\(P(X = 3) = \\frac{1}{6}\\).\n\n1.4.1 Bernoulli Distribution\nA Bernoulli random variable takes on the value \\(1\\) with probability \\(p\\) and \\(0\\) with probability \\(1 - p\\). It is often used to represent coins. When \\(p = \\frac{1}{2}\\) we refer to the coin as “fair”.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Ber(p)\n\\tag{1.2}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) =\n\\begin{cases}\n1 - p &\\text{ if } x = 0 \\\\\np &\\text{ if } x = 1\n\\end{cases} = p^x(1 - p)^{1-x}\n\\tag{1.3}\\]\n\n\n1.4.2 Binomial Distribution\nA binomial random variable is the number of events observed in \\(n\\) repeated Bernoulli trials.\nWe show that a random variable is Bernoulli distributed with\n\\[\nX \\sim Bin(n, p)\n\\tag{1.4}\\]\nThe PMF of a Bernoulli random variable is\n\\[\np(x) = {n \\choose x} p^x(1 - p)^{n - x}\n\\tag{1.5}\\]\nWe can calculate the theoretical probability of a given draw from a binomial distribution using this PDF. For example, suppose we have a binomial distribution with \\(10\\) trials and \\(p = \\frac{1}{2}\\). The probability of drawing exactly six \\(1\\)s and four \\(0\\)s is\n\\[\np(X = 6) = \\frac{10!}{6!4!} 0.5^6(1 - 0.5)^{10 - 6} \\approx 0.2051\n\\tag{1.6}\\]\nWe can do similar calculations for each value between \\(0\\) and \\(10\\).\nWe can also take random draws from the distribution. Figure 1.2 shows 1,000 random draws from a binomial distribution with 10 trials and p = 0.5. The theoretical distribution is overlaid in red.\n\n\nCode\ntibble(\n  x = rbinom(n = 1000, size = 10, prob = 0.5)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(breaks = 0:10) +\n  geom_point(data = tibble(x = 0:10, y = map_dbl(0:10, dbinom, size = 10, prob = 0.5)),\n             aes(x, y),\n             color = \"red\") +\n  labs(\n    title = \"1,000 samples of a binomial RV\",\n    subtitle = \"Size = 10; prob = 0.5\",\n    y = NULL\n  ) \n\n\n\n\nFigure 1.2: 1,000 random draws from a binomial distribution with 10 trials and p = 0.5\n\n\n\n\n\n\n\n\n\n\n\nSampling Error\n\n\n\nSampling error is the difference between sample statistics (estimates of population parameters) and population parameters.\n\n\nThe difference between the red dots and black bars in Figure 1.2 is caused by sampling error.\n\n\n1.4.3 Distributions Using R\nMost common distributions have R functions to\n\ncalculate the density of the pdf/pmf for a specific value\ncalculate the probability of observing a value less than \\(a\\)\ncalculate the value associated with specific quantiles of the pdf/pmf\nrandomly sample from the probability distribution\n\nLet’s consider a few examples:\nThe following answers the question: “What is the probability of observing 10 events in 10 trials when p = 0.5?”\n\ndbinom(x = 10, size = 10, prob = 0.5)\n\n[1] 0.0009765625\n\n\n\nThe following answers the question: “What’s the probability of observing 3 or fewer events in 10 trials when p = 0.5”\n\npbinom(q = 3, size = 10, prob = 0.5)\n\n[1] 0.171875\n\n\n\nThe following answers the question: “What is a 10th percentile number of events to see in 10 trials when p = 0.5?\n\nqbinom(p = 0.1, size = 10, prob = 0.5)\n\n[1] 3\n\n\n\nThe following randomly draw ten different binomially distributed random variables.\n\nrbinom(n = 10, size = 10, prob = 0.5)\n\n [1] 6 7 7 4 1 4 5 4 3 7\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nAdd dbinom() for 0, 1, and 2 when size = 10 and prob = 0.5.\nCalculate pbinom() with size = 10 and prob = 0.5. Why do you get the same answer?\nPlug the result from step 1/step 2 into qbinom() with size = 10 and prob = 0.5. Why do you get the answer 2?\nUse rbinom() with size = 10 and prob = 0.5 to sample 10,000 binomially distributed random variables and assign the output to x. Calculate mean(x &lt;= 2). How does the answer compare to step 1/step 2?\n\n\n\n\n\n\n\n\n\nPseudo-random numbers\n\n\n\nComputers use pseudo-random numbers to generate samples from probability distributions. Modern pseudo-random samplers are very random.\nUse set.seed() to make pseudo-random sampling reproducible.\n\n\n\n\n1.4.4 Poisson Random Variable\nA poisson random variable is the number of events that occur in a fixed period of time. For example, a poisson distribution can be used to model the number of visits in an emergency room between 1AM and 2AM.\nWe show that a random variable is poisson-distributed with\n\\[\nX \\sim Pois(\\lambda)\n\\tag{1.7}\\]\nThe parameter \\(\\lambda\\) is both the mean and variance of the poisson distribution. The PMF of a poisson random variable is\n\\[\np(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\tag{1.8}\\]\nFigure 1.3 shows 1,000 draws from a poisson distribution with \\(\\lambda = 10\\).\n\n\nCode\nset.seed(20200905)\n\ntibble(\n  x = rpois(1000, lambda = 10)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)))) +\n  scale_x_continuous(limits = c(0, 29)) +\n  stat_function(fun = dpois, n = 30, color = \"red\", args = list(lambda = 10)) + \n  labs(\n    title = \"1,000 samples of a Poisson RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\nFigure 1.3: 1,000 samples of a Poisson RV\n\n\n\n\n\n\n\n1.4.5 Categorical Random Variable\nWe can create a custom discrete probability distribution by enumerating the probability of each event in the sample space. For example, the PMF for the roll of a fair die is\n\\[\np(x) =\n\\begin{cases}\n\\frac{1}{6} & \\text{if } x = 1\\\\\n\\frac{1}{6} & \\text{if } x = 2\\\\\n\\frac{1}{6} & \\text{if } x = 3\\\\\n\\frac{1}{6} & \\text{if } x = 4\\\\\n\\frac{1}{6} & \\text{if } x = 5\\\\\n\\frac{1}{6} & \\text{if } x = 6\n\\end{cases}\n\\tag{1.9}\\]\nThis PMF is visualized in Figure 1.1. We can sample from this PMF with\n\nsample(x = 1:6, size = 1)\n\n[1] 3\n\n\nWe can also sample with probabilities that differ for each event:\n\nsample(\n  x = c(\"rain\", \"sunshine\"), \n  size = 1, \n  prob = c(0.1, 0.9)\n)\n\n[1] \"sunshine\"\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nSample 1,000 observations from a Poisson distribution with \\(\\lambda = 20\\).\nSample 1,000 observations from a normal distribution with \\(\\mu = 20\\) and \\(\\sigma = \\sqrt{20}\\).\nVisualize and compare both distribution.\n\n\n\nWhen \\(\\lambda\\) is sufficiently large, the normal distribution is a reasonable approximation of the poisson distribution."
  },
  {
    "objectID": "05_simulation-and-sampling.html#continuous-random-variables",
    "href": "05_simulation-and-sampling.html#continuous-random-variables",
    "title": "1  Simulation and Sampling",
    "section": "1.5 Continuous Random Variables",
    "text": "1.5 Continuous Random Variables\n\n\n\n\n\n\nProbability Density Function (PDF)\n\n\n\nA probability density function is a non-negative, integrable function for each real value \\(x\\) that shows the relative probability of values of \\(x\\) for an absolutely continuous random variable \\(X\\).\nWe not PDF with \\(f_X(x)\\).\n\n\n\n\n\n\n\n\nCumulative Distribution Function (CDF)\n\n\n\nA cumulative distribution function (cdf) shows the probability of a random variable \\(X\\) taking on any value less than or equal to \\(x\\).\nWe note CDF with \\(F_X(x) = P(X \\le x)\\)\n\n\nHere is the PDF for a standard normal random variable:\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"PDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) \n\n\n\n\n\nIf we integrate the entire function we get the CDF.\nCumulative Density Function (CDF): A function of a random variable \\(X\\) that returns the probability that the value \\(X &lt; x\\).\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = pnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  labs(\n    title = \"CDF for a standard normal random variable\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",\n    y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n1.5.1 Uniform Distribution\nUniform random variables have equal probability for every value in the sample space. The distribution has two parameters: minimum and maximum. A standard uniform random has minimum = 0 and maximum = 1.\nWe show that a random variable is uniform distributed with\n\\[\nX \\sim U(a, b)\n\\tag{1.10}\\]\nThe PDF of a uniform random variable is\n\\[\nf(x) =\n\\begin{cases}\n\\frac{1}{b - a} & \\text{if } x \\in [a, b] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\tag{1.11}\\]\nStandard uniform random variables are useful for generating other random processes and imputation.\n\n\nCode\nset.seed(20200904)\n\ntibble(\n  x = runif(1000)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dunif, n = 101, color = \"red\") + \n  labs(\n    title = \"1,000 samples of a standard uniform RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n1.5.2 Normal Distribution\nThe normal distribution is the backbone of statistical inference because of the central limit theorem.\nWe show that a random variable is normally distributed with\n\\[\nX \\sim N(\\mu, \\sigma)\n\\tag{1.12}\\]\nThe PDF of a normally distributed random variable is\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right) ^ 2\\right]\n\\tag{1.13}\\]\n\n\n\n\n\n\nFundamental Probability Formula for Intervals\n\n\n\nThe probability that an absolutely continuous random variable takes on any specific value is always zero because the sample space is uncountable. Accordingly, we express the probability of observing events within a region for absolutely continuous random variables.\nIf \\(X\\) has a PDF and \\(a &lt; b\\), then\n\\[\nP(a \\le X \\le b) = P(a \\le X &lt; b) = P(a &lt; X \\le b) = P(a &lt; X &lt; b) = \\int_a^bf(x)dx = F_X(b) - F_X(a)\n\\tag{1.14}\\]\n\n\nThe last portion of this inequality is fundamental to working with continuous probability distributions and is the backbone of much of any intro to statistics course. For example, the probability, \\(P(X &lt; 0)\\) is represented by the blue region below.\n\n\nCode\ntibble(x = c(-4, 4)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + \n  geom_area(stat = \"function\", fun = dnorm, fill = \"blue\", xlim = c(-4, 0)) +\n  labs(\n    title = \"PDF for a standard normal random variable\",\n    y = NULL\n  ) \n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nStudent’s t-distribution and the normal distribution are closely related.\n\nUse pnorm() to calculate \\(P(X &lt; -1)\\) for a standard normal distribution.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 10.\nUse pt() to calculate \\(P(X &lt; -1)\\) for Student’s t-distribution with df = 100.\n\n\n\nObserve how the normal distribution becomes a better approximation for Student’s t-distribution when the degrees of freedom increases.\n\n\n1.5.3 Exponential Distribution\nAn exponential random variable is the wait time between events for a poisson random variable. It is useful for modeling wait time. For example, an exponential distribution can be used to model the wait time between arrivals in an emergency room between 1AM and 2AM. It has one parameter: rate (\\(\\lambda\\)).\nWe show that a random variable is exponentially distributed with\n\\[\nX \\sim Exp(\\lambda)\n\\tag{1.15}\\]\nThe PDF of an exponential random variable is\n\\[\nf(x) = \\lambda\\exp(-\\lambda x)\n\\tag{1.16}\\]\n\n\nCode\ntibble(\n  x = rexp(n = 1000, rate = 1)\n) |&gt;\n  ggplot(aes(x)) +\n  geom_density() +\n  stat_function(fun = dexp, n = 101, args = list(rate = 1), color = \"red\") + \n  labs(\n    title = \"1,000 samples of an exponential RV\",\n    subtitle = \"Observed data in black and theoretical distribution in red\",    \n    y = NULL\n  ) \n\n\n\n\n\n\n\n1.5.4 Other Distributions\n\nGeometric RV: Number of Bernoulli trials up to and including the \\(1^{st}\\) event\nNegative Binomial RV: Number of Bernoulli trials up to and including the \\(r^{th}\\) event\nGamma RV: Time until the \\(\\alpha\\) person arrives"
  },
  {
    "objectID": "05_simulation-and-sampling.html#parametric-density-estimation",
    "href": "05_simulation-and-sampling.html#parametric-density-estimation",
    "title": "1  Simulation and Sampling",
    "section": "1.6 Parametric Density Estimation",
    "text": "1.6 Parametric Density Estimation\nA key exercise in statistics is selecting a probability distribution to represent data and then learning the parameters of probability distributions from the data. The process is often called model fitting.\nWe are focused on parametric density estimation. Later, we will focus on nonparameteric density estimation. This section will focus on frequentist inference of population parameters from observed data. Later, we will adopt a Bayesian approach to inference.\n\n1.6.1 Maximum Likelihood Estimation\nAll of the probability distributions we have observed have a finite number of parameters. Maximum likelihood estimation is a common method for estimating these parameters.\nThe general process is\n\nPick the probability distribution that fits the observed data.\nIdentify the finite number of parameters associated with the probability distribution.\nCalculate the parameters that maximize the probability of the observed data.\n\n\n\n\n\n\n\nLikelihood\n\n\n\nLet \\(\\vec{x}\\) be observed data and \\(\\theta\\) be a parameter or parameters from a chosen probability distribution. The likelihood is the joint probability of the observed data conditional on values of the parameters.\nThe likelihood of discrete data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n p(x_i|\\theta)\n\\tag{1.17}\\]\nThe likelihood of continuous data is\n\\[\nL(\\theta) = P(\\vec{x}|\\theta) = \\Pi_{i = 1}^n f(x_i|\\theta)\n\\tag{1.18}\\]\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\n\nMaximum likelihood estimation is a process for estimating parameters for a given distribution that maximizes the log likelihood.\nIn other words, MLEs find the estimated parameters that maximize the probability of observing the observed set of data.\n\n\nWe won’t unpack how to derive the maximum likelihood estimators1 but it is easy to look up most MLEs.\n\nBinomial distribution MLEs\nSuppose we have a sample of data \\(x_1, ..., x_m\\). If the number of trials \\(n\\) is already known, then \\(p\\) is the only parameter for the binomial distribution that needs to be estimated. The MLE for \\(p\\) is \\(\\hat{p} = \\frac{\\sum_{i = 1}^n x_i}{mn}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat{p}\\).\n\nset.seed(20230909)\nx &lt;- rbinom(n = 8, size = 10, prob = 0.3)\n\nx\n\n[1] 4 3 6 3 4 3 3 2\n\n\n\nmle_binom &lt;- sum(x) / (10 * 8)\n\nmle_binom\n\n[1] 0.35\n\n\n\n\nNormal distribution MLEs\n\\(\\mu\\) and \\(\\sigma\\) are the parameters of a normal distribution. The MLEs for a normal distribution are \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i = \\bar{x}\\) and \\(s^2 = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^2\\).2\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\hat\\mu = \\frac{1}{n}\\sum_{i = 1}^n x_i\\) and \\(\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\).\n\nset.seed(20230909)\nx &lt;- rnorm(n = 200, mean = 0, sd = 1)\n\n\nmean_hat &lt;- mean(x)\n\nmean_hat\n\n[1] 0.02825125\n\nsigma2_hat &lt;- mean((x - mean(x)) ^ 2)\n\nsigma2_hat\n\n[1] 0.8119682\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dnorm, color = \"red\", args = list(mean = mean_hat, sd = sqrt(sigma2_hat)))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nExponential distribution MLEs\n\\(\\lambda\\) is the only parameter of an exponential distribution. The MLE for an exponential distribution is \\(\\hat\\lambda = \\frac{1}{\\bar{x}}\\).\nSuppose we observe the following vector of observed data. Next, we calculate \\(\\frac{1}{\\bar{x}}\\).\n\nset.seed(20230909)\nx &lt;- rexp(n = 200, rate = 10)\n\nmle_exp &lt;- 1 / mean(x)\n\nmle_exp\n\n[1] 10.58221\n\ntibble(x = x) |&gt;\n  ggplot() +\n  geom_histogram(aes(x, y = after_stat(density))) +\n  stat_function(fun = dexp, color = \"red\", args = list(rate = mle_exp))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nCreate the vector in the code chunk below:\n\n\n\nCode\nx &lt;- c(\n  30970.787, 10901.544, 15070.015, 10445.772, 8972.258, \n  15759.614, 13341.328, 18498.858, 134462.066, 17498.930, \n  7112.306, 27336.795, 75526.381, 110123.606, 32910.618, \n  16764.452, 21244.380, 18952.455, 954373.470, 4219.635,\n  7078.766, 27657.996, 18337.097, 14566.525, 14220.000, \n  21457.202, 9322.311, 26018.018, 96325.728, 26780.329, \n  25833.356, 10719.360, 8642.935, 29302.623, 10517.174,\n  33831.547, 339077.456, 5805.707, 141505.710, 28168.790, \n  10446.378, 4993.349, 27502.949, 35519.162, 45761.505, \n  26163.096, 72163.668, 15515.435, 69396.895, 84972.590, \n  67248.460, 26966.374, 24624.339, 4779.110, 23330.279,\n  196311.913, 20517.739, 80257.587, 32108.466, 9735.061, \n  20502.579, 2544.004, 165909.040, 20949.512, 16643.695, \n  30267.741, 8359.024, 13355.154, 8425.988, 4491.550,\n  32071.872, 61648.149, 75074.135, 62842.985, 26040.648, \n  68733.979, 63368.710, 11157.211, 5782.610, 3629.674, \n  44399.230, 2852.381, 8200.453, 41249.003, 15006.791,\n  808974.653, 30705.915, 6341.954, 28208.144, 5409.821,\n  54566.805, 10894.864, 4583.550, 31110.875, 43474.872, \n  69059.161, 33054.574, 8789.910, 218887.477, 11051.292, \n  3366.743, 63853.329, 68756.561, 48031.259, 11707.191,\n  26593.634, 8868.942, 19225.309, 27704.670, 10666.549, \n  47151.963, 20343.604, 123932.502, 33030.986, 5412.023, \n  23540.382, 9894.513, 52742.541, 21397.990, 25100.143,\n  23757.882, 48347.300, 4325.134, 23816.776, 11907.656, \n  24179.849, 25967.574, 7531.294, 15131.240, 21595.781, \n  40473.936, 35390.849, 4060.563, 55334.157, 37058.771, \n  34050.456, 17351.500, 7453.829, 48131.565, 10576.746,\n  26450.754, 33592.986, 21425.018, 34729.337, 77370.078, \n  5819.325, 9067.356, 19829.998, 20120.706, 3637.042, \n  44812.638, 22930.229, 29683.776, 76366.822, 15464.594, \n  1273.101, 53036.266, 2846.294, 114076.200, 14492.680, \n  55071.554, 31597.849, 199724.125, 52332.510, 98411.129, \n  43108.506, 6580.620, 12833.836, 8846.348, 7599.796, \n  6952.447, 30022.143, 24829.739, 40784.581, 8997.219,\n  3786.354, 11515.298, 116515.617, 137873.967, 3282.185,\n  107886.676, 13184.850, 51083.235, 2907.886, 51827.538, \n  37564.196, 23196.399, 20169.037, 9020.364, 11118.250, \n  56930.060, 11657.302, 84642.584, 44948.450, 16610.166, \n  5509.231, 4770.262, 15614.233, 5993.999, 22628.114\n)\n\n\n\nVisualize the data with a relative frequency histogram.\nCalculate the MLEs for a normal distribution and add a normal distribution to the visualization in red.\nCalculate the MLEs for a log-normal distribution and add a log-normal distribution to the visualization in blue."
  },
  {
    "objectID": "05_simulation-and-sampling.html#multivariate-random-variables",
    "href": "05_simulation-and-sampling.html#multivariate-random-variables",
    "title": "1  Simulation and Sampling",
    "section": "1.7 Multivariate Random Variables",
    "text": "1.7 Multivariate Random Variables\nWe’ve explored univariate or marginal distributions thus far. Next, we will focus on multivariate distributions.\n\n\n\n\n\n\nMultivariate Distribution\n\n\n\nA multivariate distribution is a probability distribution that shows the probability (discrete) or relative probability (continuous) of more than one random variable.\nMultivariate distributions require describing characteristics of random variables and the relationships between random variables.\n\n\n\n1.7.1 Multivariate Normal Distribution\nThe multivariate normal distribution is a higher-dimensional version of the normal distribution.\nInstead of a single mean and a single variance, the \\(k\\)-dimensional multivariate normal distribution has a vector of means of length \\(k\\) and a \\(k\\)-by-\\(k\\) variance-covariance matrix3. The vector describes the central tendencies of each dimension of the multivariate distribution and the matrix describe the variance of the distributions and relationships between the distributions.\nWe show that a random vector is multivariate normally distributed with\n\\[\n\\vec{X} \\sim \\mathcal{N}(\\vec\\mu, \\boldsymbol\\Sigma)\n\\tag{1.19}\\]\nThe PDF of a multivariate normally distributed random variable is\n\\[\nf(x) = (2\\pi)^{-k/2}det(\\boldsymbol\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\vec{x} - \\vec\\mu)^T\\boldsymbol\\Sigma^{-1}(\\vec{x} - \\vec\\mu)\\right)\n\\tag{1.20}\\]\nFunctions for working with multi-variate normal distributions from library(MASS). Figure 1.4 shows three different random samples from 2-dimensional multivariate normal distributions.\n\nSigma1 &lt;- matrix(c(1, 0.8, \n                   0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n  \nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma1) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nSigma2 &lt;- matrix(c(1, 0.2, \n                   0.2, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma2) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nSigma3 &lt;- matrix(c(1, -0.8, \n                   -0.8, 1),\n                 nrow = 2,\n                 byrow = TRUE)\n\nmvrnorm(n = 1000, mu = c(0, 0), Sigma = Sigma3) |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(V1, V2)) +\n  geom_point()\n\nFigure 1.4: Samples from Multivariate Normal Distributions\n\n\n\n\n\n(a) Strong Positive Covariance\n\n\n\n\n\n\n\n(b) Weak Covariance\n\n\n\n\n\n\n\n(c) Strong Negative Covariance"
  },
  {
    "objectID": "05_simulation-and-sampling.html#monte-carlo-methods",
    "href": "05_simulation-and-sampling.html#monte-carlo-methods",
    "title": "1  Simulation and Sampling",
    "section": "1.8 Monte Carlo Methods",
    "text": "1.8 Monte Carlo Methods\nSimulation methods, including Monte Carlo simulation, are used for policy analysis:\n\nFiveThirtyEight and the New York Times use Monte Carlo simulation to predict the outcomes of elections.\nThe Social Security Administration uses microsimulation to evaluate the distributional impact of Social Security reforms.\nThe Census Bureau uses simulation to understand the impact of statistical disclosure control on released data.\nEconometricians and statisticians use Monte Carlo simulation to demonstrate the properties of estimators.\n\nWe can make probabilistic statements about common continuous random variables because their PDFs are integrable or at least easy enough to approximate with lookup tables. We can make probabilistic statements about common discrete random variables with summation.\nBut we often want to make probabilistic statements about uncommon or complex probability distributions. Maybe the probability distribution of the random variable doesn’t have a tractable integral (i.e. the area under the curve can’t practically be computed). Or maybe there are too many potential outcomes (e.g. rays of light emitting from a light bulb in the Marble Science video linked at the top).\nMonte Carlo: A Monte Carlo method estimates a deterministic quantity using stochastic (random) sampling.\nMonte Carlo but easier this time: A Monte Carlo method takes hundreds or thousands of independent samples from a random variable or variables and then approximates fixed population quantities with summaries of those draws. The quantities could be population parameters like a population mean or probabilities.\nMonte Carlo methods have three major applications:\n\nSampling – Monte Carlo simulation allows for sampling from complex probability distributions. The samples can be used to model real-world events (queues), to model outcomes with uncertain model inputs (election modeling), to generate fake data with known parameters to evaluate statistical methods (model selection when assumptions fail), and to draw from the posteriors of Bayesian models.\nNumerical integration – Integration, as noted above, is important to calculating probabilities and ultimately calculating quantities like expected value or the intervals. Monte Carlo methods can approximate multidimensional integrals that will never be directly solved by computers or simplify estimating probabilities when there are uncountably many potential outcomes (Solitaire).\nOptimization – Monte Carlo methods can be used for complex optimization. We will not focus on optimization.\n\nLet’s explore some examples:\n\n1.8.1 Example 1: Coin Tossing\nWe can calculate the proportion of tosses of a fair coin that we expect to turn up heads by finding the expected value of the binomial distribution and dividing by the number of tosses. But suppose we can’t… Or maybe we wish to confirm our calculations with simulations…\nLet’s try repeated sampling from a binomial distribution to approximate this process:\n\n#' Count the proportion of n tosses that turn up heads\n#'\n#' @param n An integer for the number of tosses\n#'\n#' @return The proportion of n tosses that turn up heads\n#' \ncount_heads &lt;- function(n) {\n  \n  # toss the fair coin n times\n  coin_tosses &lt;- rbinom(n = n, size = 1, prob = 0.5)\n    \n  coin_tosses &lt;- if_else(coin_tosses == 1, \"heads\", \"tails\")\n  \n  # calculate the proportion of heads\n  prop_heads &lt;- mean(coin_tosses == \"heads\")\n  \n  return(prop_heads)\n  \n}\n\nLet’s toss the coin ten times.\n\nset.seed(11)\ncount_heads(n = 10)\n\n[1] 0.3\n\n\nOk, we got 0.3, which we know isn’t close to the expected proportion of 0.5. What if we toss the coin 1 million times.\n\nset.seed(20)\ncount_heads(n = 1000000)\n\n[1] 0.499872\n\n\nOk, that’s more like it.\n\\[\\cdot\\cdot\\cdot\\]\nMonte Carlo simulation works because of the law of large numbers. The law of large numbers states that the probability that the average of trials differs from the expected value converges to zero as the number of trials goes to infinity.\nMonte Carlo simulation basically repeats the ideas behind frequentist inferential statistics. If we can’t measure every unit in a population then we can sample a representative population and estimate parameters about that population.\nThe keys to Monte Carlo simulation are randomness and independent and identically distributed sampling (i.i.d.).\n\n\n1.8.2 Example 2: Bootstrap Sampling\nOn average, a bootstrap sample includes about 63% of the observations from the data that are sampled. This means that an individual bootstrap sample excludes 37% of the observations from the source data!\nSo if we bootstrap sample from a vector of length 100, then \\(\\frac{63}{100}\\) values will end up in the bootstrap sample on average and \\(\\frac{37}{100}\\) of the values will be repeats on average.\nWe can explore this fact empirically with Monte Carlo simulation using repeated samples from a categorical distribution. We will use sample().\n\n#' Calculate the proportion of unique values from a vector of integers included \n#' in a bootstrap sample\n#'\n#' @param integers A vector of integers\n#'\n#' @return The proportion of integers included in the bootstrap sample\n#' \ncount_uniques &lt;- function(integers) {\n  \n  # generate a bootstrap sample\n  samples &lt;- sample(integers, size = length(integers), replace = TRUE)\n  \n  # calculate the proportion of unique values from the original vector\n  prop_unique &lt;- length(unique(samples)) / length(integers)\n  \n  return(prop_unique)\n  \n}\n\nLet’s bootstrap sample 100,000 times.\n\n# pre-allocate the output vector for efficient computation\nprop_unique &lt;- vector(mode = \"numeric\", length = 100000)\nfor (i in seq_along(prop_unique)) {\n  \n  prop_unique[i] &lt;- count_uniques(integers = 1:100)\n  \n}\n\nFinally, calculate the mean proportion and estimate the expected value.\n\nmean(prop_unique)\n\n[1] 0.6337935\n\n\nWe can also calculate a 95% confidence interval using the bootstrap samples.\n\nquantile(prop_unique, probs = c(0.025, 0.975))\n\n 2.5% 97.5% \n 0.57  0.69 \n\n\n\n\n1.8.3 Example 3: \\(\\pi\\)\nConsider one of the examples from Marble Science: Monte Carlo Simulation. Imagine we don’t know \\(\\pi\\) but we know that the equation for the area of a square is \\(r ^ 2\\) and the equation for the area of a circle is \\(\\pi r ^ 2\\). If we know the ratio of the areas of the circle and the square, then we can solve for \\(\\pi\\).\n\\[\n\\frac{\\text{Area of Cirle}}{\\text{Area of Square}} = \\frac{\\pi r ^ 2}{r ^ 2} = \\pi\n\\tag{1.21}\\]\nThis is simply solved with Monte Carlo simulation. Randomly sample a bivariate uniform random variables and count how frequently the values are inside of the square or inside the circle.\n\nexpand_grid(\n  x = seq(0, 4, 0.1),\n  y = seq(0, 2, 0.1)\n) |&gt;\nggplot() +\n  ggforce::geom_circle(aes(x0 = 2, y0 = 1, r = 1), fill = \"blue\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), fill = \"red\", color = NA) +\n  geom_rect(aes(xmin = 0, xmax = 3, ymin = 0, ymax = 2), fill = NA, color = \"black\") +\n  coord_fixed()\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\nnumber_of_samples &lt;- 2000000\n\n# sample points in a rectangle with x in [0, 3] and y in [0, 2]\nset.seed(20210907)\nsamples &lt;- tibble(\n  x = runif(number_of_samples, min = 0, max = 3),\n  y = runif(number_of_samples, min = 0, max = 2)\n)\n\n# calculate if (x, y) is in the circle, the square, or neither\nsamples &lt;- samples |&gt;\n  mutate(\n    in_square = between(x, 0, 1) & between(y, 0, 1),\n    in_circle = (x - 2) ^ 2 + (y - 1) ^ 2 &lt; 1\n  ) \n\n# calculate the proportion of samples in each shape\nprop_in_shapes &lt;- samples |&gt;\n  summarize(\n    prop_in_square = mean(in_square), \n    prop_in_circle = mean(in_circle)\n  ) \n\n# calculate the ratio\nprop_in_shapes |&gt;\n  mutate(prop_in_circle / prop_in_square) |&gt;\n  print(digits = 3)\n\n# A tibble: 1 × 3\n  prop_in_square prop_in_circle `prop_in_circle/prop_in_square`\n           &lt;dbl&gt;          &lt;dbl&gt;                           &lt;dbl&gt;\n1          0.166          0.524                            3.15\n\n\nThe answer approximates \\(\\pi\\)!\n\n\n1.8.4 Example 4: Simple Linear Regression\nThe goal of statistical inference is to use data, statistics, and assumptions to infer parameters and probabilities about a population. Typically we engage in point estimation and interval estimation.\nSometimes it is useful to reverse this process to understand and confirm the properties of estimators. That means starting with known population parameters, simulating hundreds or thousands of samples from that population, and then observing point estimates and interval estimates over those samples.\n\nLinear Regression Assumptions\n\nThe population model is of the linear form \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\nThe estimation data come from a random sample or experiment\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) independently and identically distributed (i.i.d.)\n\\(x\\) has variance and there is no perfect collinearity in \\(x\\)\n\n\n\nStatistics\nIf we have one sample of data, we can estimate points and intervals with the following estimators:\nThe residual standard error is\n\\[\n\\hat\\sigma = \\frac{\\sum e_i^2}{(n - 2)}\n\\tag{1.22}\\]\nThe estimate of the slope is\n\\[\n\\hat\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\n\\tag{1.23}\\]\nThe standard error of the estimate of the slope, which can be used to calculate t-statistics and confidence intervals, is\n\\[\n\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2}\n\\tag{1.24}\\]\nThe estimate of the intercept term is\n\\[\n\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1\\bar{x}\n\\tag{1.25}\\]\nThe standard error of the intercept is\n\\[\n\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]}\n\\tag{1.26}\\]\n\n\nMonte Carlo Simulation\nConsider a simple linear regression model with the following population model:\n\\[y = 5 + 15x + \\epsilon\\]\nWe can calculate the above statistics over repeated sampling and confirm their asymptotic properties with Monte Carlo simulation.\nFirst, create 1,000 random samples from the population.\n\nset.seed(20210906)\n\ndata &lt;- map(\n  .x = 1:1000,\n  .f = ~ tibble(\n    x = rnorm(n = 10000, mean = 0, sd = 2),\n    epsilon = rnorm(n = 10000, mean = 0, sd = 10),\n    y = 5 + 15 * x + epsilon\n  )\n)\n\nNext, estimate a simple linear regression model for each draw of the population. This step includes calculating \\(\\hat\\sigma\\), \\(\\hat\\beta_1\\), \\(\\hat\\beta_0\\), \\(\\hat{SE}(\\hat\\beta_1)\\), and \\(\\hat{SE}(\\hat\\beta_0)\\).\n\nestimated_models &lt;- map(\n  .x = data,\n  .f = ~ lm(y ~ x, data = .x)\n)\n\nNext, we extract the coefficients and confidence intervals.\n\ncoefficients &lt;- map_df(\n  .x = estimated_models,\n  .f = tidy,\n  conf.int = TRUE\n)\n\nLet’s look at estimates of the residual standard error. The center of the distribution closely matches the population standard deviation of the error term.\n\nmodel_metrics &lt;- map_df(\n  .x = estimated_models,\n  .f = glance\n) \n\nmodel_metrics |&gt;\n  ggplot(aes(sigma)) +\n  geom_histogram() +\n  labs(title = \"Plot of the estimated residual standard errors\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLet’s plot the coefficients. The centers approximately match the population intercept of 5 and slope of 15.\n\ncoefficients |&gt;\n  ggplot(aes(estimate)) +\n  geom_histogram() +\n  facet_wrap(~term, scales = \"free_x\") +\n  labs(title = \"Coefficients estimates across 10,000 samples\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe standard deviation of the coefficients also matches the standard errors.\n\\[\\hat{SE}(\\hat\\beta_1) = \\sqrt\\frac{\\hat\\sigma^2}{\\sum(x_i -\\bar{x})^2} = \\sqrt\\frac{10^2}{40,000} = 0.05\\]\n\\[\\hat{SE}(\\hat{\\beta_0}) = \\sqrt{\\hat\\sigma^2\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}\\right]} = \\sqrt{10^2\\left[\\frac{1}{10,000} + 0\\right]} = 0.1\\]\n\ncoefficients |&gt;\n  group_by(term) |&gt;\n  summarize(\n    mean(estimate), \n    sd(estimate)\n  )\n\n# A tibble: 2 × 3\n  term        `mean(estimate)` `sd(estimate)`\n  &lt;chr&gt;                  &lt;dbl&gt;          &lt;dbl&gt;\n1 (Intercept)             5.00         0.100 \n2 x                      15.0          0.0482\n\n\nLet’s look at how often the true parameter is inside the 95% confidence interval. It’s close although not exactly 95%.\n\ncoefficients |&gt;\n  filter(term == \"x\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 15 & conf.high &gt;= 15))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1           0.959\n\ncoefficients |&gt;\n  filter(term == \"(Intercept)\") |&gt;\n  summarize(ci_contain_beta = mean(conf.low &lt;= 5 & conf.high &gt;= 5))\n\n# A tibble: 1 × 1\n  ci_contain_beta\n            &lt;dbl&gt;\n1            0.95\n\n\n\n\n\n1.8.5 Example 5: Queuing Example\nSuppose we have a queue at a Social Security field office. Let \\(t\\) be time. When the office opens, \\(t = 0\\) and the queue is empty.\nLet, \\(T_i\\) be the interarrival time and \\(T_i \\sim exp(\\lambda_1)\\)\nLet, \\(S_i\\) be the service time time and \\(S_I \\sim exp(\\lambda_2)\\)\nFrom these two random variables, we can calculate the arrival times, departure times, and wait times for each customer.\n\nThe arrival times are the cumulative sum of the interarrival times.\nThe wait times are zero if a person arrives after the person before them and the difference between the prior person’s departure and the current person’s arrival otherwise.\nThe departure time is arrival time plus the wait time plus the service time.\n\n\nset.seed(19920401)\nqueue &lt;- generate_queue(t = 100, lambda = 1, mu = 1)\n\nqueue\n\n# A tibble: 100 × 5\n   interarrival_time arrival_time service_time wait_time departure_time\n               &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1             0.467        0.467       5.80        0              6.27\n 2             1.97         2.43        0.0892      3.83           6.36\n 3             2.70         5.13        1.26        1.22           7.61\n 4             0.335        5.47        4.85        2.14          12.5 \n 5             0.372        5.84        1.89        6.62          14.3 \n 6             1.72         7.56        0.507       6.78          14.9 \n 7             2.28         9.84        0.932       5.01          15.8 \n 8             0.339       10.2         1.18        5.61          17.0 \n 9             2.54        12.7         1.42        4.25          18.4 \n10             0.572       13.3         0.157       5.10          18.5 \n# ℹ 90 more rows\n\n\n\nflow &lt;- tibble::tibble(\n  time = c(queue$arrival_time, queue$departure_time),\n  type = c(rep(\"arrival\", length(queue$arrival_time)), \n           rep(\"departure\", length(queue$departure_time))), \n  change = c(rep(1, length(queue$arrival_time)), rep(-1, length(queue$departure_time))),\n) |&gt;\n  arrange(time) |&gt; \n  filter(time &lt; 100) |&gt; \n  mutate(queue = cumsum(change) - 1)\n\nflow |&gt;\n  ggplot(aes(time, queue)) +\n  geom_step() +\n  labs(title = \"Simulated queue at the Social Security office\")\n\n\n\n\nThis is interesting, but it’s still only one draw from a Monte Carlo simulation. What if we are interested in the distribution of wait times for the fifth customer?\n\n#' Generate wait times at the queue\n#'\n#' @param person_number An integer for the person of interest\n#' @param iterations An integer for the number of Monte Carlo iterations\n#' @param t A t for the maximum time\n#'\n#' @return A vector of wait times\n#' \ngenerate_waits &lt;- function(person_number, iterations, t) {\n  \n  wait_time &lt;- vector(mode = \"numeric\", length = iterations)\n  for (i in seq_along(wait_time)) {\n    \n    wait_time[i] &lt;- generate_queue(t = t, lambda = 1, mu = 1)$wait_time[person_number]\n    \n  }\n  \n  return(wait_time)\n  \n}\n\nset.seed(20200908)\nwait_time &lt;- generate_waits(person_number = 5, iterations = 10000, t = 50)\n\nmean(wait_time)\n\n[1] 1.464371\n\nquantile(wait_time, probs = c(0.025, 0.5, 0.975))\n\n     2.5%       50%     97.5% \n0.0000000 0.9222193 5.8742015 \n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\nCreate a Monte Carlo simulation of an unfair coin toss where p = 0.6.\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nSuppose we have three independent normally-distributed random variables.\n\\[X_1 \\sim N(\\mu = 0, \\sigma = 1)\\]\n\\[X_2 \\sim N(\\mu = 1, \\sigma = 1)\\]\n\\[X_3 \\sim N(\\mu = 2, \\sigma = 1)\\]\n\nUse Monte Carlo simulation with 10,000 repetitions to estimate how often \\(X_{i1} &lt; X_{i2} &lt; X_{i3}\\).\n\n\n\n\n\n1.8.6 More examples of Monte Carlo simulation\n\nfivethirtyeight 2020 election forecast use\nU.S. Census Bureau simulation of data collection operations\n\n\nMarkov Chain Monte Carlo\nBayesian statisticians estimate posterior distributions of parameters that are combinations of prior distributions and sampling distributions. Outside of special cases, posterior distributions are difficult to identify. Accordingly, most Bayesian estimation uses an extension of Monte Carlo simulation called Markov Chain Monte Carlo or MCMC.\n\n\n\n1.8.7 One Final Note\nMonte Carlo simulations likely underestimate uncertainty. Monte Carlo simulations only capture aleatoric uncertainty and they don’t capture epistemic uncertainty.\nAleatoric uncertainty: Uncertainty due to probabilistic variety\nEpistemic uncertainty: Uncertainty due to a lack of knowledge\nIn other words, Monte Carlo simulations estimates assume the model is correct, which is almost certainly never fully true. Be transparent. Be humble."
  },
  {
    "objectID": "05_simulation-and-sampling.html#sampling-from-observed-data",
    "href": "05_simulation-and-sampling.html#sampling-from-observed-data",
    "title": "1  Simulation and Sampling",
    "section": "1.9 Sampling from Observed Data",
    "text": "1.9 Sampling from Observed Data\nUntil now, we’ve only discussed sampling from closed-form theoretical distributions. We also called this process simulation. There are many applications where we may want to sample from observed data.\nWe can break these methods into two general approaches:\n\nSampling\nResampling\n\n\n1.9.1 Sampling\n\n\n\n\n\n\nSampling\n\n\n\nSampling is the process of selecting a subset of data. Probability sampling is the process of selecting a sample when the selection uses randomization.\n\n\nSampling has many applications:\n\nReducing costs for the collection of data\nImplementing machine learning algorithms\nResampling\n\n\n\n1.9.2 Resampling\n\n\n\n\n\n\nResampling\n\n\n\nResampling is the process of repeatedly sampling from observed data to approximate the generation of new data.\n\n\nThere are at least three popular resampling methods:\n\nCross Validation: Partitioning the data and shuffling the partitions to understand the accuracy of predictive models.\nBootstrap sampling: Repeated sampling with replacement to estimate sampling distributions from observed data.\nJackknife: Leave-one-out sampling to estimate the bias and standard error of a statistic.\n\nWe focused on cross-validation for machine learning and predictive modeling in data science for public policy. We will use this approach again for predictive modeling.\nWe will also learn about bootstrap sampling when we discuss nonparametric statistics.\n\n\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical Inference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning."
  },
  {
    "objectID": "05_simulation-and-sampling.html#footnotes",
    "href": "05_simulation-and-sampling.html#footnotes",
    "title": "1  Simulation and Sampling",
    "section": "",
    "text": "(Casella and Berger 2002) offers a robust introduction to deriving maximum likelihood estimators.↩︎\nNote that the MLE for variance is biased.↩︎\nCorrelation may be more familiar than covariance. Sample correlation is standardized sample covariance. \\(Corr(\\vec{x}, \\vec{y}) = \\frac{Cov(\\vec{x}, \\vec{y})}{S_{\\vec{x}}S_{\\vec{y}}}\\). Correlation is also between -1 and 1 inclusive. Covariance can take on any real value.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Casella, George, and Roger L. Berger. 2002. Statistical\nInference. 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning."
  }
]